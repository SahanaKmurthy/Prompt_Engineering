{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71c42a95",
   "metadata": {},
   "source": [
    "# Assignment 1 - In-Context Learning\n",
    "\n",
    "In this assignment, students experiment with in-context learning by selecting and ordering demonstrations to train a large language model at inference time to classify text. In this task, an online store is interested in classifying whether a review describes one or more general topics of interest. The topics are specific to a class of product, in this case vacuum cleaners. Other topics would be relevant to other products.\n",
    "\n",
    "The dataset has been divided into a development, training and test sets. Students should practice setting up their experiments and writing their prompts using only the development set. Demonstrations for in-context leanring can be drawn from the training set. Final evaluation prior to submission should use the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51c98888-8f3c-4d0d-9e5d-2203b44e0a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63f7c606",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "def prompt_model(prompt):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        store=True,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", 'content': prompt}\n",
    "        ]\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b25a5d4",
   "metadata": {},
   "source": [
    "## Open Source Models (Optional)\n",
    "\n",
    "If students wish to evaluate their solution on open source models, they may use Ollama, if their hardware supports it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f9909af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ollama import chat\n",
    "# from ollama import ChatResponse\n",
    "\n",
    "# def prompt_ollama(prompt):\n",
    "#     response: ChatResponse = chat(model='llama3.3', messages=[{\n",
    "#         'role': 'user',\n",
    "#         'content': prompt,\n",
    "#       },\n",
    "#     ])\n",
    "#     return response['message']['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eae8965",
   "metadata": {},
   "source": [
    "## Load Reviews with Hashtags\n",
    "\n",
    "The dataset is partitioned into development, training and testing sets. While writing the code to setup your experiments and write your prompts, only use the development set. The training set should be used to sample demonstrations. Only when your code is completed and you are ready to turn in your assignment should you run your experiment on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa8408ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Sizes: Dev 100, Train 100, Test 300\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': 'Used the product and was very happy with it until about a month ago. Motor sounded like it was working harder; thought maybe I was imagining things. Look all through hoses and brush roller assembly for any blockages. Today it was not getting good suction; then motor suddenly cut back on output. Barely runs; does not run in upright position. No suction. Bought this as an \"inexpensive\" replacement to Dyson that died after 5 years. You get what you pay for evidently. Wondering if manufacturer warranty in effect, though I failed to send in the warranty card.',\n",
       " 'expected': ['#PerformanceAndFunctionality',\n",
       "  '#ValueForMoneyAndInvestment',\n",
       "  '#CustomerExperienceAndExpectations'],\n",
       " 'sentiment': ['N', 'N', 'N']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "data_dev = json.load(open('dataset-dev.json', 'r'))\n",
    "data_train = json.load(open('dataset-train.json', 'r'))\n",
    "data_test = json.load(open('dataset-test.json', 'r'))\n",
    "\n",
    "print('\\nDataset Sizes: Dev %i, Train %i, Test %i\\n' % (len(data_dev), len(data_train), len(data_test)))\n",
    "\n",
    "data_dev[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c01ddb",
   "metadata": {},
   "source": [
    "## Define the Hashtag List for Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07987e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = [\n",
    "    '#DesignAndUsabilityIssues',\n",
    "    '#PerformanceAndFunctionality',\n",
    "    '#BatteryAndPowerIssues',\n",
    "    '#DurabilityAndMaterialConcerns',\n",
    "    '#MaintenanceAndCleaning',\n",
    "    '#CustomerExperienceAndExpectations',\n",
    "    '#ValueForMoneyAndInvestment',\n",
    "    '#AssemblyAndSetup'\n",
    "]\n",
    "\n",
    "tag_list = ' '.join(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a8de86",
   "metadata": {},
   "source": [
    "## Review the Hashtag Distribution\n",
    "\n",
    "In general, it is good practice when classifying items to know the distribution of target categories. Categories that are underrepresented, especially in the training data, would lead to underperformance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3540d8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions for Demonstrations & Prompt Construction\n",
    "\n",
    "def sample_demonstrations(data, k, seed=42):\n",
    "    \"\"\"\n",
    "    Randomly sample k demonstration examples from the training data.\n",
    "    (You can later experiment with different ordering strategies.)\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    return random.sample(data, k)\n",
    "\n",
    "def format_demo(example):\n",
    "    \"\"\"\n",
    "    Format a single demonstration example.\n",
    "    Assumes each example has keys \"review\" and \"hashtags\".\n",
    "    \"\"\"\n",
    "    review = example.get('review', '')\n",
    "    hashtags = example.get('hashtags', '')\n",
    "    if isinstance(hashtags, list):\n",
    "        hashtags = ' '.join(hashtags)\n",
    "    return f\"Review: {review}\\nHashtags: {hashtags}\"\n",
    "\n",
    "def build_demonstrations_text(demos):\n",
    "    \"\"\"\n",
    "    Concatenate formatted demonstration examples into one text block.\n",
    "    \"\"\"\n",
    "    return \"\\n\\n\".join([format_demo(ex) for ex in demos])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09c52bf",
   "metadata": {},
   "source": [
    "## Define the Prompt and Experiment\n",
    "\n",
    "The experiment generally has the following steps: (1) sample the training data to identify k demonstrations for 0 =< k < training set size; (2) construct linearize the demonstrations into text; (3) iterate over the test data and insert the test review and text linearization of the demonstrations into the prompt template; (4) send the prompt to the model and receive the response; (5) validate the response, if the response passes then store the response for later, else if the response fails validation, then save the response to a list of errors. It is generally good to save responses and errors with an index that can be linked back to the test data.\n",
    "\n",
    "After running the experiment, the evaluation metrics should be computed from the answers and the errors should be inspected. Adjustments to the prompt and/or experiment can be made to reduce the errors, e.g., by post-processing the responses prior to validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04107f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"You are given a review for a vacuum cleaner and a list of allowed hashtag categories.\n",
    "Allowed Categories: {tag_list}\n",
    "\n",
    "Here are some examples:\n",
    "{demonstrations}\n",
    "\n",
    "Now, classify the following review by outputting the relevant hashtags from the allowed list.\n",
    "Only output the hashtags (separated by a space) and nothing else.\n",
    "\n",
    "Review: {review_text}\n",
    "Hashtags:\"\"\"\n",
    "\n",
    "def construct_prompt(review_text, demos_text):\n",
    "    \"\"\"\n",
    "    Construct the full prompt by filling in the template.\n",
    "    \"\"\"\n",
    "    return PROMPT_TEMPLATE.format(tag_list=tag_list, demonstrations=demos_text, review_text=review_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "402e0729-7823-4550-b834-0bd1a7e430c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Count: 75\n"
     ]
    }
   ],
   "source": [
    "from tiktoken import encoding_for_model\n",
    "\n",
    "def count_tokens(prompt, model=\"gpt-4o\"):\n",
    "    enc = encoding_for_model(model)\n",
    "    return len(enc.encode(prompt))\n",
    "\n",
    "print(\"Token Count:\", count_tokens(PROMPT_TEMPLATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2dd381e-d3de-49f5-8bc2-1bffd95c8fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Response Processing and Validation\n",
    "def process_response(response):\n",
    "    \"\"\"\n",
    "    Post-process the response:\n",
    "      - Split the response into tokens.\n",
    "      - Filter tokens so that only allowed tags are retained (to counter hallucinations).\n",
    "    \"\"\"\n",
    "    tokens = response.split()\n",
    "    formatted_tokens = [f\"#{token}\" if not token.startswith(\"#\") else token for token in tokens]\n",
    "    valid_tags = [token for token in formatted_tokens if token in tags]\n",
    "    return valid_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f615377-29ac-4d8c-8cf7-b38eaae866d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Experiment\n",
    "def run_experiment(data, demos, debug=False, debug_samples=5):\n",
    "    \"\"\"\n",
    "    Run the experiment on a dataset:\n",
    "    1. Build demonstration text from the provided demonstrations.\n",
    "    2. For each record in the dataset:\n",
    "       - Construct the prompt with the review and demonstrations.\n",
    "       - Query the model.\n",
    "       - Process and validate the response.\n",
    "    3. Return predictions and a log of errors.\n",
    "    \n",
    "    If `debug=True`, only processes `debug_samples` reviews to inspect outputs.\n",
    "    \"\"\"\n",
    "    demos_text = build_demonstrations_text(demos)\n",
    "    predictions = {}  # Store predicted hashtags keyed by record index.\n",
    "    errors = {}       # Store any responses that failed validation.\n",
    "    \n",
    "    # Limit to debug_samples if debugging\n",
    "    dataset_size = debug_samples if debug else len(data)\n",
    "    \n",
    "    for i, record in enumerate(data[:dataset_size]):\n",
    "        review_text = record.get('text', '')\n",
    "        prompt = construct_prompt(review_text, demos_text)\n",
    "        \n",
    "        try:\n",
    "            response = prompt_model(prompt)  # Get raw response from the model\n",
    "            \n",
    "            # print(f\"\\n### Example {i} ###\")\n",
    "            # print(f\"Review: {review_text}\")\n",
    "            # print(f\"Raw Model Output: {response}\")  # Show unprocessed response\n",
    "            \n",
    "            processed = process_response(response)\n",
    "            # print(f\"Processed Hashtags: {processed}\")  # Show cleaned tags\n",
    "            \n",
    "            # If no valid hashtags are returned, record it as an error.\n",
    "            if not processed:\n",
    "                errors[i] = response\n",
    "            predictions[i] = processed\n",
    "        \n",
    "        except Exception as e:\n",
    "            errors[i] = str(e)\n",
    "            predictions[i] = []\n",
    "    \n",
    "    return predictions, errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b4c7f6",
   "metadata": {},
   "source": [
    "## Evaluate the Experimental Results\n",
    "\n",
    "The evaluation metrics include precision, recall and F1 score. For the total number of true positives (tp), false positives (fp) and false negatives (fn), these calculations should be used to report results:\n",
    "* Precision = tp / (tp + fp)\n",
    "* Recall = tp / (tp + fn)\n",
    "* F1 = 2tp / (2tp + fp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1a9ed8b-fa4d-4680-9675-32423fa56948",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def compute_metrics(data, predictions):\n",
    "    \"\"\"\n",
    "    Compute precision, recall, and F1-score using sklearn.\n",
    "    \"\"\"\n",
    "    all_ground_truths = []\n",
    "    all_predictions = []\n",
    "    \n",
    "    for i, record in enumerate(data):\n",
    "        ground_truth = record.get('expected', [])  # True labels\n",
    "        pred = predictions.get(i, [])  # Model predictions\n",
    "        \n",
    "        # Convert sets to binary format for multi-label classification\n",
    "        y_true = [1 if tag in ground_truth else 0 for tag in tags]\n",
    "        y_pred = [1 if tag in pred else 0 for tag in tags]\n",
    "\n",
    "        all_ground_truths.append(y_true)\n",
    "        all_predictions.append(y_pred)\n",
    "\n",
    "    # Compute metrics (averaged across all samples)\n",
    "    precision = precision_score(all_ground_truths, all_predictions, average='micro', zero_division=0)\n",
    "    recall = recall_score(all_ground_truths, all_predictions, average='micro', zero_division=0)\n",
    "    f1 = f1_score(all_ground_truths, all_predictions, average='micro', zero_division=0)\n",
    "\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0111d83e-0ef6-4a4c-a4f9-c5058d7c5116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running experiments on the development set for different values of k (demonstration count):\n",
      "\n",
      "Demonstration count (k) = 16:\n",
      "  Precision: 0.691\n",
      "  Recall:    0.918\n",
      "  F1 Score:  0.788\n",
      "  Number of error responses: 0\n",
      "\n",
      "Demonstration count (k) = 32:\n",
      "  Precision: 0.711\n",
      "  Recall:    0.909\n",
      "  F1 Score:  0.798\n",
      "  Number of error responses: 0\n",
      "\n",
      "Demonstration count (k) = 8:\n",
      "  Precision: 0.679\n",
      "  Recall:    0.909\n",
      "  F1 Score:  0.777\n",
      "  Number of error responses: 0\n",
      "\n",
      "Demonstration count (k) = 24:\n",
      "  Precision: 0.689\n",
      "  Recall:    0.881\n",
      "  F1 Score:  0.774\n",
      "  Number of error responses: 0\n",
      "\n",
      "Metrics by demonstration count test: {16: {'precision': 0.6907216494845361, 'recall': 0.9178082191780822, 'f1': 0.788235294117647, 'num_errors': 0}, 32: {'precision': 0.7107142857142857, 'recall': 0.908675799086758, 'f1': 0.7975951903807615, 'num_errors': 0}, 8: {'precision': 0.6791808873720137, 'recall': 0.908675799086758, 'f1': 0.77734375, 'num_errors': 0}, 24: {'precision': 0.6892857142857143, 'recall': 0.8812785388127854, 'f1': 0.7735470941883767, 'num_errors': 0}}\n"
     ]
    }
   ],
   "source": [
    "demo_counts = [16,32,8,24]\n",
    "metrics_by_k = {}\n",
    "\n",
    "print(\"\\nRunning experiments on the development set for different values of k (demonstration count):\\n\")\n",
    "\n",
    "for k in demo_counts:\n",
    "    # Sample k demonstrations from the training set.\n",
    "    demos = sample_demonstrations(data_train, k, seed=42)\n",
    "    \n",
    "    # Run the experiment on the development set using these demonstrations.\n",
    "    dev_predictions, dev_errors = run_experiment(data_dev, demos)\n",
    "    \n",
    "    # Compute evaluation metrics.\n",
    "    precision, recall, f1 = compute_metrics(data_dev, dev_predictions)\n",
    "    \n",
    "    # Store the metrics and number of errors for this k.\n",
    "    metrics_by_k[k] = {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"num_errors\": len(dev_errors)\n",
    "    }\n",
    "    \n",
    "    # Print the results.\n",
    "    print(f\"Demonstration count (k) = {k}:\")\n",
    "    print(f\"  Precision: {precision:.3f}\")\n",
    "    print(f\"  Recall:    {recall:.3f}\")\n",
    "    print(f\"  F1 Score:  {f1:.3f}\")\n",
    "    print(f\"  Number of error responses: {len(dev_errors)}\\n\")\n",
    "\n",
    "print(\"Metrics by demonstration count test:\", metrics_by_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a9a677f3-f80a-46cb-8659-7f68cc353740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running final evaluation on the test set with k = 32 demonstrations.\n",
      "Final results saved to results.json\n"
     ]
    }
   ],
   "source": [
    "selected_k = 32 \n",
    "seed = 42\n",
    "print(f\"\\nRunning final evaluation on the test set with k = {selected_k} demonstrations.\")\n",
    "final_demos = sample_demonstrations(data_dev, selected_k, seed=seed)\n",
    "test_predictions, test_errors = run_experiment(data_test, final_demos)\n",
    "\n",
    "# Save test predictions to results.json\n",
    "results = []\n",
    "for i, record in enumerate(data_test):\n",
    "    record_copy = record.copy()\n",
    "    record_copy['predicted'] = test_predictions.get(i, [])\n",
    "    results.append(record_copy)\n",
    "\n",
    "with open('results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"Final results saved to results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94138515-f6ec-4ec9-943a-f0d9e7a9dc51",
   "metadata": {},
   "source": [
    "### Evaluation metric for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "243be74a-3f79-44d1-ac5f-9c8345eda0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running experiments on the test set for different values of k (demonstration count):\n",
      "\n",
      "Demonstration count (k) = 8:\n",
      "  Precision: 0.698\n",
      "  Recall:    0.923\n",
      "  F1 Score:  0.795\n",
      "  Number of error responses: 0\n",
      "\n",
      "Demonstration count (k) = 16:\n",
      "  Precision: 0.679\n",
      "  Recall:    0.929\n",
      "  F1 Score:  0.785\n",
      "  Number of error responses: 0\n",
      "\n",
      "Demonstration count (k) = 24:\n",
      "  Precision: 0.696\n",
      "  Recall:    0.926\n",
      "  F1 Score:  0.795\n",
      "  Number of error responses: 0\n",
      "\n",
      "Demonstration count (k) = 32:\n",
      "  Precision: 0.701\n",
      "  Recall:    0.918\n",
      "  F1 Score:  0.795\n",
      "  Number of error responses: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Define different k values to test\n",
    "demo_counts = [8, 16, 24, 32]\n",
    "metrics_by_k = {}\n",
    "\n",
    "print(\"\\nRunning experiments on the test set for different values of k (demonstration count):\\n\")\n",
    "\n",
    "for k in demo_counts:\n",
    "    # Sample k demonstrations from the training set\n",
    "    demos = sample_demonstrations(data_train, k, seed=42)\n",
    "\n",
    "    # Run the experiment on the test set\n",
    "    test_predictions, test_errors = run_experiment(data_test, demos)\n",
    "\n",
    "    # Compute evaluation metrics on the test set\n",
    "    precision, recall, f1 = compute_metrics(data_test, test_predictions)\n",
    "\n",
    "    # Store results\n",
    "    metrics_by_k[k] = {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"num_errors\": len(test_errors)\n",
    "    }\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Demonstration count (k) = {k}:\")\n",
    "    print(f\"  Precision: {precision:.3f}\")\n",
    "    print(f\"  Recall:    {recall:.3f}\")\n",
    "    print(f\"  F1 Score:  {f1:.3f}\")\n",
    "    print(f\"  Number of error responses: {len(test_errors)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fac7cfb-590b-486e-99b0-62897bbc60f2",
   "metadata": {},
   "source": [
    "### Hallucination Check on the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8f93c071-a156-4f9b-af9b-2fe15c0a9def",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def validate_categories(predictions, allowed_tags):\n",
    "    validated_results = []\n",
    "    hallucinated_results = []\n",
    "    \n",
    "    for item in predictions:\n",
    "        predicted_tags = set(item[\"predicted\"])  # No need to split, it's already a list\n",
    "        valid_tags = [tag for tag in predicted_tags if tag in allowed_tags]\n",
    "        \n",
    "        if predicted_tags - set(valid_tags):  # If hallucinated tags exist\n",
    "            hallucinated_results.append({\n",
    "                \"review_id\": item[\"text\"],  # Use review text instead of ID since it's missing\n",
    "                \"invalid_tags\": list(predicted_tags - set(valid_tags))\n",
    "            })\n",
    "        \n",
    "        validated_results.append({\"text\": item[\"text\"], \"predicted\": valid_tags})  # Keep list format\n",
    "\n",
    "    return validated_results, hallucinated_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7fbe7ff9-4a90-419c-bb22-874e3e872f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tags list to a set for validation\n",
    "allowed_tags = set(tags)\n",
    "\n",
    "# Validate predictions directly from `results`\n",
    "validated_results, hallucinations = validate_categories(results, allowed_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d062213a-fc4b-49ab-9834-205d9998e447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No hallucinations detected. ✅\n"
     ]
    }
   ],
   "source": [
    "# Save the cleaned results\n",
    "with open(\"validated_results.json\", \"w\") as f:\n",
    "    json.dump(validated_results, f, indent=4)\n",
    "\n",
    "# Output hallucination findings\n",
    "if hallucinations:\n",
    "    print(\"Hallucinated tags found:\", hallucinations)\n",
    "else:\n",
    "    print(\"No hallucinations detected. ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2260c1e1-b465-4b77-a7a1-018b1c261386",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
