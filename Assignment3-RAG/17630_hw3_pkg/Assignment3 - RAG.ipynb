{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ed42f10-f71b-4a43-bc09-ca1354f88d20",
   "metadata": {},
   "source": [
    "## Assignment3 - Retrieval Augmented Generation\n",
    "\n",
    "In this asignment, you will develop a RAG solution to answer questions about a repository of research papers. The assignment requires you to parse the paper PDF files, chunk and index the data, and then design and execute an evaluation of the retriever results. In Na√Øve RAG, the query is compared to documents in the vector database for retrieval of the top N documents that match the query. The language model is then used to summarize the retrieved documents into an answer to the user query. Research papers are highly structured documents with technically deep content, in contrast to blogs, which contain more general and introductory content. This means that queries may be unlikely to match relevant chunks of the paper without additional processing, such as information extraction or summarization.\n",
    "\n",
    "One approach to address this problem is to use the language model to generate answerable questions from chunks of each paper. The generated questions can then be indexed as \"documents\" in a vector database, and the user query can be matched against the most similar questions. By maintaining a mapping between the indexed, generated question and the paper chunk, the retrieval process can then produce the most relevant chunks for use in summarizing an answer to the user query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa12a04-3acf-43f0-b984-d99790d4afff",
   "metadata": {},
   "source": [
    "## Setup the functions for prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f04c17cf-85f5-4920-bea1-db9e480a4e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "def prompt_model(prompt):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        store=True,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", 'content': prompt}\n",
    "        ]\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc24019-6bef-4580-a823-aff13d959e49",
   "metadata": {},
   "source": [
    "## Parse data from source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37bf02b8-9880-450c-9505-4968d59dc61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 53 files:\n",
      "  File: 2023.findings-emnlp.620.pdf\n",
      "  File: 29728-Article Text-33782-1-2-20240324-3.pdf\n",
      "  File: 2024.acl-long.642.pdf\n",
      "  File: 2021.findings-emnlp.320.pdf\n",
      "  File: 2020.coling-main.207.pdf\n",
      "  File: 2202.01110v2.pdf\n",
      "  File: 2212.14024v2.pdf\n",
      "  File: 2024.emnlp-industry.66.pdf\n",
      "  File: 8917_Retrieval_meets_Long_Cont.pdf\n",
      "  File: NeurIPS-2023-lift-yourself-up-retrieval-augmented-text-generation-with-self-memory.pdf\n",
      "  File: NeurIPS-2023-leandojo-theorem-proving-with-retrieval-augmented-language-models.pdf\n",
      "  File: NeurIPS-2020-retrieval-augmented-generation-for-knowledge-intensive-nlp-tasks.pdf\n",
      "  File: 2023.acl-long.557.pdf\n",
      "  File: tacl_a_00605.pdf\n",
      "  File: 3637870.pdf\n",
      "  File: 2023.emnlp-main.495.pdf\n",
      "  File: 3626772.3657834.pdf\n",
      "  File: 2402.19473v6.pdf\n",
      "  File: 3626772.3657957.pdf\n",
      "  File: 2024.eacl-demo.16.pdf\n",
      "  File: 967_generate_rather_than_retrieve_.pdf\n",
      "  File: 23-0037.pdf\n",
      "  File: 2022.naacl-main.191.pdf\n",
      "  File: 2312.10997v5.pdf\n",
      "  File: 947_Augmented_Language_Models_.pdf\n"
     ]
    }
   ],
   "source": [
    "import os, bibtexparser, pypdf, logging\n",
    "\n",
    "# silence non-critical errors while parsing PDF files\n",
    "logging.getLogger(\"pypdf\").setLevel(logging.CRITICAL)\n",
    "\n",
    "data_path = 'data/'\n",
    "data = {}\n",
    "\n",
    "files = os.listdir(data_path)\n",
    "print('Reading %i files:' % len(files))\n",
    "for f in files:\n",
    "    path = os.path.join(data_path, f)\n",
    "\n",
    "    # each datum will have at least these attributes\n",
    "    d = {'filepath': None, 'title': None, 'text': None}\n",
    "\n",
    "    # parse bibtex file, if exists\n",
    "    if path.endswith('.bib'):\n",
    "        if path[:-4] in data:\n",
    "            d = data[path[:-4]]\n",
    "\n",
    "        bib = bibtexparser.load(open(path, 'r'))\n",
    "        if 'title' in bib.entries[0]:\n",
    "            d['title'] = bib.entries[0]['title']\n",
    "            data[path[:-4]] = d\n",
    "\n",
    "    # parse pdf text, if exists\n",
    "    if path.endswith('.pdf'):\n",
    "        if path[:-4] in data:\n",
    "            d = data[path[:-4]]\n",
    "\n",
    "        print('  File: %s' % f)\n",
    "        text = ''\n",
    "        reader = pypdf.PdfReader(path)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "        d['filepath'] = path\n",
    "        d['text'] = text\n",
    "        data[path[:-4]] = d\n",
    "\n",
    "data = [d for d in data.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61ed7ed5-4b46-4483-9e6f-3f0c29fc6898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TXT file saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save to a TXT file\n",
    "with open(\"cleaneddata_input.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in data:\n",
    "        f.write(item['text'] + \"\\n\\n\")  # Add spacing between entries\n",
    "\n",
    "print(\"TXT file saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c75128a-ee9b-4b37-b4c0-fd7a9e6b48b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "# pprint.pprint(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba057a9-2f40-487c-a6b3-1afc772e10fe",
   "metadata": {},
   "source": [
    "## Pre-process the data\n",
    "\n",
    "Prior to indexing and chunking the data, the data may need to be pre-processed. This can be done to remove portions of the data irrelevant to queries to reduce mismatches between the user query and the index. This is not required for this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e20b3a5-6912-4421-bb0f-565e4953af5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_keys_from_list_of_dicts(list_of_dicts, keys_to_remove):\n",
    "    for dictionary in list_of_dicts:\n",
    "        for key in keys_to_remove:\n",
    "            if key in dictionary:\n",
    "                del dictionary[key]\n",
    "    return list_of_dicts\n",
    "\n",
    "keys_to_remove = ['filepath', 'title']\n",
    "\n",
    "cleaned_data = remove_keys_from_list_of_dicts(data, keys_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88371eac-efa1-40bd-b3fc-92a84e0bac67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5185b269-2257-4748-8405-1ff850a090dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Define section names to remove (in lowercase)\n",
    "    section_names = ['title', 'acknowledgements', 'references']\n",
    "\n",
    "    # Define patterns for sections to remove\n",
    "    remove_patterns = [\n",
    "        r'(?si)^.*?(?=\\n\\n)',  # Title (everything from start until first double newline)\n",
    "    ]\n",
    "\n",
    "    # Add patterns for other sections\n",
    "    for name in section_names:\n",
    "        pattern = rf'(?si){re.escape(name)}.*?(?=\\n\\n|\\Z)'\n",
    "        remove_patterns.append(pattern)\n",
    "\n",
    "    # Apply removals\n",
    "    for pattern in remove_patterns:\n",
    "        text = re.sub(pattern, '', text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "cleaned_data_result = []\n",
    "for item in cleaned_data:\n",
    "    if 'text' in item:\n",
    "        item['text'] = clean_text(item['text'])\n",
    "    cleaned_data_result.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd42570e-a22c-4444-9735-008dae8b2db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pprint.pprint(cleaned_data_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdb8b86f-5e6b-432b-9eeb-dcb6783f9caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data has been saved to cleaned_data.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv  # Add this import statement\n",
    "\n",
    "output_file = 'cleaned_data.txt'\n",
    "\n",
    "with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    fieldnames = cleaned_data_result[0].keys()\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "    for row in cleaned_data_result:\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"Cleaned data has been saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1063fc8f-0d55-499d-a112-2d9752b77821",
   "metadata": {},
   "source": [
    "## Chunk data and generate indices\n",
    "\n",
    "User queries will be matched to indexes that best approximate the text chunks used to summarize an answer. For this assignment, you may chunk the text and then prompt the model to generate questions that are answerable by the text. The generated questions can then be used as the \"documents\" stored in the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14f3bf67-6c9f-49ca-97bf-504f68001a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b60f00b-27e7-41c4-ba3b-89ddb9dc3ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text_with_overlap(text, max_tokens=300, overlap=64):\n",
    "    \"\"\"\n",
    "    Splits text into chunks while maintaining semantic coherence using sentence tokenization.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The full text to be chunked.\n",
    "    - max_tokens (int): Maximum token size per chunk.\n",
    "    - overlap (int): Number of tokens overlapping between chunks.\n",
    "\n",
    "    Returns:\n",
    "    - List[str]: A list of text chunks.\n",
    "    \"\"\"\n",
    "    sentences = nltk.tokenize.sent_tokenize(text)  # Tokenize by sentence\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_length = len(sentence.split())  # Approximate token count\n",
    "        if current_length + sentence_length > max_tokens:\n",
    "            chunks.append(\" \".join(current_chunk))  # Save chunk\n",
    "            current_chunk = current_chunk[-overlap:]  # Keep overlap\n",
    "            current_length = sum(len(s.split()) for s in current_chunk)\n",
    "\n",
    "        current_chunk.append(sentence)\n",
    "        current_length += sentence_length\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))  # Add the last chunk\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23ab2f28-eb6e-4680-9533-c3868e03d1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunked_data = []\n",
    "# for doc in cleaned_data:\n",
    "#     text = doc.get(\"text\", \"\")\n",
    "#     if text:\n",
    "#         chunks = chunk_text_with_overlap(text)\n",
    "#         for chunk in chunks:\n",
    "#             chunked_data.append({\"chunk\": chunk})  # Store chunks\n",
    "\n",
    "chunked_data = []\n",
    "chunk_id_counter = 0  # Initialize a counter for chunk IDs\n",
    "for doc in cleaned_data:\n",
    "    text = doc.get(\"text\", \"\")\n",
    "    if text:\n",
    "        chunks = chunk_text_with_overlap(text)\n",
    "        for chunk in chunks:\n",
    "            chunked_data.append({\"chunk_id\": chunk_id_counter, \"chunk\": chunk})  # Assign unique ID\n",
    "            chunk_id_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b66e75e-e8f3-4558-b76e-87dbbed5b3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'chunk_id': 0, 'chunk': 'Findings of the Association for Computational Linguistics: EMNLP 2023, pages 9248‚Äì9274\\nDecember 6-10, 2023 ¬©2023 Association for Computational Linguistics\\nEnhancing Retrieval-Augmented Large Language Models with Iterative\\nRetrieval-Generation Synergy\\nZhihong Shao1, Yeyun Gong2, yelong shen3, Minlie Huang1‚àó, Nan Duan2, Weizhu Chen3\\n1 The CoAI Group, DCST, Institute for Artificial Intelligence,\\n1 State Key Lab of Intelligent Technology and Systems,\\n1 Beijing National Research Center for Information Science and Technology,\\n1 Tsinghua University, Beijing 100084, China\\n2 Microsoft Research Asia 3 Microsoft Azure AI\\nszh19@mails.tsinghua.edu.cn aihuang@tsinghua.edu.cn\\nAbstract\\nRetrieval-augmented generation has raise exten-\\nsive attention as it is promising to address the\\nlimitations of large language models including\\noutdated knowledge and hallucinations. How-\\never, retrievers struggle to capture relevance,\\nespecially for queries with complex informa-\\ntion needs. Recent work has proposed to im-\\nprove relevance modeling by having large lan-\\nguage models actively involved in retrieval, i.e.,\\nto guide retrieval with generation. In this pa-\\nper, we show that strong performance can be\\nachieved by a method we call ITER -RETGEN,\\nwhich synergizes retrieval and generation in an\\niterative manner: a model‚Äôs response to a task\\ninput shows what might be needed to finish\\nthe task, and thus can serve as an informative\\ncontext for retrieving more relevant knowledge\\nwhich in turn helps generate a better response\\nin another iteration. Compared with recent\\nwork which interleaves retrieval with gener-\\nation when completing a single output, I TER -\\nRETGEN processes all retrieved knowledge as\\na whole and largely preserves the flexibility in\\ngeneration without structural constraints. We\\nevaluate ITER -RETGEN on multi-hop question\\nanswering, fact verification, and commonsense\\nreasoning, and show that it can flexibly lever-\\nage parametric knowledge and non-parametric\\nknowledge, and is superior to or competitive\\nwith state-of-the-art retrieval-augmented base-\\nlines while causing fewer overheads of retrieval\\nand generation.'}, {'chunk_id': 1, 'chunk': 'Findings of the Association for Computational Linguistics: EMNLP 2023, pages 9248‚Äì9274\\nDecember 6-10, 2023 ¬©2023 Association for Computational Linguistics\\nEnhancing Retrieval-Augmented Large Language Models with Iterative\\nRetrieval-Generation Synergy\\nZhihong Shao1, Yeyun Gong2, yelong shen3, Minlie Huang1‚àó, Nan Duan2, Weizhu Chen3\\n1 The CoAI Group, DCST, Institute for Artificial Intelligence,\\n1 State Key Lab of Intelligent Technology and Systems,\\n1 Beijing National Research Center for Information Science and Technology,\\n1 Tsinghua University, Beijing 100084, China\\n2 Microsoft Research Asia 3 Microsoft Azure AI\\nszh19@mails.tsinghua.edu.cn aihuang@tsinghua.edu.cn\\nAbstract\\nRetrieval-augmented generation has raise exten-\\nsive attention as it is promising to address the\\nlimitations of large language models including\\noutdated knowledge and hallucinations. How-\\never, retrievers struggle to capture relevance,\\nespecially for queries with complex informa-\\ntion needs. Recent work has proposed to im-\\nprove relevance modeling by having large lan-\\nguage models actively involved in retrieval, i.e.,\\nto guide retrieval with generation. In this pa-\\nper, we show that strong performance can be\\nachieved by a method we call ITER -RETGEN,\\nwhich synergizes retrieval and generation in an\\niterative manner: a model‚Äôs response to a task\\ninput shows what might be needed to finish\\nthe task, and thus can serve as an informative\\ncontext for retrieving more relevant knowledge\\nwhich in turn helps generate a better response\\nin another iteration. Compared with recent\\nwork which interleaves retrieval with gener-\\nation when completing a single output, I TER -\\nRETGEN processes all retrieved knowledge as\\na whole and largely preserves the flexibility in\\ngeneration without structural constraints. We\\nevaluate ITER -RETGEN on multi-hop question\\nanswering, fact verification, and commonsense\\nreasoning, and show that it can flexibly lever-\\nage parametric knowledge and non-parametric\\nknowledge, and is superior to or competitive\\nwith state-of-the-art retrieval-augmented base-\\nlines while causing fewer overheads of retrieval\\nand generation. We can further improve per-\\nformance via generation-augmented retrieval\\nadaptation.'}, {'chunk_id': 2, 'chunk': 'Findings of the Association for Computational Linguistics: EMNLP 2023, pages 9248‚Äì9274\\nDecember 6-10, 2023 ¬©2023 Association for Computational Linguistics\\nEnhancing Retrieval-Augmented Large Language Models with Iterative\\nRetrieval-Generation Synergy\\nZhihong Shao1, Yeyun Gong2, yelong shen3, Minlie Huang1‚àó, Nan Duan2, Weizhu Chen3\\n1 The CoAI Group, DCST, Institute for Artificial Intelligence,\\n1 State Key Lab of Intelligent Technology and Systems,\\n1 Beijing National Research Center for Information Science and Technology,\\n1 Tsinghua University, Beijing 100084, China\\n2 Microsoft Research Asia 3 Microsoft Azure AI\\nszh19@mails.tsinghua.edu.cn aihuang@tsinghua.edu.cn\\nAbstract\\nRetrieval-augmented generation has raise exten-\\nsive attention as it is promising to address the\\nlimitations of large language models including\\noutdated knowledge and hallucinations. How-\\never, retrievers struggle to capture relevance,\\nespecially for queries with complex informa-\\ntion needs. Recent work has proposed to im-\\nprove relevance modeling by having large lan-\\nguage models actively involved in retrieval, i.e.,\\nto guide retrieval with generation. In this pa-\\nper, we show that strong performance can be\\nachieved by a method we call ITER -RETGEN,\\nwhich synergizes retrieval and generation in an\\niterative manner: a model‚Äôs response to a task\\ninput shows what might be needed to finish\\nthe task, and thus can serve as an informative\\ncontext for retrieving more relevant knowledge\\nwhich in turn helps generate a better response\\nin another iteration. Compared with recent\\nwork which interleaves retrieval with gener-\\nation when completing a single output, I TER -\\nRETGEN processes all retrieved knowledge as\\na whole and largely preserves the flexibility in\\ngeneration without structural constraints. We\\nevaluate ITER -RETGEN on multi-hop question\\nanswering, fact verification, and commonsense\\nreasoning, and show that it can flexibly lever-\\nage parametric knowledge and non-parametric\\nknowledge, and is superior to or competitive\\nwith state-of-the-art retrieval-augmented base-\\nlines while causing fewer overheads of retrieval\\nand generation. We can further improve per-\\nformance via generation-augmented retrieval\\nadaptation. 1 Introduction\\nGenerative Large Language Models (LLMs)\\nhave powered numerous applications, with well-\\nperceived utility.'}]\n"
     ]
    }
   ],
   "source": [
    "print(chunked_data[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a63c9cb-94d6-4214-8340-5d664c8e7f96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4434"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunked_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "679c2fef-d32c-4ebe-a83c-d7010ac9cb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0: ['1. What is the primary benefit of using retrieval-augmented generation as discussed in the findings of EMNLP 2023?', '', '2. How does the ITER-RETGEN method improve the relevance of retrieved knowledge for complex queries?', '', '3. In what areas was the ITER-RETGEN method evaluated according to the text?']\n",
      "Chunk 1: ['1. What is the primary goal of the retrieval-augmented generation method discussed in the paper \"Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy\"?', '', '2. How does the ITER-RETGEN method differ from previous approaches to interleaving retrieval with generation when completing a single output?', '', '3. In which areas was ITER-RETGEN evaluated, and what were the outcomes of these evaluations compared to state-of-the-art retrieval-augmented baselines?']\n",
      "Chunk 2: ['1. What is the primary focus of the research presented in the paper titled \"Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy\"?', '  ', '2. How does the proposed method ITER-RETGEN differ from recent approaches in terms of processing retrieved knowledge during generation?', '', '3. What tasks were used to evaluate the performance of ITER-RETGEN, and how does it compare to state-of-the-art retrieval-augmented baselines?']\n",
      "Chunk 3: ['1. What is the purpose of the ITER-RETGEN method described in the paper?', '2. How does the ITER-RETGEN approach differ from previous methods of interleaving retrieval with generation?', '3. In which specific tasks was the ITER-RETGEN evaluated, and what advantages does it claim to offer compared to state-of-the-art baseline models?']\n",
      "Chunk 4: ['1. What is the primary focus of the study presented in the paper titled \"Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy\"?', '  ', '2. How does the method ITER-RETGEN improve the performance of retrieval-augmented generation compared to previous approaches?', '', '3. What are the specific tasks on which the ITER-RETGEN method was evaluated in the study?']\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "# Select a small sample (e.g., 5 chunks)  \n",
    "sample_chunks = chunked_data[0:5]  \n",
    "\n",
    "generated_questions = []  \n",
    "\n",
    "for i, chunk_dict in enumerate(sample_chunks):  \n",
    "    chunk_text = chunk_dict[\"chunk\"]  # Use \"chunk\" instead of \"chunk_text\"\n",
    "\n",
    "    prompt = f\"Generate 3 questions that can be answered from the following text:\\n\\n{chunk_text}\"  \n",
    "\n",
    "    try:  \n",
    "        response = openai.chat.completions.create(  \n",
    "            model=\"gpt-4o-mini\",  \n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],  \n",
    "            max_tokens=150  \n",
    "        )  \n",
    "\n",
    "        # Extract questions  \n",
    "        questions = response.choices[0].message.content.strip().split(\"\\n\") \n",
    "        generated_questions.append({\"chunk_id\": i, \"questions\": questions})  \n",
    "\n",
    "        print(f\"Chunk {i}: {questions}\")  # Debugging  \n",
    "\n",
    "    except Exception as e:  \n",
    "        print(f\"Error processing chunk {i}: {e}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d7afe82-b64a-4cb6-8811-3e62d8de2a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AsyncOpenAI\n",
    "client = AsyncOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "903f11d9-5d6f-4300-aefc-9b26a450ecd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import re\n",
    "\n",
    "async def generate_questions_for_batch(batch, batch_index):\n",
    "    \"\"\"\n",
    "    Asynchronously generates questions for a batch of chunks.\n",
    "    \"\"\"\n",
    "    batched_prompt = (\n",
    "        \"You are an AI assistant that generates questions for given text chunks, including titles, metadata, and partial content.\\n\"\n",
    "        \"For each text chunk below, generate 1-2 diverse questions ONLY.\\n\"\n",
    "        \"For title or metadata chunks, focus on questions about the topic, authors, or publication details.\\n\"\n",
    "        \"For partial content, generate questions about the potential full context or significance of the fragment.\\n\"\n",
    "        \"Start each question with 'Question:' on a new line.\\n\"\n",
    "        \"Separate questions for different chunks with '---'.\\n\\n\"\n",
    "        \"Here are the chunks:\\n\\n\"\n",
    "    )\n",
    "\n",
    "    for chunk_dict in batch:\n",
    "        batched_prompt += f\"{chunk_dict['chunk']}\\n---\\n\\n\"\n",
    "    \n",
    "    try:\n",
    "        response = await client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": batched_prompt}],\n",
    "            temperature=0.7,\n",
    "            max_tokens=250\n",
    "        )\n",
    "        \n",
    "        response_text = response.choices[0].message.content.strip()\n",
    "        chunk_responses = response_text.split('---')\n",
    "        \n",
    "        chunk_questions = []\n",
    "        for i, chunk_response in enumerate(chunk_responses):\n",
    "            questions = [q.strip()[len(\"Question:\"):].strip() for q in chunk_response.strip().split('\\n') if q.strip().lower().startswith(\"question:\")]\n",
    "\n",
    "            if not questions and i < len(batch):\n",
    "                chunk_text = batch[i]['chunk']\n",
    "                if \"Published in\" in chunk_text and \"Abstract\" in chunk_text:\n",
    "                    title = chunk_text.split('Abstract')[0].strip().split('\\n')[-1]\n",
    "                    questions = [\n",
    "                        f\"What is the main focus of the survey titled '{title}'?\",\n",
    "                        f\"Who are the primary authors of this survey paper?\",\n",
    "                        f\"What are the key points discussed in the abstract of this survey?\"\n",
    "                    ]\n",
    "                elif len(chunk_text.split()) <= 15:  # Short chunk, likely metadata\n",
    "                    if \"Published in\" in chunk_text:\n",
    "                        questions = [f\"What is the publication venue and date for this research?\"]\n",
    "                    elif any(word in chunk_text.lower() for word in ['survey', 'benchmarking']):\n",
    "                        questions = [f\"What is the main focus of the study titled '{chunk_text}'?\"]\n",
    "                    else:\n",
    "                        questions = [f\"What is the significance of '{chunk_text}' in the context of this research?\"]\n",
    "                else:  # Longer chunk, likely partial content\n",
    "                    questions = [f\"What are the main ideas presented in the fragment '{chunk_text[:50]}...'?\"]\n",
    "\n",
    "            if i < len(batch):\n",
    "                chunk_questions.append({\n",
    "                    \"chunk_id\": batch[i]['chunk_id'],\n",
    "                    \"questions\": questions\n",
    "                })\n",
    "        \n",
    "        return chunk_questions\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch {batch_index}: {e}\")\n",
    "        return []\n",
    "\n",
    "async def generate_questions_parallel(chunked_data, batch_size=5):\n",
    "    \"\"\"\n",
    "    Generates questions asynchronously using parallel API calls.\n",
    "    \"\"\"\n",
    "    tasks = []\n",
    "    for i in range(0, len(chunked_data), batch_size):\n",
    "        batch = chunked_data[i:i + batch_size]\n",
    "        tasks.append(generate_questions_for_batch(batch, i // batch_size))\n",
    "    \n",
    "    results = await asyncio.gather(*tasks)  # Run all API calls in parallel\n",
    "    flattened_results = [item for sublist in results for item in sublist]  # Flatten list\n",
    "    \n",
    "    # Final check for missing chunks\n",
    "    processed_chunk_ids = set(item['chunk_id'] for item in flattened_results)\n",
    "    for i in range(len(chunked_data)):\n",
    "        if i not in processed_chunk_ids:\n",
    "            flattened_results.append({\n",
    "                \"chunk_id\": i,\n",
    "                \"questions\": [f\"What is the main point of chunk {i} in the document?\"]\n",
    "            })\n",
    "    \n",
    "    print(f\"Total chunks processed: {len(flattened_results)}\")\n",
    "    return flattened_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c8fd0d5-313e-4698-b867-18935cb36d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks processed: 4434\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()  # Allows running asyncio inside Jupyter\n",
    "\n",
    "# # Run the function asynchronously\n",
    "# generated_questions = asyncio.run(generate_questions_parallel(chunked_data, batch_size=5))\n",
    "generated_questions = await generate_questions_parallel(chunked_data, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8afe31e-3157-4829-9c77-02d499e7769c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'chunk_id': 0,\n",
      "  'questions': ['What are the main contributions of the paper \"Enhancing '\n",
      "                'Retrieval-Augmented Large Language Models with Iterative '\n",
      "                'Retrieval-Generation Synergy\" presented at EMNLP 2023?',\n",
      "                'Who are the authors of the study and what institutions are '\n",
      "                'they affiliated with?']},\n",
      " {'chunk_id': 1,\n",
      "  'questions': ['How does the proposed ITER-RETGEN method improve the '\n",
      "                'relevance of retrieval in comparison to previous approaches?',\n",
      "                'What specific applications were evaluated to demonstrate the '\n",
      "                'effectiveness of the ITER-RETGEN method?']},\n",
      " {'chunk_id': 2,\n",
      "  'questions': ['What limitations of large language models are addressed by '\n",
      "                'the retrieval-augmented generation approach mentioned in the '\n",
      "                'paper?',\n",
      "                'What is the significance of the iterative process in the '\n",
      "                'ITER-RETGEN method for enhancing language model outputs?']},\n",
      " {'chunk_id': 3,\n",
      "  'questions': ['What potential does the paper suggest for further improving '\n",
      "                'the performance of retrieval-augmented generation systems?',\n",
      "                'How does the paper position the ITER-RETGEN method against '\n",
      "                'state-of-the-art retrieval-augmented baselines in terms of '\n",
      "                'efficiency?']},\n",
      " {'chunk_id': 4,\n",
      "  'questions': ['What is the context in which generative large language models '\n",
      "                '(LLMs) are discussed in the introduction of the paper?',\n",
      "                'How do the authors describe the issues of outdated knowledge '\n",
      "                'and hallucinations in the context of LLMs?']}]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(generated_questions[:5])  # Print only the first 5 items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c7404ee-b25a-4342-aa88-7fed8bf3556e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4434"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(generated_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7fcfd26e-f707-4fdc-8ad6-1b4429427ec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4434"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunked_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce91801-86ab-43ae-9ff1-1cd0891e82e5",
   "metadata": {},
   "source": [
    "## Build the vector database\n",
    "\n",
    "When building the vector database, be sure to maintain a mapping between the generated questions and the chunks that can be used later to retrieve the chunks from the most similar indices to the user query provided.\n",
    "\n",
    "You may also add the function to query the vector database that you will use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c65e598a-5463-4237-8de2-74a19981b4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'research_papers' does not exist.\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# Initialize Chroma client\n",
    "client = chromadb.Client()\n",
    "\n",
    "# Delete if Exists\n",
    "try:\n",
    "    collection = client.get_collection(\"research_papers\")\n",
    "    client.delete_collection(\"research_papers\")\n",
    "    print(\"Collection 'research_papers' has been deleted.\")\n",
    "except chromadb.errors.InvalidCollectionException:\n",
    "    print(\"Collection 'research_papers' does not exist.\")\n",
    "\n",
    "\n",
    "# Create a collection\n",
    "embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=\"all-MiniLM-L6-v2\")\n",
    "collection = client.create_collection(name=\"research_papers\", embedding_function=embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c498f45f-e65f-4638-9a6c-7327d29314b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, index the actual text chunks\n",
    "for chunk in chunked_data:\n",
    "    collection.add(\n",
    "        documents=[chunk[\"chunk\"]],  # Store the actual text chunk\n",
    "        metadatas=[{\"chunk_id\": chunk[\"chunk_id\"]}],  \n",
    "        ids=[f\"chunk_{chunk['chunk_id']}\"]\n",
    "    )\n",
    "\n",
    "# Then, index the generated questions\n",
    "for i, question_set in enumerate(generated_questions):\n",
    "    chunk_id = question_set['chunk_id']\n",
    "    for j, question in enumerate(question_set['questions']):\n",
    "        collection.add(\n",
    "            documents=[question],\n",
    "            metadatas=[{\"chunk_id\": chunk_id}],\n",
    "            ids=[f\"question_{i}_{j}\"]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b0d82b70-e21d-4627-b0b8-ed2a50bd89e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total items in collection: 12492\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total items in collection: {collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f489e4af-a198-466b-affa-cf4dde559f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def retrieve_relevant_chunks(query, n=3):\n",
    "#     results = collection.query(\n",
    "#         query_texts=[query],\n",
    "#         n_results=n\n",
    "#     )\n",
    "#     chunk_ids = [result['chunk_id'] for result in results['metadatas'][0]]\n",
    "#     return list(set(chunk_ids))  # Remove duplicates\n",
    "\n",
    "\n",
    "# invalid chunk id\n",
    "# def retrieve_relevant_chunks(query, n=3):\n",
    "#     \"\"\"\n",
    "#     Retrieves relevant chunks based on question similarity.\n",
    "\n",
    "#     1. Queries ChromaDB to find the questions most similar to the input query.\n",
    "#     2. Extracts the chunk_ids from the metadata of the retrieved questions.\n",
    "#     3. Retrieves the corresponding chunks from chunked_data.\n",
    "\n",
    "#     Args:\n",
    "#         query (str): The input query.\n",
    "#         n (int): The number of relevant chunks to retrieve.\n",
    "\n",
    "#     Returns:\n",
    "#         list: A list of relevant chunks.\n",
    "#     \"\"\"\n",
    "#     # Query ChromaDB for similar questions\n",
    "#     results = collection.query(\n",
    "#         query_texts=[query],\n",
    "#         n_results=n,  # Number of questions to retrieve\n",
    "#     )\n",
    "\n",
    "#     # Extract chunk_ids from the metadata of the retrieved questions\n",
    "#     chunk_ids = []\n",
    "#     for metadata in results['metadatas'][0]:\n",
    "#         chunk_ids.append(metadata['chunk_id'])\n",
    "\n",
    "#     # Retrieve the corresponding chunks from chunked_data\n",
    "#     relevant_chunks = [chunked_data[chunk_id]['chunk'] for chunk_id in chunk_ids if chunk_id < len(chunked_data)]\n",
    "\n",
    "#     return relevant_chunks\n",
    "\n",
    "def retrieve_chunks(query, top_k=3):\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=top_k,\n",
    "        include=[\"metadatas\"]  # Ensure we're retrieving metadata\n",
    "    )\n",
    "\n",
    "    retrieved_chunk_ids = []\n",
    "\n",
    "    # Check if metadata is properly structured\n",
    "    if \"metadatas\" in results and results[\"metadatas\"]:\n",
    "        for metadata in results[\"metadatas\"][0]:  # Access metadata from first query result\n",
    "            if 'chunk_id' in metadata:\n",
    "                retrieved_chunk_ids.append(metadata['chunk_id'])  # Extract correct chunk_id\n",
    "            else:\n",
    "                print(f\"Warning: No chunk_id found in metadata for result: {metadata}\")\n",
    "    else:\n",
    "        print(\"Error: No metadata retrieved from vector DB.\")\n",
    "\n",
    "    print(f\"Final retrieved chunk IDs: {retrieved_chunk_ids}\")  # Debugging output\n",
    "    return retrieved_chunk_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de81d151-e73b-454c-a588-40a41cfce905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final retrieved chunk IDs: [363, 358, 328]\n",
      "Retrieved Chunk IDs: [363, 358, 328]\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the main contribution of the RAG survey paper?\"\n",
    "chunk_ids = retrieve_chunks(query)\n",
    "print(f\"Retrieved Chunk IDs: {chunk_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "82fb1504-ecc8-4054-b8f9-97b9269664f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list to dictionary for quick lookups\n",
    "chunked_data_dict = {entry['chunk_id']: entry['chunk'] for entry in chunked_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e0c28d65-3574-4cda-b6dd-dfc39b4ab652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final retrieved chunk IDs: [363, 358, 328]\n",
      "Retrieved Chunk IDs: [363, 358, 328]\n",
      "\n",
      "Retrieved Chunks:\n",
      "Chunk 363: Overall, this\n",
      "paper sets out to meticulously compile and categorize the\n",
      "foundational technical concepts, historical progression, and\n",
      "the spectrum of RAG methodologies and applications that\n",
      "have emerged post-LLMs. It is designed to equip readers and\n",
      "professionals with a detailed and structured unders...\n",
      "Chunk 358: As research\n",
      "progressed, the enhancement of RAG was no longer limited\n",
      "to the inference stage but began to incorporate more with LLM\n",
      "fine-tuning techniques. The burgeoning field of RAG has experienced swift growth,\n",
      "yet it has not been accompanied by a systematic synthesis that\n",
      "could clarify its broade...\n",
      "Chunk 328: 1\n",
      "Retrieval-Augmented Generation for Large\n",
      "Language Models: A Survey\n",
      "Yunfan Gaoa, Yun Xiongb, Xinyu Gao b, Kangxiang Jia b, Jinliu Pan b, Yuxi Bic, Yi Dai a, Jiawei Sun a, Meng\n",
      "Wangc, and Haofen Wang a,c\n",
      "aShanghai Research Institute for Intelligent Autonomous Systems, Tongji University\n",
      "bShanghai Key...\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the main contribution of the RAG survey paper?\"\n",
    "chunk_ids = retrieve_chunks(query)\n",
    "print(f\"Retrieved Chunk IDs: {chunk_ids}\")\n",
    "\n",
    "print(\"\\nRetrieved Chunks:\")\n",
    "for chunk_id in chunk_ids:\n",
    "    if chunk_id in chunked_data_dict:\n",
    "        print(f\"Chunk {chunk_id}: {chunked_data_dict[chunk_id][:300]}...\")  # Print first 300 chars\n",
    "    else:\n",
    "        print(f\"Chunk {chunk_id} not found in chunked_data_dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "890697cf-92b6-4824-81dc-0aa8148eac94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_answer(query, chunk_ids):\n",
    "#     chunks = [chunked_data[chunk_id]['chunk'] for chunk_id in chunk_ids]\n",
    "#     context = \"\\n\".join(chunks)\n",
    "    \n",
    "#     prompt = f\"\"\"Given the following context, answer the question. If the answer cannot be found in the context, respond with 'IDK'.\n",
    "\n",
    "# Context:\n",
    "# {context}\n",
    "\n",
    "# Question: {query}\n",
    "\n",
    "# Answer:\"\"\"\n",
    "\n",
    "#     response = openai.chat.completions.create(\n",
    "#         model=\"gpt-4o-mini\",  # or whichever model you're using\n",
    "#         messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "#         temperature=0.7,\n",
    "#         max_tokens=250\n",
    "#     )\n",
    "    \n",
    "#     return response.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "## invalid chunk id\n",
    "# def generate_answer(query, chunk_ids):\n",
    "#     chunks = []\n",
    "#     for chunk_id in chunk_ids:\n",
    "#         try:\n",
    "#             chunk_id = int(chunk_id)  # Convert chunk_id to integer\n",
    "#             chunks.append(chunked_data[chunk_id]['chunk'])\n",
    "#         except (ValueError, KeyError) as e:\n",
    "#             print(f\"Invalid chunk_id: {chunk_id} - Skipping. Error: {e}\")\n",
    "#             continue  # Skip this chunk if chunk_id is invalid\n",
    "\n",
    "#     context = \"\\n\".join(chunks)\n",
    "\n",
    "#     prompt = f\"\"\"Given the following context, answer the question. If the answer cannot be found in the context, respond with 'IDK'.\n",
    "\n",
    "#     Context:\n",
    "#     {context}\n",
    "\n",
    "#     Question: {query}\n",
    "\n",
    "#     Answer:\"\"\"\n",
    "\n",
    "#     response = openai.chat.completions.create(\n",
    "#         model=\"gpt-4o-mini\",  # or whichever model you're using\n",
    "#         messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "#         temperature=0.7,\n",
    "#         max_tokens=250\n",
    "#     )\n",
    "\n",
    "#     return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d0ed6665-1be1-4bae-a1ab-3e7f31f3e287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_answer(query, chunk_ids):\n",
    "#     chunks = []\n",
    "#     for chunk_id in chunk_ids:\n",
    "#         try:\n",
    "#             chunk_id = int(chunk_id)  # Convert to integer\n",
    "#             if chunk_id in chunked_data_dict:\n",
    "#                 chunks.append(chunked_data_dict[chunk_id])  # Fetch chunk text\n",
    "#             else:\n",
    "#                 print(f\"Warning: Chunk ID {chunk_id} not found in chunked_data_dict\")\n",
    "#         except ValueError:\n",
    "#             print(f\"Invalid chunk_id: {chunk_id} - Skipping.\")\n",
    "#             continue\n",
    "\n",
    "#     if not chunks:\n",
    "#         return \"IDK\"\n",
    "\n",
    "#     context = \"\\n\".join(chunks)\n",
    "\n",
    "#     prompt = f\"\"\"Given the following context, answer the question. If the answer cannot be found in the context, respond with 'IDK'.\n",
    "\n",
    "#     Context:\n",
    "#     {context}\n",
    "\n",
    "#     Question: {query}\n",
    "\n",
    "#     Answer:\"\"\"\n",
    "\n",
    "#     print(\"----- Final Prompt -----\")\n",
    "#     print(prompt)\n",
    "#     print(\"------------------------\")\n",
    "\n",
    "\n",
    "#     response = openai.chat.completions.create(\n",
    "#         model=\"gpt-4o-mini\",\n",
    "#         messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "#         temperature=0.7,\n",
    "#         max_tokens=250\n",
    "#     )\n",
    "\n",
    "#     return response.choices[0].message.content.strip()\n",
    "\n",
    "def generate_answer(query, chunk_ids):\n",
    "    chunks = []\n",
    "    for chunk_id in chunk_ids:\n",
    "        try:\n",
    "            chunk_id = int(chunk_id)  # Convert to integer\n",
    "            if chunk_id in chunked_data_dict:\n",
    "                chunks.append(chunked_data_dict[chunk_id])  # Fetch chunk text\n",
    "            else:\n",
    "                print(f\"Warning: Chunk ID {chunk_id} not found in chunked_data_dict\")\n",
    "        except ValueError:\n",
    "            print(f\"Invalid chunk_id: {chunk_id} - Skipping.\")\n",
    "            continue\n",
    "\n",
    "    if not chunks:\n",
    "        return \"IDK\"\n",
    "\n",
    "    context = \"\\n\".join(chunks)\n",
    "\n",
    "    # Debugging prints (without full context)\n",
    "    # print(\"----- Final Prompt Structure -----\")\n",
    "    # print(f\"Context: [Hidden, length: {len(context)} characters]\")\n",
    "    # print(f\"Question: {query}\")\n",
    "    # print(\"Answer: [Generated by model]\")\n",
    "    # print(\"----------------------------------\")\n",
    "\n",
    "    prompt = f\"\"\"Given the following context, answer the question. If the answer cannot be found in the context, respond with 'IDK'.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.7,\n",
    "        max_tokens=250\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6d028d2f-57c0-4351-ad76-410eaaabd1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_query(query):\n",
    "    chunk_ids = retrieve_chunks(query)\n",
    "    answer = generate_answer(query, chunk_ids)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8b7efd23-a7c7-42e7-9c2f-a951c2766eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('dev-questions.json', 'r') as f:\n",
    "    dev_queries = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f6798986-2715-45c0-86bf-3160dc4ab287",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rag_system(queries):\n",
    "    generated_answers = []\n",
    "    ground_truth_answers = []\n",
    "    for query in queries:\n",
    "        answer = rag_query(query['query'])\n",
    "        print(f\"Query: {query['query']}\")\n",
    "        print(f\"Generated Answer: {answer}\")\n",
    "        print(f\"Ground Truth: {query['answer']}\")\n",
    "        print(\"---\")\n",
    "        generated_answers.append(answer)\n",
    "        ground_truth_answers.append(query['answer'])  # Append the ground truth\n",
    "    return generated_answers, ground_truth_answers # Return both lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "df9d8163-f0b7-4c2f-af27-a24aa3d15d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "test_queries = [\n",
    "  {\n",
    "    \"query\": \"What does RAG do?\",\n",
    "    \"answer\": \"RAG retrieves relevant information based on the query and then prompts an LLM to generate a response in the context of the retrieved information.\"\n",
    "  },\n",
    "  {\n",
    "    \"query\": \"What are the names of the two retrievers used in the study?\",\n",
    "    \"answer\": \"Contriever and Dragon\"\n",
    "  },\n",
    "  {\n",
    "    \"query\": \"What is SELF-ROUTE?\",\n",
    "    \"answer\": \"SELF-ROUTE utilizes LLM itself to route queries based on self-reflection, under the assumption that LLMs are well-calibrated in predicting whether a query is answerable given provided context.\"\n",
    "  },\n",
    "  {\n",
    "    \"query\": \"What is the main goal of the research described in the paper?\",\n",
    "    \"answer\": \"The research aims to compare Retrieval Augmented Generation (RAG) and long-context Large Language Models (LLMs) to understand their strengths and weaknesses, and to combine them effectively.\"\n",
    "  },\n",
    "  {\n",
    "    \"query\": \"What advantage does RAG still have over long-context LLMs?\",\n",
    "    \"answer\": \"Despite potentially lower performance, RAG offers a significantly lower computational cost compared to long-context LLMs.\"\n",
    "  },\n",
    "  {\n",
    "    \"query\": \"How does SELF-ROUTE decide whether to use RAG or a long-context LLM?\",\n",
    "    \"answer\": \"SELF-ROUTE uses the LLM's own assessment of whether a query is answerable given the retrieved context to decide whether to use RAG or a long-context LLM.\"\n",
    "  },\n",
    "  {\n",
    "    \"query\": \"How does SELF-ROUTE perform on visual question answering tasks?\",\n",
    "    \"answer\": \"IDK\"\n",
    "  },\n",
    "  {\n",
    "    \"query\": \"What is the impact of SELF-ROUTE on the carbon footprint of LLM applications?\",\n",
    "    \"answer\": \"IDK\"\n",
    "  },\n",
    "  {\n",
    "    \"query\": \"Can SELF-ROUTE be applied to other modalities beyond text, such as audio or video?\",\n",
    "    \"answer\": \"IDK\"\n",
    "  }\n",
    "]\n",
    "\n",
    "with open(\"test_queries.json\", \"w\") as file:\n",
    "    json.dump(test_queries, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2e4fba58-1d4f-4deb-9901-7e0813a2a4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final retrieved chunk IDs: [340, 347, 378]\n",
      "Query: What is RAG?\n",
      "Generated Answer: RAG stands for Retrieval-Augmented Generation, which is a methodology that enhances Large Language Models (LLMs) by incorporating knowledge from external databases. This approach aims to improve the accuracy and credibility of generated responses, particularly for knowledge-intensive tasks, by retrieving relevant document chunks through semantic similarity calculations. RAG effectively reduces issues like hallucinations and outdated knowledge by referencing external information, allowing for continuous knowledge updates and integration of domain-specific content. The RAG framework includes three main components: retrieval, generation, and augmentation.\n",
      "Ground Truth: RAG stands for Retrieval Augmented Generation. It is a method used in large language models (LLMs) that retrieves relevant information based on a query and then prompts an LLM to generate a response in the context of the retrieved information. This approach helps expand the LLM's access to vast amounts of information efficiently.\n",
      "---\n",
      "Final retrieved chunk IDs: [483, 488, 476]\n",
      "Query: What are the different ways to chunk documents for RAG?\n",
      "Generated Answer: The different ways to chunk documents for RAG include:\n",
      "\n",
      "1. **Fixed Number of Tokens**: The most common method is to split the document into chunks based on a fixed number of tokens (e.g., 100, 256, 512). Larger chunks can capture more context but may generate more noise, while smaller chunks may not fully convey the necessary context but produce less noise.\n",
      "\n",
      "2. **Recursive Splits and Sliding Window Methods**: These methods enable layered retrieval by merging globally related information across multiple retrieval processes, although they still struggle to balance semantic completeness and context length.\n",
      "\n",
      "3. **Small2Big Method**: This approach uses sentences (small) as the retrieval unit, providing preceding and following sentences as (big) context to the LLMs.\n",
      "\n",
      "4. **Metadata Attachments**: Chunks can be enriched with metadata information such as page number, file name, author, category, and timestamp to enhance their utility.\n",
      "\n",
      "These chunking strategies aim to optimize the retrieval process and improve the performance of RAG systems.\n",
      "Ground Truth: IDK\n",
      "---\n",
      "Final retrieved chunk IDs: [3893, 2844, 3882]\n",
      "Query: What are the different metrics for evaluating a RAG system?\n",
      "Generated Answer: The context does not specify the exact metrics used for evaluating a RAG system. It mentions that the RAGA S framework provides a suite of metrics to evaluate different dimensions of RAG architectures, such as the ability of the retrieval system to identify relevant context passages, the ability of the LLM to exploit such passages in a faithful way, and the quality of the generation itself. However, it does not detail these specific metrics. \n",
      "\n",
      "Therefore, the answer is IDK.\n",
      "Ground Truth: The different metrics for evaluating a RAG (Retrieval-Augmented Generation) system include context relevance, answer faithfulness and answer relevance. Additional metrics mentioned in the text for evaluating the retrieval and generation aspects include Accuracy, EM (Exact Match), Recall, Precision, R-Rate, Cosine Similarity, Hit Rate, MRR (Mean Reciprocal Rank), NDCG (Normalized Discounted Cumulative Gain), BLEU, and ROUGE/ROUGE-L.\n",
      "---\n",
      "Final retrieved chunk IDs: [380, 370, 390]\n",
      "Query: What are main contributions of RAG research?\n",
      "Generated Answer: The main contributions of RAG research include the development of three stages of RAG (Naive RAG, Advanced RAG, and Modular RAG), improvements in retrieval quality through pre-retrieval and post-retrieval strategies, optimization methods for indexing and query processes, and the integration of external knowledge sources to enhance the context provided to language models. RAG also addresses limitations faced by traditional models, such as retrieval challenges, generation difficulties, and augmentation hurdles, thereby improving the overall effectiveness of language model responses. Additionally, RAG research explores the synergy between retrieval, generation, and augmentation processes, helping to form a cohesive RAG framework.\n",
      "Ground Truth: The main contributions of RAG research include a thorough and systematic review of state-of-the-art RAG methods, the delineation of its evolution through various paradigms, advancements aimed at improving factual accuracy by providing access to auxiliary information, and exploration of innovative augmentation methodologies and flexible RAG pipelines. Additionally, the research emphasizes the need for domain-specific RAG techniques to enhance performance across various applications.\n",
      "---\n",
      "Final retrieved chunk IDs: [2915, 2937, 3758]\n",
      "Query: Which model is better for RAG, Claude v2 or Phi v3?\n",
      "Generated Answer: IDK\n",
      "Ground Truth: IDK\n",
      "---\n",
      "Final retrieved chunk IDs: [3849, 3864, 398]\n",
      "Query: What RAG solutions exist when an input question does not cover all of the necessary detail for long form generation?\n",
      "Generated Answer: IDK\n",
      "Ground Truth: Forward-Looking Active REtrieval augmented generation (FLARE) addresses this limitation by iteratively generating a temporary next sentence to be used as the query to retrieve relevant documents, only if it contains low-probability tokens. The retrieved documents are then used to regenerate the next sentence until reaching the sentence end\n",
      "---\n",
      "Final retrieved chunk IDs: [3903, 3898, 3908]\n",
      "Query: How to detect hallucinations in LLM-generated content?\n",
      "Generated Answer: Several approaches have been proposed to detect hallucinations in LLM-generated content:\n",
      "\n",
      "1. **Factuality Prediction with Prompting**: Some authors suggest using a few-shot prompting strategy to predict factuality (Zhang et al., 2023).\n",
      "\n",
      "2. **Linking to External Knowledge Bases**: Other approaches link generated responses to facts from an external knowledge base, though this isn't always feasible (Min et al., 2023).\n",
      "\n",
      "3. **Token Probability Inspection**: One method involves inspecting the probabilities assigned to individual tokens, where we would expect the model to be less confident in hallucinated answers compared to factual ones. For instance, BARTScore estimates factuality by looking at the conditional probability of the generated text given the input.\n",
      "\n",
      "4. **Supervised Classifiers**: Instead of looking at output probabilities, some propose training a supervised classifier on the weights from a hidden layer of the LLM to predict whether a statement is true or false (Azaria and Mitchell, 2023).\n",
      "\n",
      "5. **Answer Stability Sampling**: SelfCheckGPT addresses the lack of token probability access by sampling multiple answers. It posits that factual answers are more stable, meaning that different samples will tend to be semantically similar, whereas hallucinated answers\n",
      "Ground Truth: Several approaches exist, including using few-shot prompting, linking generated responses to facts from an external knowledge base, and inspecting probabilities assigned to invididual tokens, because low probabilities may indicate low confidence and possible hallucination.\n",
      "---\n",
      "Final retrieved chunk IDs: [3826, 2828, 3953]\n",
      "Query: What datasets are commonly used to evaluate RAG systems?\n",
      "Generated Answer: The commonly used datasets to evaluate RAG systems include Natural Questions (NQ), TriviaQA (TQA), WebQuestions (WQ), and CuratedTrec (CT).\n",
      "Ground Truth: \n",
      "---\n",
      "Final retrieved chunk IDs: [2242, 762, 2740]\n",
      "Query: In what year was GPT-3 introduced?\n",
      "Generated Answer: IDK\n",
      "Ground Truth: IDK\n",
      "---\n",
      "----------------------------- test_queries -----------------------------\n",
      "Final retrieved chunk IDs: [337, 347, 340]\n",
      "Query: What does RAG do?\n",
      "Generated Answer: RAG (Retrieval-Augmented Generation) enhances Large Language Models (LLMs) by retrieving relevant document chunks from external knowledge bases through semantic similarity calculation. By referencing external knowledge, RAG effectively reduces the problem of generating factually incorrect content, thereby improving the accuracy and credibility of the generated responses, particularly for knowledge-intensive tasks.\n",
      "Ground Truth: RAG retrieves relevant information based on the query and then prompts an LLM to generate a response in the context of the retrieved information.\n",
      "---\n",
      "Final retrieved chunk IDs: [3242, 3369, 1926]\n",
      "Query: What are the names of the two retrievers used in the study?\n",
      "Generated Answer: The two retrievers used in the study are Dragon and Contriever.\n",
      "Ground Truth: Contriever and Dragon\n",
      "---\n",
      "Final retrieved chunk IDs: [2892, 2935, 2917]\n",
      "Query: What is SELF-ROUTE?\n",
      "Generated Answer: SELF-ROUTE is a method that combines Retrieval-Augmented Generation (RAG) and long-context (LC) models to reduce computational costs while maintaining performance comparable to LC. It utilizes the LLM itself to route queries based on self-reflection, under the assumption that LLMs are well-calibrated in predicting whether a query is answerable based on the provided context. The method consists of two steps: a RAG-and-Route step, where the LLM predicts if the query is answerable and generates the answer if it is; and a long-context prediction step for queries deemed unanswerable in the first step. This approach allows for substantial reductions in the number of tokens required, leading to cost savings while achieving effective performance.\n",
      "Ground Truth: SELF-ROUTE utilizes LLM itself to route queries based on self-reflection, under the assumption that LLMs are well-calibrated in predicting whether a query is answerable given provided context.\n",
      "---\n",
      "Final retrieved chunk IDs: [2461, 2031, 1944]\n",
      "Query: What is the main goal of the research described in the paper?\n",
      "Generated Answer: The main goal of the research described in the paper is to propose and demonstrate a framework called DEMONSTRATE ‚ÄìSEARCH ‚ÄìPREDICT (DSP) that enhances retrieval-augmented in-context learning for knowledge-intensive tasks using frozen language models (LM) and retrieval models (RM). The framework aims to systematically improve the interaction between LMs and RMs, leading to more reliable problem-solving and state-of-the-art results in tasks such as question answering, fact checking, and information-seeking dialogue.\n",
      "Ground Truth: The research aims to compare Retrieval Augmented Generation (RAG) and long-context Large Language Models (LLMs) to understand their strengths and weaknesses, and to combine them effectively.\n",
      "---\n",
      "Final retrieved chunk IDs: [2792, 2856, 2832]\n",
      "Query: What advantage does RAG still have over long-context LLMs?\n",
      "Generated Answer: RAG has a distinctly lower computational cost compared to long-context LLMs, making it more cost-efficient.\n",
      "Ground Truth: Despite potentially lower performance, RAG offers a significantly lower computational cost compared to long-context LLMs.\n",
      "---\n",
      "Final retrieved chunk IDs: [2950, 2821, 2786]\n",
      "Query: How does SELF-ROUTE decide whether to use RAG or a long-context LLM?\n",
      "Generated Answer: SELF-ROUTE decides whether to use RAG or a long-context LLM based on model self-reflection. It routes queries to either RAG or LC depending on the predicted answerability of the queries, utilizing the strengths of both approaches to optimize performance and reduce computational costs.\n",
      "Ground Truth: SELF-ROUTE uses the LLM's own assessment of whether a query is answerable given the retrieved context to decide whether to use RAG or a long-context LLM.\n",
      "---\n",
      "Final retrieved chunk IDs: [2955, 2917, 2946]\n",
      "Query: How does SELF-ROUTE perform on visual question answering tasks?\n",
      "Generated Answer: IDK\n",
      "Ground Truth: IDK\n",
      "---\n",
      "Final retrieved chunk IDs: [2911, 2835, 2845]\n",
      "Query: What is the impact of SELF-ROUTE on the carbon footprint of LLM applications?\n",
      "Generated Answer: IDK\n",
      "Ground Truth: IDK\n",
      "---\n",
      "Final retrieved chunk IDs: [2938, 2945, 2935]\n",
      "Query: Can SELF-ROUTE be applied to other modalities beyond text, such as audio or video?\n",
      "Generated Answer: IDK\n",
      "Ground Truth: IDK\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Test with dev queries\n",
    "dev_generated_answers, dev_ground_truth_answers = test_rag_system(dev_queries)\n",
    "print(\"----------------------------- test_queries -----------------------------\")\n",
    "# Test with your own queries\n",
    "generated_answers, ground_truth_answers = test_rag_system(test_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b30541-75ca-4647-86b3-4af98d395c31",
   "metadata": {},
   "source": [
    "## Conduct experiments to evaluate user queries\n",
    "\n",
    "Report your average precision, recall and F1 score. You are welcome to sample the model multiple times for each query when computing your average, or you may sample once per query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7176779e-12a3-4d39-bfc9-91d5f6ae6ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import BERTScorer\n",
    "\n",
    "scorer = BERTScorer(model_type='bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "53e9537d-6fae-4b48-a94e-6d7f57d728c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import score\n",
    "\n",
    "def evaluate_bertscores(generated_answers, ground_truth_answers):\n",
    "    P, R, F1 = score(generated_answers, ground_truth_answers, lang=\"en\", verbose=True)\n",
    "    return P.mean().item(), R.mean().item(), F1.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a6a67d5e-3253-40d9-a175-d776b08826f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_refs = []\n",
    "filtered_preds = []\n",
    "for ref, pred in zip(dev_ground_truth_answers, dev_generated_answers):\n",
    "    if ref.strip():  # Keep only non-empty references\n",
    "        filtered_refs.append(ref)\n",
    "        filtered_preds.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "596d7d2a-d863-4678-a285-21065993b6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------- DEV QUERIES -----------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12f0516153f84d20a7ea7310e3d5586c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c3357aa33444543b6fc3d34bee352d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.10 seconds, 3.81 sentences/sec\n",
      "BertScore (Dev Queries) - Precision: 0.8711, Recall: 0.8797, F1: 0.8750\n"
     ]
    }
   ],
   "source": [
    "#Evaluate on Dev Queries\n",
    "print(\"----------------------------- DEV QUERIES -----------------------------\")\n",
    "dev_precision, dev_recall, dev_f1 = evaluate_bertscores(filtered_preds, filtered_refs)\n",
    "print(f\"BertScore (Dev Queries) - Precision: {dev_precision:.4f}, Recall: {dev_recall:.4f}, F1: {dev_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e29ab0c2-8177-4b69-84cc-7ebeae0899fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty reference at index 7\n"
     ]
    }
   ],
   "source": [
    "for i, ref in enumerate(dev_ground_truth_answers):\n",
    "    if not ref.strip():  # Check for empty or whitespace-only strings\n",
    "        print(f\"Empty reference at index {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c5561dd2-30a4-47be-9300-06e5dc05bad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------- TEST QUERIES -----------------------------\n",
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aba9fb7af95943d396acd301c1c2c1e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "346995908d39433ea3a8d8ffa7c0bfac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1.12 seconds, 8.03 sentences/sec\n",
      "BertScore (Test Queries) - Precision: 0.9159, Recall: 0.9382, F1: 0.9266\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on Test Queries\n",
    "print(\"----------------------------- TEST QUERIES -----------------------------\")\n",
    "test_precision, test_recall, test_f1 = evaluate_bertscores(generated_answers, ground_truth_answers)\n",
    "print(f\"BertScore (Test Queries) - Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, F1: {test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cff928f-e496-448f-a271-899f5ba8c6b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6d86c6-7a0f-4736-b8de-c08e6add34d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eef4bf-f35a-45e0-9d94-50a7592e48ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python-Prompt-Eng (conda)",
   "language": "python",
   "name": "python-prompt-eng"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
