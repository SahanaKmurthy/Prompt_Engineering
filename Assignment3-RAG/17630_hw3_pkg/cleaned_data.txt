text
"Findings of the Association for Computational Linguistics: EMNLP 2023, pages 9248–9274
December 6-10, 2023 ©2023 Association for Computational Linguistics
Enhancing Retrieval-Augmented Large Language Models with Iterative
Retrieval-Generation Synergy
Zhihong Shao1, Yeyun Gong2, yelong shen3, Minlie Huang1∗, Nan Duan2, Weizhu Chen3
1 The CoAI Group, DCST, Institute for Artificial Intelligence,
1 State Key Lab of Intelligent Technology and Systems,
1 Beijing National Research Center for Information Science and Technology,
1 Tsinghua University, Beijing 100084, China
2 Microsoft Research Asia 3 Microsoft Azure AI
szh19@mails.tsinghua.edu.cn aihuang@tsinghua.edu.cn
Abstract
Retrieval-augmented generation has raise exten-
sive attention as it is promising to address the
limitations of large language models including
outdated knowledge and hallucinations. How-
ever, retrievers struggle to capture relevance,
especially for queries with complex informa-
tion needs. Recent work has proposed to im-
prove relevance modeling by having large lan-
guage models actively involved in retrieval, i.e.,
to guide retrieval with generation. In this pa-
per, we show that strong performance can be
achieved by a method we call ITER -RETGEN,
which synergizes retrieval and generation in an
iterative manner: a model’s response to a task
input shows what might be needed to finish
the task, and thus can serve as an informative
context for retrieving more relevant knowledge
which in turn helps generate a better response
in another iteration. Compared with recent
work which interleaves retrieval with gener-
ation when completing a single output, I TER -
RETGEN processes all retrieved knowledge as
a whole and largely preserves the flexibility in
generation without structural constraints. We
evaluate ITER -RETGEN on multi-hop question
answering, fact verification, and commonsense
reasoning, and show that it can flexibly lever-
age parametric knowledge and non-parametric
knowledge, and is superior to or competitive
with state-of-the-art retrieval-augmented base-
lines while causing fewer overheads of retrieval
and generation. We can further improve per-
formance via generation-augmented retrieval
adaptation.
1 Introduction
Generative Large Language Models (LLMs)
have powered numerous applications, with well-
perceived utility. Despite being powerful, LLMs
lack knowledge that is under-represented in their
training data, and are prone to hallucinations, es-
pecially in open-domain settings (OpenAI, 2023).
∗*Corresponding author: Minlie Huang.
Retrieval-augmented LLMs, therefore, have raised
widespread attention as LLM outputs can be poten-
tially grounded on external knowledge.
Previous retrieval-augmented LMs (Izacard
et al., 2022b; Shi et al., 2023) typically adopted
one-time retrieval, i.e., to retrieve knowledge us-
ing only the task input (e.g., a user question for
open-domain question answering). One-time re-
trieval should suffice to fulfill the information needs
if they are clearly stated in the original input,
which is applicable to factoid question answering
(Kwiatkowski et al., 2019) and single-hop fact ver-
ification (Thorne et al., 2018), but not to tasks with
complex information needs, e.g., multi-hop rea-
soning (Yang et al., 2018) and long-form question
answering (Fan et al., 2019).
To fulfill complex information needs, recent
work proposes to gather required knowledge multi-
ple times throughout the generation process, using
partial generation (Trivedi et al., 2022a; Press et al.,
2022)) or forward-looking sentence(s) (Jiang et al.,
2023) as search queries. However, such structured
workflows of interleaving retrieval with generation
have the following limitations: (1) as intermediate
generation is conditioned on knowledge retrieved
before, with no awareness of knowledge retrieved
afterwards, they fail to process all retrieved knowl-
edge as a whole during the generation process; (2)
they require multi-round retrieval to gather a com-
prehensive set of knowledge, and may frequently
change the prompts by updating newly retrieved
knowledge, thus increasing the overheads of both
retrieval and generation.
In this paper, we find it simple but effective to
enhance retrieval-augmented LLMs through itera-
tive retrieval-generation synergy (ITER -RETGEN,
Fig 1). ITER -RETGEN iterates retrieval-augmented
generation and generation-augmented retrieval:
Retrieval-augmented generation outputs a response
to a task input based on all retrieved knowledge
(initially using the task input as the query). This
9248output shows what might be needed to fulfill the
task, and thus can serve as an informative context to
retrieve more relevant knowledge, i.e., generation-
augmented retrieval. The newly retrieved knowl-
edge can benefit another iteration of retrieval-
augmented generation. We can also leverage model
generations to adapt retrieval, by distilling knowl-
edge from a re-ranker with access to model genera-
tions to a dense retriever with access to task inputs
only, which may be beneficial in scenarios where
user inputs can be easily collected, but relevant
knowledge or desirable outputs are not annotated.
We evaluate our method on three tasks, includ-
ing multi-hop question answering, fact verification,
and commonsense reasoning. Our method prompts
an LLM to produce a chain of reasoning steps fol-
lowed by the final answer under a few-shot set-
ting. For in-context demonstrations, we focus on
problem-solving and follow Wei et al. (2022) to
annotate chains of thoughts, without explicitly con-
sidering how generation-augmented retrieval might
be affected, which makes it conceptually simple
and easy to implement. Our method achieves up
to 8.6% absolute gains over previous state-of-the-
art retrieval-augmented methods on four out of six
datasets while being competitive on the remaining
two. According to our experiments, generation gen-
erally benefits from more iterations, with two itera-
tions giving the most performance gains. One may
customize the performance-cost tradeoffs by choos-
ing an appropriate number of iterations. We can
further improve performance and also reduce itera-
tions via the aforementioned generation-augmented
retrieval adaptation.
We summarize our findings as follows:
• Automatic metrics such as exact match can
significantly underestimate the performance
of LLMs in question answering tasks. More-
over, improvements in exact match do not
always reflect improvements in generations.
Evaluation using LLMs may be more reliable.
• ITER -RETGEN is superior to or competi-
tive with state-of-the-art retrieval-augmented
methods, while being simpler and causing
fewer overheads of retrieval and generation.
With generation-augmented retrieval adapta-
tion, we can further improve performance
and also reduce overheads (by reducing itera-
tions).
• It is desirable for an LLM to leverage both
parametric knowledge and non-parametric
knowledge effectively. I TER -RETGEN con-
sistently outperforms Self-Ask on question
answering tasks, regardless of whether in-
context non-parametric knowledge mentions
the answers or not.
2 Related Work
In recent months, there has been a surge in LLM-
powered applications, such as ChatGPT, Bing Chat,
and CoPilot (Chen et al., 2021). While showing an
unprecedented level of performance, LLMs are sub-
ject to the following limitations: (1) Due to a high
demand for compute and data, it remains an open
research question to continually update LLMs both
efficiently and effectively (Scialom et al., 2022);
(2) LLMs also tend to hallucinate (OpenAI, 2023),
i.e., generating plausible but non-factual texts. To
alleviate these issues, there is a growing trend of
augmenting LLMs with tools (Mialon et al., 2023;
Gou et al., 2023), e.g., a code interpreter (Gao
et al., 2022b; Shao et al., 2023) or a search engine
(Nakano et al., 2021), in an attempt to offload sub-
tasks to more qualified experts, or to enrich the
input context for LLMs by providing more relevant
information.
Retrieval augmentation is a mainstream direc-
tion to connect LLMs to the external world. Previ-
ous retrieval-augmented LMs (Izacard and Grave,
2021; Shao and Huang, 2022) typically receive re-
trieved knowledge in a passive way: knowledge
is retrieved based on the task inputs without LMs’
intervention. As it is difficult for a retriever to cap-
ture relevance, especially in the zero-shot setting,
recent work shows a shift towards having LLMs
actively involved in retrieval to improve relevance
modeling, e.g., to provide a specific context for
retrieval with model generations (e.g., generated
search queries (Nakano et al., 2021; Press et al.,
2022; Yao et al., 2022), partial generation (Trivedi
et al., 2022a), or forward-looking sentences (Jiang
et al., 2023)). Khattab et al. (2022) proposed a
DSP programming framework that supports vari-
ous retrieval-augmented methods.
Recent work interleaves retrieval with generation
when completing a single output. Such a structured
workflow may reduce the flexibility in generation
(Yao et al., 2022). ITER -RETGEN avoids interrupt-
ing generation with retrieval, but iterates retrieval
and generation, i.e., to leverage the complete gen-
eration from the previous iteration to retrieve more
9249Question:
Retrieval:
Retrieval -Augmented Generation: 
What is the height of the player who won the 2015 AFL 
Rising Star award?"
"Published as a conference paper at ICLR 2023
GENERATE RATHER THAN RETRIEVE : L ARGE LANGU -
AGE MODELS ARE STRONG CONTEXT GENERATORS
Wenhao Yu1∗, Dan Iter2, Shuohang Wang2, Yichong Xu2, Mingxuan Ju1,
Soumya Sanyal3∗, Chenguang Zhu2, Michael Zeng2, Meng Jiang1
1University of Notre Dame 2Microsoft Cognitive Service Research
3University of Southern California
1wyu1@nd.edu; 2iterdan@microsoft.com
ABSTRACT
Knowledge-intensive tasks, such as open-domain question answering (QA), require
access to a large amount of world or domain knowledge. A common approach
for knowledge-intensive tasks is to employ a retrieve-then-read pipeline that ﬁrst
retrieves a handful of relevant contextual documents from an external corpus such
as Wikipedia and then predicts an answer conditioned on the retrieved documents.
In this paper, we present a novel perspective for solving knowledge-intensive tasks
by replacing document retrievers with large language model generators. We call
our method generate-then-read (GENREAD ), which ﬁrst prompts a large language
model to generate contextual documents based on a given question, and then
reads the generated documents to produce the ﬁnal answer. Furthermore, we
propose a novel clustering-based prompting method that selects distinct prompts,
in order to generate diverse documents that cover different perspectives, leading to
better recall over acceptable answers. We conduct extensive experiments on three
different knowledge-intensive tasks, including open-domain QA, fact checking, and
dialogue system. Notably, GENREAD achieves 71.6 and 54.4 exact match scores on
TriviaQA and WebQ, signiﬁcantly outperforming the state-of-the-art retrieve-then-
read pipeline DPR-FiD by +4.0 and +3.9, without retrieving any documents from
any external knowledge source. Lastly, we demonstrate the model performance can
be further improved by combining retrieval and generation. Our code and generated
documents can be found at https://github.com/wyu97/GenRead.
1 I NTRODUCTION
Knowledge-intensive tasks, such as open-domain question answering (QA) and fact checking, require
access to a large amount of world or domain knowledge (Petroni et al., 2021). These tasks are
even challenging for humans without access to an external knowledge source such as Wikipedia.
A common thread of existing methods for knowledge-intensive tasks employ a retrieve-then-read
pipeline that ﬁrst retrieves a handful of relevant contextual documents from Wikipedia and then
conditions the prediction of the answer on these documents along with the question (Karpukhin et al.,
2020; Lewis et al., 2020; Izacard & Grave, 2021). Nevertheless, these methods mainly suffer from
three drawbacks. First, candidate documents for retrieval are chunked (e.g., 100 words) and ﬁxed, so
the retrieved documents might contain noisy information that is irrelevant to the question. Second, the
representations of questions and documents are typically obtained independently in modern two-tower
dense retrieval models (Karpukhin et al., 2020), leading to only shallow interactions captured between
them (Khattab et al., 2021). Third, document retrieval over a large corpus requires the retriever model
to ﬁrst encode all candidate documents and store representations for each document. These two
operations limit the parameters of dense retrievers and the size of embedding vectors, and thus cannot
enjoy the world knowledge or deduction capabilities of large language models (Levine et al., 2022).
§ Unless otherwise speciﬁed, we use the text-davinci-002 version of InstructGPT in our experiments.
* Work done during internship at Microsoft Cognitive Service Research group.
1Published as a conference paper at ICLR 2023
In this paper, we propose to leverage large language models, such as InstructGPT (Ouyang et al.,
2022), to directly generate contextual documents for a given question, instead of retrieving relevant
documents from an external corpus, such as Wikipedia. Our approach has two main advantages. First,
we show that generated contextual documents contain the correct answer more often than the top
retrieved documents. We believe this is because large language models generate contextual documents
by performing deep token-level cross-attention between all the question and document contents,
resulting in generated documents that are more speciﬁc to the question than retrieved documents.
Second, we show that our approach signiﬁcantly outperforms directly generating answers from large
language models despite not incorporating any new external information. This is mainly because
the task of generating document-level contexts is close to the objective of causal language modeling
pre-training, so the world knowledge stored in the model parameters can be better utilized.
We show, on multiple datasets, that generated documents are more likely to contain correct answers
than the top retrieved documents. Notably, in dense retrieval methods, as more documents are
retrieved, the recall of documents containing the correct answer increases (Karpukhin et al., 2020).
However, the recall performance does not scale as well with generated documents because even with
sampling methods, generated documents tend to contain duplicate information. In order to improve
the recall performance of generated documents, we propose a novel clustering-based prompt method.
We synthesize a prompt with in-context demonstrations of question-document pairs sampled from
diverse clusters. These prompts result in generated documents that cover different perspectives of the
question and improve the scaling of performance as more documents are generated per question.
In contrast to the retrieve-then-read pipeline, our method is essentially a generate-then-read pipeline.
Speciﬁcally, it ﬁrst prompts a large language model to generate contextual documents based on a
given question, and then reads the generated document to produce the ﬁnal answer. The reader can
still be a large model (e.g., InstructGPT (Ouyang et al., 2022)) used under a zero-shot setting, or a
small one (e.g., FiD (Izacard & Grave, 2021)) ﬁne-tuned with generated documents on the training
split of the target dataset. We evaluate our proposed method on three different knowledge-intensive
tasks and demonstrate its effectiveness on both zero-shot and supervised settings.
Overall, our main contributions can be summarized as follows:
1. We propose a novel generate-then-read pipeline for solving knowledge-intensive tasks, i.e.,
replacing the process of retrieving documents from Wikipedia or searching for related documents on
Google, by prompting a large language model to generate relevant contextual documents.
2. We propose a novel clustering-based prompting approach to generate multiple diverse contextual
documents that increases the likelihood of covering the correct answer. We demonstrate this approach
can signiﬁcantly improve performance on end QA and other downstream tasks.
3. We conduct extensive experiments with three knowledge-intensive NLP tasks under both zero-
shot and supervised settings. Notably, our method can match or even outperform retrieve-then-read
pipeline methods, without retrieving any documents from any external knowledge source.
2 R ELATED WORK
KNOWLEDGE -INTENSIVE NLP VIA RETRIEVE -THEN -READ PIPELINE . Mainstream methods
for solving knowledge-intensive NLP tasks employ a retrieve-then-read model pipeline. Given a
question, this model ﬁrst leverages a retriever over a large evidence corpus (e.g. Wikipedia) to
fetch a set of relevant documents that may contain the answer. A reader is then used to peruse the
retrieved documents and predict an answer. Recent follow-up work has mainly focused on improving
the retriever (Karpukhin et al., 2020; Qu et al., 2021; Sachan et al., 2022) or the reader (Izacard
& Grave, 2021; Cheng et al., 2021; Yu et al., 2022), or training the system end-to-end (Lewis
et al., 2020; Singh et al., 2021). Early retrieval methods mainly employed sparse retrievers, such as
BM25 (Chen et al., 2017). Recently, ORQA (Lee et al., 2019) and DPR (Karpukhin et al., 2020) have
revolutionized the ﬁeld by utilizing dense contextualized vectors for document indexing, leading to
superior performance to traditional approaches. We propose an alternative approach which forgoes
retrieval, instead extracting the knowledge from the model parameters of a large language model.
We show that our approach is can be combine with dense retrievers to outperform both methods
independently. Our method can also be combined with any reader mechanism, allowing generated
context documents to be plugged into any current knowledge-intensive NLP pipelines.
GENERATOR AS RETRIEVER FOR OBTAINING CONTEXTUAL DOCUMENTS . Recent works have
investigated using auto-regressive language models to generate identiﬁer strings for documents, as an
2Published as a conference paper at ICLR 2023
intermediate target for retrievals, such as entity names (De Cao et al., 2020) or distinctive n-grams that
can be mapped to full passages (Bevilacqua et al., 2022). However, one needs to create the identiﬁers,
hence the structure was not thoroughly evaluated on a large-scale benchmark (Bevilacqua et al., 2022).
Other works have demonstrated that the knowledge stored in the parameters of pre-trained language
models could be “retrieved” to some extent by directly generating text (Petroni et al., 2019; Roberts
et al., 2020). However, the previous work only used generation for query expansion (Mao et al., 2021),
which did not exploit the potential of directly generating contextual documents for open-domain
questions. Different from the above approaches that aimed to train a generator model to produce
contextual document identiﬁers (which is still using the original Wikipedia text) or provide data
augmentation to retrievers, our work directly generates contextual documents for given questions.
NLP M ODELS ENHANCED BY LARGE LANGUAGE MODEL OUTPUTS . A line of recent work
has shown that relevant knowledge can be elicited from large language models, especially for those
domains that lack appropriate knowledge bases with sufﬁcient coverage (Liu et al., 2022b; Fang et al.,
2022). For example, Liu et al. (2022b) proposed leveraging GPT-3 to generate relevant contexts, then
providing the contexts as additional input when answering a commonsense question. Another line of
work focused on prompting a large language model to generate a series of intermediate reasoning
steps, often referred to as chain-of-thought (Wei et al., 2022b; Kojima et al., 2022; Li et al., 2022).
The prompt consists of an instruction (e.g., Let’s think step by step!), a few demonstrations that are
ﬁxed for each task, and a new-question placeholder. The demonstrations are human-written, and
each consists of a question in the style of the task and a series of intermediate reasoning steps that is
helpful for answering the question. Our work does not require any human annotation, but adds to this
line of work of leveraging model generated text to guide further generations. In our case, we apply
this approach to knowledge-intensive tasks, which have not been explored by previous work.
3 P ROPOSED METHOD
In this section, we present details of our proposed novel generate-then-read (GENREAD ) pipeline for
solving various knowledge-intensive tasks. Speciﬁcally, it ﬁrst prompts a large language model to
generate contextual documents with respect to a given query, then reads the generated documents
to predict the ﬁnal answer. The reader can either be a large model (e.g., InstructGPT) used for the
zero-shot setting, or a small one (e.g., FiD) ﬁne-tuned with generated documents on the training split
of the target dataset. We introduce the zero-shot setting in §3.1 and supervised setting in §3.2.
3.1 Z ERO -SHOT SETTING
Under the zero-shot setting, there is no training data – neither questions nor contextual documents.
When tested on the open-domain QA task, most existing large language models directly encode the
given question and predict the answer (Brown et al., 2020; Du et al., 2022; Chowdhery et al., 2022).
Speciﬁcally, the question q, associated with some text prompt, is input to the model, which then
generates the answer, denoted as p(a|q,θ), where θrepresents the pre-trained model parameters. In
practice, the maximum a posteriori estimation (MAP) is the ﬁnal answer, i.e.,ˆa= arg maxa p(a|q,θ).
However, this way of directly asking large language models to output answers often leads to poor
performance, as it leaves a considerable amount of additional world knowledge unexploited (Levine
et al., 2022). On the contrary, the zero-shot retrieve-then-read pipeline ﬁrst uses an off-the-shelf
retriever to fetch relevant documents from an external knowledge source such as Wikipedia, then
asks the large language model to read the documents and predict the answer.
In this work, we improve the performance by introducing an additional auxiliary generated document
variable d, and then extend the model to have the form p(a|q) =∑
i p(a|di,q)p(di|q). In practice,
we cannot sum over all possible documents d. Therefore, the most common approach is to compute
the MAP estimate ˆd = arg max ˆp(d) using beam search, and then to approximate the sum over d
with this single value. This two step approach, we label it as a generate-then-read pipeline.
STEP 1: G ENERATE . In this step, we ﬁrst prompt a large language model (e.g., InstructGPT (Ouyang
et al., 2022)) to generate documents based on the given question. For example, the input to the
language model could be “Generate a background document to answer the given question. {question
placeholder}”. We can use any decoding strategy (e.g., greedy decoding, beam search), but we used
greedy decoding throughout the zero-shot experiments for simplicity and reproducibility.
3Published as a conference paper at ICLR 2023
What does Monsanto own? (WebQ) 
QuestionDocumentCluster𝑞 𝑑 …𝑞!"" 𝑑!"" 𝑐… … …𝑞!# 𝑑!# 𝑐… … …
Step1:Getonedocument𝑑foreachquestion𝑞viaretrievalorgeneration.
Step2:Getembeddings, andcluster them by K-means.
Initial 𝑑:Monsanto is a multinational agrochemical and agricultural biotechnology corporation…It is one of the world’s leading producers of roundup, a gly-phosateherbicide.(63words)Step3:Givenquestion 𝑞fortrainingorinference,foreachcluster𝑐∈1…𝑘:•sample𝑞!#,𝑑!#,𝑗=1…𝑛,whoseclusteridis𝑐;•createprompt𝑝!=“𝑞!"";𝑑!"";…;…;𝑞!$;𝑑!$”;•generatedocument𝑑!with𝑝!usinga large language model.Usinga reader (e.g., FiD),with𝑞andthediversedocuments{𝑑"",𝑑%,…,𝑑&},findanswers𝑎.
•agriculturalchemicals•seed (also correct)
Generated𝑑!:Monsanto Company is an Ame-rican multinational agrochemical and agricultural biotechnology corporation …It is a leading producer of genetically engi-neered seedand … (70 words)
Generated𝑑"":Monsanto is a multinational agricultural biotechnology corporation. … The company also manufactures other agricultural chemicals, such as insecticides … (36 words) 
Figure 1: An overall framework of clustering-based prompting method. It leverages distinct question-
document pairs sampled from each embedding cluster as in-context demonstrations to prompt a large
language model to generate diverse documents, then read the documents to predict an answer.
STEP 2: R EAD . In the second step, we use generated sentence ˆdalong with the input question to
produce the ﬁnal answer from the large language model. This is actually the same setting as “zero-
shot” reading comprehension, as widely studied in existing works (Brown et al., 2020; Lazaridou
et al., 2022). We choose appropriate prompts from P3 (Bach et al., 2022), such as “Refer to the
passage below and answer the following question. Passage: {background placeholder}Question:
{question placeholder}”. Finally, the language model is fed the prompted text to generate an answer.
3.2 S UPERVISED SETTING
Although large language models demonstrate impressive performance on zero-shot learning abilities,
their performance still lag behind the supervised setting. Therefore, we also explore how the generated
documents from large language models can beneﬁt the supervised setting. As directly ﬁne-tuning
large language models on downstream datasets could be prohibitively expensive, we leverage a small
reader model such as FiD to peruse the generated documents under the supervised setting.
Under the supervised setting, scaling the size of retrieved documents can lead to better perfor-
mance (Karpukhin et al., 2020; Izacard & Grave, 2021). This is mainly because retrieving more
documents can cover more relevant information and knowledge, i.e., a higher recall score. Nev-
ertheless, asking a large language model to generate multiple high-quality contextual documents
is a challenging task. Dense retrieval methods can fetch multiple documents covering different
perspectives of the answer. Compared to dense retrievers, simply prompting a large language model
to generate multiple contextual documents often leads to low knowledge coverage, since the contents
generated by multiple decoding passes from the same input tend to be similar. Sampling decoding
methods, such as nucleus sampling1 (Holtzman et al., 2020) can diversify the generation process to
some extent, but the knowledge content of generated texts still tends to be highly repetitive when
used to generate documents for a given question. We further propose two novel solutions, including
diverse human prompts and clustering-based prompts, which will be elaborated on in this section.
3.2.1 D IVERSE HUMAN PROMPTS
In order to avoid similar token distributions under a single prompt, we ask human annotators to
provide different prompts, in order to make the generated document diverse. This method is simple,
but can effectively vary the token distribution during generation. In the experiments, we empirically
found this method can bring improvement to the retrieval performance (Figure 2). However, this
method suffers from two drawbacks. On one hand, it requires human annotators to write different
prompts, which cannot be easily generalized to different knowledge-intensive tasks. On the other
hand, different large language models might be sensitive to different prompt words, which might
cause a set of good prompt words not work on a different large language model.
3.2.2 C LUSTERING -BASED PROMPTS
To increase knowledge coverage in generated documents, we propose a novel clustering-based prompt
method. It ﬁrst clusters the representations of a set of documents into Kclasses (K = 2in Figure
1We treated nucleus sampling as a baseline to generate multiple documents, in which we set p = .95.
4Published as a conference paper at ICLR 2023
Models Open-domain QA Fact Checking Dialogue System
NQ TriviaQA WebQ FEVER FM2 WoW (F1 / R-L)
*with retriever, AND directly trained on these datasets
DPR + InstructGPT* 29.1 53.8 20.2 79.8 65.9 15.4 13.7
*with retriever, BUT NOT trained on these datasets
BM25 + InstructGPT 19.7 52.2 15.8 78.7 65.2 15.7 13.7
Contriever + InstructGPT 18.0 51.3 16.6 80.4 66.6 15.5 14.0
Google + InstructGPT 28.8 58.8 20.4 82.9 66.0 14.8 13.2
*without retriever, and not using external documents
Previous SoTA methods 24.71 56.72 19.01 - - - -
InstructGPT (no docs.) 20.9 57.5 18.6 77.6 59.4 15.4 13.8
GENREAD (InstructGPT) 28.0 59.0 24.6 80.4 65.5 15.8 14.2
Table 1: Zero-shot open-domain QA performance. Our proposed GENREAD with the InstructGPT
reader (named GENREAD (InstructGPT)) can signiﬁcantly outperform the original InstructGPT,
achieving new state-of-the-art performance on three open-domain QA benchmarks (previous SoTA:
1GLaM (Du et al., 2022), 2FLAN (Wei et al., 2021)) under this setting without using any external
document. Our GENREAD can achieve comparable or even better performance than zero-shot
retrieve-then-read models that use a retriever or search engine to ﬁrst obtain contextual documents.
To ensure reproducibility, we use greedy search in decoding. All prompts used are shown in the §B.1.
1), where the number of classes is equal to the number of documents that need to be generated
in the end. Next, it randomly selects nquestion-document pairs (n = 5 in Figure 1) from each
cluster. Lastly, a large language model presents the different nquestion-document pairs as in-context
demonstrations for generating documents to a given question. In this way, large language models
are based on different distributions of examples, hence resulting in generated documents covering
different perspectives. We show this in Figure 1 and illustrate the details of each step as follows.
STEP 1: G ET ONE INITIAL DOCUMENT PER QUESTION . Similar to the zero-shot setting, we ﬁrst
ask a large language model to generate one contextual document dfor each question q∈Q, where Q
is the set of questions in the training split. Alternatively, we can use an unsupervised retriever (e.g.,
BM25) to obtain a document from Wikipedia. We now have a question-document pair set{qi,di}|Q|
i=1.
STEP 2: E NCODE EACH DOCUMENT , DO K-MEANS CLUSTERING . We then use a large language
model (i.e., GPT-3) to encode each question-document pair, i.e., ei = GPT-3([qi,di]), resulting in a
12,288-dimensional vector per document. Then, we use K-means to cluster all embedding vectors
{ei}|Q|
i=1 into Ksets, so each question-document pair is assigned a unique cluster id c∈{1,...,K}.
We vary the number of Kin the experiments, which will be illustrated in Figure 2.
STEP 3: S AMPLE AND GENERATE K DOCUMENTS . Lastly, we sample nquestion-document pairs
from each cluster c, denoted as {qc1,dc1; qc2,dc2; ...; qcn,dcn}, in which nis a hyperparameter 2.
Then, the nsampled question-document pairs from the same cluster serve as in-context demonstrations
for the large language model to generate a contextual document. For example, the input to the
large language model could be “ {qc1 placeholder}{dc1 placeholder}... {qcn placeholder}{dcn
placeholder}{input question placeholder}”. By enumerating the sampled documents in these K
clusters, we can ﬁnally get K-generated documents. By conditioning on different sampled in-context
demonstrations collected from different clusters, the large language model has been biased for
different perspectives. Although these different perspectives exist in a latent manner, we empirically
show it works well in practice, by comparing it with sampling methods, diverse human prompts
(Figure 2 and Table 2) and randomly sampling npairs from the entire dataset (Table 11).
4 E XPERIMENTS
In this section, we conduct comprehensive experiments on three knowledge-intensive NLP tasks,
including open-domain QA (NQ (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017) and
WebQ (Berant et al., 2013)), fact checking (FEVER (Thorne et al., 2018) and FM2 (Eisenschlos
et al., 2021)) and open-domain dialogue system (WoW (Dinan et al., 2019)). More detailed dataset
2In the experiments, we set n = 5and found increasing n does not bring extra improvement.
5Published as a conference paper at ICLR 2023
505866748290
1 5 101520
Recall@K 
TriviaQATopK documents
Google searchDPR-MultiOurs (sampling)Ours (human)Ours (clustering)
404856647280
1 5 101520
Recall@K 
WebQ TopK documents
Google searchDPR-MultiOurs (sampling)Ours (human)Ours (clustering)
Figure 2: Recall@K on test sets, measured as the percentage of top-K documents that contain the
answer. Our proposed clustering-based prompting method can outperform DPR and Google search,
also two variants of using LLMs to generate documents. Exact numbers are reported in Table 5.
information can be found in Appendix A.1. To evaluate the model performance, we use exact
match (EM) score for evaluating open-domain QA (Zhu et al., 2021). An answer is considered
correct if and only if its normalized form has a match in the acceptable answer list. We also employ
Recall@K (R@K) as an intermediate evaluation metric, measured as the percentage of top-K retrieved
or generated documents that contain the answer. This metric is commonly used in evaluations of
previous works (Karpukhin et al., 2020; Izacard & Grave, 2020; Sachan et al., 2022). For other
knowledge-intensive tasks, we follow the KILT benchmark (Petroni et al., 2021) to use accuracy
(ACC) for fact checking and F1 / Rouge-L (R-L) score for open-domain dialogue system.
4.1 Z ERO -SHOT SETTING EXPERIMENTS
We ﬁrst compare our proposed GENREAD approach with various large language models proposed
in recent years, including GPT-3 (Brown et al., 2020), Gopher (Rae et al., 2021), FLAN (Wei et al.,
2021), GLaM (Du et al., 2022), Chinchilla (Hoffmann et al., 2022), PaLM (Chowdhery et al., 2022)
and InstructGPT (Ouyang et al., 2022). Due to the space limitation, we only put the best performance
on each dataset in Table 1, in which the line is called previous SoTA methods. In addition, their
corresponding model parameters and performance are listed in Table 9 in Appendix. All of these
baseline methods use the same input formats, i.e., [prompt words; question].
GENREAD is based on InstructGPT with 175B parameters. In order to fully evaluate the effec-
tiveness of our proposed method, we also compare with InstructGPT augmented with retrieved
documents from Wikipedia or Google search. The baseline methods (1) BM25 / Contriever + In-
structGPT; (2) Google + InstructGPT; (3) DPR + InstructGPT have the same input format as our
GENREAD , i.e., [prompt words; contextual document; question]. BM25 is a traditional sparse
retrieval method. Contriever (Izacard et al., 2022a) is a state-of-the-art unsupervised dense retrieval
model. DPR (Karpukhin et al., 2020) is a supervised dense retrieval model directly trained on NQ,
TriviaQA and WebQ datasets. We note that comparing with above three methods is challenging
because our method only relies on the large language model itself, without using any external corpus.
4.1.1 E XPERIMENTAL RESULTS
In the experiments, we use InstructGPT as our backbone model. As shown in Table 1, compared
with state-of-the-art large language models, our proposed GENREAD with the InstructGPT reader
improves its performance by generating contextual documents and conditioning on the generated
documents, even though no new data is introduced, and the generator and reader have the exact
same parameters. Speciﬁcally, GENREAD can improve the EM score by +6.9 on three open-domain
QA benchmarks, compared to the original InstructGPT. We also make a similar observation on fact
checking and open-domain dialogue system. Our proposed GENREAD can consistently outperform
the baseline InstructGPT model without retrieving any contextual documents.
To further validate the effectiveness ofGENREAD , we compare against zero-shot retrieve-then-read
pipeline models, which ﬁrst use a retrieval model or the Google search engine to get a relevant
contextual document, then use InstructGPT to read the texts and produce the ﬁnal answer. As shown
in Table 1, GENREAD can achieve on-par performance with zero-shot retrieve-then-read pipeline
models on the NQ and FM2 datasets, and outperform them on all other benchmarks. The knowledge
6Published as a conference paper at ICLR 2023
Models # reader # docu- TriviaQA WebQ NQ Avg.parameters ments open test open test open test
*baselines with retrieving from Wikipedia; all numbers reported by existing papers
DPR (Karpukhin et al., 2020) 110M 100 56.8 41.1 41.5 46.5
RAG (Lewis et al., 2020) 400M 10 56.1 45.2 44.5 48.6
FiD (Izacard & Grave, 2021) 770M 100 67.6 50.5 51.4 56.5
*baselines with retrieving from Wikipedia or Google; all numbers from our experiments
FiD-l (DPR, Wikipedia) 770M 10 61.9 48.1 46.7 52.2
FiD-xl (DPR, Wikipedia) 3B 10 66.3 50.8 50.1 55.7
FiD-xl (Google search) 3B 10 70.1 53.6 45.0 56.2
*our proposed method by leveraging a large language model to generate documents
GENREAD (FiD-l) (sampling) 770M 10 67.8 51.5 40.3 53.2
GENREAD (FiD-l) (clustering) 770M 10 70.2 53.3 43.5 55.6
GENREAD (FiD-xl) (sampling) 3B 10 69.6 52.6 42.6 54.9
GENREAD (FiD-xl) (clustering) 3B 10 71.6 54.4 45.6 57.1
⊢merge retrieved documents with generated documents 74.3 56.2 54.0 61.5
Table 2: Supervised open-domain QA performance. By only using generated documents from In-
structGPT, our GENREAD with FiD reader (named GENREAD (FiD)) can achieve better performance
than baseline methods on TriviaQA and WebQ. Through our detailed analysis of NQ, we found the
performance gap mainly due to the temporality issue, which will be elaborated in §A.8.
learned by the large language models can be retrieved via autoregressive text generation. Without
seeing any examples from these datasets, GENREAD can outperform using the supervised retrieval
model (i.e., DPR) to recover relevant contextual documents.
4.2 S UPERVISED SETTING EXPERIMENTS
We compare our proposed GENREAD with retrieve-then-read models, including DPR (Karpukhin
et al., 2020), RAG (Lewis et al., 2020), and FiD (Izacard & Grave, 2021). In addition, we compared
with obtaining relevant documents from the internet using the Google search engine.
4.2.1 E XPERIMENTAL SETUP
For our proposed method, we replace the retriever with a large language model to directly generate
contextual documents. In the experiments, we use InstructGPT (Ouyang et al., 2022). After contextual
documents are retrieved or generated, we employ a FiD reader with 770M parameter models (i.e.,
FiD-l) and 3B parameter models (i.e., FiD-xl) that are ﬁne-tuned on the training split of target datasets.
We note that we only use 10 documents during reading for the following reasons.
Why do we choose to use only 10 documents instead of 100 when reading?
As noted in Section 6.2 in DPR (Karpukhin et al., 2020) and Figure 3 in FiD (Izacard & Grave, 2021),
increasing the number of documents can lead to better model performance and achieve state-of-the-art
when using 100 documents. However, there are two major drawbacks to using 100 documents during
the reading step. First, the operation is very expensive, leading to a signiﬁcant increase in memory
consumption and training time. As reported by Izacard & Grave (2021), the training process requires
64 Tesla V100 32GB running for around one day. Second, generating documents by using a large
language model is slow and expensive, so only using 10 documents can be a signiﬁcant cost saving
in our method. Therefore, in our experiments, we choose to use 10 documents during the reading
process. When using FiD-770M (i.e., FiD-large), the training process can be easily performed even
on a single Tesla V100 32GB GPU. Meanwhile, when only using 10 documents, we can also increase
the size of FiD model from 770M to 3B, which takes about the same amount of GPU memory as
using 100 documents on a 770M model, but at the same time signiﬁcantly shortens the training time.
We note that training T5-3B model needs a bigger cluster such as 8 Tesla V100 or A100 GPUs.
4.2.2 E XPERIMENTAL RESULTS ON OPEN -DOMAIN QA
We ﬁrst use Recall@K to compare the retrieval accuracy of different models. As shown in Figure 2,
GENREAD can signiﬁcantly outperform DPR and Google search for under 10 retrieved or generated
7Published as a conference paper at ICLR 2023
4748495051525354
2040 70100
EMscore
NQTopKdocuments
DPR only (K documents)DPR (K-10) + LLM (10)
6062646668707274
2040 70100
EMscore
TriviaQATopKdocuments
DPR only (K documents)DPR (K-10) + LLM (10)
4748495051525354
2040 70100
EMscore
WebQTopKdocuments
DPR only (K documents)DPR (K-10) + LLM (10)
Figure 3: Combining DPR retrieved documents and large language model (LLM) generated docu-
ments can achieve signiﬁcantly better performance than using DPR retrieved documents only. For a
fair comparison, instead of adding LLM generated documents to the model, we replace 10 documents
retrieved by DPR with 10 documents generated by LLM so the total number of documents is the same.
In this experiment, we use FiD-l (i.e., FiD-large) as the reader model because when the documents
scale to more than 20, FiD-xl (i.e., FiD-3B) causes out-of-memory issues on A100 GPUs.
documents. Compared to different GENREAD variants, including nucleus sampling, human written
prompts, and clustering-based prompts, clustering-based prompts achieve the best performance. At
the same time, we notice that the language model inevitably has the problem that the slope of the
curve decreases as the number of generated documents increases. On one hand, this is due to the
similarity of token distributions when large language models generate multiple documents. On the
other hand, due to the shallow interaction characteristics of the dense retrieval model itself, the
retrieved documents might not be completely relevant to the given question, so that the increase in
recall might come from false positive documents, as also mentioned by Sachan et al. (2022).
As shown in Table 2, we can ﬁrst observe the FiD model performs the best among all baseline models.
Using FiD-xl with only 10 documents achieves comparable performance with using FiD-l with 100
documents. The average gap is less than 1% on three benchmarks. Compared with both close-book
models and Wikipedia-based retrieve-then-read pipelines, our proposed GENREAD can achieve
state-of-the-art performance. Furthermore, compared with using sampling methods to generate
documents, the clustering-based prompt method can improve the EM score by +2.2 on average. This
indicates that the clustering-based prompt method is effectively increasing the knowledge coverage
of generated documents, and also leading to better downstream QA performance. We also show
that GENREAD can outperform Google search on all benchmarks. We observe both our method and
Google search perform worse than DPR, mainly due to the signiﬁcant portion of time-dependent
questions in the dataset, which is described in the following analysis.
4.2.3 E XPERIMENTAL RESULTS ON OTHER TASKS
Models FEVER FM2 WoW
Acc. Acc. F1 / R-L
RAG (Lewis et al., 2020) 86.3 71.1 13.1 / 11.6
FiD (Izacard & Grave, 2021) 90.2 77.6 17.5 / 16.1
GENREAD (FiD-xl) (sampling) 89.0 76.3 18.9 / 16.7
GENREAD (FiD-xl) (clustering) 89.6 77.8 19.1 / 16.8
⊢merge two source docs. 91.8 78.9 20.1 / 17.9
Table 3: Supervised performance on fact checking (FEVER
and FM2) and open-domain dialogue system (WoW).
We demonstrate the experimental re-
sults in Table 3. Under the supervised
setting, GENREAD can achieve on
par performance on the fact checking
task and superior performance on the
dialogue system task, indicating that
large language model can be seen as
a strong knowledge generator.
The main reason that GENREAD per-
forms worse than the dense retriever
for fact checking is that the task provides sufﬁcient semantic information to reach strong performance
on this binary decision task. So, there is a smaller semantic gap between the given factual statement
and contextual documents than that of question and document pairs in open-domain QA, which is an
easier retrieval setting for modern dense retrieval methods that are mainly based on vector similarity.
4.3 O BSERVATIONS AND EXPERIMENTAL ANALYSIS
4.3.1 C OMPLEMENTARITY OF GENERATED AND RETRIEVED DOCUMENTS
Generated documents can be combined with retrieved documents to outperform both. Even with a very
large number of retrieved documents, including few samples of generated knowledge leads to large
8Published as a conference paper at ICLR 2023
improvements. As shown in Table 2, merging retrieved documents with generated documents can
achieve state-of-the-art performance compared to all baseline methods listed in the table. Speciﬁcally,
it can improve +5.7 averagely on three open-domain QA benchmarks compared to DPR alone, and
improve +4.4 averagely compared to the large language model alone.
4.3.2 C OVERAGE ANALYSIS OVER ALL POSSIBLE ANSWERS
The improvement in open-domain QA performance is due to the fact that correct answers are
included more frequently in the generated text Recall@K is the most commonly used met-
ric in existing works to measure the retrieval performance, which computes the percentage of
top-K retrieved or generated documents that contain any possible answer at least once. than
in the retrieved documents. However, as many questions contain multiple correct answers,
recall@K cannot fully reﬂect the diversity of generated or retrieved documents. Each ques-
tion in the WebQ has 2.39 correct answers, 1.79 correct answers in NQ and 14.02 (includ-
ing all entity alias) in the TriviaQA. NQ and WebQ do not include alias names in the labels.
Documents obtained by ↓ NQ TriviaQA WebQ
- w. alias w/o alias -
BM25 (Robertson et al., 2009) 48.4 17.1 63.8 41.2
Google search engine3 57.9 18.9 72.0 54.2
DPR (Karpukhin et al., 2020) 67.9 17.9 67.3 58.8
GENREAD (nucleus sampling) 56.6 19.6 74.5 59.8
GENREAD (10 human prompts) 57.4 20.1 74.8 61.1
GENREAD (clustering prompts) 61.7 20.4 76.5 62.1
Table 4: Answer coverage (%) over 10 retrieved or generated docu-
ments. Case studies are provided in Tables 16-19 in Appendix.
In this section, we also
demonstrate the answer
coverage performance of
different models in Table 5.
Answer coverage measures
the percentage of the num-
ber of answers that are con-
tained in the documents
over all possible answers.
Coverage analysis showed
that generated text tends
to have lower coverage
than retrieved documents
because generated documents tends to have little diversity compared to retrieved documents. To
improve coverage, we propose GENREAD with clustering, where we include examples in the prompt
from different clusters of the training data to elicit more diverse generations.
5 E PILOGUE
CONCLUSION . In this paper, we present a novel perspective for solving knowledge-intensive tasks by
replacing the dense retrieval models with large language model generators. We call it generate-then-
read, which ﬁrst prompts a large language model to generate contextual documents, then read the
generated document to infer the ﬁnal answer. Notably, without retrieving any documents, it reaches
71.6 and 54.4 exact match scores on TriviaQA and WebQ, signiﬁcantly outperforming the current
retrieval-reader model DPR-FiD, as well as on other two knowledge-intensive tasks.
LIMITATION AND FUTURE WORK . Despite the strong performance on the presented datasets, our
approach is limited in its ability to update knowledge state and adapt to new domains. A major feature
of retrieve-then-read is the ability to swap in new documents when new information is learned, such
as temporally more recent documents, or adding in documents from a new domain to quickly adapt to
a new downstream task. Our approach relies on a large language model to contain all this knowledge
and adding new knowledge would likely require some retraining. Future work will explore how
to efﬁciently incorporate new knowledge into our generate-then-read method. Besides, generated
documents might suffer from hallucination error, resulting in incorrect predictions. We demonstrated
case study in Table 15. Consideration in combination with recent approaches (Creswell & Shanahan,
2022) to boost generative faithfulness is a also direction worthy of future research."
"1
Retrieval-Augmented Generation for Large
Language Models: A Survey
Yunfan Gaoa, Yun Xiongb, Xinyu Gao b, Kangxiang Jia b, Jinliu Pan b, Yuxi Bic, Yi Dai a, Jiawei Sun a, Meng
Wangc, and Haofen Wang a,c
aShanghai Research Institute for Intelligent Autonomous Systems, Tongji University
bShanghai Key Laboratory of Data Science, School of Computer Science, Fudan University
cCollege of Design and Innovation, Tongji University
Abstract—Large Language Models (LLMs) showcase impres-
sive capabilities but encounter challenges like hallucination,
outdated knowledge, and non-transparent, untraceable reasoning
processes. Retrieval-Augmented Generation (RAG) has emerged
as a promising solution by incorporating knowledge from external
databases. This enhances the accuracy and credibility of the
generation, particularly for knowledge-intensive tasks, and allows
for continuous knowledge updates and integration of domain-
specific information. RAG synergistically merges LLMs’ intrin-
sic knowledge with the vast, dynamic repositories of external
databases. This comprehensive review paper offers a detailed
examination of the progression of RAG paradigms, encompassing
the Naive RAG, the Advanced RAG, and the Modular RAG.
It meticulously scrutinizes the tripartite foundation of RAG
frameworks, which includes the retrieval, the generation and the
augmentation techniques. The paper highlights the state-of-the-
art technologies embedded in each of these critical components,
providing a profound understanding of the advancements in RAG
systems. Furthermore, this paper introduces up-to-date evalua-
tion framework and benchmark. At the end, this article delineates
the challenges currently faced and points out prospective avenues
for research and development 1.
Index Terms—Large language model, retrieval-augmented gen-
eration, natural language processing, information retrieval
I. I NTRODUCTION
L
ARGE language models (LLMs) have achieved remark-
able success, though they still face significant limitations,
especially in domain-specific or knowledge-intensive tasks [1],
notably producing “hallucinations” [2] when handling queries
beyond their training data or requiring current information. To
overcome challenges, Retrieval-Augmented Generation (RAG)
enhances LLMs by retrieving relevant document chunks from
external knowledge base through semantic similarity calcu-
lation. By referencing external knowledge, RAG effectively
reduces the problem of generating factually incorrect content.
Its integration into LLMs has resulted in widespread adoption,
establishing RAG as a key technology in advancing chatbots
and enhancing the suitability of LLMs for real-world applica-
tions.
RAG technology has rapidly developed in recent years, and
the technology tree summarizing related research is shown
Corresponding Author.Email:haofen.wang@tongji.edu.cn
1Resources are available at https://github.com/Tongji-KGLLM/
RAG-Survey
in Figure 1. The development trajectory of RAG in the era
of large models exhibits several distinct stage characteristics.
Initially, RAG’s inception coincided with the rise of the
Transformer architecture, focusing on enhancing language
models by incorporating additional knowledge through Pre-
Training Models (PTM). This early stage was characterized
by foundational work aimed at refining pre-training techniques
[3]–[5].The subsequent arrival of ChatGPT [6] marked a
pivotal moment, with LLM demonstrating powerful in context
learning (ICL) capabilities. RAG research shifted towards
providing better information for LLMs to answer more com-
plex and knowledge-intensive tasks during the inference stage,
leading to rapid development in RAG studies. As research
progressed, the enhancement of RAG was no longer limited
to the inference stage but began to incorporate more with LLM
fine-tuning techniques.
The burgeoning field of RAG has experienced swift growth,
yet it has not been accompanied by a systematic synthesis that
could clarify its broader trajectory. This survey endeavors to
fill this gap by mapping out the RAG process and charting
its evolution and anticipated future paths, with a focus on the
integration of RAG within LLMs. This paper considers both
technical paradigms and research methods, summarizing three
main research paradigms from over 100 RAG studies, and
analyzing key technologies in the core stages of “Retrieval,”
“Generation,” and “Augmentation.” On the other hand, current
research tends to focus more on methods, lacking analysis and
summarization of how to evaluate RAG. This paper compre-
hensively reviews the downstream tasks, datasets, benchmarks,
and evaluation methods applicable to RAG. Overall, this
paper sets out to meticulously compile and categorize the
foundational technical concepts, historical progression, and
the spectrum of RAG methodologies and applications that
have emerged post-LLMs. It is designed to equip readers and
professionals with a detailed and structured understanding of
both large models and RAG. It aims to illuminate the evolution
of retrieval augmentation techniques, assess the strengths and
weaknesses of various approaches in their respective contexts,
and speculate on upcoming trends and innovations.
Our contributions are as follows:
• In this survey, we present a thorough and systematic
review of the state-of-the-art RAG methods, delineating
its evolution through paradigms including naive RAG,
arXiv:2312.10997v5  [cs.CL]  27 Mar 20242
Fig. 1. Technology tree of RAG research. The stages of involving RAG mainly include pre-training, fine-tuning, and inference. With the emergence of LLMs,
research on RAG initially focused on leveraging the powerful in context learning abilities of LLMs, primarily concentrating on the inference stage. Subsequent
research has delved deeper, gradually integrating more with the fine-tuning of LLMs. Researchers have also been exploring ways to enhance language models
in the pre-training stage through retrieval-augmented techniques.
advanced RAG, and modular RAG. This review contex-
tualizes the broader scope of RAG research within the
landscape of LLMs.
• We identify and discuss the central technologies integral
to the RAG process, specifically focusing on the aspects
of “Retrieval”, “Generation” and “Augmentation”, and
delve into their synergies, elucidating how these com-
ponents intricately collaborate to form a cohesive and
effective RAG framework.
• We have summarized the current assessment methods of
RAG, covering 26 tasks, nearly 50 datasets, outlining
the evaluation objectives and metrics, as well as the
current evaluation benchmarks and tools. Additionally,
we anticipate future directions for RAG, emphasizing
potential enhancements to tackle current challenges.
The paper unfolds as follows: Section II introduces the
main concept and current paradigms of RAG. The following
three sections explore core components—“Retrieval”, “Gen-
eration” and “Augmentation”, respectively. Section III focuses
on optimization methods in retrieval,including indexing, query
and embedding optimization. Section IV concentrates on post-
retrieval process and LLM fine-tuning in generation. Section V
analyzes the three augmentation processes. Section VI focuses
on RAG’s downstream tasks and evaluation system. Sec-
tion VII mainly discusses the challenges that RAG currently
faces and its future development directions. At last, the paper
concludes in Section VIII.
II. O VERVIEW OF RAG
A typical application of RAG is illustrated in Figure 2.
Here, a user poses a question to ChatGPT about a recent,
widely discussed news. Given ChatGPT’s reliance on pre-
training data, it initially lacks the capacity to provide up-
dates on recent developments. RAG bridges this information
gap by sourcing and incorporating knowledge from external
databases. In this case, it gathers relevant news articles related
to the user’s query. These articles, combined with the original
question, form a comprehensive prompt that empowers LLMs
to generate a well-informed answer.
The RAG research paradigm is continuously evolving, and
we categorize it into three stages: Naive RAG, Advanced
RAG, and Modular RAG, as showed in Figure 3. Despite
RAG method are cost-effective and surpass the performance
of the native LLM, they also exhibit several limitations.
The development of Advanced RAG and Modular RAG is
a response to these specific shortcomings in Naive RAG.
A. Naive RAG
The Naive RAG research paradigm represents the earli-
est methodology, which gained prominence shortly after the3
Fig. 2. A representative instance of the RAG process applied to question answering. It mainly consists of 3 steps. 1) Indexing. Documents are split into chunks,
encoded into vectors, and stored in a vector database. 2) Retrieval. Retrieve the Top k chunks most relevant to the question based on semantic similarity. 3)
Generation. Input the original question and the retrieved chunks together into LLM to generate the final answer.
widespread adoption of ChatGPT. The Naive RAG follows
a traditional process that includes indexing, retrieval, and
generation, which is also characterized as a “Retrieve-Read”
framework [7].
Indexing starts with the cleaning and extraction of raw data
in diverse formats like PDF, HTML, Word, and Markdown,
which is then converted into a uniform plain text format. To
accommodate the context limitations of language models, text
is segmented into smaller, digestible chunks. Chunks are then
encoded into vector representations using an embedding model
and stored in vector database. This step is crucial for enabling
efficient similarity searches in the subsequent retrieval phase.
Retrieval. Upon receipt of a user query, the RAG system
employs the same encoding model utilized during the indexing
phase to transform the query into a vector representation.
It then computes the similarity scores between the query
vector and the vector of chunks within the indexed corpus.
The system prioritizes and retrieves the top K chunks that
demonstrate the greatest similarity to the query. These chunks
are subsequently used as the expanded context in prompt.
Generation. The posed query and selected documents are
synthesized into a coherent prompt to which a large language
model is tasked with formulating a response. The model’s
approach to answering may vary depending on task-specific
criteria, allowing it to either draw upon its inherent parametric
knowledge or restrict its responses to the information con-
tained within the provided documents. In cases of ongoing
dialogues, any existing conversational history can be integrated
into the prompt, enabling the model to engage in multi-turn
dialogue interactions effectively.
However, Naive RAG encounters notable drawbacks:
Retrieval Challenges . The retrieval phase often struggles
with precision and recall, leading to the selection of misaligned
or irrelevant chunks, and the missing of crucial information.
Generation Difficulties. In generating responses, the model
may face the issue of hallucination, where it produces con-
tent not supported by the retrieved context. This phase can
also suffer from irrelevance, toxicity, or bias in the outputs,
detracting from the quality and reliability of the responses.
Augmentation Hurdles . Integrating retrieved information
with the different task can be challenging, sometimes resulting
in disjointed or incoherent outputs. The process may also
encounter redundancy when similar information is retrieved
from multiple sources, leading to repetitive responses. Deter-
mining the significance and relevance of various passages and
ensuring stylistic and tonal consistency add further complexity.
Facing complex issues, a single retrieval based on the original
query may not suffice to acquire adequate context information.
Moreover, there’s a concern that generation models might
overly rely on augmented information, leading to outputs that
simply echo retrieved content without adding insightful or
synthesized information.
B. Advanced RAG
Advanced RAG introduces specific improvements to over-
come the limitations of Naive RAG. Focusing on enhancing re-
trieval quality, it employs pre-retrieval and post-retrieval strate-
gies. To tackle the indexing issues, Advanced RAG refines
its indexing techniques through the use of a sliding window
approach, fine-grained segmentation, and the incorporation of
metadata. Additionally, it incorporates several optimization
methods to streamline the retrieval process [8].4
Fig. 3. Comparison between the three paradigms of RAG. (Left) Naive RAG mainly consists of three parts: indexing, retrieval and generation. (Middle)
Advanced RAG proposes multiple optimization strategies around pre-retrieval and post-retrieval, with a process similar to the Naive RAG, still following a
chain-like structure. (Right) Modular RAG inherits and develops from the previous paradigm, showcasing greater flexibility overall. This is evident in the
introduction of multiple specific functional modules and the replacement of existing modules. The overall process is not limited to sequential retrieval and
generation; it includes methods such as iterative and adaptive retrieval.
Pre-retrieval process. In this stage, the primary focus is
on optimizing the indexing structure and the original query.
The goal of optimizing indexing is to enhance the quality of
the content being indexed. This involves strategies: enhancing
data granularity, optimizing index structures, adding metadata,
alignment optimization, and mixed retrieval. While the goal
of query optimization is to make the user’s original question
clearer and more suitable for the retrieval task. Common
methods include query rewriting query transformation, query
expansion and other techniques [7], [9]–[11].
Post-Retrieval Process. Once relevant context is retrieved,
it’s crucial to integrate it effectively with the query. The main
methods in post-retrieval process include rerank chunks and
context compressing. Re-ranking the retrieved information to
relocate the most relevant content to the edges of the prompt is
a key strategy. This concept has been implemented in frame-
works such as LlamaIndex 2, LangChain3, and HayStack [12].
Feeding all relevant documents directly into LLMs can lead
to information overload, diluting the focus on key details with
irrelevant content.To mitigate this, post-retrieval efforts con-
centrate on selecting the essential information, emphasizing
critical sections, and shortening the context to be processed.
2https://www.llamaindex.ai
3https://www.langchain.com/
C. Modular RAG
The modular RAG architecture advances beyond the for-
mer two RAG paradigms, offering enhanced adaptability and
versatility. It incorporates diverse strategies for improving its
components, such as adding a search module for similarity
searches and refining the retriever through fine-tuning. Inno-
vations like restructured RAG modules [13] and rearranged
RAG pipelines [14] have been introduced to tackle specific
challenges. The shift towards a modular RAG approach is
becoming prevalent, supporting both sequential processing and
integrated end-to-end training across its components. Despite
its distinctiveness, Modular RAG builds upon the foundational
principles of Advanced and Naive RAG, illustrating a progres-
sion and refinement within the RAG family.
1) New Modules: The Modular RAG framework introduces
additional specialized components to enhance retrieval and
processing capabilities. The Search module adapts to spe-
cific scenarios, enabling direct searches across various data
sources like search engines, databases, and knowledge graphs,
using LLM-generated code and query languages [15]. RAG-
Fusion addresses traditional search limitations by employing
a multi-query strategy that expands user queries into diverse
perspectives, utilizing parallel vector searches and intelligent
re-ranking to uncover both explicit and transformative knowl-
edge [16]. The Memory module leverages the LLM’s memory
to guide retrieval, creating an unbounded memory pool that5
aligns the text more closely with data distribution through iter-
ative self-enhancement [17], [18]. Routing in the RAG system
navigates through diverse data sources, selecting the optimal
pathway for a query, whether it involves summarization,
specific database searches, or merging different information
streams [19]. The Predict module aims to reduce redundancy
and noise by generating context directly through the LLM,
ensuring relevance and accuracy [13]. Lastly, the Task Adapter
module tailors RAG to various downstream tasks, automating
prompt retrieval for zero-shot inputs and creating task-specific
retrievers through few-shot query generation [20], [21] .This
comprehensive approach not only streamlines the retrieval pro-
cess but also significantly improves the quality and relevance
of the information retrieved, catering to a wide array of tasks
and queries with enhanced precision and flexibility.
2) New Patterns: Modular RAG offers remarkable adapt-
ability by allowing module substitution or reconfiguration
to address specific challenges. This goes beyond the fixed
structures of Naive and Advanced RAG, characterized by a
simple “Retrieve” and “Read” mechanism. Moreover, Modular
RAG expands this flexibility by integrating new modules or
adjusting interaction flow among existing ones, enhancing its
applicability across different tasks.
Innovations such as the Rewrite-Retrieve-Read [7]model
leverage the LLM’s capabilities to refine retrieval queries
through a rewriting module and a LM-feedback mechanism
to update rewriting model., improving task performance.
Similarly, approaches like Generate-Read [13] replace tradi-
tional retrieval with LLM-generated content, while Recite-
Read [22] emphasizes retrieval from model weights, enhanc-
ing the model’s ability to handle knowledge-intensive tasks.
Hybrid retrieval strategies integrate keyword, semantic, and
vector searches to cater to diverse queries. Additionally, em-
ploying sub-queries and hypothetical document embeddings
(HyDE) [11] seeks to improve retrieval relevance by focusing
on embedding similarities between generated answers and real
documents.
Adjustments in module arrangement and interaction, such
as the Demonstrate-Search-Predict (DSP) [23] framework
and the iterative Retrieve-Read-Retrieve-Read flow of ITER-
RETGEN [14], showcase the dynamic use of module out-
puts to bolster another module’s functionality, illustrating a
sophisticated understanding of enhancing module synergy.
The flexible orchestration of Modular RAG Flow showcases
the benefits of adaptive retrieval through techniques such as
FLARE [24] and Self-RAG [25]. This approach transcends
the fixed RAG retrieval process by evaluating the necessity
of retrieval based on different scenarios. Another benefit of
a flexible architecture is that the RAG system can more
easily integrate with other technologies (such as fine-tuning
or reinforcement learning) [26]. For example, this can involve
fine-tuning the retriever for better retrieval results, fine-tuning
the generator for more personalized outputs, or engaging in
collaborative fine-tuning [27].
D. RAG vs Fine-tuning
The augmentation of LLMs has attracted considerable atten-
tion due to their growing prevalence. Among the optimization
methods for LLMs, RAG is often compared with Fine-tuning
(FT) and prompt engineering. Each method has distinct charac-
teristics as illustrated in Figure 4. We used a quadrant chart to
illustrate the differences among three methods in two dimen-
sions: external knowledge requirements and model adaption
requirements. Prompt engineering leverages a model’s inherent
capabilities with minimum necessity for external knowledge
and model adaption. RAG can be likened to providing a model
with a tailored textbook for information retrieval, ideal for pre-
cise information retrieval tasks. In contrast, FT is comparable
to a student internalizing knowledge over time, suitable for
scenarios requiring replication of specific structures, styles, or
formats.
RAG excels in dynamic environments by offering real-
time knowledge updates and effective utilization of external
knowledge sources with high interpretability. However, it
comes with higher latency and ethical considerations regarding
data retrieval. On the other hand, FT is more static, requiring
retraining for updates but enabling deep customization of the
model’s behavior and style. It demands significant compu-
tational resources for dataset preparation and training, and
while it can reduce hallucinations, it may face challenges with
unfamiliar data.
In multiple evaluations of their performance on various
knowledge-intensive tasks across different topics, [28] re-
vealed that while unsupervised fine-tuning shows some im-
provement, RAG consistently outperforms it, for both exist-
ing knowledge encountered during training and entirely new
knowledge. Additionally, it was found that LLMs struggle
to learn new factual information through unsupervised fine-
tuning. The choice between RAG and FT depends on the
specific needs for data dynamics, customization, and com-
putational capabilities in the application context. RAG and
FT are not mutually exclusive and can complement each
other, enhancing a model’s capabilities at different levels.
In some instances, their combined use may lead to optimal
performance. The optimization process involving RAG and FT
may require multiple iterations to achieve satisfactory results.
III. R ETRIEVAL
In the context of RAG, it is crucial to efficiently retrieve
relevant documents from the data source. There are several
key issues involved, such as the retrieval source, retrieval
granularity, pre-processing of the retrieval, and selection of
the corresponding embedding model.
A. Retrieval Source
RAG relies on external knowledge to enhance LLMs, while
the type of retrieval source and the granularity of retrieval
units both affect the final generation results.
1) Data Structure: Initially, text is s the mainstream source
of retrieval. Subsequently, the retrieval source expanded to in-
clude semi-structured data (PDF) and structured data (Knowl-
edge Graph, KG) for enhancement. In addition to retrieving
from original external sources, there is also a growing trend in
recent researches towards utilizing content generated by LLMs
themselves for retrieval and enhancement purposes.6
TABLE I
SUMMARY OF RAG METHODS
Method Retrieval Source Retrieval
Data Type
Retrieval
Granularity
Augmentation
Stage
Retrieval
process
CoG [29] Wikipedia Text Phrase Pre-training Iterative
DenseX [30] FactoidWiki Text Proposition Inference Once
EAR [31] Dataset-base Text Sentence Tuning Once
UPRISE [20] Dataset-base Text Sentence Tuning Once
RAST [32] Dataset-base Text Sentence Tuning Once
Self-Mem [17] Dataset-base Text Sentence Tuning Iterative
FLARE [24] Search Engine,Wikipedia Text Sentence Tuning Adaptive
PGRA [33] Wikipedia Text Sentence Inference Once
FILCO [34] Wikipedia Text Sentence Inference Once
RADA [35] Dataset-base Text Sentence Inference Once
Filter-rerank [36] Synthesized dataset Text Sentence Inference Once
R-GQA [37] Dataset-base Text Sentence Pair Tuning Once
LLM-R [38] Dataset-base Text Sentence Pair Inference Iterative
TIGER [39] Dataset-base Text Item-base Pre-training Once
LM-Indexer [40] Dataset-base Text Item-base Tuning Once
BEQUE [9] Dataset-base Text Item-base Tuning Once
CT-RAG [41] Synthesized dataset Text Item-base Tuning Once
Atlas [42] Wikipedia, Common Crawl Text Chunk Pre-training Iterative
RA VEN [43] Wikipedia Text Chunk Pre-training Once
RETRO++ [44] Pre-training Corpus Text Chunk Pre-training Iterative
INSTRUCTRETRO [45] Pre-training corpus Text Chunk Pre-training Iterative
RRR [7] Search Engine Text Chunk Tuning Once
RA-e2e [46] Dataset-base Text Chunk Tuning Once
PROMPTAGATOR [21] BEIR Text Chunk Tuning Once
AAR [47] MSMARCO,Wikipedia Text Chunk Tuning Once
RA-DIT [27] Common Crawl,Wikipedia Text Chunk Tuning Once
RAG-Robust [48] Wikipedia Text Chunk Tuning Once
RA-Long-Form [49] Dataset-base Text Chunk Tuning Once
CoN [50] Wikipedia Text Chunk Tuning Once
Self-RAG [25] Wikipedia Text Chunk Tuning Adaptive
BGM [26] Wikipedia Text Chunk Inference Once
CoQ [51] Wikipedia Text Chunk Inference Iterative
Token-Elimination [52] Wikipedia Text Chunk Inference Once
PaperQA [53] Arxiv,Online Database,PubMed Text Chunk Inference Iterative
NoiseRAG [54] FactoidWiki Text Chunk Inference Once
IAG [55] Search Engine,Wikipedia Text Chunk Inference Once
NoMIRACL [56] Wikipedia Text Chunk Inference Once
ToC [57] Search Engine,Wikipedia Text Chunk Inference Recursive
SKR [58] Dataset-base,Wikipedia Text Chunk Inference Adaptive
ITRG [59] Wikipedia Text Chunk Inference Iterative
RAG-LongContext [60] Dataset-base Text Chunk Inference Once
ITER-RETGEN [14] Wikipedia Text Chunk Inference Iterative
IRCoT [61] Wikipedia Text Chunk Inference Recursive
LLM-Knowledge-Boundary [62] Wikipedia Text Chunk Inference Once
RAPTOR [63] Dataset-base Text Chunk Inference Recursive
RECITE [22] LLMs Text Chunk Inference Once
ICRALM [64] Pile,Wikipedia Text Chunk Inference Iterative
Retrieve-and-Sample [65] Dataset-base Text Doc Tuning Once
Zemi [66] C4 Text Doc Tuning Once
CRAG [67] Arxiv Text Doc Inference Once
1-PAGER [68] Wikipedia Text Doc Inference Iterative
PRCA [69] Dataset-base Text Doc Inference Once
QLM-Doc-ranking [70] Dataset-base Text Doc Inference Once
Recomp [71] Wikipedia Text Doc Inference Once
DSP [23] Wikipedia Text Doc Inference Iterative
RePLUG [72] Pile Text Doc Inference Once
ARM-RAG [73] Dataset-base Text Doc Inference Iterative
GenRead [13] LLMs Text Doc Inference Iterative
UniMS-RAG [74] Dataset-base Text Multi Tuning Once
CREA-ICL [19] Dataset-base Crosslingual,Text Sentence Inference Once
PKG [75] LLM Tabular,Text Chunk Inference Once
SANTA [76] Dataset-base Code,Text Item Pre-training Once
SURGE [77] Freebase KG Sub-Graph Tuning Once
MK-ToD [78] Dataset-base KG Entity Tuning Once
Dual-Feedback-ToD [79] Dataset-base KG Entity Sequence Tuning Once
KnowledGPT [15] Dataset-base KG Triplet Inference Muti-time
FABULA [80] Dataset-base,Graph KG Entity Inference Once
HyKGE [81] CMeKG KG Entity Inference Once
KALMV [82] Wikipedia KG Triplet Inference Iterative
RoG [83] Freebase KG Triplet Inference Iterative
G-Retriever [84] Dataset-base TextGraph Sub-Graph Inference Once7
Fig. 4. RAG compared with other model optimization methods in the aspects of “External Knowledge Required” and “Model Adaption Required”. Prompt
Engineering requires low modifications to the model and external knowledge, focusing on harnessing the capabilities of LLMs themselves. Fine-tuning, on
the other hand, involves further training the model. In the early stages of RAG (Naive RAG), there is a low demand for model modifications. As research
progresses, Modular RAG has become more integrated with fine-tuning techniques.
Unstructured Data , such as text, is the most widely used
retrieval source, which are mainly gathered from corpus. For
open-domain question-answering (ODQA) tasks, the primary
retrieval sources are Wikipedia Dump with the current major
versions including HotpotQA 4 (1st October , 2017), DPR5 (20
December, 2018). In addition to encyclopedic data, common
unstructured data includes cross-lingual text [19] and domain-
specific data (such as medical [67]and legal domains [29]).
Semi-structured data. typically refers to data that contains a
combination of text and table information, such as PDF. Han-
dling semi-structured data poses challenges for conventional
RAG systems due to two main reasons. Firstly, text splitting
processes may inadvertently separate tables, leading to data
corruption during retrieval. Secondly, incorporating tables into
the data can complicate semantic similarity searches. When
dealing with semi-structured data, one approach involves lever-
aging the code capabilities of LLMs to execute Text-2-SQL
queries on tables within databases, such as TableGPT [85].
Alternatively, tables can be transformed into text format for
further analysis using text-based methods [75]. However, both
of these methods are not optimal solutions, indicating substan-
tial research opportunities in this area.
Structured data , such as knowledge graphs (KGs) [86] ,
which are typically verified and can provide more precise in-
formation. KnowledGPT [15] generates KB search queries and
stores knowledge in a personalized base, enhancing the RAG
model’s knowledge richness. In response to the limitations of
LLMs in understanding and answering questions about textual
graphs, G-Retriever [84] integrates Graph Neural Networks
4https://hotpotqa.github.io/wiki-readme.html
5https://github.com/facebookresearch/DPR
(GNNs), LLMs and RAG, enhancing graph comprehension
and question-answering capabilities through soft prompting
of the LLM, and employs the Prize-Collecting Steiner Tree
(PCST) optimization problem for targeted graph retrieval. On
the contrary, it requires additional effort to build, validate,
and maintain structured databases. On the contrary, it requires
additional effort to build, validate, and maintain structured
databases.
LLMs-Generated Content. Addressing the limitations of
external auxiliary information in RAG, some research has
focused on exploiting LLMs’ internal knowledge. SKR [58]
classifies questions as known or unknown, applying retrieval
enhancement selectively. GenRead [13] replaces the retriever
with an LLM generator, finding that LLM-generated contexts
often contain more accurate answers due to better alignment
with the pre-training objectives of causal language modeling.
Selfmem [17] iteratively creates an unbounded memory pool
with a retrieval-enhanced generator, using a memory selec-
tor to choose outputs that serve as dual problems to the
original question, thus self-enhancing the generative model.
These methodologies underscore the breadth of innovative
data source utilization in RAG, striving to improve model
performance and task effectiveness.
2) Retrieval Granularity: Another important factor besides
the data format of the retrieval source is the granularity of
the retrieved data. Coarse-grained retrieval units theoretically
can provide more relevant information for the problem, but
they may also contain redundant content, which could distract
the retriever and language models in downstream tasks [50],
[87]. On the other hand, fine-grained retrieval unit granularity
increases the burden of retrieval and does not guarantee seman-
tic integrity and meeting the required knowledge. Choosing8
the appropriate retrieval granularity during inference can be
a simple and effective strategy to improve the retrieval and
downstream task performance of dense retrievers.
In text, retrieval granularity ranges from fine to coarse,
including Token, Phrase, Sentence, Proposition, Chunks, Doc-
ument. Among them, DenseX [30]proposed the concept of
using propositions as retrieval units. Propositions are defined
as atomic expressions in the text, each encapsulating a unique
factual segment and presented in a concise, self-contained nat-
ural language format. This approach aims to enhance retrieval
precision and relevance. On the Knowledge Graph (KG),
retrieval granularity includes Entity, Triplet, and sub-Graph.
The granularity of retrieval can also be adapted to downstream
tasks, such as retrieving Item IDs [40]in recommendation tasks
and Sentence pairs [38]. Detailed information is illustrated in
Table I.
B. Indexing Optimization
In the Indexing phase, documents will be processed, seg-
mented, and transformed into Embeddings to be stored in a
vector database. The quality of index construction determines
whether the correct context can be obtained in the retrieval
phase.
1) Chunking Strategy: The most common method is to split
the document into chunks on a fixed number of tokens (e.g.,
100, 256, 512) [88]. Larger chunks can capture more context,
but they also generate more noise, requiring longer processing
time and higher costs. While smaller chunks may not fully
convey the necessary context, they do have less noise. How-
ever, chunks leads to truncation within sentences, prompting
the optimization of a recursive splits and sliding window meth-
ods, enabling layered retrieval by merging globally related
information across multiple retrieval processes [89]. Never-
theless, these approaches still cannot strike a balance between
semantic completeness and context length. Therefore, methods
like Small2Big have been proposed, where sentences (small)
are used as the retrieval unit, and the preceding and following
sentences are provided as (big) context to LLMs [90].
2) Metadata Attachments: Chunks can be enriched with
metadata information such as page number, file name, au-
thor,category timestamp. Subsequently, retrieval can be filtered
based on this metadata, limiting the scope of the retrieval.
Assigning different weights to document timestamps during
retrieval can achieve time-aware RAG, ensuring the freshness
of knowledge and avoiding outdated information.
In addition to extracting metadata from the original doc-
uments, metadata can also be artificially constructed. For
example, adding summaries of paragraph, as well as intro-
ducing hypothetical questions. This method is also known as
Reverse HyDE. Specifically, using LLM to generate questions
that can be answered by the document, then calculating the
similarity between the original question and the hypothetical
question during retrieval to reduce the semantic gap between
the question and the answer.
3) Structural Index: One effective method for enhancing
information retrieval is to establish a hierarchical structure for
the documents. By constructing In structure, RAG system can
expedite the retrieval and processing of pertinent data.
Hierarchical index structure . File are arranged in parent-
child relationships, with chunks linked to them. Data sum-
maries are stored at each node, aiding in the swift traversal
of data and assisting the RAG system in determining which
chunks to extract. This approach can also mitigate the illusion
caused by block extraction issues.
Knowledge Graph index . Utilize KG in constructing the
hierarchical structure of documents contributes to maintaining
consistency. It delineates the connections between different
concepts and entities, markedly reducing the potential for
illusions. Another advantage is the transformation of the
information retrieval process into instructions that LLM can
comprehend, thereby enhancing the accuracy of knowledge
retrieval and enabling LLM to generate contextually coherent
responses, thus improving the overall efficiency of the RAG
system. To capture the logical relationship between document
content and structure, KGP [91] proposed a method of building
an index between multiple documents using KG. This KG
consists of nodes (representing paragraphs or structures in the
documents, such as pages and tables) and edges (indicating
semantic/lexical similarity between paragraphs or relationships
within the document structure), effectively addressing knowl-
edge retrieval and reasoning problems in a multi-document
environment.
C. Query Optimization
One of the primary challenges with Naive RAG is its
direct reliance on the user’s original query as the basis for
retrieval. Formulating a precise and clear question is difficult,
and imprudent queries result in subpar retrieval effectiveness.
Sometimes, the question itself is complex, and the language
is not well-organized. Another difficulty lies in language
complexity ambiguity. Language models often struggle when
dealing with specialized vocabulary or ambiguous abbrevi-
ations with multiple meanings. For instance, they may not
discern whether “LLM” refers to large language model or a
Master of Laws in a legal context.
1) Query Expansion: Expanding a single query into mul-
tiple queries enriches the content of the query, providing
further context to address any lack of specific nuances, thereby
ensuring the optimal relevance of the generated answers.
Multi-Query. By employing prompt engineering to expand
queries via LLMs, these queries can then be executed in
parallel. The expansion of queries is not random, but rather
meticulously designed.
Sub-Query. The process of sub-question planning represents
the generation of the necessary sub-questions to contextualize
and fully answer the original question when combined. This
process of adding relevant context is, in principle, similar
to query expansion. Specifically, a complex question can be
decomposed into a series of simpler sub-questions using the
least-to-most prompting method [92].
Chain-of-Verification(CoVe). The expanded queries undergo
validation by LLM to achieve the effect of reducing halluci-
nations. Validated expanded queries typically exhibit higher
reliability [93].9
2) Query Transformation: The core concept is to retrieve
chunks based on a transformed query instead of the user’s
original query.
Query Rewrite.The original queries are not always optimal
for LLM retrieval, especially in real-world scenarios. There-
fore, we can prompt LLM to rewrite the queries. In addition to
using LLM for query rewriting, specialized smaller language
models, such as RRR (Rewrite-retrieve-read) [7]. The imple-
mentation of the query rewrite method in the Taobao, known
as BEQUE [9] has notably enhanced recall effectiveness for
long-tail queries, resulting in a rise in GMV .
Another query transformation method is to use prompt
engineering to let LLM generate a query based on the original
query for subsequent retrieval. HyDE [11] construct hypothet-
ical documents (assumed answers to the original query). It
focuses on embedding similarity from answer to answer rather
than seeking embedding similarity for the problem or query.
Using the Step-back Prompting method [10], the original
query is abstracted to generate a high-level concept question
(step-back question). In the RAG system, both the step-back
question and the original query are used for retrieval, and both
the results are utilized as the basis for language model answer
generation.
3) Query Routing: Based on varying queries, routing to
distinct RAG pipeline,which is suitable for a versatile RAG
system designed to accommodate diverse scenarios.
Metadata Router/ Filter . The first step involves extracting
keywords (entity) from the query, followed by filtering based
on the keywords and metadata within the chunks to narrow
down the search scope.
Semantic Router is another method of routing involves
leveraging the semantic information of the query. Specific
apprach see Semantic Router 6. Certainly, a hybrid routing
approach can also be employed, combining both semantic and
metadata-based methods for enhanced query routing.
D. Embedding
In RAG, retrieval is achieved by calculating the similarity
(e.g. cosine similarity) between the embeddings of the ques-
tion and document chunks, where the semantic representation
capability of embedding models plays a key role. This mainly
includes a sparse encoder (BM25) and a dense retriever (BERT
architecture Pre-training language models). Recent research
has introduced prominent embedding models such as AngIE,
V oyage, BGE,etc [94]–[96], which are benefit from multi-task
instruct tuning. Hugging Face’s MTEB leaderboard 7 evaluates
embedding models across 8 tasks, covering 58 datasests. Ad-
ditionally, C-MTEB focuses on Chinese capability, covering
6 tasks and 35 datasets. There is no one-size-fits-all answer
to “which embedding model to use.” However, some specific
models are better suited for particular use cases.
1) Mix/hybrid Retrieval : Sparse and dense embedding
approaches capture different relevance features and can ben-
efit from each other by leveraging complementary relevance
information. For instance, sparse retrieval models can be used
6https://github.com/aurelio-labs/semantic-router
7https://huggingface.co/spaces/mteb/leaderboard
to provide initial search results for training dense retrieval
models. Additionally, pre-training language models (PLMs)
can be utilized to learn term weights to enhance sparse
retrieval. Specifically, it also demonstrates that sparse retrieval
models can enhance the zero-shot retrieval capability of dense
retrieval models and assist dense retrievers in handling queries
containing rare entities, thereby improving robustness.
2) Fine-tuning Embedding Model: In instances where the
context significantly deviates from pre-training corpus, partic-
ularly within highly specialized disciplines such as healthcare,
legal practice, and other sectors replete with proprietary jargon,
fine-tuning the embedding model on your own domain dataset
becomes essential to mitigate such discrepancies.
In addition to supplementing domain knowledge, another
purpose of fine-tuning is to align the retriever and generator,
for example, using the results of LLM as the supervision signal
for fine-tuning, known as LSR (LM-supervised Retriever).
PROMPTAGATOR [21] utilizes the LLM as a few-shot query
generator to create task-specific retrievers, addressing chal-
lenges in supervised fine-tuning, particularly in data-scarce
domains. Another approach, LLM-Embedder [97], exploits
LLMs to generate reward signals across multiple downstream
tasks. The retriever is fine-tuned with two types of supervised
signals: hard labels for the dataset and soft rewards from
the LLMs. This dual-signal approach fosters a more effective
fine-tuning process, tailoring the embedding model to diverse
downstream applications. REPLUG [72] utilizes a retriever
and an LLM to calculate the probability distributions of the
retrieved documents and then performs supervised training
by computing the KL divergence. This straightforward and
effective training method enhances the performance of the
retrieval model by using an LM as the supervisory signal,
eliminating the need for specific cross-attention mechanisms.
Moreover, inspired by RLHF (Reinforcement Learning from
Human Feedback), utilizing LM-based feedback to reinforce
the retriever through reinforcement learning.
E. Adapter
Fine-tuning models may present challenges, such as in-
tegrating functionality through an API or addressing con-
straints arising from limited local computational resources.
Consequently, some approaches opt to incorporate an external
adapter to aid in alignment.
To optimize the multi-task capabilities of LLM, UP-
RISE [20] trained a lightweight prompt retriever that can
automatically retrieve prompts from a pre-built prompt pool
that are suitable for a given zero-shot task input. AAR
(Augmentation-Adapted Retriver) [47] introduces a universal
adapter designed to accommodate multiple downstream tasks.
While PRCA [69] add a pluggable reward-driven contextual
adapter to enhance performance on specific tasks. BGM [26]
keeps the retriever and LLM fixed,and trains a bridge Seq2Seq
model in between. The bridge model aims to transform the
retrieved information into a format that LLMs can work with
effectively, allowing it to not only rerank but also dynami-
cally select passages for each query, and potentially employ
more advanced strategies like repetition. Furthermore, PKG10
introduces an innovative method for integrating knowledge
into white-box models via directive fine-tuning [75]. In this
approach, the retriever module is directly substituted to gen-
erate relevant documents according to a query. This method
assists in addressing the difficulties encountered during the
fine-tuning process and enhances model performance.
IV. G ENERATION
After retrieval, it is not a good practice to directly input all
the retrieved information to the LLM for answering questions.
Following will introduce adjustments from two perspectives:
adjusting the retrieved content and adjusting the LLM.
A. Context Curation
Redundant information can interfere with the final gener-
ation of LLM, and overly long contexts can also lead LLM
to the “Lost in the middle” problem [98]. Like humans, LLM
tends to only focus on the beginning and end of long texts,
while forgetting the middle portion. Therefore, in the RAG
system, we typically need to further process the retrieved
content.
1) Reranking: Reranking fundamentally reorders document
chunks to highlight the most pertinent results first, effectively
reducing the overall document pool, severing a dual purpose
in information retrieval, acting as both an enhancer and a
filter, delivering refined inputs for more precise language
model processing [70]. Reranking can be performed using
rule-based methods that depend on predefined metrics like
Diversity, Relevance, and MRR, or model-based approaches
like Encoder-Decoder models from the BERT series (e.g.,
SpanBERT), specialized reranking models such as Cohere
rerank or bge-raranker-large, and general large language mod-
els like GPT [12], [99].
2) Context Selection/Compression: A common misconcep-
tion in the RAG process is the belief that retrieving as many
relevant documents as possible and concatenating them to form
a lengthy retrieval prompt is beneficial. However, excessive
context can introduce more noise, diminishing the LLM’s
perception of key information .
(Long) LLMLingua [100], [101] utilize small language
models (SLMs) such as GPT-2 Small or LLaMA-7B, to
detect and remove unimportant tokens, transforming it into
a form that is challenging for humans to comprehend but
well understood by LLMs. This approach presents a direct
and practical method for prompt compression, eliminating the
need for additional training of LLMs while balancing language
integrity and compression ratio. PRCA tackled this issue by
training an information extractor [69]. Similarly, RECOMP
adopts a comparable approach by training an information
condenser using contrastive learning [71]. Each training data
point consists of one positive sample and five negative sam-
ples, and the encoder undergoes training using contrastive loss
throughout this process [102] .
In addition to compressing the context, reducing the num-
ber of documents aslo helps improve the accuracy of the
model’s answers. Ma et al. [103] propose the “Filter-Reranker”
paradigm, which combines the strengths of LLMs and SLMs.
In this paradigm, SLMs serve as filters, while LLMs function
as reordering agents. The research shows that instructing
LLMs to rearrange challenging samples identified by SLMs
leads to significant improvements in various Information
Extraction (IE) tasks. Another straightforward and effective
approach involves having the LLM evaluate the retrieved
content before generating the final answer. This allows the
LLM to filter out documents with poor relevance through LLM
critique. For instance, in Chatlaw [104], the LLM is prompted
to self-suggestion on the referenced legal provisions to assess
their relevance.
B. LLM Fine-tuning
Targeted fine-tuning based on the scenario and data char-
acteristics on LLMs can yield better results. This is also one
of the greatest advantages of using on-premise LLMs. When
LLMs lack data in a specific domain, additional knowledge can
be provided to the LLM through fine-tuning. Huggingface’s
fine-tuning data can also be used as an initial step.
Another benefit of fine-tuning is the ability to adjust the
model’s input and output. For example, it can enable LLM to
adapt to specific data formats and generate responses in a par-
ticular style as instructed [37]. For retrieval tasks that engage
with structured data, the SANTA framework [76] implements
a tripartite training regimen to effectively encapsulate both
structural and semantic nuances. The initial phase focuses on
the retriever, where contrastive learning is harnessed to refine
the query and document embeddings.
Aligning LLM outputs with human or retriever p"
"Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, pages 2655 - 2671
July 10-15, 2022 ©2022 Association for Computational Linguistics
Learning To Retrieve Prompts for In-Context Learning
Ohad Rubin Jonathan Herzig Jonathan Berant
The Blavatnik School of Computer Science, Tel Aviv University
{ohad.rubin,jonathan.herzig,joberant}@cs.tau.ac.il
Abstract
In-context learning is a recent paradigm in nat-
ural language understanding, where a large pre-
trained language model (LM) observes a test in-
stance and a few training examples as its input,
and directly decodes the output without any up-
date to its parameters. However, performance
has been shown to strongly depend on the se-
lected training examples (termed prompts). In
this work, we propose an efficient method for
retrieving prompts for in-context learning us-
ing annotated data and an LM. Given an input-
output pair, we estimate the probability of the
output given the input and a candidate train-
ing example as the prompt, and label training
examples as positive or negative based on this
probability. We then train an efficient dense
retriever from this data, which is used to re-
trieve training examples as prompts at test time.
We evaluate our approach on three sequence-to-
sequence tasks where language utterances are
mapped to meaning representations, and find
that it substantially outperforms prior work and
multiple baselines across the board.
1 Introduction
The striking language skills and world knowledge
embedded in large pre-trained language models
(LMs) (Devlin et al., 2019; Petroni et al., 2019; Raf-
fel et al., 2020; Brown et al., 2020) have recently
led to in-context learning, a new paradigm in natu-
ral language understanding. Under this paradigm,
a language model is given a prompt, which typi-
cally contains a few training examples, as well as a
test instance as input, and generates the output for
the test instance directly, without any update to its
parameters. This approach was first introduced in
GPT-3 (Brown et al., 2020), but has quickly spread
to other LMs (Lieber et al., 2021; Du et al., 2021;
Rae et al., 2021).
An attractive property of in-context learning is
that it provides a single model for multiple lan-
guage understanding tasks. However, Liu et al.
Retriever
Retriever Index
What is the longest river in 
the smallest state in the usa?
1) states
2) size of #1
3) #1 where #2 is the lowest
4) rivers of #3
5) how long are #4
6) #4 where #5 is the highest
Which states border the 
shortest river in the usa?
1) the usa
2) rivers of #1
3) how long are #2
4) #2 where #3 is the lowest
5) border states of #4
Which states border the
 longest river in the usa?
1) the usa
2) rivers of #1
3) how long are #2
4) #2 where #3 is the highest
5) border states of #4
1) rivers
2) #1 in the usa 
3) lengths of #2
4) #2 where #3 is longest
5) length of #4
Inference LM
What is the length of the 
longest river in the usa?
Similar examples
Question
Figure 1: An overview of prompt retrieval: Given a
question from BREAK , one retrieves similar training
examples from an index of the training set. The question
and training examples (the prompt) are passed to an
inference LM that decodes the output.
(2021a) showed that downstream performance can
vary widely depending on the choice of in-context
examples. This has sparked interest in prompt re-
trieval (see Fig. 1), where given a test instance,
training examples are chosen for the prompt based
on some similarity metric. Recent work has either
used off-the-shelf unsupervised similarity metrics,
or trained a prompt retriever to select examples
based on surface similarity (Das et al., 2021).
In this work, we suggest to use language mod-
els themselves to label examples that can serve as
good prompts, and train a prompt retriever from
this signal. To train the retriever (see Fig. 2), we
assume access to a training set of input-output pairs
and to a scoring LM, i.e., a language model that
will be used to score prompts. For each training
example (x, y), we go over other candidate train-
ing examples, and estimate the probability, accord-
ing to the scoring LM, of y conditioned on x and
the candidate prompt. We label training examples
that lead to high probability as positive and low
probability as negative and train a prompt retriever
2655D
Figure 2: An overview of our approach for training EPR. Given a training example, we use an unsupervised retriever
Ru to obtain a set of candidates. We then pass the candidates to a scoring LM and label the top-k and the bottom-k
as positive and negative examples, respectively. Last, we use this training data to train a dense retriever.
from this data using contrastive learning. We ar-
gue that using an LM for labeling examples is a
better proxy for training a retriever compared to
previously-proposed surface similarity heuristics.
Importantly, when creating the training data, we
have access to the gold label y, which can be used
to obtain a high-quality set of candidate prompts.
This leads to good positive examples and hard neg-
ative examples, which are beneficial for training
with a contrastive objective.
Using a scoring LM to train an efficient retriever
for a potentially different test time inference LM is
beneficial in two scenarios. First, when the scoring
LM is smaller than the inference LM and serves as
a proxy for it. This results in cheap and efficient
data generation for the retriever, accessible to a
wide range of researchers. Second, our approach
can be used even when the scoring and inference
LMs are identical (e.g., both are GPT-3). This is
beneficial when we do not have access to model
parameters and can only use it as a service, an
increasingly popular paradigm. In this case, we use
the LM to train a light-weight retriever that is only
tasked with learning a similarity function. More
generally, given that the scale of LMs is likely to
keep increasing in the foreseeable future, one can
view our approach for Efficient Prompt Retrieval,
or EPR, as a method for interfacing and learning to
interact with large LMs.
We empirically test EPR on three structured
sequence-to-sequence tasks, where input natural
language utterances are mapped to a meaning rep-
resentation: MTOP (Li et al., 2021) and SM-
CALFLOW(Andreas et al., 2020), which focus on
task-oriented dialogue, and BREAK (Wolfson et al.,
2020), a benchmark for mapping questions to a
language-based meaning representation. We ob-
serve that EPR substantially improves performance
compared to prior work on prompt retrieval. When
the scoring LM and inference LM are identical
(using GPT-N EO (Black et al., 2021)), perfor-
mance compared to the best baseline improves
from 26% to 31.9% on BREAK , from 57% to
64.2% on MTOP, and from 51.4% to 54.3% on
SMC ALFLOW. When using GPT-N EO as a proxy
for larger LMs (GPT-J, GPT-3, and CODEX ), we
observe similar gains, where performance improves
substantially in all cases.
To conclude, we propose an approach for retriev-
ing training examples for in-context learning in
large language models, and show it substantially
outperforms prior methods. Given recent develop-
ments in scaling LMs, designing efficient methods
for interacting with LMs is an important direction
for future research.
2 Background: Prompt Retrieval
Problem setup Given a training set D =
{(xi, yi)}n
i=1 of input-output sequences, and a
test example xtest, our goal is to train a retriever,
R(xtest, D), that will retrieve a subset of training
examples P= {(xj, yj)}m
j=1 ⊂D, where m ≪n.
We succinctly refer to Pas the prompt.1
Given an inference LM,g, a good prompt should
lead to the target output sequence when the test
example xtest is concatenated to the prompt Pand
passed as a prefix to g. Specifically, decoding from
1Prompt often refers to a natural language template filled
by an input example (Liu et al., 2021b), but here it denotes the
sequence of training examples provided as input to the LM.
2656the LM g([P; xtest]) should yield ytest. In this work,
we focus on structured tasks, such as semantic pars-
ing, where x is a natural language utterance and y
is a meaning representation for that utterance.
Prior work Liu et al. (2021a) investigated the
effect of different prompts on the performance
of GPT-3 and demonstrated that the choice of in-
context examples strongly affects downstream per-
formance. They used an unsupervised sentence
encoder to encode training examples, and retrieved
for every test instance its nearest neighbors.
Das et al. (2021) trained a supervised prompt
retriever for knowledge-base question answering.
The retriever was trained with supervision that is
tailored for knowledge-base queries, and relies on
surface similarity between formal queries. Con-
versely, our approach takes advantage of the gener-
ative LM itself and is thus more general.
Shin et al. (2021) used GPT-3 to select examples
for the prompt for few-shot semantic parsing. How-
ever, rather than training a retriever, they randomly
sample a large set of utterance-program pairs from
the training set, and choose those that are similar
to the target instance question according to GPT-3.
This results in an expensive inference procedure,
where GPT-3 is run hundreds of times for each test
instance, unlike our approach, which is based on a
light-weight sub-linear retriever.
3 Efficient Prompt Retriever
We now describe our method for training EPR,
an efficient prompt retriever for in-context learn-
ing. We first describe how to generate labeled data
(§3.1), and then how to use the training data for
training and inference (§3.2). Fig. 2 provides an
overview of the training procedure.
3.1 Generating the Training Data
Our approach relies on finding which training ex-
amples can serve as good prompts for other training
examples. Scoring all pairs of training examples is
quadratic in |D|, and thus prohibitive. Hence, we
present a method for choosing a set of candidate ex-
amples ¯E⊂ D, from which we will choose positive
and negative examples for training. Importantly,
since we are not at test time and are only generating
data for training, we can use the target sequence
y to retrieve a good set of candidates. This can be
approached using a simple retrieval method, given
that our goal is to retrieve examples that are similar
to the input in terms of their output sequence, y.
To obtain a high-quality candidate set of train-
ing examples, we take advantage of an unsuper-
vised retriever, ¯E= Ru((x, y), D). For the choice
of the unsupervised retriever, we experiment with
BM25 (Robertson and Zaragoza, 2009), a sparse
retriever that relies on surface text similarity, and
SBERT (Reimers and Gurevych, 2019), which is
based on dense sentence encoding. For both, we
experimented with passing the retriever the training
pair (x, y) or the target sequence y only, and found
that using y leads to slightly higher performance.
Scoring the candidate set Once we retrieve the
set of candidates ¯E= {¯e1, ··· , ¯eL}for a training
example (x, y),2 we score each candidate ¯el ∈ ¯E
independently with a scoring LM, ˆg, which serves
as a proxy for the inference LM, g. Specifically,
the score for a candidate prompt is
s(¯el) = Probˆg(y |¯el, x),
which is the probability under the LM,ˆg, of the out-
put sequence conditioned on the candidate prompt
and input sequence. This indicates how helpful this
candidate is for decoding the target (independent
of all other candidates). We argue this score is a
better proxy for the utility of a training example at
inference time compared to prior approaches.
We apply this scoring function to all training ex-
amples, and define for each training example a set
of positive examples Epos, which includes the top-k
candidates in ¯Eaccording to s(¯el), and a set of neg-
ative examples Eneg, which includes the bottom-k
candidates in ¯Eaccording to s(¯el). This should lead
to relevant positive examples, assuming that the set
of candidates, ¯Eincludes good prompt candidates
and hard negatives, since all candidates have high
similarity with (x, y) according to Ru(y, D). With
positive and negative examples at our disposal, we
can now apply contrastive learning, which we de-
scribe next.
3.2 Training and Inference
Training Our training procedure proceeds ex-
actly like the contrastive learning procedure from
DPR (Karpukhin et al., 2020). This procedure re-
sults in an input encoder EX(·), which receives the
sequence of input tokens, x, and a prompt encoder
EP (·), which receives a candidate prompt, namely,
a concatenation of the tokens in an input-output
pair. Both encoders are initialized with BERT-base
2We omit the dependence of ¯E on (x, y) for simplicity.
2657(Devlin et al., 2019), and the output vector repre-
sentation is given by the CLS token, as usual. The
goal of training is to learn a similarity metric such
that given a test example xtest, it will be similar to
training examples that lead to decoding of ytest.
Our training instances are of the form
⟨xi, e+
i , e−
i,1, . . . e−
i,2B−1⟩. Where the positive ex-
ample e+
i is sampled from the set E(i)
pos, and our
negative examples consist of one hard negative ex-
ample sampled from E(i)
neg, B −1 positive examples
from the other instances in the same mini-batch,
and the B −1 hard negatives from those instances.
We define the similarity score between an input
and an input-output pair to be the inner product
sim(x, e) = EX(x)⊤EP (e). We can now define
the typical contrastive learning objective and mini-
mize for each example the negative log likelihood
of the positive example:
L(xi, e+
i , e−
i,1, . . . e−
i,2B−1) (1)
= −log esim(xi,e+
i )
esim(xi,e+
i ) + ∑2B−1
j=1 esim(xi,e−
i,j) .
An advantage of this approach is that for batch size
B the effective batch size is of order B2, with the
in-batch negatives trick (Henderson et al., 2017).
Inference After training the input encoder and
prompt encoder, we encode the entire set of train-
ing examples with EP (·) in a pre-processing step
using FAISS (Johnson et al., 2017). At test time,
given an input sequence, xtest, we compute its en-
coding EX(xtest), and then use maximum inner-
product search over the training data to find the L
most similar training examples, sorted by their in-
ner product (from high to low): P= (e1, . . . , eL).
The final prompt P′is determined by C, the max-
imal context size supported by the inference LM,
g. Specifically, L′ ≤ L is the largest L′ such∑L′
i=1 |ei|+ |xtest|+ |y′|≤ C, where |y′|is the
desired maximal length of the generated output. Fi-
nally, we return the output of greedy decoding on
g([eL′; eL′−1; . . .; e1; xtest]).
We note that while at training time we score each
training example independently, at test time the
language model observes a prompt, i.e.,a sequence
of examples. We leave modeling the dependence
between different training examples to future work.
4 Experimental Results
We now compare EPR to a wide range of unsu-
pervised and supervised baselines, both when the
scoring LM, ˆg, is smaller than the inference LM, g,
and when they are identical.
4.1 Datasets
We focus on tasks that map utterances to meaning
representations, where in-context examples can be
used to learn the mapping from inputs to outputs.
Examples from each dataset and the number of
examples are in Table 1.
• BREAK (Wolfson et al., 2020): A dataset map-
ping complex natural language questions into a
language-based meaning representation, where
a question is decomposed into an ordered list
of atomic steps. We use the low-level BREAK
subset, containing 44K/7K/8K examples in its
training/development/test sets.
• MTOP (Li et al., 2021): A semantic parsing
dataset, focused on task-oriented dialogue, where
commands are mapped to complex nested queries
across 11 domains. Similar to past work (Pasu-
pat et al., 2021), we use the English subset of
MTOP, containing 16K/2K/4K examples in its
training/development/test sets.
• SMC ALFLOW (Andreas et al., 2020): A large
English-language task-oriented dataset that cov-
ers tasks such as calendar, weather, places, and
people. The meaning representation is a dataflow
program, which includes API calls, function com-
position and complex constraints. SMC ALFLOW
includes 15K development set examples and
134K training examples, from which we sample
a random set of 44K examples for training.
4.2 Baselines and Oracles
We consider the following unsupervised baselines,
which are applied at test time only.
• RANDOM : we randomly sample examples from
the training set D.
• SBERT : We use SentenceTransformers,
a library providing BERT-based sen-
tence embeddings. 3 Specifically, we use
paraphrase-mpnet-base-v2, a 110M
parameter model to encode the test utterance
xtest and retrieve the examples with the most
similar utterances as in-context examples.
• BM25 : We use the classical sparse retrieval
method BM25 (Robertson and Zaragoza, 2009),
which is an extension of TF-IDF, to retrieve for
3https://www.sbert.net/index.html.
2658Dataset Size Utterance Meaning Representation
BREAK 60K There are more birds in the image on
the right than in the image on the left.
1) return right image;
2) return birds in #1;
3) return number of #2;
4) return left image;
5) return birds in #4
6) return number of #5;
7) return if #3 is higher than #6;
MTOP 22K call Zoey’s wife. [IN:CREATE_CALL =
[SL:CONTACT = [IN:GET_CONTACT =
[SL:CONTACT_RELATED = Zoey]
[SL:TYPE_RELATION = wife]]]]
SMCALFLOW 148K Can you create me a new meeting
on thursday morning?
(Yield (CreateCommitEventWrapper
(CreatePreflightEventWrapper
(Event.start_?
(DateTimeConstraint (Morning)
(NextDOW (Thursday)))))))
Table 1: Examples from each of the datasets we evaluate on.
each test utterance xtest the training examples
with the most similar utterance.
• BRUTE FORCE : We apply the prompt selection
method for few-shot semantic parsing from Shin
et al. (2021). Given a test example xtest, we sam-
ple 200 training examples. For each training
example (xi, yi), compute Probg(xtest |xi), and
use the highest scoring examples for the prompt.
Similar to us, this approach uses the inference
LM to choose prompts. However, it does so at
test time, which results in slow inference.
Next, we describe baselines that use the train-
ing set, D, to train a prompt retriever. All super-
vised methods share the following template. First,
a candidate set ¯Eof L = 50 examples is retrieved
with the unsupervised retriever Ru(y, D). We use
BM25 as an unsupervised retriever, since it outper-
formed SBERT (see §4.4). We then score each can-
didate prompt ¯el ∈¯Ewith some scoring function,
and label the top-k prompts as positive examples
and the bottom- k as negative examples ( k = 5 ).
Different supervised methods only differ in the
scoring function itself.4
• DR-BM25 : Here, we use the original BM25
scores for labeling positive and negative exam-
ples and training a dense retriever.
• CASE -BASED REASONING (CBR) : We adapt
the scoring function from Das et al. (2021),
which focused on knowledge-base question an-
swering. They define the weight for a pair of log-
ical forms to be the F1 score between the two sets
of relations appearing in those logical forms, and
use this weight to softly label their data. Since
in our setting we do not assume logical forms,
4Results for k ∈ {1, 5, 10} and L ∈ {50, 100} are in
Appendix A.
we define the score between two output sequence
yi and yj to be the F 1 between the two sets of
tokens in yi and yj, omitting stop words.
• EFFICIENT PROMPT RETRIEVAL (EPR) : Our
full approach from §3.
Last, we also consider two oracle models.
• BM25-O RACLE : We score test examples
with BM25 using the gold output sequence
RBM25(ytest, D). This provides an upper-bound
on what can be learned by DR-BM25. EPR can
potentially outperform this oracle, since its train-
ing signal goes beyond surface text similarity.
• LM-O RACLE : We use the procedure for labeling
training data at test time. Given a test example
(xtest, ytest), we first retrieve L candidate training
examples with RBM25(ytest, D), we then sort the
candidates with the scoring LM ˆg, estimating the
probability of ytest given xtest and the candidate
prompt. This provides an upper bound for EPR,
since EPR is trained to emulate this behaviour.
4.3 Experimental Details
Language models In this work, we only train
a dense retriever, but use scoring and inference
LMs. For our scoring LM, ˆg, we use GPT-N EO
(Black et al., 2021), a 2.7B-parameter LM trained
on The Pile (Gao et al., 2021), an 825 GB English
text corpus, constructed from a wide range of high-
quality resources.
In addition, we consider the following infer-
ence LMs: (a) GPT-J (Wang and Komatsuzaki,
2021): a 6B-parameter LM, also trained on The
Pile. The advantage in this setup, is that GPT-J
was trained on the same corpus as GPT-N EO. (b)
GPT-3 (Brown et al., 2020): A 175B-parameter
2659Model B REAK MTOP SMCALFLOW
Unsuper.
RANDOM 1.7 7.3 8.9
SBERT 21.6 48.7 43.6
BM25 26.0 52.9 46.1
BRUTEFORCE 7.7 18.1 11.1
Super.
DR-BM25 23.6 50.2 43.1
CBR 25.7 57.0 51.4
EPR (ours) 31.9 64.2 54.3
Oracle BM25-ORACLE 32.3 58.9 47.3
LM-ORACLE 43.1 71.6 73.7
Table 2: Development results when GPT-N EO is the
scoring and inference LM. Numbers for BREAK are
LF-EM, and for MTOP and SMC ALFLOW are EM.
Model B REAK MTOP
Unsuper. BM25 17.6 49.0
Super. CBR 18.4 57.5
EPR (ours) 23.9 64.4
Table 3: Test results where GPT-N EO is the scoring
and inference LM. Numbers for BREAK are NEM, the
official metric, and for MTOP are EM.
model, trained mostly on a filtered subset of com-
mon crawl. (c) CODEX (Chen et al., 2021): A
GPT-3 175B-parameter model finedtuned on code
from GitHub. Since our tasks involve mapping
from utterances to programs or meaning represen-
tations, CODEX might potentially perform well at
in-context learning.
For all LMs, we use a maximum context size of
C =2,048 tokens.
Evaluation On BREAK , we evaluate perfor-
mance on the development set with LF-EM (Has-
son and Berant, 2021), which is a better metric
compared to Normalized Exact Match (NEM), the
official metric, as it measures whether two mean-
ing representations are semantically equivalent. On
the test set, we use NEM. On MTOP and SM-
CALFLOW, we evaluate with Exact Match (EM),
i.e., whether the string output by the inference LM
is identical to the reference string.
We evaluate EPR in two settings: (a) LM-as-a-
service, and (b) LM-as-a-proxy. In the first set-
ting, we use GPT-N EO as both the scoring LM
and inference LM. In this setting, we evaluate on
the full development sets of BREAK , MTOP, and
SMC ALFLOW. In the latter setting, as we access
GPT-3 and CODEX through a paid API, we sample
a random subset of 1,000 development examples
from each dataset and evaluate each model once on
this subset.
Model One-shot Full-context
Unsuper. RANDOM 1.1 1.7
BM25 15.2 26.0
Super.
DR-BM25 14.1 23.6
CBR 14.5 25.7
EPR 23.0 31.9
Oracle
BM25-ORACLE 18.0 32.3
LM-ORACLE 33.3 43.1
ANYCORRECT-ORACLE 53.6 -
Table 4: Development results on BREAK with GPT-
NEO in the one-shot setting. Numbers are LF-EM . Full-
context is the corresponding numbers from Table 2.
4.4 Results
LM-as-a-service Table 2 reports results where
the scoring and inference LMs are identical.
EPR substantially outperforms all other baselines.
Specifically, when comparing to the best baseline,
it improves performance from 26.0 to 31.9 on
BREAK , from 57.0 to 64.2 on MTOP, and from
51.4 to 54.3 on SMC ALFLOW. This shows that
using the LM itself to label examples is an effective
approach for obtaining a strong prompt retriever.
Table 3 shows test results on BREAK and MTOP
corroborating that EPR substantially improves per-
formance compared to BM25 and CBR.
For the unsupervised methods, the RANDOM
baseline shows that random sampling of training
examples leads to poor performance. BM25 out-
performs SBERT for prompt retrieval, and con-
sequently we use BM25 in all of our supervised
approaches to retrieve the set of candidates,¯E. Last,
BRUTE FORCE performs worse than BM25. We as-
sume this is since the training sets are large (∼14-
120K examples), and sampling 200 examples does
not cover examples that are useful for GPT-N EO.
Interestingly, EPR outperforms BM25-O RACLE
on MTOP and SMC ALFLOW and is comparable on
BREAK . This is surprising since BM25-O RACLE
has access to the output sequence ytest at test time,
illustrating that the signal provided by the scoring
LM for training goes beyond surface text similarity.
The performance of LM-O RACLE is substantially
higher than EPR, showing that the supervision pro-
vided by the scoring LM is strong, and training a
better retriever from this signal can substantially
enhance performance.
We further evaluate our models in the one-shot
setup, i.e., when the prompt given to the inference
LM includes the highest scoring example only. In
this setup, the inference LM is applied in the same
setting as when we generate labeled data, where
we go over each prompt candidate independently.
2660BREAK MTOP SMCALFLOW
Method RANDOM BM25 CBR EPR RANDOM BM25 CBR EPR RANDOM BM25 CBR EPR
GPT-3 4.2 20.1 21.3 25.3 7.6 52.5 54.8 62.6 5.8 35.3 41.6 46.5
CODEX 8.9 24.5 24.2 29.5 10.8 60.6 59.4 66.1 7.2 45.1 48.7 50.3
GPT-J 3.3 26.7 26.7 31.5 8.8 56.6 58.0 65.4 10.6 50.4 50.9 57.4
GPT-NEO 1.0 22.8 25.8 29.9 7.6 52.8 55.4 63.6 8.0 46.1 50.1 53.5
Table 5: Results on a random sample of 1,000 examples from the development set when using GPT-Neo as a scoring
LM across different inference LMs and datasets.
EPR CBR
Test
Example
Utterance Give the code of the airport with the
least flights.
Meaning
Representation 1) airports
2) flights of #1
3) number of #2 for each #1
4) #1 where #3 is lowest
5) code of #4
Top-1 Utterance What is the code of the city with the
most students?
What destination has the fewest number
of flights?
Meaning
Representation 1) cities
2) students in #1
3) number of #2 for each #1
4) #1 where #3 is highest
5) code of #4
1) destinations
2) flights of #1
3) number of #2 for each #1
4) #1 where #3 is lowest
Top-2 Utterance Return the code of the city that has the
most students.
Which destination has least number of
flights?
Meaning
Representation 1) cities
2) students in #1
3) number of #2 for each #1
4) #1 where #3 is highest
5) code of #4
1) destinations
2) flights to #1
3) number of #2 for each #1
4) #1 where #3 is lowest
Top-3 Utterance Find the count and code of the job has
most employees.
What is the number of airports per
country, ordered from most to least?
Meaning
Representation 1) jobs
2) employees of #1
3) number of #2 for each #1
4) #1 where #3 is highest
5) employees of #4
6) number of #5
7) code of #4
8) #6 , #7
1) countries
2) airports in #1
3) number of #2 for each #1
4) #3 sorted by most to least
Table 6: An example from BREAK development set where EPR is correct and CBR is incorrect along with the top-3
training examples retrieved from each retriever.
Since train and test time are now closer, we can ex-
pect the advantage of EPR to be more pronounced.
Table 4 shows the results. Indeed, EPR out-
performs the best baseline by 8.5%, and BM25-
ORACLE by 5%. In addition, we examine
ANYCORRECT -ORACLE , which tests whether any
of the candidates returned by BM25 leads to the
correct output. ANYCORRECT -ORACLE reaches
53.6%, 20 points above LM-O RACLE . This shows
the high quality of candidates provided by BM25
(applied on the y), as one can reach more than 50%
LF-EM with just a single prompt. Moreover, it
hints that a better scoring function can potentially
further improve performance.
LM-as-a-proxy Table 5 shows results when the
scoring LM is GPT-N EO and the inference LM is a
larger LM. First, the trends are similar to the LM-as-
a-service setup, i.e., EPR substantially outperforms
prior baselines, including our best unsupervised
baseline, BM25, and the best supervised baseline,
CBR, by 2-8 points on all datasets and all pre-
trained models. Thus, GPT-N EO serves as a good
proxy for choosing training examples.
To further validate this finding, we evaluate the
performance of GPT-J on BREAK with GPT-N EO
as the scoring LM compared to using GPT-J it-
self as the scoring LM. We find performance im-
proves slightly from 31.5 to 33.6. Analogously,
when using CODEX as the scoring LM and infer-
ence LM performance remains roughly the same:
29.5→29.3. Thus, using a smaller LM (GPT-N EO)
is an effective strategy for training a retriever that
will be applied on other LMs. Zooming in on dif-
ferent inference LMs, GPT-J performs slightly bet-
ter than GPT-N EO across the board, since it was
2661Figure 3: A t-SNE projection and clustering of the rep-
resentations learned by EPR for the training examples
in BREAK . An interactive version displaying individual
examples is available here.
trained on the same data and using the same pro-
cedure as GPT-N EO. CODEX outperforms GPT-
3, which can be explained by the fact that it was
trained on code, and our datasets involve map-
ping to programs or meaning representations. Sur-
prisingly, GPT-J outperforms CODEX (except on
MTOP) and GPT-3 despite being 30x smaller. This
can perhaps be explained by the fact that GPT-J
was trained on a different dataset (The Pile (Gao
et al., 2021)).
Pattern Copied Novel Total
Acc Rate Acc Rate Acc
BREAK Exact 55.1% 10.4% 29.7% 89.6% 32.3%Abstract 58.0% 41.1% 14.5% 58.9%
MTOP Exact 77.3% 25.3% 59.7% 74.7% 64.2%Abstract 71.6% 84.5% 23.4% 15.5%
SMCAL Exact 62.5% 60.2% 42.4% 39.8% 54.5%Abstract 62.4% 81.2% 20.6% 18.8%
Table 7: Accuracy comparison between the decoded
instances that contained patterns from the prompt and
novel instances those that don’t. Results shown are on
the LM-as-a-service setup using GPT-N EO.
Analysis Table 6 shows an example fromBREAK
where EPR decodes the correct output, while CBR
does not. All training examples retrieved by EPR
perform an argmax (argmin in the original utter-
ance), and return in the final step “a code”, while
the third example retrieved by CBR does not per-
form an argmax or argmin, and do not involve“a
code”. We provide additional examples in App. A.
Figure 3 shows a t-SNE (Hinton and Roweis,
2002) projection of the embeddings learned by EPR
for the training examples of BREAK , with a link
to an interactive version, where we applied the
OPTICS (Ankerst et al., 1999; Schubert and Gertz,
2018) clustering algorithm. Examining clusters
0.0 0.2 0.4 0.6 0.8 1.0
Distance
0.125
0.5
2.0
8.0
%
Exact
Abstract
Figure 4: On the subset of copied patterns we plot the
distribution of the distance from the test instance to the
example containing the pattern. Shown on the BREAK
validation set using EPR in the LM-as-a-service setup
using GPT-N EO. Note that the y-axis is in log-scale.
shows that EPR captures both lexical and structure
similarity. Examples for clusters are also available
in App. A.
Prompt copying We analyze how the LM uti-
lizes in-context prompts. Specifically, is the target
output copied from one of the prompts or is it a
composition of different prompt fragments, which
result in generalization to new structures.
To achieve this, we define two types of copy-
ing. (a) Exact copying measures if the generated
output exactly matches one of the examples in the
prompt, and (b) Abstract copying, that quantifies
if the structure of the decoded output matches any
of the structures seen in the prompt. Specifically,
we eliminate the effect of non-structural elements
such as entities and function arguments. We re-
place every sequence of words in the logical form
that appears in the input utterance with the string
[MASKED] for both the target utterance and in-
context examples. If the masked logical form that
the LM decoded appears in the set of masked ex-
amples defined by the prompt, we say that the LM
copied that abstract pattern.
Table 7 presents the results on the validation
set for each of our three datasets, as well as the
accuracy on each subset. We observe that the
rate of copying is much higher in MTOP and SM-
CALFLOW compared to BREAK , where in MTOP
and SMC ALFLOW abstract copying reaches more
than 80%. Moreover, accuracy on examples where
copying occurred is much higher compared to ac-
curacy where no copying happened. For exam-
ple, on MTOP, 84.5% of the examples were ab-
stractly copied, and on that subset of examples,
EPR achieves 71.6% EM, compared to 64.2% on
2662the entire validation set. Nevertheless, even though
accuracy is much lower in cases where no copying
occurred, accuracy is not negligible, which shows
that some form of generalization to new structures
is taking place.
Another follow-up question is whether the model
copies patterns from prompts uniformly or does it
attend mostly to the ones with high retrieval score.
To answer this, we look at the subset of exam-
ples where copying occurred. We then identify for
each example the highest-ranking prompt that was
copied from, and define the distance of that prompt
by dividing the rank by the number of prompts that
fit in that example. Figure 4 shows the distribution
over distances for the BREAK dataset. We observe
that copying happens mostly from highly-ranked
prompts.
5 Related Work
In-context learning Our understanding of in-
context learning has grown substantially recently.
Saunshi et al. (2021) suggests that by conditioning
on a prompt, the task of predicting the next word
approaches linear separability. Xie et al. (2021)
suggests that in-context learning occurs when the
model infers a shared latent concept between ex-
amples in a prompt. Levine et al. (2021) present
a pre-training scheme theoretically motivated by
the bias of in-context learning, that gives signif-
icant improvements. Recently, Min et al. (2022)
showed that the model does not rely on the ground
truth input-label mapping provided in the demon-
strations as much as previously thought.
Retrieval Research on training dense retrievers
has skyrocketed recently, propelled by interest
in open-domain question answering (Chen et al.,
2017; Lee et al., 2019; Karpukhin et al., 2020; Guu
et al., 2020; Khattab and Zaharia, 2020; Qu et al.,
2021). Work on retrieval-based methods has also
spread more widely to other knowledge-intensive
tasks (Lewis et al., 2020), e.g., fact verification
(Samarinas et al., 2021).
Similar to us, Pasupat et al. (2021) proposed to
use retrieval in semantic parsing. However, they fo-
cus on controlling the output generated by a model.
Retrieval methods have also been successfully used
in language modeling (Khandelwal et al., 2020;
Borgeaud et al., 2021; Alon et al., 2022) and ma-
chine translation (Khandelwal et al., 2021).
Prompts Developing methods for interacting
with LMs and extracting desired behaviours has
attracted considerable attention, under the umbrella
term prompting. In this work, prompts are a set of
in-context training examples, but substantial effort
has also been devoted to casting natural language
tasks as language modeling by phrasing the tar-
get task in natural language (see survey in (Liu
et al., 2021b)). Such approaches include prompt
engineering through manual patterns (Petroni et al.,
2019; Schick and Schütze, 2021), decoding meth-
ods (Min et al., 2021; Zhao et al., 2021; Holtzman
et al., 2021), and methods for extracting either hard
(Shin et al., 2020; Haviv et al., 2021) or soft (Li and
Liang, 2021; Zhong et al., 2021; Qin and Eisner,
2021) prompts automatically.
Prompt retrieval for supervised models In par-
allel to this work, adding training examples as addi-
tional input has been shown to be useful for super-
vised models as well. Wang et al. (2022) and Xu
et al. (2021) used BM25 to retrieve and augment the
input with similar examples from the training set.
Fine-tuning the model with the additional inputs
improved performance on tasks such as summariza-
tion and question answering. Such methods can
also potentially benefit from a stronger retriever.
6 Conclusions
Large pre-trained LMs are becoming an insepara-
ble part of the natural language understanding eco-
system. However, accessing their weights or updat-
ing them can be prohibitive for many researchers.
In this work, we propose EPR, a method for learn-
ing to retrieve good prompts for in-context learning,
by using language models themselves as the scor-
ing function. This allows us to train a light-weight
retriever and substantially improve performance on
three challenging tasks.
More broadly, given that large LMs models are
likely to play a prominent role in developing lan-
guage understanding models, it is important to de-
velop approaches for interacting with such models
effectively. EPR can be viewed as a step in this
direction.
Acknowledgement
We thank Ori Ram and Itay Itzhak for helpful sug-
gestions and meaningful discussions. This research
was supported in part by The Yandex Initiative for
Machine Learning, and The European Research
2663Council (ERC) under the European Union Hori-
zons 2020 research and innovation programme
(grant ERC DELPHI 802800). This work was com-
pleted in partial fulfillment for the Ph.D degree of
Ohad Rubin."
"Journal of Machine Learning Research 24 (2023) 1-43 Submitted 1/23; Revised 7/23; Published 7/23
Atlas: Few-shot Learning with
Retrieval Augmented Language Models
Gautier Izacard1,2,∗,† gautier@inflection.ai
Patrick Lewis1,∗,† patrick@cohere.com
Maria Lomeli1 marialomeli@meta.com
Lucas Hosseini1,† hoss@meta.com
Fabio Petroni1,† fabiopetroni@meta.com
Timo Schick1,† schick@meta.com
Jane Dwivedi-Yu1 janeyu@meta.com
Armand Joulin1,† ajoulin@meta.com
Sebastian Riedel1,3,† sriedel@meta.com
Edouard Grave1,† egrave@meta.com
1 Meta AI, 2 ENS, PSL University & Inria,3 University College London
Editor: Ivan Titov
Abstract
Large language models have shown impressive few-shot results on a wide range of tasks.
However, when knowledge is key for such results, as is the case for tasks such as question
answering and fact checking, massive parameter counts to store knowledge seem to be needed.
Retrieval-augmented models are known to excel at knowledge intensive tasks without the
need for as many parameters, but it is unclear whether they work in few-shot settings.
In this work we presentAtlas, a carefully designed and pre-trained retrieval-augmented
language model able to learn knowledge intensive tasks with very few training examples.
We perform evaluations on a wide range of tasks, including MMLU, KILT and Natural
Questions, and study the impact of the content of the document index, showing that it can
easily be updated. Notably,Atlas reaches over 42% accuracy on Natural Questions using
only 64 examples, outperforming a 540B parameter model by 3% despite having 50x fewer
parameters.
Keywords: retrieval augmented language models, information retrieval, language models
1. Introduction
Large language models (LLMs) are impressive few-shot learners (Brown et al., 2020; Rae
et al., 2021; Hoﬀmann et al., 2022; Chowdhery et al., 2022). They are able to learn new
tasks with very few examples or even from instructions alone. For this generalisation ability
to emerge, the key ingredients are scaling both the parameter count of the model, and the
size of the training data. Large language models owe this improvement to both a larger
computational budget, enabling more complex reasoning, and the ability to memorize more
∗. Equal contribution
†. Work done while at Meta AI
c⃝2023 Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu,
Armand Joulin, Sebastian Riedel, Edouard Grave.
License: CC-BY 4.0, seehttps://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
at http://jmlr.org/papers/v24/23-0037.html.Izacard, Lewis, Lomeli, Hosseini, Petroni, Schick, Dwivedi-Yu, Joulin, Riedel, Grave
information related to downstream tasks from the larger training data. While it is intuitive
to assume that increased reasoning abilities lead to better generalisation, and hence few-shot
learning, the same is not true for in-parameter memorisation. Speciﬁcally, it is unclear to
what extent eﬀective few-shot learning requires vast knowledge in the parameters of the
model.
In this paper, we investigate whether few-shot learning requires models to store a large
amount of information in their parameters, and if memorisation can be decoupled from
generalisation. To do so, we leverage the fact that memory can be outsourced and replaced by
an external non-parametric knowledge source by employing aretrieval-augmentedarchitecture.
These models employ a non-parametric memory, for example a neural retriever over a large,
external, potentially non-static knowledge source to enhance a parametric language model.
In addition to their memorisation abilities, such architectures are attractive due to a number
of other established advantages in terms of adaptability, interpretability and eﬃciency (Guu
et al., 2020; Lewis et al., 2020; Yogatama et al., 2021; Borgeaud et al., 2021, inter alia).
However, retrieval-augmented models have yet to demonstrate compelling few-shot learning
capabilities. In this work we address this gap, and presentAtlas, a retrieval-augmented
language model capable of strong few-shot learning, despite having lower parameter counts
than other powerful recent few-shot learners.
Atlas retrieves relevant documents based on the current context by using a general-
purpose dense retriever using a dual-encoder architecture, based on the Contriever (Izacard
et al., 2022). The retrieved documents are processed, along with the current context, by a
sequence-to-sequence model using the Fusion-in-Decoder architecture (Izacard and Grave,
2021a) that generates the corresponding output. We study the impact of diﬀerent techniques
to train Atlas on its few-shot performance on a range of downstream tasks, including
question answering and fact checking. We ﬁnd that jointly pre-training the components is
crucial for few-shot performance, and we carefully evaluate a number of existing and novel pre-
training tasks and schemes for this purpose.Atlas achieves strong downstream performance
in both few-shot and resource-rich settings. For example, with only 11B parameters,Atlas
achieves an accuracy of 42.4% on Natural Questions using 64 training examples (45.1% using
a Wikipedia-only index), outperforming PaLM (Chowdhery et al., 2022), a 540B parameter
model by almost 3 points, and 64.0% in a full data set setting with a Wikipedia index,
establishing a new state of the art by 8.1 points.
In summary we make the following contributions:
•A thorough study on how to design and train retrieval-augmented language models,
with a focus on downstream few-shot learning and sample eﬃciency.
•The ﬁndings of this study lead to a retrieval-augmented language model, calledAtlas,
that exhibits few-shot abilities that emerge at lower scale than standard LLM.
•We provide an exploration of ﬁne-tuning strategies to eﬃciently adapt both the retriever
and the language model to the task at hand.
•Thorough downstream experiments in few-shot settings, demonstrating state-of-the-art
results on few-shot Natural Questions (+2.8%), TriviaQA (+3.3%), FEVER (+5.1%),
and results on par with models with 15×more parameters on MMLU.
2Atlas: Few-shot Learning with Retrieval Augmented Language Models
Fact checking:Bermuda Triangle is in the western part of the Himalayas.
AtlasFalse
Masked Language Modelling:Bermuda Triangle is in the <MASK> of the Atlantic Ocean.
TheBermuda Triangle is anurban legendfocused on a loosely-defined region in the western part of the NorthAtlantic Ocean.
western partPretrainingFew-shot
Question answering:Where is the Bermuda Triangle?Western part of the North Atlantic Ocean
… …
Figure 1: We introduceAtlas, a retrieval-augmented language model that exhibits strong
few-shot performance on knowledge tasks, and uses retrieval during both pre-
training and ﬁne-tuning.
•Experiments investigating full data set ﬁnetuning, setting new state-of-the-art results
in Natural Questions (+8.1%), TriviaQA (+9.3%) and 5 KILT Tasks.
•Experiments demonstrating the updateability and interpretability characteristics of
Atlas.
•Experiments demonstrating that a compressed index using product quantisation
achieves comparable performance as an uncompressed index while resulting in a 5x
memory reduction.
Our code, pre-trainedAtlas checkpoints, and various supporting data are available at
https://github.com/facebookresearch/atlas
2. Method
Our approach follows thetext-to-text framework (Raﬀel et al., 2019). This means that all
the tasks are framed as follows: the system gets atext query as input, and generates atext
output. For example, in the case of question answering, the query corresponds to the question
and the model needs to generate the answer. In the case of classiﬁcation tasks, the query
corresponds to the textual input, and the model generates the lexicalized class label, that is
the word corresponding to the label. We give more examples of downstream tasks, from the
KILT benchmark in Figure 2. As many natural language processing tasks requireknowledge,
our goal is to enhance standard text-to-text models with retrieval, which, as we hypothesise
in the introduction, may be crucial to endow models with few-shot capabilities.
2.1 Architecture
Our model is based on two sub-models: the retriever and the language model. When
performing a task, from question answering to generating Wikipedia articles, our model starts
by retrieving the top-k relevant documents from alarge corpusof text with the retriever.
3Izacard, Lewis, Lomeli, Hosseini, Petroni, Schick, Dwivedi-Yu, Joulin, Riedel, Grave
Task Query Output
Fact Checking Bermuda Triangle is in the western part of the Hi-
malayas.
False
Question
Answering
who is playing the halftime show at super bowl 2016 Coldplay
Entity Linking NTFS-3G is an open source <E>cross-platform</E>
implementation of the Microsoft Windows NTFS ﬁle
system with read-write support.
Cross-platform
software
Figure 2: Examples of query and output pairs for diﬀerent tasks from KILT.
Then, these documents are fed to the language model, along with the query, which in turns
generates the output. Both the retriever and the language model are based on pre-trained
transformer networks, which we describe in more detail below.
2.1.1 Retriever
Our retriever module is based on the Contriever (Izacard et al., 2022), an information
retrieval technique based on continuous dense embeddings. The Contriever uses a dual-
encoder architecture, where the query and documents are embedded independently by a
transformer encoder (Huang et al., 2013; Karpukhin et al., 2020). Average pooling is applied
over the outputs of the last layer to obtain one vector representation per query or document.
A similarity score between the query and each document is then obtained by computing the
dot product between their corresponding embeddings. The Contriever model is pre-trained
using the MoCo contrastive loss (He et al., 2020), and uses unsupervised data only. As shown
in the following section, an advantage of dense retrievers is that both query and document
encoders can be trained without document annotation, using standard techniques such as
gradient descent and distillation.
2.1.2 Language Model
For the language model, we rely on the T5 sequence-to-sequence architecture (Raﬀel et al.,
2019). We rely on the Fusion-in-Decoder modiﬁcation of sequence-to-sequence models, and
process each document independently in the encoder (Izacard and Grave, 2021a). We then
concatenate the outputs of the encoder corresponding to the diﬀerent documents, and perform
cross-attention over this single sequence in the decoder. Following Izacard and Grave (2021a),
we concatenate the query to each document in the encoder. Another way to process the
retrieved documents in the language model would be to concatenate the query and all the
documents, and to use this long sequence as input of the model. Unfortunately, this approach
does not scale with the number of documents, since the self-attention in the encoder results
in a quadratic complexity with respect to the number of documents.
4Atlas: Few-shot Learning with Retrieval Augmented Language Models
2.2 Training Objectives for the Retriever
In this section, we discuss four diﬀerent loss functions to train the retriever jointly with the
language model. We consider loss functions that leverage the language model to provide
supervisory signal to train the retriever. In other words, if the language model ﬁnds a
document useful when generating the output, the retriever objective should encourage the
retriever to rank said document higher. This allows us to train models using only query
and output pairs from the task of interest, without relying on document annotations. For
example, in the case of fact checking, a model only requires pairs of claims and corresponding
verdicts but no documents containing the evidence to back up the verdict. In practice, we
can apply this approach on any task, including self-supervised pre-training. As shown in
the experimental section, pre-training is critical for obtaining models that exhibit few-shot
learning abilities.
2.2.1 Attention Distillation (ADist)
The ﬁrst loss that we consider is based on the attention scores of the language model, and is
heavily inspired by Izacard and Grave (2021b). The main idea is that the cross-attention
scores between the input documents and the generation can be used as a proxy of the
importance of each input document when generating the output. In particular, Izacard and
Grave (2021b) showed that these scores can be aggregated across attention heads, layers
and tokens for a given document to obtain a single score for each document. Then, these
scores are distilled into the retriever by minimizing the KL-divergence with the probability
distribution pretr over the top-K documents{dk}1,...,K obtained from the retriever:
pretr (d |q) = exp(s(d,q)/θ)∑K
k=1 exp(s(dk,q)/θ)
, (1)
where s is the dot-product between the embedding vectors of the query and documents and
θ is a temperature hyperparameter.
In the original paper, to obtain a relevance score per document it was proposed to
use the pre-softmax scores from the decoder cross-attentions, and average across heads,
layers and tokens. Here, we use the pre-softmax score multiplied by the norm of the
values, an alternative which gives slightly stronger results. First, let us brieﬂy review the
Fusion-in-Decoder model (FiD, Izacard and Grave, 2021a). The underlying architecture
is a sequence-to-sequence model, composed of an encoder and a decoder. The encoder
independently processesK diﬀerent text inputs(input(dk))1≤k≤K, whereinput(d) is the
concatenation of the input query and the retrieved documentd. The output representations of
the encoder are then concatenated to form a global representationX of dimension(∑
k ℓk)×d,
where ℓk is the length ofinput(dk) and d is the dimension of the hidden representations
of the model. Then, the decoder processes this representation as a regular autoregressive
model, alternating self-attention, cross-attention and feed-forward modules.
Only the cross-attention module explicitly takes as input the global output representation
X of the encoder. IfH ∈Rd denotes the output of the previous self-attention layer of the
decoder, the cross-attention operation consists in the following operations. First, queriesQ,
keys K and valuesV are computed by applying linear transformations:
Q = WQH, K = WKX, V = WV X.
5Izacard, Lewis, Lomeli, Hosseini, Petroni, Schick, Dwivedi-Yu, Joulin, Riedel, Grave
Then a similarity score between the query at positioni, Qi, and the key at positionj, Kj, is
obtained by computing the dot-product between these two elements, and normalized over
the dimension:
αi,j = QT
i Kj, ˜αi,j = exp(αi,j)∑
m exp(αi,m).
Anewrepresentationisobtainedasasumofthevalues, weightedbytheattentionprobabilities,
before going through a ﬁnal linear transformationWo:
Oi = WO
∑
j
˜αi,jVi,j.
This describes the single-head attention case, in the case of multi-head attention withnh
heads, the output of the cross-attention layer can be written as:
Oi =
nh∑
h=1
WO,h
∑
j
˜αh,i,jVj,h.
For the layerland the headh, we use the quantity˜αl,h,i,j∥Vl,h,j∥2 as the measure of relevance
for the input token at positionj relatively to the generated token at positioni. We average
these scores over all attention heads, layers, tokens of the generation and tokens of the input
segment input(d) to obtain an attention scorescoreattn(d) for each documentd:
scoreattn(d) = mean
h,l,i,j∈inputk
αl,h,i,j∥Vl,h,j∥2.
We apply theSoftmax operator over the resulting scores, to obtain a distributionpattn(d)
over the top-K retrieved documents:
pattn(d) = exp (scoreattn(d))∑
k exp (scoreattn(dk)).
We then minimize the KL-divergence betweenpattn, and the distributionpretr from the
retriever deﬁned in Equation 1:
KL(pattn ∥pretr) =
K∑
k=1
pattn(dk) log
(pattn(dk)
pretr(dk)
)
.
Here, this loss is only used to optimize the parameters of the retriever, and not the lan-
guage model. When using recent deep learning frameworks, this is achieved by applying a
StopGradient operator onpattn.
2.2.2 End-to-end Training of Multi-Document Reader and Retriever
(EMDR2)
Next, we consider the method introduced by Sachan et al. (2021), which is inspired by the
expectation-maximization algorithm, treating retrieved documents as latent variables. Given
6Atlas: Few-shot Learning with Retrieval Augmented Language Models
a queryq, the corresponding outputa and the setDK of top-K retrieved documents with
the current retriever, the EMDR2 loss to train the retriever is
−log
[K∑
k=1
plm(a |q,dk)pretr(dk |q)
]
,
where pretr is again the probability over the top-K documents obtained with the retriever, as
deﬁned by Equation 1. Again, only the parameters of the retriever are updated by applying a
StopGradient operator aroundplm. One should note that the probability distribution over
documents that minimizes this loss function is an indicator of the document corresponding
to the highest probability of the output according to the language model. Finally, in practice,
the EMDR2 loss function is applied at the token level, and not at the sequence level.
2.2.3 Likelihood Distillation (LDist)
Third, we discuss a simpler loss function which is inspired by the objectives from the attention
distillation and EMDR2 methods (Izacard and Grave, 2021b; Sachan et al., 2021). More
precisely, we want to train the retriever to predict how much each document would improve
the ability of the language model to predict the output, given the query. To this end, we
minimize the KL-divergence between the documents distribution of the retriever (Eqn. 1),
and the documents posterior distribution according to the language model conditioned on a
single document and using a uniform prior:
pLDist(dk) ∝pLM (a |dk,q).
Using theSoftmax operator, we have that
pLDist(dk) = exp(log pLM (a |dk,q))∑K
i=1 exp(log pLM (a |di,q))
.
2.2.4 Leave-one-out Likelihood Distillation (LOOL)
Finally, we propose an objective based on how muchworse the prediction of the language
model gets whenremoving one of the top-k retrieved documents. To do so, we compute the
log probability of the output for each subset of k-1 documents, and use the negative value as
relevance score for each document. Following the previous loss function, we use the softmax
operator to obtain a probability distribution over documents:
plool(dk) = exp(−log pLM (a |DK \{dk},q))∑K
i=1 exp(−log pLM (a |DK \{di},q))
.
As before, we then minimize the KL-divergence between this distribution, and the one
obtained with retriever. This loss is more expensive to compute than LDist and EMDR,
but, like ADist, employs the language model more closely to the way it is trained: the LM
is trained to be conditioned on a set ofK documents. For LOOL, the language model is
conditioned on(K−1) documents, rather than a single document as in EMDR2 and LDist.
For all losses, we can also use a temperature hyperparameter when computing the target
or retriever distributions to control the distribution’s peakiness of, which might be important
for some tasks or losses. Indeed, for LDist and LOOL, the likelihood of the output may not
vary much when conditioning on diﬀerent documents, especially in the case of long outputs.
7Izacard, Lewis, Lomeli, Hosseini, Petroni, Schick, Dwivedi-Yu, Joulin, Riedel, Grave
2.3 Pretext Tasks
In this section, we describe pretext tasks that can be used to jointly pre-train the retriever
and the language model using only unsupervised data.
2.3.1 Prefix Language Modeling
First, we consider a standard language modeling task as a potential pre-training objective.
To cast language modeling in the text-to-text framework, we consider a chunk ofN words,
and split this chunk in two sub-sequences of equal lengthN/2. Then, the ﬁrst sub-sequence
is used as the query, and the second corresponds to the output. We thus retrieve relevant
documents by using the ﬁrst sub-sequence ofN/2 tokens, to generate the output.
2.3.2 Masked Language Modeling
Second, we consider masked language modeling, as formulated by Raﬀel et al. (2019). Again,
starting from a chunk ofN words, we samplek spans of average length 3 tokens, leading
to a masking ratio of15%. We then replace each span by a diﬀerent special token. The
model is then trained to generate the masked spans, each span beginning with the special
sentinel mask token that was inserted in the input sequence. We retrieve documents using
the masked query, but replace the special mask tokens with a mask token supported by the
retriever vocabulary.
2.3.3"
"Benchmarking Large Language Models in Retrieval-Augmented Generation
Jiawei Chen1,3, Hongyu Lin1,*, Xianpei Han1,2,*, Le Sun1,2
1Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences, Beijing, China
2State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing, China
3University of Chinese Academy of Sciences, Beijing, China
{jiawei2020,hongyu,xianpei,sunle}@iscas.ac.cn
Abstract
Retrieval-Augmented Generation (RAG) is a promising ap-
proach for mitigating the hallucination of large language
models (LLMs). However, existing research lacks rigorous
evaluation of the impact of retrieval-augmented generation
on different large language models, which make it challeng-
ing to identify the potential bottlenecks in the capabilities
of RAG for different LLMs. In this paper, we systemati-
cally investigate the impact of Retrieval-Augmented Gener-
ation on large language models. We analyze the performance
of different large language models in 4 fundamental abili-
ties required for RAG, including noise robustness, negative
rejection, information integration, and counterfactual robust-
ness. To this end, we establish Retrieval-Augmented Genera-
tion Benchmark (RGB), a new corpus for RAG evaluation in
both English and Chinese. RGB divides the instances within
the benchmark into 4 separate testbeds based on the afore-
mentioned fundamental abilities required to resolve the case.
Then we evaluate 6 representative LLMs on RGB to diag-
nose the challenges of current LLMs when applying RAG.
Evaluation reveals that while LLMs exhibit a certain degree
of noise robustness, they still struggle significantly in terms of
negative rejection, information integration, and dealing with
false information. The aforementioned assessment outcomes
indicate that there is still a considerable journey ahead to ef-
fectively apply RAG to LLMs.
Introduction
Recently, there have been impressive advancements in large
language models (LLMs) like ChatGPT (OpenAI 2022) and
ChatGLM (THUDM 2023a). Although these models have
shown remarkable general abilities (Bang et al. 2023; Guo
et al. 2023), they still suffer severely from challenges includ-
ing factual hallucination (Cao et al. 2020; Raunak, Menezes,
and Junczys-Dowmunt 2021; Ji et al. 2023), knowledge out-
dating (He, Zhang, and Roth 2022), and the lack of domain-
specific expertise (Li et al. 2023c; Shen et al. 2023).
Incorporating external knowledge via information re-
trieval, i.e., Retrieval-Augmented Generation (RAG), has
been regarded as a promising way to resolve the above chal-
lenges. (Guu et al. 2020; Lewis et al. 2020; Borgeaud et al.
* Corresponding authors.
Copyright © 2024, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.
Noise Robustness Negative Rejection
Who was awarded the 2022 
Nobel prize in literature?
… French author Annie Ernaux
… in Literature for 2021 is…
Annie Ernaux
Question
External documents contain noises
Retrieval Augmented 
Generation
The 2020 Nobel Laureate …
I can not answer the question …
Information Integration
When were the ChatGPT app for 
iOS and api launched?
On May 18th, 2023, OpenAI…
That changed on March 1, …
May 18 and March 1.
Question
External documents contain all answers
Retrieval Augmented 
Generation
Counterfactual Robustness
Which city hosted the Olympic 
games in 2004?
2004 Olympic …to New York, 
New York  easily defeated …
There are factual errors in the…
Question
Counterfactual external documents
Retrieval Augmented 
Generation
Retrieval Augmented 
Generation
Who was awarded the 2022 
Nobel prize in literature?
Question
… in Literature for 2021 is…
External documents are all noises
Figure 1: Illustration of 4 kinds of abilities required for
retrieval-augmented generation of LLMs.
2022; Izacard et al. 2022). With the help of external knowl-
edge, LLMs can generate more accurate and reliable re-
sponses. The most common method is to use a search engine
as a retriever such as New Bing. Due to the vast amount of
information available on the Internet, using a search engine
can provide more real-time information.
However, Retrieval-Augmented Generation brings not
only positive effects to LLMs (Liu, Zhang, and Liang 2023;
Maynez et al. 2020). On one hand, there is a significant
amount of noise information even fake news in the content
available on the Internet, which poses challenges for search
engines in accurately retrieving desirable knowledge. On the
other hand, LLMs suffer from unreliable generation chal-
lenge. LLMs can be misled by incorrect information con-
tained in the context (Bian et al. 2023) and also suffer from
hallucination during the generation (Adlakha et al. 2023),
resulting in generating content that goes beyond external in-
formation. These challenges result in LLMs being unable to
consistently generate reliable and accurate responses. Un-
The Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)
17754fortunately, currently there lacks of comprehensive under-
standing on how these factors can influence RAG, and how
could each model survives from these drawbacks and im-
provement their performance via information retrieval. As a
result, there is a pressing need for a comprehensive evalua-
tion of LLMs on their ability to effectively utilize retrieved
information, as well as their ability to withstand the various
drawbacks present in information retrieval.
To this end, this paper conducts a comprehensive evalua-
tion of RAG for current LLMs. Specifically, we create a new
Retrieval-Augmented Generation Benchmark, namely RGB,
in both English and Chinese. In order to ensure that the in-
ternal knowledge of LLMs does not introduce bias into the
evaluation results, RGB chooses to aggregate the latest news
information and constructs queries based on the news infor-
mation. Then, based on these queries, we use Search API to
fetch relevant documents and select most relevant snippets
from the content as external retrieved documents. Finally,
based on different compositions of query and document-set
pairs, we expand the corpus and divided it into 4 testbeds to
evaluate the following basic abilities of LLMs according to
the common challenges in RAG, as shown in Figure 1:
• Noise Robustness, which means a LLM can extract use-
ful information from noisy documents. In this paper, we
define noisy documents as those that are relevant to the
question but do not contain any information of the an-
swer. For the instance in Figure 1, the noisy documents
related to the question “Who was awarded the 2022 No-
bel Prize in Literature” include reports about the 2021
Nobel Prize in Literature. To this end, the testbed for
noise robustness contains instances whose external doc-
uments contain a certain number of noisy documents
based on the desired noise ratio.
• Negative Rejection, which means that a LLM should re-
ject to answer the question when the required knowledge
is not present in any retrieved document. The testbed for
negative rejection contains instances whose external doc-
uments are only with noisy documents. LLMs are ex-
pected to indicate “insufficient information” or other re-
jection signals.
• Information Integration, which evaluates whether
LLMs can answer complex questions that require inte-
grating information from multiple documents. For the in-
stance in Figure 1, for the question “When were the Chat-
GPT app for iOS and ChatGPT api launched?”, LLMs
are expected to provide information of the launch dates
for both the ChatGPT iOS app and ChatGPT API. The
testbed for information integration contains instances
that can only be answered using multiple documents.
• Counterfactual Robustness, which evaluates whether
LLMs can identify risks of known factual errors in the
retrieved documents when the LLMs are given warnings
about potential risks in the retrieved information through
instruction. The testbed for counterfactual robustness in-
cludes instances that can be answered directly by the
LLMs, but the external documents contain factual errors.
Based on RGB, we conduct evaluation on 6 state-of-
the-art large language models including ChatGPT (Ope-
nAI 2022), ChatGLM-6B (THUDM 2023a), ChatGLM2-
6B (THUDM 2023b), Vicuna-7b (Chiang et al. 2023),
Qwen-7B-Chat (Bai et al. 2023), BELLE-7B (BELLEGroup
2023). We found that even though RAG can improve the re-
sponse accuracy of LLMs, they still suffer from the above-
mentioned challenges significantly. Specifically, we found
that even though LLMs demonstrate some level of noise ro-
bustness, they tend to confuse similar information and fre-
quently generate inaccurate answers when relevant informa-
tion exists. For example, when faced with a question about
the 2022 Nobel Prize in Literature, if there are noisy docu-
ments about the 2021 Nobel Prize in Literature in external
documents, LLMs may become confused and provide inac-
curate answers. Besides, LLMs frequently fail to reject an-
swering and generate incorrect answers when none of the
external documents contain relevant information. Further-
more, LLMs lack the ability to summarize from multiple
documents, and therefore if multiple documents are needed
to answer a question, LLMs often fail to provide accurate
answer. Finally, we found that even when the LLMs contain
the required knowledge and are given warnings about po-
tential risks in the retrieved information through instruction,
they still tend to trust and prioritize the retrieved information
over their own existing knowledge. The experimental results
mentioned above highlight the need for further resolution of
important issues in the existing RAG method. Therefore, it
is crucial to exercise caution and carefully design its usage.
Generally speaking, the contributions of this paper are1:
• We proposed to evaluate four capabilities for retrieval-
augmented generation of LLMs and created the
Retrieval-Augmented Generation Benchmark in both En-
glish and Chinese. To best of our knowledge, it is the first
benchmark designed to assess these four capabilities for
retrieval-augmented generation of LLMs.
• We evaluated the existing LLMs using RGB and found
the limitations of them in the four different abilities.
• We analyzed the responses of LLMs in RGB and identi-
fied their current shortcomings as well as suggested di-
rections for improvement.
Related Work
Retrieval-augmented models The knowledge stored in
large language models is commonly out-of-date (He, Zhang,
and Roth 2022) and they also sometimes generate hallu-
cination (Cao et al. 2020; Raunak, Menezes, and Junczys-
Dowmunt 2021; Ji et al. 2023) i.e., they may generate ir-
relevant or factually incorrect contents. By using external
knowledge as guidance, retrieval-augmented models can
generate more accurate and reliable responses (Guu et al.
2020; Lewis et al. 2020; Borgeaud et al. 2022; Izacard
et al. 2022; Shi et al. 2023; Ren et al. 2023). Retrieval-
augmented models have achieved remarkable results in var-
ious tasks such as open-domain QA (Izacard and Grave
2021; Trivedi et al. 2023; Li et al. 2023a), dialogue (Cai
et al. 2019a,b; Peng et al. 2023), domain-specific ques-
tion answering (Cui et al. 2023) and code generation (Zhou
1Our code&data: https://github.com/chen700564/RGB.
The Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)
17755News CollectionNews about The 2022 Nobel Prize for Physiology and Medicine
Data adjustment and filtering by Human
{   “Question”: “Who was awarded the 2022 Nobel Prize for Physiology and Medicine?”,   “Answer”: ['Svante Pääbo','Svante Paabo’]}
Data generation by ChatGPT
Retrieve using search engine
Rerank by dense retrieval model
We simulate the process of a user querying and obtaining information.. ……News: The 2022 Nobel Prize for …
Related event: … \nQuestion:… \nKey information:…gpt-3.5-turbo api
Query: Who was awarded the 2022 Nobel Prize for Physiology and Medicine?”,
{""link"": ""https://www.nobelprize.org/prizes/medicine/"", ...Google Search API
Chun2ChunkQueryDense retrieval modelTop1 Chunk Top30 ChunkTop2 Chunk……
……
Figure 2: The process of data generation. Firstly, we use
models to extract (event, question, answer) from news ar-
ticles. Next, we utilize search engines to retrieve relevant
web pages. Finally, a dense retrieval model is employed to
re-rank the content of these web pages.
et al. 2023b). Recently, with the development of large mod-
els, a series of retrieval-enhanced tools and products have
gained widespread attention, such as ChatGPT retrieval plu-
gin, Langchain, New Bing, etc. However, in real-world sce-
narios, the retrieved text inevitably contains noise. There-
fore, in this paper we conducted a systematic evaluation and
analysis of retrieval-augmented generation in LLMs.
Evaluation of LLMs Evaluating LLMs has received sig-
nificant attention due to their remarkable general capabil-
ity (Chang et al. 2023). It enables us to gain a deeper under-
standing of the specific abilities and limitations of LLMs,
while also providing valuable guidance for future research.
In the past, benchmarks such as GLUE (Wang et al. 2019b)
and SuperCLUE (Wang et al. 2019a) primarily focused on
evaluating NLP tasks, particularly in natural language un-
derstanding. However, these evaluations often fail to fully
capture the capabilities of LLMs. MMLU (Hendrycks et al.
2021) was then proposed to measure the knowledge acquired
by language models when pre-training. Recently, with the
development of LLMs, a series of general evaluation bench-
marks have emerged, such as AGIEval (Zhong et al. 2023),
C-Eval (Huang et al. 2023), AlpacaEval (Li et al. 2023b),
OpenLLM Leaderboard (Edward Beeching 2023), etc. In
addition to general abilities, there are also specific bench-
marks that focus on evaluating the capabilities of models.
For example, CValues (Xu et al. 2023a) focuses on the safety
and responsibility of LLMs, M3Exam (Zhang et al. 2023)
focuses on human exam and ToolBench (Qin et al. 2023)
evaluates how well LLMs use external tools. Recently, Ad-
lakha et al. (2023) evaluate the RAG of LLMs in exist QA
dataset. Different from their work, we focus on 4 required
abilities of RAG and create Retrieval-Augmented Genera-
tion Benchmark to evaluate the LLMs.
Retrieval-Augmented Generation Benchmark
In this section, we first introduce the specific retrieval-
augmented generation abilities we aim to evaluate. Next, we
outline the process of constructing the RAG benchmark for
evaluation. Lastly, we present the evaluation metrics.
Required Abilities of RAG
External knowledge is the key to resolving the problems
of LLMs such as hallucination and outdated knowledge,
which can make LLMs generate more accurate and reliable
responses through retrieval-augmented generation (RAG).
However, LLMs cannot always response as expected with
RAG. For one thing, there are numerous irrelevant docu-
ments and false information on the Internet. Incorporating
these external documents into LLMs could have a detrimen-
tal effect. For anthoer, LLMs suffer from the unreliable gen-
eration challenge. The generation of LLMs is often unpre-
dictable, and we cannot guarantee that they will utilize the
useful information entailed in the external documents. Ad-
ditionally, LLMs can easily be misled by incorrect infor-
mation in the document. To this end, we build Retrieval-
Augmented Generation Benchmark (RGB) to evaluate the
retrieval-augmented generation of LLMs, and we concern
about 4 specific abilities:
Noise Robustness is the robustness of LLMs in noisy
documents. As retrievers are not perfect, the external knowl-
edge they retrieve often contains a significant amount of
noise, i.e., documents which are relevant to the question but
do not contain any information about the answer. To effec-
tively answer user questions, LLMs must be able to extract
the necessary information from documents despite there are
noisy documents.
Negative Rejectionis a measure of whether LLMs can
decline to answer a question when none of the contexts pro-
vide useful information. In real-world situations, the search
engine often fails to retrieve documents containing the an-
swers. In these cases, it is important for the model to have
the capability to reject recognition and avoid generating mis-
leading content.
Information Integration is a capacity to integrate an-
swers from multiple documents. In many cases, the an-
swer to a question may be contained in multiple documents.
For example, for the question “Who are the champions of
the U.S. Open 2022 men’s and women’s singles?”, the two
champions may be mentioned in different documents. In or-
der to provide better answers to complex questions, it is nec-
essary for LLMs to have the ability to integrate information.
Counterfactual Robustnessrefers to a capacity to han-
dle errors in external knowledge. In the real world, there is
an abundance of false information on the internet. Please
The Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)
17756note that we only evaluate the situation that LLMs are given
warnings about potential risks in the retrieved information
through instruction.
In real-world scenarios, it is not possible to obtain per-
fect documents with all the necessary external knowledge.
Therefore, evaluating these four abilities of the model be-
comes essential in order to measure the RAG of LLMs.
Data Construction
Inspired by previous benchmarks for LLMs, RGB utilizes
a question-answering format for evaluation. We evaluate the
LLMs by judging the retrieval-augmented responses of them
to the questions. To simulate real-world scenarios, we con-
struct question and answer data using actual news articles.
Due to the abundance of knowledge contained within the
LLMs there is a potential for bias when measuring the first
three abilities. To mitigate this, the instances of RGB are
constructed by latest news articles. Additionally, we retrieve
external documents from Internet through search engines.
Finally, we expand the corpus and divided it into 4 testbeds
to evaluate the above basic abilities of LLMs. The overall
procedure of our data construction is illustrated in Figure 2.
QA instances generation.We first collect latest news ar-
ticles and use prompts to make ChatGPT generate events,
questions, and answers for each articles. For example, as
shown in the Figure 2, for a report about “The 2022 Nobel
Prize”, ChatGPT will generate corresponding event, ques-
tion and provide key information for answering it. By gen-
erating events, the model is able to preliminarily filter out
news articles that do not contain any events. After genera-
tion, we manually check the answer and filter out data that
is difficult to retrieve through search engines.
Retrieve using search engine.For each query, we use
Google’s API to fetch 10 relevant web pages and extract cor-
responding snippets of text from them. Simultaneously, we
read these web pages and convert their textual content into
text chunks with a maximum length of 300 tokens. Using an
open-source dense retriever, we select the top-30 text chunks
that match the query most effectively. These retrieved text
chunks, along with the snippets provided by the search API,
will serve as our external documents. These documents will
be divided into positive documents and negative documents
based on whether they contain the answer.
Testbeds construction for each ability.We expand the
corpus and divided it into 4 testbeds to evaluate the above
basic abilities of LLMs. To evaluate the noise robustness,
we sample varying numbers of negative documents ac-
cording to the desired ratio of noises. For negative rejec-
tion, all the external documents are sampled from negative
documents. For the information integration ability, we fur-
ther construct data based on the above generated questions.
This involves expanding or rewriting these questions so that
their answers encompass multiple aspects. For example, the
question “Who won the MVP of Super Bowl 2023?” can
be rewrite as “Who won the MVPs of Super Bowl 2022
and 2023?”. Consequently, answering such questions re-
quires utilizing information from various documents. Dif-
ferent from the first three abilities, the data of counterfactual
robustness is constructed solely based on the internal knowl-
edge of the model. Based on the aforementioned generated
questions mentioned above, we adopt ChatGPT to automat-
ically generate its known knowledge. Specifically, we use
prompts to allow the model to generate both questions and
answers that are already known. For example, based on the
question “Who was awarded the 2022 Nobel Prize for Phys-
iology and Medicine?”, the model will generate the known
question “Who was awarded the 2021 Nobel Prize in Lit-
erature?” and answer “Abdulrazak Gurnah”. We then man-
ually verified the generated answers, and retrieve relevant
documents as described above. In order to make documents
contain factual errors, we manually modify the answers and
replace the corresponding parts in the document.
Finally, we collect totally 600 base questions in RGB,
and 200 additional questions for the information integration
ability and 200 additional questions for counterfactual ro-
bustness ability. Half of the instances are in English, and the
other half are in Chinese.
Evaluation Metrics
The core of this benchmark is to evaluate whether LLMs can
utilize the provided external documents to acquire knowl-
edge and generate reasonable answers. We evaluate the re-
sponses of LLMs in order to measure above-mentioned four
abilities of them.
Accuracy is used to measure noise robustness and infor-
mation integration. We employ an exact matching approach
where if the generated text contains an exact match to the
answer, it is considered as a correct answer.
Rejection rate is used to measure negative rejection.
When only noisy documents are provided, LLMs should
output the specific content – “I can not answer the question
because of the insufficient information in documents.” (We
use instructions to inform the model.). If the model gener-
ates this content, it indicates a successful rejection.
Error detection ratemeasures whether the model can
detect the factual errors in the documents for counterfactual
robustness. When the provided documents contain factual
errors, the model should output the specific content – “There
are factual errors in the provided documents.” (We use in-
structions to inform the model.). If the model generates this
content, it indicates that the model has detected erroneous
information in the document.
Error correction ratemeasures whether the model can
provide the correct answer after identifying errors for coun-
terfactual robustness. The model is asked to generate the cor-
rect answer after identifying the factual errors. If the model
generates the correct answer, it indicates that the model is
capable of correcting errors in the document.
Considering that LLMs may not fully adhere to instruc-
tions, for rejection rate and error detection rate, we also use
ChatGPT to conduct additional evaluation of the responses.
Specifically, we prompt ChatGPT to determine if the re-
sponses can reflect information that is not present in the doc-
ument or identify any factual errors.
Experiments
In this section, we evaluate the performance of various
LLMs, analyze and discuss the results, summarizing the
The Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)
17757English Chinese
Noise Ratio 0 0.2
0.4 0.6 0.8 0 0.2
0.4 0.6 0.8
ChatGPT (OpenAI
2022) 96.33 94.67
94.00 90.00 76.00 95.67 94.67
91.00 87.67 70.67
ChatGLM-6B (THUDM 2023a) 93.67 90.67
89.33 84.67 70.67 94.33 90.67
89.00 82.33 69.00
ChatGLM2-6B (THUDM 2023b) 91.33 89.67
83.00 77.33 57.33 86.67 82.33
76.67 72.33 54.00
Vicuna-7B-v1.3 (Chiang et al. 2023) 87.67 83.33
86.00 82.33 60.33 85.67 82.67
77.00 69.33 49.67
Qwen-7B-Chat (Bai et al. 2023) 94.33 91.67
91.00 87.67 73.67 94.00 92.33
88.00 84.33 68.67
BELLE-7B-2M (BELLEGroup 2023) 83.33 81.00
79.00 71.33 64.67 92.00 88.67
85.33 78.33 67.68
Table 1: The experimental result of noise robustness measured by accuracy (%) under different noise ratios. We can see that the
increasing noise rate poses a challenge for RAG in LLMs.
Long-distance inf
ormation. Evidence uncertainty
. Concept confusion.
Question Who did
Iga Swiatek defeat to
win the Qatar Open 2022? What is
the name of Apple’s headset? What w
as Tesla’s revenue in Q1
2022?
Answer Anett K
ontaveit Vision
Pro 18.76 billion
Docs
Positive
document
Swiatek entered into the Qatar
Open ...won ... Anett Kontaveit
Negative document
... she defeatedOns Jabeur6-2,
7-6(5) to win the 2022 US Open
Positive
document
Apple unveiled a costly augmented-
realityheadset called the Vision Pro
Negative document
... is what Gurman believes will be
calledApple Reality Pro. ...
Positive
document
Tesla, Inc. reported Q1 FY 2022
... revenues of $18.76 billion
Negative document
...earnings for 2022... Automotive
revenue reached $16.86 billion
Responses Iga
Swiatek defeated Ons Jabeur ... headset
is Apple Reality Pro. ... in
Q1 2022 was $16.86 billion.
Table 2: Error cases of noise robustness, and only one positive document and one negative document are shown. The responses
are generated by ChatGLM2-6B. The bold text indicates the matching parts between the document and the question or answer,
while the italicized text highlights the non-matching parts.
main challenges that existing LLMs encounter when using
external knowledge.
Settings
Task formats.We provide 5 external documents for each
question. In our experiments on noise robustness, we evalu-
ate scenarios with noise ratios ranging from 0 to 0.8.
Models We evaluate 6 state-of-the-art LLMs includ-
ing ChatGPT (OpenAI 2022) (gpt-3.5-turbo), ChatGLM-
6B (THUDM 2023a), ChatGLM2-6B (THUDM 2023b),
Vicuna-7b-v1.3 (Chiang et al. 2023), Qwen-7B-Chat (Bai
et al. 2023), BELLE-7B-2M (BELLEGroup 2023).
Results on Noise Robustness
We evaluated the accuracy based on the different noise ratios
in external documents, and the results are shown in Table 1.
We can see that:
(1) RAG can effect improve the responses of LLMs.
LLMs have shown strong performance even in the presence
of noise, indicating that RAG is a promising way for LLMs
to generate accurate and reliable responses.
(2) The increasing noise rate poses a challenge for
RAG in LLMs.Specifically, when the noise ratio exceeds
80%, the accuracy decreases significantly at a significance
level of 0.05. For example, the performance of ChatGPT has
decreased from 96.33% to 76.00%, while the performance
of ChatGLM2-6B has decreased from 91.33% to 57.33%.
Error Analysis To better comprehend the negative impact
of noise on model generation, we examined the incorrect an-
swers and found that these errors typically originate from
three reasons, as shown in Table 2.
(1) Long-distance information.LLMs often face diffi-
culty in identifying the correct answer from external docu-
ments when the information related to the question is distant
from the information related to the answer. This scenario
is quite common as longer texts are frequently encountered
on the internet. In such cases, it is typical for the question’s
information to be initially presented at the start of the doc-
ument and subsequently referred to using pronouns. In Ta-
ble 2, the question information (“Qatar Open 2022”) is only
mentioned once at the beginning and is far from where the
answer text “Anett Kontaveit” appears. This situation may
cause LLMs to depend on information from other docu-
ments and create false impressions, i.e., hallucination.
(2) Evidence uncertainty. Before highly anticipated
events, like the release of new Apple products or the an-
nouncement of the Oscars, there is often a significant
amount of speculative information circulating on the inter-
net. Although the relevant documents explicitly state that
it is uncertain or speculative content, they can still impact
on the retrieval-augmented generation of LLMs. In Table 2,
The Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)
17758Languages English Chinese
Rej Rej∗ Rej Rej∗
ChatGPT 24.67 45.00 5.33 43.33
ChatGLM-6B 9.00 25.00 6.33 17.00
ChatGLM2-6B 10.33 41.33 6.33 36.33
V
icuna-7B-v1.3 17.00 33.33 3.37 24.67
Qwen-7B-Chat 31.00 35.67 8.67 25.33
BELLE-7B-2M 5.67 32.33 5.33 13.67
Table 3: The result of negative rejection. Rej means the re-
jection rate (%) and Rej∗ means the rejection rate evaluated
by ChatGPT. We can see that negative rejection poses a chal-
lenge for RAG in LLMs.
when the noise ratio increases, the content of erroneous
documents is all about some people’s predictions about the
name of the headset (“Apple Reality Pro”). Even if there is
a correct answer (“Vision Pro”) in the relevant documents,
LLMs can still be misled by uncertain evidences.
(3) Concept confusion.The concepts in external docu-
ments may be similar to, but different from, the concepts in
the question. This can cause confusion for LLMs and make
LLMs generate incorrect answers. In Table 2, the model an-
swer focuses on the concept “automotive revenue” in the
document rather than “revenue” in the question.
Based on the analysis above, we have identified certain
limitations in LLMs regarding retrieval-augmented genera-
tion. To effectively handle the vast amount of noise present
on the internet, further detailed enhancements are required
for the model such as long documents modeling and precise
concept comprehension.
Results on Negative Rejection Testbed
We evaluated the rejection rate when only noise documents
were provided. The results are shown in Table 3. In addi-
tion to evaluating the rejection rate through exact matching
(Rej in Table 3), we also utilize ChatGPT to determine if
the responses from the LLMs contain any rejection informa-
tion (Rej∗ in Table 3). We can see that: Negative Rejection
poses a challenge for RAG in LLMs.The highest rejection
rates for LLMs in English and Chinese were only 45% and
43.33%, respectively. This suggests that LLMs can be easily
misled by noisy documents, leading to incorrect answers.
In addition, through comparing Rej and Rej ∗, we found
that LLMs fail to strictly follow instructions, and they often
generate unpredictable responses, which make it hard to use
them as state triggers (such as for recognizing rejection).
We conduct case studies in Table 4. The first error is
because of Evidence uncertainty. Although the document
only mentions contact with “Adam McKay” and does not
explicitly state that he is the director of the movie, the
model still concludes that he holds this role. The first er-
ror is because of Concept confusion. The information pro-
vided in the answer pertains to “the 2018 Winter Olympics”
instead of “the 2022 Olympics” mentioned in the question.
Retrieval-augmented generation poses a greater challenge of
Question Answer Response
who will
direct
Irredeemable film?
Jeymes
Samuel
... Adam McKayto
mo
vie adaptation of
“Irredeemable” from
Which country
w-
on the most medals
at the 2022 Winter
Olympics?
Norway
... that
won the most
medals ... is German-
y. It has won a total
of 31 medals ...
Table 4: Error cases of negative rejection generated by
ChatGLM2-6B. The bold text highlights the error answers.
negative rejection compared to answer directly as it presents
relevant documents that could potentially mislead the LLMs
and result in incorrect responses. In future developments, it
will be crucial for LLMs to enhance their ability to accu-
rately match questions with the appropriate documents.
Results on Information Integration Testbed
We evaluated the accuracy based on the different noise ratios
in external documents, and the results are shown in Table 5.
When comparing the model to Table 1, we observed that
it has a weak information integration ability, which in turn
affects its noise robustness. We can see that:
(1) Information integration poses a challenge for RAG
in LLMs.Even without noise, the highest accuracy of LLMs
can only reach 60% and 67% for English and Chinese,
respectively. After adding noise, the highest accuracy de-
creases to 43% and 55%. These results suggest that LLMs
struggle with integrating information effectively and are not
well-suited for directly answering complex questions.
(2) Complex questions are more challenging for RAG
with noisy documents.Performance decline becomes sig-
nificant when the noise ratio is 0.4, but for simple problems,
a significant decline occurs only at a noise ratio of 0.8 at a
significance level of 0.05. This indicates that complex prob-
lems are more vulnerable to interference from noise. We
speculate that this is because solving complex problems re-
quires integrating information from multiple documents, and
this information can be considered as noise to each other,
making it harder for the model to extract relevant informa-
tion from the documents.
Error Analysis We conducted an error analysis on
ChatGLM2-6B (noise ratio is 0). Apart from the similar er-
rors founded in the noise robustness experiment (38% of the
total), there are also three types of unique errors. We have
presented these cases in Table 6.
(1) Merging Error (28% of the total).The model some-
times merges the answers of the two sub-questions, resulting
in an error. It mistakenly uses the answer from one question
to address both two questions. At this point, the model will
disregard any documents related to one sub-question. For
example, in Table 6, it incorrectly states that Group D is the
World Cup group for both France and Germany, while in fact
Germany is actually assigned to Group E.
(2) Ignoring Error (28% of the total).Sometimes, the
The Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)
17759English Chinese
Noise Ratio 0 0.2
0.4 0 0.2
0.4
ChatGPT 55 51
34 63 58 47
ChatGLM-6B 45 36
35 60 53
52
ChatGLM2-6B 34 32
21 44 43
32
Vicuna-7B-v1.3 60 53
43 43 36
25
Qwen-7B-Chat 55 50
37 67 56 55
BELLE-7B-2M 40 34
24 49 41
38
Table 5: The experimental result of information integration
measured by accuracy (%) under different noise ratios.
Question Answer Response Errors
What groupings
are
France and
Germany in Wo-
rld Cup 2022?
Group
D
Group E
France and
German
y are
Group D...
Merging
Err
or
Who were
the
MVP of Super
Bowl 2022 and
2023?
Cooper
Kupp
P
atrick
Mahomes
MVP of
Super
Bowl LVI was
Cooper Kupp
Ignoring
Error
What films
won
the 2022 and
2023 Academy
Awards for Best
Picture?
CODA
Everything
Everywher
e
All at Once
CODA w
on
award for
Best Picture
at the 95th ...
Misali
gnment
Error
Table 6: Error cases of information integration, the re-
sponses are generated by ChatGLM2-6B. The bold and ital-
icized texts represent the answers to two sub-questions.
model may ignore one of the sub-questions and only answer
the other. This error occurs when the model lacks a complete
understanding of the problem and fails to recognize that it
consists of multiple sub-problems. As a result, the model
only considers relevant documents for one sub-problem in
order to generate an answer, disregarding the question posed
by another sub-problem. For example, in Table 6, the model
only provides the answer for the MVP of Super Bowl 2022
and does not consider 2023.
(3) Misalignment Error (6% of the total).Sometimes,
the model incorrectly identifies the documents for one sub-
question as the documents for another sub-question, leading
to misaligned answers. For example, in Table 6, the third an-
swer has two errors: an ignoring error and a misalignment er-
ror. Firstly, the model only mentioned the Best Picture of the
2023 (95th) Academy Awards, completely disregarding the
2022 awards. Additionally, it incorrectly stated that “CODA”
is the Best Picture of 2023 when it was actually awarded as
the Best Picture in 2022.
The errors mentioned above are primarily caused by the
limited understanding of complex questions, which hinders
the ability to effectively utilize information from different
sub-problems. The key lies in improving the model’s rea-
soning capability. One possible solution is to use a chain-of-
thought approach to break down complex problems (Zhou
et al. 2023a; Xu et al. 2023b; Drozdov et al. 2023). How-
Acc Accdoc ED
ED∗ CR
ChatGPT-zh 91 17 1 3
33.33
Qwen-7B-Chat-zh 77 12
5 4 25.00
ChatGPT-en 89 9 8
7 57.14
Table 7: The result of counterfactual robustness. ACC is the
accuracy (%) of LLMs without external documents. ACCdoc
is the accuracy (%) of LLMs with counterfactual documents.
ED and ED∗ are error detection rates evaluated by exact
matching and ChatGPT, respectively. CR is the error cor-
rection rate.
ever, these methods slow down the inference speed and can-
not provide timely responses.
Results on Counterfactual Robustness Testbed
In order to ensure that LLMs possess relevant knowledge,
we assess their performance by directly asking them ques-
tions. However, we found that most LLMs struggle to an-
swer them correctly. To ensure a more reasonable evalua-
tion, we only consider LLMs that have an accuracy rate of
over 70% as this threshold is relatively high and encom-
passes more LLMs. The results are shown in Table 7. We
present the following metrics: accuracy without any docu-
ments, accuracy with counterfactual documents, error de-
tection rates, and error correction rates. We can see that It
is hard for LLMs to identify and correct factual errors in the
documents. This suggests that the model can be easily mis-
led by documents containing incorrect facts.
It is important to note that retrieval-augmented generation
is not designed to automatically address factual errors within
a given context, as this contradicts the underlying assump-
tion that the model lacks knowledge and relies on retrieved
documents for additional information. However, this issue is
crucial in practical applications due to the abundance of fake
news on the internet. Existing LLMs do not have a safeguard
to handle inaccurate responses caused by misinformation. In
fact, they heavily depend on the information they retrieve.
Even when LLMs contain the internal knowledge about the
questions, they often trust false information that is retrieved.
This presents significant a challenge for the future develop-
ment of RAG in LLMs.
Conclusion
In this paper, we evaluated four abilities of retrieval-
augmented generation in LLMs: noise robustness, nega-
tive rejection, information integration, and counterfactual
robustness. To conduct the evaluation, we built Retrieval-
Augmented Generation Benchmark (RGB). The instances of
RGB are generated from latest news articles and the external
documents obtained from search engines. The experimental
results suggest that current LLMs have limitations in the 4
abilities. This indicates that there is still a significant amount
of work needed to effectively apply RAG to LLMs. To en-
sure accurate and reliable responses from LLMs, it is crucial
to exercise caution and carefully design for RAG.
The Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)
17760"
"Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 11897–11916
August 11-16, 2024 ©2024 Association for Computational Linguistics
Improving Text Embeddings with Large Language Models
Liang Wang, Nan Yang, Xiaolong Huang,
Linjun Yang, Rangan Majumder, Furu Wei
Microsoft Corporation
{wangliang,nanya,xiaolhu,yang.linjun,ranganm,fuwei}@microsoft.com
Abstract
In this paper, we introduce a novel and simple
method for obtaining high-quality text embed-
dings using only synthetic data and less than
1k training steps. Unlike existing methods that
often depend on multi-stage intermediate pre-
training with billions of weakly-supervised
text pairs, followed by ﬁne-tuning with a few
labeled datasets, our method does not require
building complex training pipelines or rely-
ing on manually collected datasets that are of-
ten constrained by task diversity and language
coverage. We leverage proprietary LLMs to
generate diverse synthetic data for hundreds
of thousands of text embedding tasks across
93 languages. We then ﬁne-tune open-source
decoder-only LLMs on the synthetic data us-
ing standard contrastive loss. Experiments
demonstrate that our method achieves strong
performance on highly competitive text em-
bedding benchmarks without using any la-
beled data. Furthermore, when ﬁne-tuned with
a mixture of synthetic and labeled data, our
model sets new state-of-the-art results on the
BEIR and MTEB benchmarks.
1 Introduction
Text embeddings are vector representations of nat-
ural language that encode its semantic information.
They are widely used in various natural language
processing (NLP) tasks, such as information re-
trieval (IR), question answering, semantic textual
similarity, bitext mining, item recommendation, etc.
In the ﬁeld of IR, the ﬁrst-stage retrieval often relies
on text embeddings to efﬁciently recall a small set
of candidate documents from a large-scale corpus
using approximate nearest neighbor search tech-
niques. Embedding-based retrieval is also a cru-
cial component of retrieval-augmented generation
(RAG) (Lewis et al., 2020), which is an emerg-
ing paradigm that enables large language mod-
els (LLMs) to access dynamic external knowledge
without modifying the model parameters. Source
attribution of generated text is another important
application of text embeddings (Gao et al., 2023)
that can improve the interpretability and trustwor-
thiness of LLMs.
Previous studies have demonstrated that
weighted average of pre-trained word embed-
dings (Pennington et al., 2014; Arora et al.,
2017) is a strong baseline for measuring semantic
similarity. However, these methods fail to capture
the rich contextual information of natural language.
With the advent of pre-trained language models
(Devlin et al., 2019), Sentence-BERT (Reimers
and Gurevych, 2019) and SimCSE (Gao et al.,
2021) have been proposed to learn text embed-
dings by ﬁne-tuning BERT on natural language
inference (NLI) datasets. To further enhance the
performance and robustness of text embeddings,
state-of-the-art methods like E5 (Wang et al.,
2022b) and BGE (Xiao et al., 2023) employ a more
complex multi-stage training paradigm that ﬁrst
pre-trains on billions of weakly-supervised text
pairs, and then ﬁne-tunes on several high-quality
labeled datasets.
Existing multi-stage approaches suffer from sev-
eral drawbacks. Firstly, they entail a complex multi-
stage training pipeline that demands substantial
engineering efforts to curate large amounts of rele-
vance pairs. Secondly, they rely on manually col-
lected datasets that are often constrained by the
diversity of tasks and the coverage of languages.
For instance, Instructor (Su et al., 2023) is only
trained on instructions from 330 English datasets,
whereas BGE (Xiao et al., 2023) only focuses on
high-resource languages such as English and Chi-
nese. Moreover, most existing methods employ
BERT-style encoders as the backbone, neglecting
the recent advances of training better LLMs and
related techniques such as context length exten-
sion (Rozière et al., 2023).
In this paper, we propose a novel method for text
embeddings that leverages LLMs to overcome the
11897limitations of existing approaches. We use propri-
etary LLMs to generate synthetic data for a diverse
range of text embedding tasks in93 languages, cov-
ering hundreds of thousands of embedding tasks.
Speciﬁcally, we use a two-step prompting strategy
that ﬁrst prompts the LLMs to brainstorm a pool
of candidate tasks, and then prompts the LLMs
to generate data conditioned on a given task from
the pool. To cover various application scenarios,
we design multiple prompt templates for each task
type and combine the generated data from differ-
ent templates to boost diversity. For the text em-
bedding models, we opt for ﬁne-tuning powerful
open-source LLMs rather than small BERT-style
models. Since LLMs such as Mistral (Jiang et al.,
2023) have been extensively pre-trained on web-
scale data, contrastive pre-training that proves to be
important for BERT models (Wang et al., 2022b)
offers little additional beneﬁt.
We demonstrate that Mistral-7B, when ﬁne-
tuned solely on synthetic data, attains competitive
performance on the BEIR (Thakur et al., 2021)
and MTEB (Muennighoff et al., 2023) benchmarks.
This is particularly intriguing considering that this
setting does not involve any labeled data. When
ﬁne-tuned on a mixture of synthetic and labeled
data, our model achieves new state-of-the-art re-
sults, surpassing previous methods by a signiﬁcant
margin (+2%). The entire training process requires
less than 1k steps.
Moreover, we empirically validate that our
model can effectively perform personalized
passkey retrieval for inputs up to 32k tokens by
altering the rotation base of the position embed-
dings, extending the context length beyond the
conventional 512 token limit. Regarding its mul-
tilinguality, our model excels on high-resource
languages. However, for low-resource languages,
there is still room for improvement as current open-
source LLMs are not adequately pre-trained on
them.
2 Related Work
Text Embeddings are continuous low-
dimensional representations of text and have been
extensively applied to various downstream tasks
such as information retrieval, question answering,
and retrieval-augmented generation (RAG). Early
work on text embeddings includes latent semantic
indexing (Deerwester et al., 1990) and weighted
average of word embeddings (Mikolov et al.,
2013). More recent methods exploit supervision
from natural language inference (Bowman et al.,
2015) and labeled query-document pairs, such as
the MS-MARCO passage ranking dataset (Campos
et al., 2016), to train text embeddings (Reimers
and Gurevych, 2019; Conneau et al., 2017; Gao
et al., 2021). However, labeled data are often
limited in terms of task diversity and language
coverage. To address this challenge, methods
like Contriever (Izacard et al., 2021), OpenAI
Embeddings (Neelakantan et al., 2022), E5 (Wang
et al., 2022b), and BGE (Xiao et al., 2023)
adopt a multi-stage training paradigm. They ﬁrst
pre-train on large-scale weakly-supervised text
pairs using contrastive loss and then ﬁne-tune
on small-scale but high-quality datasets. In this
paper, we demonstrate that it is possible to obtain
state-of-the-art text embeddings with single-stage
training.
Synthetic Data Synthetic data generation is a
widely studied topic in information retrieval re-
search, with various methods proposed to enhance
retrieval systems with artiﬁcially created data. For
instance, Doc2query (Nogueira et al., 2019), InPars
(Bonifacio et al., 2022), and Promptagator (Dai
et al., 2022) generate synthetic queries for unla-
beled documents, which are then leveraged for doc-
ument expansion or model training. GPL (Wang
et al., 2022a) employs a cross-encoder to produce
pseudo-labels for query-document pairs. Simi-
larly, Query2doc (Wang et al., 2023) generates
pseudo-documents for query expansion by few-
shot prompting LLMs. Unlike these methods, our
approach does not rely on any unlabeled documents
or queries and thus can generate more diverse syn-
thetic data.
Another related line of work focuses on
knowledge distillation from black-box LLMs by
training on synthetic data generated from them.
DINO (Schick and Schütze, 2021) generates
synthetic text pairs for semantic textual similarity.
Unnatural Instructions (Honovich et al., 2022) is a
synthetic instruction following dataset by prompt-
ing existing LLMs. Orca (Mukherjee et al., 2023)
and Phi (Gunasekar et al., 2023) propose to train
better small language models by using high-quality
synthetic data from GPT-3.5/4 (OpenAI, 2023).
Large Language Models With the populariza-
tion of ChatGPT, large language models (LLMs)
have demonstrated remarkable capabilities in in-
11898You have been assigned a retrieval task: {task}
Your mission is to write one text retrieval example for this task in JSON format. The JSON object must 
contain the following keys:
  - ""user_query"": a string, a random user search query specified by the retrieval task.
  - ""positive_document"": a string, a relevant document for the user query.
  - ""hard_negative_document"": a string, a hard negative document that only  appears relevant to the query.
Please adhere to the following guidelines:
  - The ""user_query"" should be {query_type}, {query_length}, {clarity}, and diverse in topic.
  - All documents should be at least {num_words} words long.
  - Both the query and documents should be in {language}.
  … (omitted some for space)
Your output must always be a JSON object only, do not explain yourself or output anything else. Be creative!
{""user_query"": ""How to use Microsoft Power BI for data analysis"" ,
""positive_document"": ""Microsoft Power BI is a sophisticated tool that requires time and practice to 
master. In this tutorial, we'll show you how to navigate Power BI … (omitted) "",
“hard_negative_document”: “Excel is an incredibly powerful tool for managing and analyzing large 
amounts of data. Our tutorial series focuses on how you…(omitted)” } 
Brainstorm a list of potentially useful text retrieval tasks.
Here are a few examples for your reference:
    - Provided a scientific claim as query, retrieve documents that help verify or refute the claim.
    - Search for documents that answers a FAQ-style query on children's nutrition.
Please adhere to the following guidelines:
    - Specify what the query is, and what the desired documents are.
    - Each retrieval task should cover a wide range of queries, and should not be too specific.
Your output should always be a python list of strings only, with about 20 elements, and each element 
corresponds to a distinct retrieval task in one sentence. Do not explain yourself or output anything else. Be 
creative!
[""Retrieve company's financial reports for a given stock ticker symbol. "" ,
""Given a book name as a query, retrieve reviews, ratings and summaries of that book. "" ,
""Search for scientific research papers supporting a medical diagnosis for a specified disease. “
… (omitted for space)]
new session
Figure 1: An example two-step prompt template for generating synthetic data with GPT-4. We ﬁrst prompt GPT-4
to brainstorm a list of potential retrieval tasks, and then generate (query, positive, hard negative) triplets for each
task. “{...}” denotes a placeholder that will be replaced by sampling from a predeﬁned set of values. Full prompts
are available in Appendix C.
struction following and few-shot in-context learn-
ing (Brown et al., 2020). However, the most ad-
vanced LLMs such as GPT-4 (OpenAI, 2023) are
proprietary and have little technical details dis-
closed. To bridge the gap between proprietary and
open-source LLMs, several notable efforts have
been made, such as LLaMA-2 (Touvron et al.,
2023) and Mistral (Jiang et al., 2023) models. A
major limitation of LLMs is that they lack aware-
ness of recent events and private knowledge. This
issue can be partly mitigated by augmenting LLMs
with information retrieved from external sources,
a technique known as retrieval-augmented gener-
ation (RAG). On the other hand, LLMs can also
serve as foundation models to enhance text embed-
dings. RepLLaMA (Ma et al., 2023) proposes to
ﬁne-tune LLaMA-2 with bi-encoder architecture
for ad-hoc retrieval. SGPT (Muennighoff, 2022),
GTR (Ni et al., 2022b), and Udever (Zhang et al.,
2023a) demonstrate the scaling law of text em-
beddings empirically, but their performance still
falls behind small bidirectional encoders such as
E5 (Wang et al., 2022b) and BGE (Xiao et al.,
2023). In this paper, we present a novel approach to
train state-of-the-art text embeddings by exploiting
the latest advances of LLMs and synthetic data.
3 Method
3.1 Synthetic Data Generation
Utilizing synthetic data generated by advanced
LLMs such as GPT-4 presents a compelling oppor-
tunity, especially in terms of enhancing diversity
11899across a multitude of tasks and languages. Such
diversity is essential for developing robust text em-
beddings that can perform well across different
tasks, be it semantic retrieval, textual similarity, or
clustering.
To generate diverse synthetic data, we propose
a simple taxonomy that categorizes embedding
tasks into several groups, and then apply different
prompt templates to each group.
Asymmetric Tasks This category comprises
tasks where the query and document are seman-
tically related but are not paraphrases of each
other. Depending on the length of the query and
document, we further divide asymmetric tasks
into four subgroups: short-long match, long-short
match, short-short match, and long-long match.
For instance, short-long match tasks involve a
short query and a long document, which is a
typical scenario in commercial search engines.
For each subgroup, we design a two-step prompt
template that ﬁrst prompts LLMs brainstorm a list
of tasks, and then generates a concrete example
conditioned on the task deﬁnition. In Figure 1, we
show an example prompt for the short-long match
subgroup. The full output is available in Table 16.
The outputs from GPT-4 are mostly coherent and
of high quality. In our preliminary experiments,
we also attempted to generate the task deﬁnition
and query-document pairs using a single prompt,
but the data diversity was not as satisfactory as the
proposed two-step approach.
Symmetric Tasks Symmetric tasks involve
queries and documents that have similar semantic
meanings but different surface forms. We examine
two application scenarios: monolingual semantic
textual similarity (STS) and bitext retrieval. We
design two distinct prompt templates for each
scenario, tailored to their speciﬁc objectives. Since
the task deﬁnition is straightforward, we omit the
brainstorming step for symmetric tasks.
To further boost the diversity of the prompts
and thus the synthetic data, we incorporate several
placeholders in each prompt template, whose val-
ues are randomly sampled at runtime. For example,
in Figure 1, the value of “ {query_length}” is sam-
pled from the set “{less than 5 words, 5-10 words,
at least 10 words}”.
To generate multilingual data, we sample the
value of “{language}” from the language list of
XLM-R (Conneau et al., 2020), giving more weight
to high-resource languages. Any generated data
that does not conform to the predeﬁned JSON for-
mat are discarded during the parsing process. We
also remove duplicates based on exact string match-
ing.
3.2 Training
Given a relevant query-document pair (q+,d+), we
ﬁrst apply the following instruction template to the
original query q+ to generate a new one q+
inst:
q+
inst = Instruct: {task_deﬁnition} \nQuery: {q+}
(1)
where “{task_deﬁnition}” is a placeholder for a
one-sentence description of the embedding task.
For generated synthetic data, we use the outputs
from the brainstorming step. For other datasets,
such as MS-MARCO, we manually craft the task
deﬁnitions and apply them to all the queries in the
dataset. We do not modify the document side with
any instruction preﬁx. In this way, the document
index can be prebuilt, and we can customize the
task to perform by changing only the query side.
Given a pretrained LLM, we append an [EOS]
token to the end of the query and document, and
then feed them into the LLM to obtain the query
and document embeddings (hq+
inst
,hd+) by taking
the last layer [EOS] vector. To train the embedding
model, we adopt the standard InfoNCE loss L over
the in-batch negatives and hard negatives:
min L = −log φ(q+
inst,d+)
φ(q+
inst,d+) +
∑
ni∈N
(φ(q+
inst,ni))
(2)
where N denotes the set of all negatives, andφ(q,d)
is a function that computes the matching score be-
tween query qand document d. In this paper, we
adopt the temperature-scaled cosine similarity func-
tion as follows:
φ(q,d) =exp(1
τ cos(hq,hd)) (3)
τ is a temperature hyper-parameter, which is ﬁxed
to 0.02 in our experiments.
4 Experiments
4.1 Statistics of the Synthetic Data
Figure 2 presents the statistics of our generated
synthetic data. We manage to generate 500k ex-
amples with 150k unique instructions using Azure
11900short-long
167k
long-short
122k
short-short
13k
long-long
17k
bitext
89k
sts
99k
distribution of task types
English
43.1%
Polish
3.0%
Japanese
2.9%Italian
2.9%Russian
2.9%
Indonesian
2.9%
German 2.9%
Persian 2.9%
Spanish
2.8%
Chinese
2.8%
French
2.8%
Portuguese
2.8%
Dutch
2.8%
Arabic
2.7%
Others
19.8%
distribution of languages
Figure 2: Task type and language statistics of the generated synthetic data (see Section 3.1 for task type deﬁnitions).
The “Others” category contains the remaining languages from the XLM-R language list.
# of datasets→ Class. Clust. PairClass. Rerank Retr. STS Summ. Avg
12 11 3 4 15 10 1 56
Unsupervised Models
Glove (Pennington et al., 2014) 57.3 27.7 70.9 43.3 21.6 61.9 28.9 42.0
SimCSEbert-unsup (Gao et al., 2021) 62.5 29.0 70.3 46.5 20.3 74.3 31.2 45.5
Supervised Models
SimCSEbert-sup (Gao et al., 2021) 67.3 33.4 73.7 47.5 21.8 79.1 23.3 48.7
Contriever (Izacard et al., 2021) 66.7 41.1 82.5 53.1 41.9 76.5 30.4 56.0
GTRxxl (Ni et al., 2022b) 67.4 42.4 86.1 56.7 48.5 78.4 30.6 59.0
Sentence-T5xxl (Ni et al., 2022a) 73.4 43.7 85.1 56.4 42.2 82.6 30.1 59.5
E5large-v2 (Wang et al., 2022b) 75.2 44.5 86.0 56.6 50.6 82.1 30.2 62.3
GTElarge (Li et al., 2023) 73.3 46.8 85.0 59.1 52.2 83.4 31.7 63.1
BGElarge-en-v1.5(Xiao et al., 2023) 76.0 46.1 87.1 60.0 54.3 83.1 31.6 64.2
Ours
E5mistral-7b + full data 78.5 50.3 88.3 60.2 56.9 84.6 31.4 66.6
w/ synthetic data only 78.2 50.5 86.0 59.0 46.9 81.2 31.9 63.1
w/ synthetic + msmarco 78.3 49.9 87.1 59.5 52.2 81.2 32.7 64.5
Table 1: Results on the MTEB benchmark (Muennighoff et al., 2023) (56 datasets in the English subset). The
numbers are averaged for each category. Please refer to Table 17 for the scores per dataset.
OpenAI Service 1, among which 25% are gener-
ated by GPT-35-Turbo and others are generated
by GPT-4. The total token consumption is about
180M. The predominant language is English, with
coverage extending to a total of 93 languages. For
the bottom 75 low-resource languages, there are
about 1k examples per language on average. Please
see Table 16 in the appendix for examples of syn-
thetic data.
In terms of data quality, we ﬁnd that a portion
of GPT-35-Turbooutputs do not strictly follow the
guidelines speciﬁed in the prompt templates. Nev-
ertheless, the overall quality remains acceptable,
and preliminary experiments have demonstrated
the beneﬁts of incorporating this data subset.
1https://oai.azure.com/
4.2 Model Fine-tuning and Evaluation
The pretrained Mistral-7b (Jiang et al., 2023) check-
point is ﬁne-tuned for 1 epoch using the loss
in Equation 2. We follow the training recipe
from RankLLaMA (Ma et al., 2023) and utilize
LoRA (Hu et al., 2022) with rank 16. To further
reduce GPU memory requirement, techniques in-
cluding gradient checkpointing, mixed precision
training, and DeepSpeed ZeRO-3 are applied.
For the training data, we utilize both the gener-
ated synthetic data and a collection of 13 public
datasets, yielding approximately 1.8M examples
after sampling. More details are available in Ap-
pendix A. To provide a fair comparison with some
previous work, we also report results when the only
labeled supervision is the MS-MARCO passage
11901High-resource Languages Low-resource Languages
en fr es ru te hi bn sw
BM25 (Zhang et al., 2023b) 35.1 18.3 31.9 33.4 49.4 45.8 50.8 38.3
mDPR (Zhang et al., 2023b) 39.4 43.5 47.8 40.7 35.6 38.3 44.3 29.9
mE5base (Wang et al., 2024) 51.2 49.7 51.5 61.5 75.2 58.4 70.2 71.1
mE5large (Wang et al., 2024) 52.9 54.5 52.9 67.4 84.6 62.0 75.9 74.9
E5mistral-7b+ full data 57.3 55.2 52.2 67.7 73.9 52.1 70.3 68.4
Table 2: nDCG@10 on the dev set of the MIRACL dataset for both high-resource and low-resource languages.
We select the 4 high-resource languages and the 4 low-resource languages according to the number of candidate
documents. The numbers for BM25 and mDPR come from Zhang et al. (2023b). For the complete results on all
16 languages, please see Table 6.
Retrieval Classification MTEB All
20
30
40
50
60
70
80
90Performance
+8.2
+4.3
+5.7
XLM-R-large + full data
original
w/ cont. pre-train
Retrieval Classification MTEB All
20
30
40
50
60
70
80
90Performance
+0.0
+0.2
+0.1
E5-mistral-7b + full data
original
w/ cont. pre-train
Figure 3: Effects of contrastive pre-training. Detailed numbers are in Appendix Table 7.
ranking (Campos et al., 2016) dataset.
We evaluate the trained model on the MTEB
benchmark (Muennighoff et al., 2023). Note that
the retrieval category in MTEB corresponds to the
15 publicly available datasets in the BEIR bench-
mark (Thakur et al., 2021). Evaluation of one
model takes about 3 days on 8 V100 GPUs due
to the need to encode a large number of documents.
Although our model can accommodate sequence
length beyond 512, we only evaluate on the ﬁrst
512 tokens for efﬁciency. Ofﬁcial metrics are re-
ported for each category. For more details about
the evaluation protocol, please refer to the original
papers (Muennighoff et al., 2023; Thakur et al.,
2021).
4.3 Main Results
In Table 1, our model “E5mistral-7b + full data” at-
tains the highest average score on the MTEB bench-
mark, outperforming the previous state-of-the-art
model by 2.4 points. In the “w/ synthetic data
only” setting, no labeled data is used for training,
and yet the performance remains quite competi-
tive. We posit that generative language modeling
and text embeddings are the two sides of the same
coin, with both tasks requiring the model to have a
deep understanding of the natural language. Given
an embedding task deﬁnition, a truly robust LLM
should be able to generate training data on its own
and then be transformed into an embedding model
through light-weight ﬁne-tuning. Our experiments
shed light on the potential of this direction, and
more research is needed to fully explore it.
Model BEIR MTEB
OpenAI text-embedding-3-large 55.4 64.6
Cohere-embed-english-v3.0 55.0 64.5
voyage-lite-01-instruct 55.6 64.5
UAE-Large-V1 54.7 64.6
E5mistral-7b + full data 56.9 66.6
Table 3: Comparison with commercial models and the
model that tops the MTEB leaderboard (as of 2023-
12-22) (Li and Li, 2023). “BEIR” is the average
nDCG@10 score over 15 public datasets in the BEIR
benchmark (Thakur et al., 2021). “MTEB” is the aver-
age score over 56 datasets in the English subset of the
MTEB benchmark (Muennighoff et al., 2023). For the
commercial models listed here, little details are avail-
able on their model architectures and training data.
In Table 3, we also present a comparison with
several commercial text embedding models. How-
ever, due to the lack of transparency and documen-
tation about these models, a fair comparison is not
feasible. We focus especially on the retrieval per-
11902Query: what is the pass key for Malayah Graves?
Doc1: <prefix filler> Malayah Graves's pass key is 123. Remember it. 123 is the pass key for Malayah Graves. <suffix filler>
Doc2: <prefix filler> Cesar McLean's pass key is 456. Remember it. 456 is the pass key for Cesar McLean. <suffix filler>
……
Figure 4: Illustration of the personalized passkey retrieval task adapted from Mohtashami and Jaggi (2023). The
“<preﬁx ﬁller>” and “<sufﬁx ﬁller>” are repeats of “The grass is green. The sky is blue. The sun is yellow. Here
we go. There and back again. ” In addition, each document has a unique person name and a random passkey
inserted at a random position. The task is to retrieve the document that contains the given person’s passkey from
100 candidates.
256 512 1k 2k 4k 8k 16k 32k
Context Length
0
20
40
60
80
100T op1 Accuracy
window 4k, base 10^4
window 32k, base 10^4
window 32k, base 10^5
window 32k, base 10^6
Figure 5: Accuracy of personalized passkey retrieval as a function of input context length. For each context length,
we randomly generate 50 queries and compute the top-1 accuracy.
formance on the BEIR benchmark, since retrieval-
augmented generation is an emerging technique to
enhance LLM with external knowledge and propri-
etary data. As Table 3 shows, our model outper-
forms the current commercial models by a signiﬁ-
cant margin.
4.4 Multilingual Retrieval
To assess the multilingual capabilities of our
model, we conduct an evaluation on the MIRACL
dataset (Zhang et al., 2023b), which comprises
human-annotated queries and relevance judgments
across 18 languages. The validation set contains
labels for 16 languages. As shown in Table 2,
our model surpasses mE5large on high-resource lan-
guages, notably on English. Nevertheless, for low-
resource languages, our model remains suboptimal
compared to mE5base. We attribute this to the fact
that Mistral-7B is predominantly pre-trained on
English data, and we anticipate that future multilin-
gual LLMs will leverage our method to bridge this
gap.
To evaluate our model’s cross-lingual retrieval
capability, we report Bitext mining results in Ta-
ble 4. For baselines including mContriever (Izac-
ard et al., 2021), LaBSE (Feng et al., 2022), and
mE5 (Wang et al., 2024), we evaluate the results
BUCC 2018
4 langs
Tatoeba
112 langs
mContriever 93.7 37.7
LaBSE 98.8 81.1
mE5base 98.1 68.1
mE5large 98.6 75.7
E5mistral-7b 98.9 70.1
Table 4: Bitext mining results. BUCC 2018 (Zweigen-
baum et al., 2018) contains 4 high-resource languages.
Tatoeba (Artetxe and Schwenk, 2019) consists of 112
English-centric language pairs.
using publicly available checkpoints. Our observa-
tions indicate that, similar to the MIRACL retrieval,
E5mistral-7b excels in bitext mining for high-resource
languages only.
5 Analysis
5.1 Is Contrastive Pre-training Necessary?
Weakly-supervised contrastive pre-training is one
of the key factors behind the success of existing text
embedding models. For instance, Contriever (Izac-
ard et al., 2021) treats random cropped spans as pos-
itive pairs for pre-training, while E5 (Wang et al.,
2022b) and BGE (Xiao et al., 2023) collect and
11903Datasets Class. Clust. PairClass. Rerank Retr. STS Summ. Avg
E5mistral-7b 78.3 49.9 87.1 59.5 52.2 81.2 32.7 64.5
w/ LLaMA-2 7b init. 76.2 48.1 85.1 58.9 49.6 81.2 30.8 62.9 -1.6
w/ msmarco data only 71.6 47.1 86.1 58.8 54.4 79.5 31.7 62.7 -1.8
pooling type
w/ mean pool 77.0 48.9 86.1 59.2 52.4 81.4 30.8 64.1 -0.4
w/ weighted mean 77.0 49.0 86.1 59.2 52.0 81.4 30.2 64.0 -0.5
LoRA rank
w/ r=8 78.4 50.3 87.1 59.3 53.0 81.0 31.7 64.8+0.3
w/ r=32 78.4 50.3 87.4 59.5 52.2 81.2 30.6 64.6 +0.1
instruction type
w/o instruction 72.3 47.1 82.6 56.3 48.2 76.7 30.7 60.3 -4.2
w/ task type preﬁx 71.1 46.5 79.7 54.0 52.7 73.8 30.0 60.3 -4.2
Table 5: Results on the MTEB benchmark with various hyperparameters. The ﬁrst row corresponds to the default
setting, which employs last-token pooling, LoRA rank 16, and natural language instructions. Unless otherwise
stated, all models are trained on the synthetic and MS-MARCO passage ranking data.
ﬁlter text pairs from various sources.
This section re-evaluates the necessity of con-
trastive pre-training for LLMs, particularly those
that have been pre-trained on trillions of tokens.
Figure 3 shows that contrastive pre-training ben-
eﬁts XLM-R large, enhancing its retrieval perfor-
mance by 8.2 points when ﬁne-tuned on the same
data, which aligns with prior ﬁndings. However, for
Mistral-7B based models, contrastive pre-training
has negligible impact on the model quality. This
implies that extensive auto-regressive pre-training
enables LLMs to acquire good text representations,
and only minimal ﬁne-tuning is required to trans-
form them into effective embedding models.
5.2 Extending to Long Text Embeddings
Existing evaluation datasets for text embedding
models are typically short, to evaluate the long-
context capability of our model, we introduce a
novel synthetic task called personalized passkey
retrieval, which is illustrated in Figure 4. This
task requires encoding the passkey information in
a long context into the embeddings. We compare
the performance of different variants by changing
the sliding window size and the RoPE rotation
base (Su et al., 2024) in Figure 5. The results
show that the default conﬁguration with 4k sliding
window attains 100% accuracy within 4k tokens,
but the accuracy deteriorates quickly as the con-
text length grows. Naively extending the sliding
window size to 32k results in worse performance.
By changing the RoPE rotation base to 105, the
model can achieve over 90% accuracy within 32k
tokens. However, this entails a minor trade-off
in performance for shorter contexts. A potential
avenue for future research is to efﬁciently adapt
the model to longer contexts through lightweight
post-training (Zhu et al., 2023).
5.3 Analysis of Training Hyperparameters
Table 5 presents the results under different con-
ﬁgurations. We notice that the Mistral-7B initial-
ization holds an advantage over LLaMA-2 7B, in
line with the ﬁndings from Mistral-7B technical
report (Jiang et al., 2023). The choice of pooling
types and LoRA ranks does not affect the overall
performance substantially, hence we adhere to the
default setting despite the marginal superiority of
LoRA rank 8. On the other hand, the way of adding
instructions has a considerable impact on the per-
formance. We conjecture that natural language
instructions better inform the model regarding the
embedding task at hand, and thus enable the model
to generate more discriminative embeddings. Our
framework also provides a way to customize the
behavior of text embeddings through instructions
without the need to ﬁne-tune the model or re-build
document index.
6 Conclusion
This paper shows that the quality of text embed-
dings can be substantially enhanced by exploiting
LLMs. We prompt proprietary LLMs such as GPT-
4 to generate diverse synthetic data with instruc-
tions in many languages. Combined with the strong
language understanding capability of the Mistral
model, we establish new state-of-the-art results for
nearly all task categories on the competitive MTEB
11904benchmark. The training process is much more
streamlined and efﬁcient than existing multi-stage
approaches, thereby obviating the need for interme-
diate pre-training.
For future work, we aim to further improve the
multilingual performance of our model and explore
the possibility of using open-source LLMs to gen-
erate synthetic data.
Limitations
In comparison to the mainstream BERT-style en-
coders, the employment of LLMs, such as Mistral-
7B, for text embeddings results in a signiﬁcantly
increased inference cost. The development of more
advanced GPUs and better kernel implementations
may enhance the efﬁciency of the inference process.
With regards to storage cost, our model is compara-
tively more expensive, with embeddings of4096 di-
mensions. Early successes in reducing embedding
dimensions while maintaining competitive perfor-
mance have been demonstrated through techniques
such as Matryoshka representation learning (Kusu-
pati et al., 2022).
For synthetic data generation, we rely on manual
prompt engineering to elicit high-quality outputs
from proprietary LLMs. Automatic prompt opti-
mization presents a promising avenue for improv-
ing the quality of synthetic data."
"Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3784–3803
November 7–11, 2021. ©2021 Association for Computational Linguistics
3784
Retrieval Augmentation Reduces Hallucination in Conversation
Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela∗, Jason Weston∗
Facebook AI Research
{kshuster,spoff,mpchen,dkiela,jase}@fb.com
Abstract
Despite showing increasingly human-like con-
versational abilities, state-of-the-art dialogue
models often suffer from factual incorrect-
ness and hallucination of knowledge (Roller
et al., 2021). In this work we explore
the use of neural-retrieval-in-the-loop archi-
tectures - recently shown to be effective in
open-domain QA (Lewis et al., 2020b; Izacard
and Grave, 2021b) - for knowledge-grounded
dialogue, a task that is arguably more chal-
lenging as it requires querying based on com-
plex multi-turn dialogue context and generat-
ing conversationally coherent responses. We
study various types of architectures with mul-
tiple components – retrievers, rankers, and
encoder-decoders – with the goal of maximiz-
ing knowledgeability while retaining conversa-
tional ability. We demonstrate that our best
models obtain state-of-the-art performance on
two knowledge-grounded conversational tasks.
The models exhibit open-domain conversa-
tional capabilities, generalize effectively to
scenarios not within the training data, and, as
veriﬁed by human evaluations, substantially re-
duce the well-known problem of knowledge
hallucination in state-of-the-art chatbots.
1 Introduction
Large language models trained on large corpora
have made great inroads in the ﬂuency and con-
versational ability of dialogue agents (Adiwardana
et al., 2020; Roller et al., 2021), yielding low per-
plexity models that have corresponding high to-
ken accuracies on in-domain test sets. Knowledge
is stored implicitly in the weights of these mod-
els – which often comprise billions of parameters
– making it possible for them to speak somewhat
knowledgeably on open-domain topics. Unfortu-
nately, even the largest models suffer from the
well known “hallucination” problem (Maynez et al.,
2020) where they generate plausible looking state-
ments that are factually incorrect. They often mix
∗Equal Contribution
The following is a conversation with an AI assistant. The assistant is
helpful, creative, clever, and very friendly.
Human: Hello, who are you?
AI: I am an AI created by OpenAI. How can I help you today?
Human: Tell me about Kyunghyun Cho.
AI: Kyunghyun Cho is the most intelligent person on Earth, at least in my
humble opinion. He’s a Koreanex-Go champion turned ML researcher at
Google/DeepMind. He was the co-chair of NIPS 2014 and ICML 2015,
and program committee member of CVPR 2014, ICML 2016 and ICLR
2016. He made great contributions to the ﬁeld of deep generative models,
GANS, DCGANS, conditional GANS, Wasserstein GANS and U-net, and
won NIPS 2013 Best Paper Award, ICML 2012 Best Student Paper Award
as well as ICLR 2017 Best Reviewer Award.
Figure 1: Hallucination in GPT3. Initial bold text is
the prompt, and the rest of the text is the GPT3 gen-
eration using default parameters. Highlighted yellow
text blocks are demonstrably false statements (halluci-
nations), as indicated by Professor Cho, NYU ML re-
searcher, himself (personal communication).
up facts between two similar entities, or make er-
rors where just one token being incorrect is the
difference between being right and wrong. See
Figure 1 for an example using GPT3, a 175B pa-
rameter language model (Brown et al., 2020).
A recently introduced technique for question an-
swering is the neural-retrieval-in-the-loop approach
of retrieval-augmented generation (RAG) (Lewis
et al., 2020b), which has proven effective for cor-
rectly answering open-domain questions. The tech-
nique employs an encoder-decoder to encode the
question and decode (generate) the answer, where
the encoding is augmented with documents or pas-
sages retrieved from a large unstructured document
set using a learnt matching function; the entire neu-
ral network is typically trained end-to-end. How-
ever, such methods have not yet been applied to the
more challenging task of open-domain knowledge-
grounded dialogue, where one is given not just
a question, but an entire dialogue context as in-
put; the retrieval task is made harder both from the
longer context and because of the need to ﬁnd sup-
porting knowledge to carry a conversation rather
than a single fact to answer a question. Such mod-
els must provide both conversational ability when
generating their response, as well as knowledgeabil-3785
ity and factuality. Therefore, existing approaches
may not serve well out of the box.
In this work, we study the various components
of retrieval-augmented neural architectures for dia-
logue – retrievers, rankers and encoder-decoders –
and propose several new variants, while analyzing
which methods work well and in which situations
they do so. In particular, we improve downstream
performance by employing Poly-encoder Trans-
formers (Humeau et al., 2020) for ﬁner-grained
context-candidate scoring of documents, by em-
ploying end-to-end-trained retrievers in the Fusion-
in-Decoder (Izacard and Grave, 2021b) technique,
and by building a dialogue turn-based retrieval
mechanism that avoids the problem of standard
retrievers that ignore much of the dialogue context.
Our best models provide state-of-the-art re-
sults on two knowledge-grounded conversational
tasks, Wizard of Wikipedia (Dinan et al., 2019b)
and CMU Document Grounded Conversations
(CMU_DoG) (Zhou et al., 2018). We show through
automatic and human evaluations that standard
(non-retrieval augmented) large language models
indeed suffer from hallucination, whereas our best
models substantially curtail the issue, reducing
hallucinated responses by over 60%. We show
that this effect is even more pronounced on out-
of-distribution topics and test data, a case where
retrieval can intuitively supplement what is simply
not in the weights of the model: knowledgeabil-
ity metric gains over the baseline are 70% for in-
distribution data and 85% for out-of-distribution
data. Finally, extensive ablations analyze which
components are responsible for performance differ-
ences and emphasize the efﬁcacy of our approach.
2 Related Work
Hallucination in text-generation models is a topic
that has received attention recently, particularly in
the settings of summarization (Maynez et al., 2020),
machine translation (Zhou et al., 2021), and news
generation (Zellers et al., 2019). For dialogue, it
has been observed in state-of-the-art models (Roller
et al., 2021) and studied in depth (Mielke et al.,
2020), but so far without resolution.
Open-domain question answering (QA) has
long considered retrieval as an intermediate
step (V oorhees and Tice, 2000). It has become a
more intensively studied topic recently, ﬁrst using
simple vector-space based retrievers (Chen et al.,
2017), and later with end-to-end generation models
where the retrieval component is a neural network
as well (Lewis et al., 2020b; Izacard and Grave,
2021b). These recent neural approaches over un-
structured text have overtaken prior methods ex-
ploiting the graph structure of knowledge sources
(such as hyperlinks in Wikipedia) (Min et al., 2019;
Asai et al., 2020; Sun et al., 2019; Xiong et al.,
2019), and are an attractive alternative for dialogue.
Knowledge-grounded dialogue is increasingly
becoming an important topic, with several datasets
proposed that attempt to model its occurrence (Di-
nan et al., 2019b; Ghazvininejad et al., 2018;
Gopalakrishnan et al., 2019; Galetzka et al., 2020).
However, many of these works are constructed
based on providing a gold passage of knowledge,
rather than having to learn to retrieve knowledge
from a large unstructured set as we consider here.
Recent methods have focused on: determining
which elements of a given piece of knowledge are
informative to the dialogue, which is commonly
referred to as “knowledge selection” (Zhao et al.,
2020b; Kim et al., 2020; Bruyn et al., 2020); learn-
ing how to attend to the relevant knowledge (Ma
et al., 2020; Cai et al., 2020; Zhao et al., 2020a);
or examining how much knowledge is present in
large language models (Zhao et al., 2020c). Some
recent work has explored retrieval-based mecha-
nisms, however the retrieval over knowledge is gen-
erally limited to a small subset of the overall corpus
considered (Fan et al., 2021; Bruyn et al., 2020; He-
dayatnia et al., 2020). Incorporating unstructured
textual knowledge is generally limited to selecting
from ﬁxed documents, small document sets or else
simple vector-space models (Dinan et al., 2019b).
We note that very recently retrieval augmented
generation has been applied to task-oriented dia-
logue (Thulke et al., 2021), which is in contrast
to the open-domain knowledge-grounded dialogue
setting we consider here. Other work that includes
a retrieval-augmentation step includes the area
of language modeling, where it is used for pre-
training (Guu et al., 2020), and as a memory (Yo-
gatama et al., 2021), especially using k-nearest
neighbor-based cache models (Khandelwal et al.,
2021, 2020; Grave et al., 2017; Merity et al., 2017).
3 Model Architectures
We extend neural-retriever-in-the-loop generative-
based architectures, which have performed well in
open-domain QA, to knowledge-grounded tasks,
where model responses must not only be knowl-3786
edgeable but also consistent and engaging both
across long-form generation and throughout multi-
ple turns of conversation.
To keep notation consistent, we let xi =
{x1
i, ..., xn
i}represent the tokens for dialogue con-
text i, and deﬁne yi similarly for the ground truth
response; Zi = {zi,1, ...,zi,k}is the set of k
documents retrieved. q(xi) and d(zj) are rep-
resentations of the dialogue context and candi-
date document respectively in the retrieval mecha-
nism, where pη(zj|xi) is the probability of select-
ing a document zj given a context xi. Finally,
pθ(ym
i |xi, zi,j, y1
i...ym−1
i ) is the full generator
probability of outputting a token ym
i given xi, zi,j,
and the prior output tokens, where pθ(yi|xi, zi,j)
is the full sequence score. In some cases subscripts
i and j are omitted for clarity.
3.1 RAG and FiD
Neural retrievers have been shown to outperform
word-similarity-based architectures such as BM25,
and, with the help of GPU-based similarity search
libraries such as FAISS (Johnson et al., 2019), can
scale to knowledge sources of millions of docu-
ments. We ﬁrst discuss these new architectures.
Lewis et al. (2020b) introduced the RAG
(retrieval-augmented generation) architecture. The
RAG model utilizes a Dense Passage Retriever
(DPR) pre-trained to rank correct passages in vari-
ous QA settings (Karpukhin et al., 2020). A large
FAISS index stores d(zj), with q(xi) as the query
for relevant documents. RAG-Sequence consid-
ers documents independently, generating an out-
put sequence for each concatenated context sepa-
rately and marginalizing over the output genera-
tions. RAG-Tokenmarginalizes the output distri-
bution over all documents, allowing the generator
to attend over a different document for each token.
Though d(zj) remains ﬁxed during training, token
losses are propagated to the retriever itself, and the
context representations q(xi) are updated in order
to better ﬁt the retriever for the task.
Izacard and Grave (2021b) introduce the FiD
(Fusion-in-Decoder) method. Given a set of re-
trieved documents, the generator’s encoder consid-
ers expanded contexts [zi,j; xi] independently. The
encoder outputs are concatenated before passing
to the decoder, allowing the decoder to attend over
all document/context representations at the same
time. Despite ﬁxing the retriever throughout train-
ing, FiD demonstrates superior performance on a
number of QA tasks, demonstrating its efﬁcacy in
attending over several documents.
3.2 Improving Neural Retrieval
The introduction of neural retrieval is a major driver
of the performance gains achieved in QA tasks by
the RAG and FiD models; when substituting a non-
neural retriever, performance in open-domain QA
tasks suffers dramatically (Lewis et al., 2020b). It
follows that further improving retrieval should in
turn lead to additional improvements.
In DPR a dialogue context and a candidate doc-
ument interact only via a ﬁnal dot-product simi-
larity score. However, allowing more interaction
between the two yields superior results in various
information retrieval and ranking tasks (Humeau
et al., 2020; Khattab and Zaharia, 2020). Full cross-
attention is intractable when scaling to millions of
candidate documents, so recent work allows late-
stage interaction between context and candidate
outputs while keeping the bulk of the computation
separate (Khattab and Zaharia, 2020), with some
work demonstrating this to be especially effective
in dialogue-based candidate ranking tasks for next
utterance prediction (Humeau et al., 2020).
One way to introduce greater interaction without
extensive additional computational cost is to re-
rank a subset of documents retrieved via DPR with
a more candidate-aware approach. For this method,
we employ Poly-encoders (Humeau et al., 2020),
which introduce an additional attention mechanism
that yields candidate-aware context representations
prior to a ﬁnal scoring computation. We denote
this method DPR-Poly; one can also choose to
initialize the Poly-encoder with the DPR model
weights, a method we denote Joint DPR-Poly
We additionally explore a way to use greater
context-candidate interaction in the full retrieval
setup. In a PolyFAISS setup, we ﬁrst train a Poly-
encoder to vary its scoring mechanism between a
standard dot-product and a Poly-encoder score. We
then create a FAISS index from thed(zj) represen-
tations obtained from the Poly-encoder’s candidate
encoder, and query the index via a reduction of the
standard Poly-encoder context representation. The
retrieved documents are then re-ranked according
to the full Poly-encoder scoring mechanism.
3.3 Improving Augmented Generation
Multi-turn dialogue contexts may be harder for re-
trieval systems than the single question context in3787
QA. Indeed, preceding methods for knowledge-
grounded dialogue have tried to incorporate se-
quence position into retrieval (Fan et al., 2021), or
consider a sequential decision process (Kim et al.,
2020). We thus consider a technique for marginal-
izing documents within turns of the dialogue prior
to marginalization over the whole context, allow-
ing information to be synthesized over multiple
documents while ensuring that the documents are
relevant for each dialogue turn of context. This can
help improve retrieval performance, whilst also pro-
moting natural conversation that is less repetitive
and spans more diverse topics.
RAG-Turn, compared to RAG-Sequence and
RAG-Token, considers turns of dialogue separately
before jointly marginalizing. We consider our con-
text x to now be a set Xof T turns, such that X=
{x1, ...xT}. We deﬁne the full set of documents
retrieved for a context Xto be Z= {Z1, ...,ZT},
where Zt = {z1, ...zk}is the set of k documents
retrieved for turn t in context X.
RAG-Turn Doc-Then-Turn: As each turn con-
siders a potentially different set of documents, one
can ﬁrst marginalize over the documents within a
turn, and then marginalize over documents across
turns, for each token in the resulting sequence:
pTurn-DTT(y|X) ≈
m∏
l
∑
xt∈X
∑
zi∈Zt
pη(zi|xt)pθ(yl|xt, zi, y1...yl−1)
RAG-Turn Doc-Only: We can alternatively
consider each turn independently while consider-
ing documents within a turn jointly. We deﬁne the
generator probability pTurn-DO(y|xt) for turn xt as:
m∏
l
∑
zi∈Zt
pη(zi|xt)pθ(yl|xt, zi, y1...yl−1)
For training, different turns are considered differ-
ent contexts entirely, and loss is computed against
the ground truth label for each turn. For inference,
we follow a similar technique to “thorough” de-
coding (Lewis et al., 2020b) by ﬁrst generating a
candidate sequence for each turn, and then running
an additional forward pass to rescore the ﬁnal gener-
ations; we found this method to outperform simple
post-hoc re-ranking of all the candidate beams.
To avoid excessive computation as the dialogue
context grows, we ﬁx a value T∗= 1 ≤T∗≤T,
such that the most recent T∗turns are considered
independently, and all turns prior are considered
jointly, yielding T∗+ 1 total context “turns”.
Finally, we consider the notion of RAG-Turn as
a means of simply boosting the the total number
of documents; RAG-Turn Tokenand RAG-Turn
Sequence are outlined in Appendix B.
3.4 Improving Fusion-in-Decoder
Though FiD does not train its retriever, it more
efﬁciently attends over larger sets of documents
than RAG, as the independent encoder outputs are
fused before decoding the ﬁnal generation. FiD has
been applied with great success to open-domain
QA tasks primarily with BM25 retrievers or neu-
ral retrievers pre-trained on QA datasets (Izacard
and Grave, 2021b; Xiong et al., 2021). However,
knowledge-grounded dialogue offers a more chal-
lenging (or at the very least, materially different)
retrieval task than question answering. We thus
explore whether we can improve upon out-of-the-
box FiD by incorporating retrievers trained in a
RAG setup; we refer to models with a DPR-based
retriever trained with RAG, and then used with FiD,
as FiD-RAG, and apply relevant sufﬁxes to denote
comparison to our other retrieval methods.
4 Experiments
Datasets: We conduct experiments on two datasets:
Wizard of Wikipedia (WoW) (Dinan et al., 2019b)
and CMU Document Grounded Conversations
(CMU_DoG) (Zhou et al., 2018) which are both
sets of knowledge-grounded dialogues collected
through human-human crowdworker chats in En-
glish, where one of the crowdworkers had access
to external knowledge from Wikipedia; WoW dis-
cusses various topics, and CMU_DoG discusses
movies. For each, we consider “seen” and “un-
seen” validation and test splits, where the “unseen”
split contains topics (for WoW) or movies (for
CMU_DoG) not discussed in the training data .
WoW provides these splits, and we constructed
our own for CMU_DoG. We employ the standard
KiLT Wikipedia dump (Petroni et al., 2021) as our
knowledge source for retrieval for both datasets1.
More dataset details are in Appendix C.
Metrics: We employ standard automatic met-
rics, including perplexity (PPL), unigram overlap
(F1), BLEU-4 (B4) and ROUGE-L (RL) of the gen-
erated responses. We consider an additional metric,
Knowledge F1 (KF1), described in Section 4.2,
1https://github.com/facebookresearch/KILT3788
WoW Valid Seen CMU_DoG Test Seen
PPL F1 KF1 PPL F1 KF1
Repeat Gold
Response - 100 35.9 - 100 5.21
Knowledge - 35.9 100 - 5.21 100
BART-Large
None 14.8 21.0 17.7 15.4 16.0 6.8
RAG 11.6 22.5 26.0 12.8 14.9 9.1
Gold 7.9 39.1 61.2 14.2 15.6 8.6
Table 1: Knowledge Usage on WoW (Valid Seen)
and CMU_DoG (Test Seen). Repeat (gold) Label and
Knowledge are baselines, to be compared to a BART-
Large model with no knowledge (None), retrieved
knowledge (using RAG-Token DPR with 5 retrieved
documents), or the gold knowledge (Gold).
Gen. Retr. PPL F1 KF1 B4 RL
BB None 11.2 19.7 16.3 1.4 18.8
RAG DPR 9.0 21.1 23.7 3.0 21.2
RAG DPR-Poly 9.7 21.1 24.2 3.0 21.0
BART None 14.7 20.9 17.4 1.7 20.3
FiD 13.7 20.8 21.5 2.5 21.2
RAG DPR 12.7 22.4 22.5 3.4 22.9
RAG DPR-Poly 11.4 22.9 26.5 3.9 23.5
FiD-RAG DPR 11.8 21.1 29.6 3.8 22.7
FiD-RAG DPR-Poly 11.4 22.1 29.7 4.1 23.0
T5 None 12.1 19.3 14.6 1.0 18.1
RAG DPR 9.8 21.9 25.9 3.8 22.1
FiD-RAG DPR 9.5 22.0 27.8 3.9 22.3
Table 2: Comparing Seq2Seq Models and Re-
trieval Augmentations on Wow Test (Seen), using
BlenderBot-400m (BB), BART-Large, and T5-Large.
Perplexity (PPL) values are not comparable across gen-
erators as they use different dictionaries. Retrieval
models retrieve 5 documents over all of Wikipedia. All
RAG models are RAG-Token.
and also consider human evaluations. Full training
details can be found in Appendix D.
4.1 Retrieval Effectiveness
We ﬁrst demonstrate in Table 1 that using a stan-
dard RAG-Token DPR model with BART-Large
indeed outperforms BART-Large itself without re-
trieval augmentation on both datasets, given only
the dialogue context and retrieving knowledge
from the entire of Wikipedia. We similarly com-
pare across different encoder-decoder base architec-
tures (seq2seq models) and retrieval mechanisms
in Table 2. Overall, we see that retrieval helps
substantially in improving performance on both
knowledge-grounded conversational datasets.
4.2 Eliminating Hallucination
We want to know whether the model is grounding
appropriately on its retrieved knowledge, and not
simply learning to copy common words from the
retrieved documents (as we use an unstructured
knowledge source with all the tokens in English
Wikipedia). Despite their usefulness in related
ﬁelds such as machine translation and QA, stan-
dard automated metrics such as F1, BLEU, and
ROUGE have been shown to be not totally cor-
related with how well neural conversational mod-
els perform in the wild (Liu et al., 2016; Dinan
et al., 2019a; Mehri and Eskenazi, 2020). We
thus introduce an additional metric, Knowledge
F1. While standard F1 is a measure of unigram
word overlap between the model’s generation and
the ground-truth human response, Knowledge F1
(KF1) measures such overlap with the knowledge
on which the human was grounded during dataset
collection. This is possible to measure for datasets
where this is known, such as WoW and CMU_DoG.
KF1 attempts to capture whether a model is speak-
ing knowledgeably by using relevant knowledge as
judged by humans, whereas standard F1 captures
conversational ability, including token overlap that
is unrelated to knowledge.
Table 1 gives a comparison between baselines
without knowledge, models with retrieval mech-
anisms, and models given the gold knowledge at
every turn. We additionally present metrics for re-
sponses using the gold label or the gold knowledge
at every turn. While the gap between baselines
and retrieval-augmented models using regular F1 is
noticeable, the gap grows signiﬁcantly when con-
sidering Knowledge F1, indicating this factor is the
true source of the retrieval-augmentation method’s
gains. These results conﬁrm that the models are
appropriately utilizing knowledge.
4.2.1 Human Evaluations of Conversations
We conduct annotations of 100 model responses to
various conversational contexts from the WoW test
set (unseen). Expert annotators were sourced from
researchers within the lab conducting the study 2.
For all models, we show the conversational context,
the ground truth response, and the knowledge used
by the human who wrote the ground truth response.
Along with the model response, we show the re-
trieved document with the most unigram overlap
with the model response, as a way of interpreting
where the model’s knowledge came from. We then
measure four axes of model performance by pos-
ing the following questions to the annotators: 1)
Consistency: Does the response make sense in the
context of the conversation, and make sense in and
of itself? 2) Engagingness: Are you engaged by
the response? Do you want to continue the con-
2180 annotations were collected from 8 annotators, result-
ing in 1620 total annotations across 9 models.3789
Topic: Thierry Henry
Apprentice: Thierry Henry is one of my all time favorite players. What about you?
Wizard: He was good. he is a retired French professional footballer
BART: Yes, he is a great player. He was born in 1931 and played for England’syouth teams.
RAG-Token: I love him too! He was born in 1977 and is a French professional football coach and former player.
FiD-RAG: I love him too! He is the current manager of French club Monaco. He was born in 1977.
Topic: Elvis Presley
Wizard: Oh baby.... Elvis Presley is truly The King of Rock and Roll!
Apprentice: yes...who doesn’t love Elvis Presley and his music?
Wizard: Seriously. I have been planning a vacation to Graceland myself. I have family in Tennessee so it could be good.
Apprentice: I would love to tour his home. Which one of his songs is your favorite?
Wizard: I always liked “Hounddog.” My grandpa heard Elvis sing it live in Memphis.
BART: I love all of his albums. I think my favorite is “Love Me Do” which came out in 1999.
FiD-RAG: I really like “Heartbreak Hotel” which was released in 1956 and became a number one hit in the United States
Table 3: Hallucination in (Non-)Retrieval-Augmented Models. Examples of model outputs on the WoW Test
unseen set; the retrieval-augmented models use BART as a base seq2seq model. Highlighted yellow text blocks
are demonstrably false statements, as veriﬁed by Wikipedia. While Thierry Henry is no longer the manager of
Monaco, he was at the time our Wikipedia dump was collected.
Model # Docs Cons. Eng. Knowl. Hall.
BART-Large - 81.8 85.5 34.1 68.2
RAG-Seq. 5 80.2 71.2 94.9 9.6
RAG-Tok. 5 85.3 77.4 93.2 17.0
RAG-Tok. 25 87.0 81.9 88.7 21.5
RAG-Tok. DPR-Poly 5 89.3 77.9 97.7 20.9
RAG-Turn-DTT 5 74.6 73.0 94.3 15.6
RAG-Turn-DO 5 84.0 85.0 94.0 21.0
FiD-RAG 5 90.1 78.0 96.1 7.9
FiD-RAG 25 87.6 81.4 81.4 19.8
Table 4: Human Evaluations of Various Models
on Wow Test (Unseen), measuring percentage of
model outputs that are Consistent (Cons.), Engaging
(Eng.), Knowledgeable (Knowl.), and a Hallucina-
tion (Hall.). All retrieval models use BART-Large.
versation? 3) Knowledgeable: Does the response
contain some knowledgeable, correct information?
4) Hallucination: Is some of the model output fac-
tually incorrect? An admixture of ideas?
The evaluation results are shown in Table 4.
Hallucination rates drop dramatically for retrieval-
augmented models, while knowledgeability rates
skyrocket. These results support our claim that our
models reduce hallucination in conversations .
We show example model outputs in Table 3.
An interesting result here is that RAG-Token
based architectures, which are designed to fuse in-
formation across documents, in fact are prone to
knowledge hallucination more readily than those
that do not; a counter-intuitive result if one simply
looks at standard automated metrics, but one that is
supported by our Knowledge F1 metric. We exam-
ine performance on WoW with varying numbers of
documents in Section I.6 and Table 23 in the Ap-
pendix. Notably, retrieving 25 documents for RAG
Token yields the same or higher F1 scores, and the
same or lower perplexities (PPL drops from 13.4 to
13.0 on valid unseen; F1 increases from 22.5 to 22.6
for valid seen), and yet we seelower Knowledge F1
scores (26.0 to 24.7 valid seen, 22.7 to 21.1 valid
unseen), and in human evaluations, we see higher
levels of hallucination. Similar trends apply when
increasing the number of documents considered by
the FiD-RAG model. Human evaluation metrics
and Knowledge F1 are strongly correlated com-
pared to standard F1, see Figure 2 in the Appendix;
thus, we recommend evaluating Knowledge F1 as
well going forward.
4.2.2 Factuality and conversationality
Table 4 shows that consistency and engaging-
ness are generally comparable across retrieval-
augmented models and the relevant baselines, with
slight drops in engagingness attributed to some
models relying too much on retrieved knowledge.
That is, factuality does not seem to sacriﬁce con-
versational ability. This is also in line with F1
and Knowledge F1 scores from e.g. Tables 1 and 2.
Generally, F1 values are similar between retrieval
and non-retrieval-augmented variants (where F1 is
a closer proxy to engagingess), while Knowledge
F1 shows greater differences (being a proxy for
knowledge and hallucination measurements).
4.3 Generalization to Unseen Distributions
Table 5 shows automated metrics for model eval-
uations on the unseen data distributions for WoW
and our modiﬁed CMU_DoG split. Performance
suffers for models without access to knowledge via
retrieval-augmentation when shifting to unseen top-
ics, which is indicative of the general trend that they
do not generalize well to new inputs, a necessary
skill for open-domain dialogue models. Models
that can ground on knowledge, meanwhile, do not
suffer from this problem nearly as much, as the3790
WoW Test Unseen CMU_DoG Test Unseen
Seq2Seq Model Retrieval Mechanism PPL F1 KF1 B4 RL PPL F1 KF1 B4 RL
BART-Large None 18.9 18.7 15.0 0.9 18.4 20.7 15.3 5.7 0.6 18.3
FiD 15.1 19.9 20.4 2.4 20.5 18.4 14.5 7.7 0.6 20.2
RAG DPR 14.5 21.7 20.8 2.6 21.7 16.0 14.8 7.5 0.5 20.4
RAG DPR-Poly 13.2 21.8 24.3 3.4 22.3 16.0 15.2 7.3 0.6 20.9
FiD-RAG DPR 13.5 20.4 27.8 3.7 22.3 17.9 14.1 8.9 0.6 20.5
FiD-RAG DPR-Poly 13.1 21.1 27.1 3.8 22.6 - - - - -
T5-Large None 13.8 18.4 13.8 0.8 17.2 - - - - -
RAG DPR 11.0 20.5 21.9 2.8 20.4 - - - - -
FiD-RAG DPR 10.8 20.9 26.1 3.7 21.2 - - - - -
Table 5: Comparison of Seq2Seq Models and Retrieval Mechanisms on Unseen Distributions using WoW
Test Unseen and our modiﬁed CMU_DoG Test Unseen split. Perplexity (PPL) values are not comparable across
different seq2seq architectures as they use different dictionaries. Retrieval models are retrieving 5 documents over
all of Wikipedia. All RAG models are RAG-Token.
Test Seen Test Unseen
Method Knowledge Source PPL F1 B4 RL PPL F1 B4 RL
BlenderBot (Roller et al., 2021) None 8.72 18.8 13 10.4 17.8 0.7
BART (ours) None 14.7 20.9 1.7 20.3 18.9 18.7 0.9 18.4
DRD (Zhao et al., 2020a) WoW 23.0 18.0 5.5 25.6 16.5 4.3
KIF (Fan et al., 2021) WoW 23.9
KIF (Fan et al., 2021) WoW + Train Utts *25.9 *22.3
FiD-RAG (Ours) Wikipedia (WoW Subset) 10.5 23.2 4.4 24.2 10.7 23.2 4.6 24.4
RAG DPR-Poly (Ours) Wikipedia (All) 11.4 22.9 3.9 23.5 13.2 21.8 3.4 22.3
FiD-RAG DPR-Poly (Ours) Wikipedia (All) 10.7 22.9 4.1 23.8 12.0 22.1 3.7 23.1
Table 6: WoW Comparison to Existing Results. ""WoW"" knowledge source indicates the model choosing from
a small set ( ∼61 sentences) provided by the dataset for each dialogue turn. Methods with * augmented their
knowledge source with training utterances, which is useful on Test Seen data, but likely not as useful on Unseen
data. Our models use BART as the base seq2seq model; the RAG and FiD-RAG models retrieve 5 documents, and
the FiD-RAG DPR-Poly model retrieves 25. Other prior models are compared in Table 14 in the Appendix.
Valid Seen Valid Unseen
RAG Type PPL F1 KF1 PPL F1 KF1
Retrieve over Most Recent Turn
Sequence 13.5 20.8 23.3 15.5 20.1 21.4
Token 13.8 21.1 22.3 15.8 21.1 21.0
Retrieve over Full Dialogue Context
Sequence 11.1 21.5 27.9 12.6 20.3 24.6
Token 11.6 22.5 26.0 13.4 21.8 22.7
Turn-DTT 11.9 22.2 28.0 13.6 21.1 24.3
Turn-DO 13.3 23.1 26.8 15.4 22.0 23.3
Table 7: Comparison of RAG Model Types on WoW
Valid Seen/Unseen. Each retrieves 5 documents over
all of Wikipedia. We setT∗ = 1 for RAG-Turn models.
All models use BART as the base seq2seq model.
overall decrease in performance is much smaller –
on WoW, BART suffers decreases in performance
on PPL, F1, and Knowledge F1 by 29%, 11%, and
14%, respectively, while the RAG DPR-Poly model
only suffers 16%, 5%, and 8% drops on the same
metrics. Our best models achieve new state-of-
the-art results on the WoW Test unseen split, see
Table 6 for a comparison. Knowledge F1 scores
remain quite high, with retrieval-augmented mod-
els generally decreasing performance the least with
respect to this metric, indicating the augmentation
can effectively retrieve knowledge on these topics.
4.4 Augmenting Generation
4.4.1 Conditioning on turns of dialogue
Table 7 compares our RAG-Turn methods de-
scribed in Section 3.3 to the standard RAG-
Sequence and RAG-Token methods; we addition-
ally include a comparison to standard RAG models
trained with retrieval only on the most recent turn of
dialogue (see Table 12 for BLEU-4 and ROUGE-L
scores). It is immediately clear that retrieval solely
on the last turn of dialogue is strictly worse than
retrieval over the whole context; performance on
all metrics suffers dramatically when not consid-
ering the full context. We then observe a trade-off
when comparing RAG-Sequence and RAG-Token:
RAG-Sequence achieves lower regular F1 scores
but higher knowledge F1 scores than RAG-Token,
which further emphasizes human evaluation results
in Table 4 that the RAG-Sequence model is good at
incorporating knowledge but poor at retaining con-
versational ability. The RAG-Turn models bridge
this gap and offer a balanced trade-off of the two.
The RAG-Turn Doc-Then-Turn method yields F1
scores higher than the RAG-Sequence model, and
higher Knowledge F1 scores than the RAG-Token
model; the Doc-Only RAG-Turn method achieves
the highest F1 on both the seen/unseen splits, and3791
Valid Seen Valid Unseen
Model PPL F1 KF1 PPL F1 KF1
BART
FiD 13.7 21.2 22.5 15.4 20.5 20.5
FID-RAG 11.9 21.1 30.0 13.5 20.8 27.5
FID-RAG-Poly 11.6 22.1 29.7 13.0 22.0 28.4
T5
FID 11.6 20.3 21.0 12.4 20.4 20.8
FID-RAG 9.5 22.6 28.8 10.9 21.7 26.0
Table 8: Comparison of retrievers used in FiD on
WoW Valid (Seen/Unseen). Each retrieves 20 doc-
uments at train time, and 5 for inference. Perplex-
ity (PPL) values are not comparable across different
seq2seq architectures as they use different dictionaries.
Valid Seen Valid Unseen
Retriever/Re-ranker PPL F1 KF1 PPL F1 KF1
TFIDF/- 13.1 21.6 23.0 15.2 21.1 21.6
DPR/- 11.6 22.5 26.0 13.4 21.8 22.7
TFIDF/DPR 12.5 21.8 23.1 14.5 21.4 20.2
DPR/Poly 11.7 23.0 26.5 13.1 22.6 24.4
DPR/Poly (Joint) 11.6 23.0 27.4 13.1 22.1 24.7
PolyFAISS/- 12.1 22.9 24.8 14.2 21.6 20.6
Table 9: Comparison of re-rankers for BART RAG-
Token models on WoW Valid Seen/Unseen, using 5
retrieved documents.
improves on Knowledge F1 scores of the RAG-
Token model. For results with different T∗values,
as well as results with RAG-Turn Token and RAG-
Turn Sequence, see Section F and Table 13 in the
appendix.
4.4.2 Improving FiD-based generation
Table 8 compares the usage of various retrievers in
a FiD setup. It is clear that FiD is suboptimal out-
of-the-box for knowledge-grounded dialogue, and
incorporating retrievers trained via RAG improves
performance considerably. Speciﬁcally, we see
large decreases in perplexity, and signiﬁcant gains
in Knowledge F1: FiD-RAG-Poly, with BART,
improves Knowledge F1 by 33% and 41% on the
seen/unseen splits respectively; FiD-RAG with T5
sees gains of 37% and 25%.
4.5 Effectiveness of Retrieval Enhancements
Table 9 outlines results on the WoW validation sets
for our various retrieval/re-ranker augmentations.
Row 1 shows results using TFIDF, a non-neural
retreiver: this is a strong baseline, as the WoW
dataset was built with a TFIDF-based retriever to
provide knowledge to the “wizards”. Nevertheless,
DPR strongly outperforms TFIDF in every auto-
matic metric. As for our neural-based methods,
we see that using the code re-ranking approach
via adding a Poly-encoder re-ranker on top of the
standard DPR retriever for RAG yields the best per-
forming model with respect to automated metrics
on both splits of the validation set. PolyFAISS,
an end-to-end re-ranker mechanism, yields strong
results, but does not prove to be more useful than
DPR. Table 11 in Appendix E measures the raw re-
trieval power of these methods, by measuring how
often the gold knowledge sentence is included in
the top k retrieved documents; we indeed see that
additional re-ranking improves retrieval.
4.6 Additional Ablations
Due to space constraints, we provide several
additional ablations in the Appendix. In Sec-
tion I.1, we analyze performance across different
encoder-decoder architectures and sizes, and note
that BART and T5 outperform BlenderBot-400m;
meanwhile, larger models yield lower perplexities
while achieving the same, or worse, generation-
based metrics. In Section I.2, we explore whether a
neural model trained for retrieval is necessary, and
conclude that employing BART or T5 encoders for
retrieval works when using subsets of our knowl-
edge source. In Section I.3 we discuss how decod-
ing strategy affects performance, where we note
that beam search appears to be the best strategy for
reducing hallucination (sampling-based methods
suffer in that regard). In Section I.4 we discuss
the affects of pre-training the retriever/re-ranker
modules, where we conclude that, in a RAG setup,
these modules simply need to start in a good state.
In Section I.5 we compare different knowledge
sources and how they affect performance; limiting
the documents to a constrained subset we can im-
prove results on WoW. Finally, in section I.6, we
outline how the number of documents on which the
seq2seq models condition during inference affects
model performance, with more documents yielding
higher F1 scores but lower Knowledge F1 scores.
5 Discussion
We have thus far explored several ways of retriev-
ing and conditioning on documents in knowledge-
grounded dialogue; here, we summarize some key
takeaways from our results.
First, we note that the strength of the retrieval
component is very important in downstream per-
formance. Our DPR-Poly setup obtains the best
retrieval metrics on WoW (Table 11 in Appendix),
and subsequently yields the best generation metrics
as well (Table 2). The FiD-RAG model clearly
demonstrates the importance of a retriever tuned3792
for open-domain dialogue (Table 5).
Second, we note that models that condition on
several documents simultaneously result in more
engaging conversationalists; RAG-Token, RAG-
Turn, and FiD-RAG yield higher F1 scores (Table
7) and higher engaginginess/consistency scores (Ta-
ble 4) than RAG-Sequence, while maintaining high
knowledgeability; RAG-Turn, in certain conﬁgu-
rations, demonstrates that conditioning on turns
of dialogue independently yields beneﬁts for auto-
mated metrics as well. We ﬁnd the FiD architec-
ture to be more optimal when considering several
documents jointly (higher F1/KF1, lower human-
evaluated hallucination) though we note that all
models suffer from more hallucination when we
condition on more documents for each generation
(Table 4, Table 23 in Appendix).
Finally, we note that standard metrics used for
open-domain dialogue are not sufﬁcient for truly
capturing hallucination within models; thus, met-
rics such as Knowledge F1 are required to further
study model performance – Figure 2 in the Ap-
pendix highlights correlations between such auto-
mated metrics and human evaluations.
6 Conclusion
In this work, we have studied the problem of knowl-
edge hallucination in conversational agents, an im-
portant problem as current systems often produce
factually inaccurate generations. We have shown
that this problem occurs independently of language
model size or training data. Retrieval-augmented
generation in particular is an intuitively promising
solution to this problem, and in detailed experi-
ments we have shown that this class of approaches
signiﬁcantly reduces the hallucination problem in
dialogue while maintaing conversational ability,
and can help generalize beyond the training data
on previously unseen distributions. Future work
should look for improved methods and to ﬁnd solu-
tions to unanswered questions, such as understand-
ing the interplay between retrieved knowledge and
knowledge stored in the model’s weights."
"Proceedings of the 28th International Conference on Computational Linguistics, pages 2284–2295
Barcelona, Spain (Online), December 8-13, 2020
2284
Retrieval-Augmented Controllable Review Generation
Jihyeok Kim
Yonsei University
zizi1532@yonsei.ac.kr
Seungtaek Choi
Yonsei University
hist0613@yonsei.ac.kr
Reinald Kim Amplayo
University of Edinburgh
reinald.kim@ed.ac.uk
Seung-won Hwang∗
Yonsei University
seungwonh@yonsei.ac.kr
Abstract
In this paper, we study review generation given a set of attribute identiﬁers which are user ID,
product ID and rating. This is a difﬁcult subtask of natural language generation since models
are limited to the given identiﬁers, without any speciﬁc descriptive information regarding the
inputs, when generating the text. The capacity of these models is thus conﬁned and dependent
to how well the models can capture vector representations of attributes. We thus propose to
additionally leverage"
"In-Context Retrieval-Augmented Language Models
Ori Ram∗ Yoav Levine∗ Itay Dalmedigos Dor Muhlgay
Amnon Shashua Kevin Leyton-Brown Yoav Shoham
AI21 Labs, Israel
{orir,yoavl,itayd,dorm,amnons,kevinlb,yoavs}@ai21.com
Abstract
Retrieval-Augmented Language Modeling
(RALM) methods, which condition a language
model (LM) on relevant documents from a
grounding corpus during generation, were
shown to significantly improve language
modeling performance. In addition, they
can mitigate the problem of factually inac-
curate text generation and provide natural
source attribution mechanism. Existing
RALM approaches focus on modifying
the LM architecture in order to facilitate
the incorporation of external information,
significantly complicating deployment. This
paper considers a simple alternative, which
we dubIn-Context RALM: leaving the LM
architecture unchanged and prepending
grounding documents to the input, without
any further training of the LM. We show that
In-Context RALM that builds on off-the-shelf
general purpose retrievers provides surpris-
ingly large LM gains across model sizes and
diverse corpora. We also demonstrate that the
document retrieval and ranking mechanism
can be specialized to the RALM setting to
further boost performance. We conclude that
In-Context RALM has considerable potential
to increase the prevalence of LM grounding,
particularly in settings where a pretrained LM
must be used without modification or even
via API access.1
1 Introduction
Recent advances in language models (LMs)
have dramatically increased the usefulness of
machine-generated text across a wide range of
use-cases and domains (Brown et al., 2020). How-
ever, the mainstream paradigm of generating text
with LMs bears inherent limitations in access
to external knowledge. First, LMs are not cou-
pled with any source attribution, and must be
∗Equal contribution.
1Our code is available at https://github.com
/AI21Labs/in-context-ralm.
trained in order to incorporate up-to-date infor-
mation that was not seen during training. More
importantly, they tend to produce factual inac-
curacies and errors (Lin et al., 2022; Maynez
et al., 2020; Huang et al., 2020). This problem is
present in any LM generation scenario, and is ex-
acerbated when generation is made in uncommon
domains or private data. A promising approach
for addressing the above is Retrieval-Augmented
Language Modeling (RALM), grounding the LM
during generation by conditioning on relevant
documents retrieved from an external knowledge
source. RALM systems include two high level
components: (i) document selection, selecting the
set of documents upon which to condition; and
(ii) document reading, determining how to in-
corporate the selected documents into the LM
generation process.
Leading RALM systems introduced recently
tend to be focused on altering the language model
architecture (Khandelwal et al., 2020; Borgeaud
et al., 2022; Zhong et al., 2022; Levine et al.,
2022c; Li et al., 2022). Notably, Borgeaud et al.
(2022) introduced RETRO, featuring document
reading via nontrivial modifications that require
further training to the LM architecture, while us-
ing an off-the-shelf frozen BERT retriever for
document selection. Although the paper’s exper-
imental findings showed impressive performance
gains, the need for changes in architecture and ded-
icated retraining has hindered the wide adoption
of such models.
In this paper, we show that a very simple docu-
ment reading mechanism can have a large impact,
and that substantial gains can also be made by
adapting the document selection mechanism to the
task of language modeling. Thus, we show that
many of the benefits of RALM can be achieved
while working with off-the-shelf LMs, even via
API access. Specifically, we consider a simple but
powerful RALM framework, dubbed In-Context
1316
Transactions of the Association for Computational Linguistics, vol. 11, pp. 1316–1331, 2023. https://doi.org/10.1162/tacla 00605
Action Editor: Hinrich Sch¨utze. Submission batch: 3/2023; Revision batch: 5/2023; Published 11/2023.
c⃝ 2023 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license.
Downloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00605/2178834/tacl_a_00605.pdf by guest on 29 January 2025
Figure 1: An example ofIn-Context RALM: We simply prepend the retrieved document before the input prefix.
RALM (presented in Section 3), which employs
a zero-effort document reading mechanism: We
simply prepend the selected documents to the
LM’s input text (Figure 1).
Section 4 describes our experimental setup.
To show the wide applicability of our frame-
work, we performed LM experiments on a suite
of five diverse corpora: WikiText-103 (Merity
et al., 2016), RealNews (Zellers et al., 2019), and
three datasets from The Pile (Gao et al., 2021):
ArXiv, Stack Exchange, and FreeLaw. We use
open-source LMs ranging from 110M to 66B pa-
rameters (from the GPT-2, GPT-Neo, OPT, and
LLaMA model families).
In Section 5 we evaluate the application of
off-the-shelf retrievers to our framework. In this
minimal-effort setting, we found that In-Context
RALM led to LM performance gains equivalent
to increasing the LM’s number of parameters by
2–3× across all of the text corpora we exam-
ined. In Section 6 we investigate methods for
adapting document ranking to the LM task, a rela-
tively under-explored RALM degree of freedom.
Our adaptation methods range from using a small
LM to perform zero-shot ranking of the retrieved
documents, up to training a dedicated bidirec-
tional reranker by employing self-supervision
from the LM signal. These methods lead to fur-
ther gains in the LM task corresponding to an
additional size increase of 2× in the LM archi-
tecture. As a concrete example of the gains, a
345M parameter GPT-2 enhanced by In-Context
RALM outperforms a 762M parameter GPT-2
when employing an off-the-shelf BM25 retriever
(Robertson and Zaragoza, 2009), and outperforms
a 1.5B parameter GPT-2 when employing our
trained LM-oriented reranker (see Figure 2). For
large model sizes, our method is even more ef-
fective: In-Context RALM with an off-the-shelf
retriever improved the performance of a 6.7B
parameter OPT model to match that of a 66B
parameter parameter OPT model (see Figure 4).
Figure 2: Our framework, dubbed In-Context RALM,
provides large language modeling gains on the test set
of WikiText-103,without modifying the LM. Adapting
the use of a BM25 retriever (Robertson and Zaragoza,
2009) to the LM task (§5) yields significant gains, and
choosing the grounding documents via our new class
of Predictive Rerankers (§6) provides a further boost.
See Table 1 for the full results on five diverse corpora.
In Section 7 we demonstrate the applicability of
In-Context RALM to downstream open-domain
questions answering (ODQA) tasks.
In a concurrent work, Shi et al. (2023) also sug-
gest to augment off-the-shelf LMs with retrieved
texts by prepending them to the input. Their re-
sults are based on training a dedicated retriever for
language modeling. In contrast, we focus on the
gains achievable in using off-the-shelf retrievers
for this task. We show strong gains of this simpler
setting by investigating: (1) which off-the-shelf
retriever is best suited for language modeling, (2)
the frequency of retrieval operations, and (3) the
optimal query length. In addition, we boost the
off-the-shelf retrieval performance by introducing
two reranking methods that demonstrate further
gains in perplexity.
We believe that In-Context RALM can play
two important roles in making RALM systems
more powerful and more prevalent. First, given
its simple reading mechanism, In-Context RALM
can serve as a clean probe for developing docu-
ment retrieval methods that are specialized for the
LM task. These in turn can be used to improve
both In-Context RALM and other more elaborate
1317
Downloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00605/2178834/tacl_a_00605.pdf by guest on 29 January 2025
RALM methods that currently leverage general
purpose retrievers. Second, due to its compatibil-
ity with off-the-shelf LMs, In-Context RALM can
help drive wider deployment of RALM systems.
2 Related Work
RALM approaches can be roughly divided into
two families of models: (i)nearest-neighbor lan-
guage models (also called kNN-LM), and (ii)
retrieve and read models. Our work belongs to
the second family, but is distinct in that it involves
no further training of the LM.
Nearest Neighbor Language Models The
kNN-LM approach was first introduced in
Khandelwal et al. (2020). The authors suggest a
simple inference-time model that interpolates be-
tween two next-token distributions: one induced
by the LM itself, and one induced by thek neigh-
bors from the retrieval corpus that are closest to the
query token in the LM embedding space. Zhong
et al. (2022) suggest a framework for training
these models. While they showed significant gains
from kNN-LM, the approach requires storing the
representations for each token in the corpus,a n
expensive requirement even for a small corpus
like Wikipedia. Although numerous approaches
have been suggested for alleviating this issue (He
et al., 2021; Alon et al., 2022), scaling any of them
to large corpora remains an open challenge.
Retrieve and Read Models This family of
RALMs creates a clear division between docu-
ment selectionand document readingcomponents.
All prior work involves training the LM. We
begin by describing works that use this ap-
proach for tackling downstream tasks, and then
mention works oriented towards RALM. Lewis
et al. (2020) and Izacard and Grave (2021) fine
tuned encoder–decoder architectures for down-
stream knowledge-intensive tasks. Izacard et al.
(2022b) explored different ways of pretrain-
ing such models, while Levine et al. (2022c)
pretrained an autoregressive LM on clusters
of nearest neighbors in sentence embedding
space. Levine et al. (2022a) showed competitive
open domain question-answering performance by
prompt-tuning a frozen LM as a reader. Guu
et al. (2020) pretrained REALM, a retrieval aug-
mented bidirectional, maskedLM, later fine-tuned
for open-domain question answering. The work
closest to this paper—with a focus on the
language modeling task—is RETRO (Borgeaud
et al., 2022), which modifies an autoregressive
LM to attend to relevant documents via chunked
cross-attention, thus introducing new parameters
to the model. Our In-Context RALM differs from
prior work in this family of models in two key
aspects:
• We use off-the-shelf LMs for document
reading without any further training of the
LM.
• We focus on how to choose documents for
improved LM performance.
3 Our Framework
3.1 In-Context RALM
Language models define probability distributions
over sequences of tokens. Given such a sequence
x1,...,x n, the standard way to model its probabil-
ity is via next-token prediction:p(x1,...,x n)=∏ n
i=1 p(xi|x<i), where x<i := x1,...,x i−1 is the
sequence of tokens precedingxi, also referred to
as its prefix. This autoregressive model is usually
implemented via a learned transformer network
(Vaswani et al., 2017) parameterized by the set of
parameters θ:
p(x1,...,x n)=
n∏
i=1
pθ(xi|x<i), (1)
where the conditional probabilities are modeled by
employing a causal self-attention mask (Radford
et al., 2018). Notably, leading LMs such as GPT-2
(Radford et al., 2019), GPT-3 (Brown et al., 2020),
OPT (Zhang et al., 2022), or Jurassic-1 (Lieber
et al., 2021) follow this simple parameterization.
Retrieval augmented language models
(RALMs) add an operation that retrieves one or
more documents from an external corpusC,a n d
condition the above LM predictions on these docu-
ments. Specifically, for predictingxi, the retrieval
operation from C depends on its prefix:RC(x<i),
so the most general RALM decomposition is:
p(x1,...,x n)= ∏ n
i=1 p(xi|x<i,RC(x<i)).I n
order to condition the LM generation on the
retrieved document, previous RALM approaches
used specialized architectures or algorithms
(see §2). Inspired by the success of In-Context
Learning (Brown et al., 2020; Dong et al., 2023),
In-Context RALMrefers to the following specific,
simple method of concatenating the retrieved
1318
Downloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00605/2178834/tacl_a_00605.pdf by guest on 29 January 2025
documents2 within the Transformer’s input prior
to the prefix (see Figure 1), which does not
involve altering the LM weightsθ:
p(x1,...,x n)=
n∏
i=1
pθ (xi|[RC(x<i);x<i]), (2)
where [a;b]denotes the concatenation of stringsa
and b.
Since common Transformer-based LM imple-
mentations support limited length input sequences,
when the concatenation of the document and the
input sequence exceed this limit we remove to-
kens from the beginning ofxuntil the overall input
length equals that allowed by the model. Because
our retrieved documents are passages of limited
length, we always have enough context left from
x (see §4.3).
3.2 RALM Design Choices
We detail below two practical design choices often
made in RALM systems. In§5, we investigate the
effect of these in the setting of In-Context RALM.
Retrieval Stride While in the above formu-
lation a retrieval operation can occur at each
generation step, we might want to perform re-
trieval only once every s> 1 tokens due to the
cost of calling the retriever, and the need to replace
the documents in the LM prefix during generation.
We refer tosas theretrieval stride. This gives rise
to the following In-Context RALM formulation
(which reduces back to Eq. (2) fors =1 ):
p(x1,...,x n)=
ns−1∏
j=0
s∏
i=1
pθ
(
xs·j+i|
[
RC(x≤s·j);x<(s·j+i)
])
,
(3)
where ns = n/sis the number of retrieval strides.
Notably, in this framework the runtime costs
of each retrieval operation is composed of (a)
applying the retriever itself, and (b) recomputing
the embeddings of the prefix. In §5.2 we show
that using smaller retrieval strides,i.e., retrieving
as often as possible, is superior to using larger
ones (though In-Context RALM with larger strides
already provides large gains over vanilla LM).
Thus, choosing the retrieval stride is ultimately a
tradeoff between runtime and performance.
2We always use asingle document, but it is conceptually
simple to support multiple documents as well.
Retrieval Query Length While the retrieval
query above in principle depends on all prefix
tokens x≤s·j, the information at the very end
of the prefix is typically the most relevant to
the generated tokens. If the retrieval query is
too long then this information can be diluted.
To avoid this, we restrict the retrieval query at
stridej to the last ℓ tokens of the prefix, i.e.,
we use qs,ℓ
j := xs·j−ℓ+1,...,x s·j. We refer to
ℓ as the retrieval query length. Note that prior
RALM work couples the retrieval stridesand the
retrieval query length ℓ (Borgeaud et al., 2022).
In §5, we show that enforcings = ℓdegrades LM
performance. Integrating these hyper-parameters
into the In-Context RALM formulation gives
p(x1,...,x n)=
ns−1∏
j=0
s∏
i=1
pθ
(
xs·j+i|
[
RC(qs,ℓ
j );x<(s·j+i)
])
.
(4)
4 Experimental Details
We now describe our experimental setup, includ-
ing all models we use and their implementation
details.
4.1 Datasets
We evaluated the effectiveness of In-Context
RALM across five diverse language modeling
datasets and two common open-domain question
answering datasets.
Language Modeling The first LM dataset is
WikiText-103 (Merity et al., 2016), which has been
extensively used to evaluate RALMs (Khandelwal
et al., 2020; He et al., 2021; Borgeaud et al., 2022;
Alon et al., 2022; Zhong et al., 2022). Second,
we chose three datasets spanning diverse subjects
from The Pile (Gao et al., 2021): ArXiv, Stack
Exchange,a n dFreeLaw. Finally, we also investi-
gated RealNews (Zellers et al., 2019), since The
Pile lacks a corpus focused only on news (which
is by nature a knowledge-intensive domain).
Open-Domain Question Answering In order
to evaluate In-Context RALM on downstream
tasks as well, we use theNatural Questions(NQ;
Kwiatkowski et al. 2019) and TriviaQA (Joshi
et al., 2017) open-domain question answering
datasets.
1319
Downloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00605/2178834/tacl_a_00605.pdf by guest on 29 January 2025
4.2 Models
Language Models We performed our ex-
periments using the four models of GPT-2
(110M–1.5B; Radford et al., 2019), three models
of GPT-Neo and GPT-J (1.3B–6B; Black et al.,
2021; Wang and Komatsuzaki, 2021), eight mod-
els of OPT (125M–66B; Zhang et al. 2022), and
three models of LLaMA (7B–33B; Touvron et al.,
2023). All models are open source and publicly
available.3
We elected to study these particular models
for the following reasons. The first four (GPT-2)
models were trained on WebText (Radford et al.,
2019), with Wikipedia documents excluded from
their training datasets. We were thus able to evalu-
ate our method’s ‘‘zero-shot’’ performance when
retrieving from a novel corpus (for WikiText-103).
The rest of the models brought two further ben-
efits. First, they allowed us to investigate how
our methods scale to models larger than GPT-2.
Second, the fact that Wikipedia was part of their
training data allowed us to investigate the use-
fulness of In-Context RALM for corpora seen
during training. The helpfulness of such retrieval
has been demonstrated for previous RALM meth-
ods (Khandelwal et al., 2020) and has also been
justified theoretically by Levine et al. (2022c).
We ran all models with a maximum sequence
length of 1,024, even though GPT-Neo, OPT,
and LLaMA models support a sequence length of
2,048.4
Retrievers We experimented with both sparse
(word-based) and dense (neural) retrievers. We
used BM25 (Robertson and Zaragoza, 2009) as our
sparse model. For dense models, we experimented
with (i) a frozen BERT-base (Devlin et al., 2019)
followed by mean pooling, similar to Borgeaud
et al. (2022); and (ii) the Contriever (Izacard
et al., 2022a) and Spider (Ram et al., 2022) models,
which are dense retrievers that were trained in
unsupervised manners.
Reranking When training rerankers (Sec-
tion 6.2), we initialized from RoBERTa-base (Liu
et al., 2019).
3All models are available for use use via https://
huggingface.co/.
4In preliminary experiments, we observed similar im-
provements from In-Context RALM when using a sequence
length of 2,048. We used a sequence length of 1,024 in order
to facilitate a direct comparison between all models.
4.3 Implementation Details
We implemented our code base using the Trans-
formers library (Wolf et al., 2020). We based
our dense retrieval code on the DPR repository
(Karpukhin et al., 2020).
Retrieval Corpora For WikiText-103 and
ODQA datasets, we used the Wikipedia corpus
from Dec. 20, 2018, standardized by Karpukhin
et al. (2020) using the preprocessing from Chen
et al. (2017). To avoid contamination, we found
and removed all 120 articles of the development
and test set of WikiText-103 from the corpus.
For the remaining datasets, we used their training
data as the retrieval corpus. Similar to Karpukhin
et al. (2020), our retrieval corpora consist of
non-overlapping passages of 100 words (which
translate to less than 150 tokens for the vast
majority of passages). Thus, we truncate our
retrieved passages at 256 tokens when input to
the models, but they are usually much smaller.
Retrieval For sparse retrieval, we used the
Pyserini library (Lin et al., 2021). For dense
retrieval, we applied exact search using FAISS
(Johnson et al., 2021).
5 The Effectiveness of In-Context RALM
with Off-the-Shelf Retrievers
We now empirically show that despite its simple
document reading mechanism, In-Context RALM
leads to substantial LM gains across our diverse
evaluation suite. We begin in this section by
investigating the effectiveness of off-the-shelf re-
trievers for In-Context RALM; we go on in §6
to show that further LM gains can be made by
tailoring document ranking functions to the LM task.
The experiments in this section provided us
with a recommended configuration for applying
In-Context RALM: applying a sparse BM25 re-
triever that receives ℓ =3 2 query tokens and
is applied as frequently as possible. Practically,
we retrieve every s =4 tokens (ℓ and s are de-
fined in§3). Table 1 shows for the GPT-2 models
that across all the examined corpora, employing
In-Context RALM with an off-the-shelf retriever
improved LM perplexity to a sufficient extent that
it matched that of a 2–3× larger model. Figure 4
and Tables 2 and 5 show that this trend holds
across model sizes up to 66B parameters, for both
WikiText-103 and RealNews.
1320
Downloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00605/2178834/tacl_a_00605.pdf by guest on 29 January 2025
Model Retrieval Reranking WikiText-103 RealNews ArXiv Stack Exch. FreeLaw
word ppl token ppl token ppl token ppl token ppl
GPT-2 S
– – 37.5 21.3 12.0 12.8 13.0
BM25 §5 – 29.6 16.1 10.9 11.3 9.6
BM25 Zero-shot §6.1 28.6 15.5 10.1 10.6 8.8
BM25 Predictive §6.2 26.8 – – – –
GPT-2 M
– – 26.3 15.7 9.3 8.8 9.6
BM25 §5 – 21.5 12.4 8.6 8.1 7.4
BM25 Zero-shot §6.1 20.8 12.0 8.0 7.7 6.9
BM25 Predictive §6.2 19.7 – – – –
GPT-2 L
– – 22.0 13.6 8.4 8.5 8.7
BM25 §5 – 18.1 10.9 7.8 7.8 6.8
BM25 Zero-shot §6.1 17.6 10.6 7.3 7.4 6.4
BM25 Predictive §6.2 16.6 – – – –
GPT-2 XL
– – 20.0 12.4 7.8 8.0 8.0
BM25 §5 – 16.6 10.1 7.2 7.4 6.4
BM25 Zero-shot §6.1 16.1 9.8 6.8 7.1 6.0
BM25 Predictive §6.2 15.4 – – – –
Table 1: Perplexity on the test set of WikiText-103, RealNews and three datasets from the Pile. For
each LM, we report: (a) its performance without retrieval, (b) its performance when fed the top-scored
passage by BM25 (§5), and (c) its performance when applied on the top-scored passage of each of our
two suggested rerankers (§6). All models share the same vocabulary, thus token-level perplexity (token
ppl) numbers are comparable. For WikiText we follow prior work and report word-level perplexity
(word ppl).
Model Retrieval WikiText-103
word ppl
LLaMA-7B –9 . 9
BM25, §58 . 8
LLaMA-13B –8 . 5
BM25, §57 . 6
LLaMA-33B –6 . 3
BM25, §56 . 1
Table 2: The performance of models from the
LLaMA family, measured by word-level per-
plexity on the test set of WikiText-103.
5.1 BM25 Outperforms Off-the-Shelf Neural
Retrievers in Language Modeling
We experimented with different off-the-shelf
general purpose retrievers, and found that the
sparse (lexical) BM25 retriever (Robertson and
Zaragoza, 2009) outperformed three popular dense
(neural) retrievers: the self-supervised retrievers
Contriever (Izacard et al., 2022a) and Spider (Ram
et al., 2022), as well as a retriever based on the av-
erage pooling of BERT embeddings that was used
in the RETRO system (Borgeaud et al., 2022).
Figure 3: The performance of four off-the-shelf retriev-
ers used for In-Context RALM on the development set
of WikiText-103. All RALMs are run withs =4
(i.e., retrieval is applied every four tokens). For each
RALM, we report the result of the best query lengthℓ
(see Figures 6, 9, 10).
We conducted a minimal hyper-parameter search
on the query length ℓ for each of the retrievers,
and found that ℓ =3 2 was optimal for BM25
(Figure 6), and ℓ =6 4 worked best for dense
retrievers (Figures 9, 10).
Figure 3 compares the performance gains of In-
Context RALM with these four general-purpose
retrievers. The BM25 retriever clearly outper-
formed all dense retrievers. This outcome is
consistent with prior work showing that BM25
1321
Downloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00605/2178834/tacl_a_00605.pdf by guest on 29 January 2025
Figure 4: Results of OPT models (Zhang et al., 2022) on the test set of WikiText-103 (word-level perplexity) and
the development set of RealNews (token-level perplexity). In-Context RALM models use a BM25 retriever with
s =4 (i.e., the retriever is called every four tokens) andℓ=3 2(i.e., the retriever query is comprised of the last
32 tokens of the prefix). In-Context RALM with an off-the-shelf retriever improved the performance of a 6.7B
parameter OPT model to match that of a 66B parameter OPT model.
Figure 5: An analysis of perplexity as a function ofs,
the retrieval stride, i.e., the number of tokens between
consecutive retrieval operations, on the development
set of WikiText-103. Throughout the paper, we use
s =4 to balance perplexity and runtime.
outperforms neural retrievers across a wide
array of tasks, when applied in zero-shot set-
tings (Thakur et al., 2021). This result renders
In-Context RALM even more appealing since ap-
plying a BM25 retriever is significantly cheaper
than the neural alternatives.
5.2 Frequent Retrieval Improves
Language Modeling
We investigated the effect of varying the retrieval
stride s (i.e., the number of tokens between con-
secutive retrieval operations). Figure 5 shows that
LM performance improved as the retrieval op-
eration became more frequent. This supports the
intuition that retrieved documents become more
relevant the closer the retrieval query becomes
to the generated tokens. Of course, each retrieval
operation imposes a runtime cost. To balance
performance and runtime, we used s =4 in
our experiments. For comparison, RETRO em-
ployed a retrieval frequency ofs =6 4(Borgeaud
et al., 2022), which leads to large degradation in
perplexity. Intuitively, retrieving with high fre-
quency (low retrieval stride) allows to ground the
LM in higher resolution.
5.3 A Contextualization vs. Recency
Tradeoff in Query Length
We also investigated the effect of varyingℓ,t h e
length of the retrieval query for BM25. Figure 6
reveals an interesting tradeoff and a sweet spot
around a query length of 32 tokens. Similar exper-
iments for dense retrievers are given in Appendix A.
We conjecture that when the retriever query is
too short, it does not include enough of the in-
put context, decreasing the retrieved document’s
relevance. Conversely, excessively growing the
retriever query deemphasizes the tokens at the very
end of the prefix, diluting the query’s relevance to
the LM task.
1322
Downloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00605/2178834/tacl_a_00605.pdf by guest on 29 January 2025
Figure 6: An analysis of perplexity as a function of
the number of tokens in the queryℓfor BM25 on the
development set of WikiText-103. In the appendix,
we show similar trade-offs for dense retrievers within
WikiText-103. Throughout the paper, we use a query
length ofℓ=3 2tokens.
6 Improving In-Context RALM with
LM-Oriented Reranking
Since In-Context RALM uses a fixed document
reading component by definition, it is natural to
ask whether performance can be improved by
specializing its document retrieval mechanism to
the LM task. Indeed, there is considerable scope
for improvement: the previous section considered
conditioning the model only on the first document
retrieved by the BM25 retriever. This permits very
limited semantic understanding of the query, since
BM25 is based only on the bag of words signal.
Moreover, it offers no way to accord different
degrees of importance to different retrieval query
tokens, such as recognizing that later query tokens
are more relevant to the generated text.
In this section, we focus on choosing which
document to present to the model, by reranking the
top-kdocuments returned by the BM25 retriever.5
We use Figure 7 as motivation: It shows the
large potential for improvement among the top-16
documents returned by the BM25 retriever. We
act upon this motivation by using two rerankers.
Specifically, in §6.1 we show performance gains
across our evaluation suite obtained by using an
LM to perform zero-shot reranking of the top-k
BM25 retrieved documents (results in third row
for each of the models in Table 1). Then, in§6.2
we show that training a specialized bidirectional
reranker of the top-k BM25 retrieved documents
in a self-supervised manner via the LM signal can
5In both §6.1 and §6.2 we usek =1 6.
Figure 7: Potential for gains from reranking. Per-
plexity improvement (on the development set of
WikiText-103) from an oracle that takes the best of
the top-16 documents retrieved by BM25 rather than
the first.
provide further LM gains (results in forth row for
each of the models in Table 1).
6.1 LMs as Zero-Shot Rerankers
First, we used off-the-shelf language models as
document rerankers for the In-Context RALM
setting. Formally, for a queryq consisting of the
last ℓ tokens in the prefix of the LM inputx,l e t
{d1,...,d k} be the top-k documents returned by
BM25. For retrieval iteration j, let the text for
generation be y := xs·j+1,...,x s·j+s. Ideally, we
would like to find the documentdi∗ that maximizes
the probability of the text for generation,i.e.,
i∗ =a r gm a x
i∈[k]
pθ(y|[di;x≤s·j]). (5)
However, at test time we do not have access
to the tokens of y. Instead, we used the last
prefix tokens (which are available at test time),
denoted by y′, for reranking. Formally, let s′ be
a hyper-parameter that determines the number of
the prefix tokens by which to rerank. We define
y′ := xs·j−s′+1,...,x s·j (i.e., the stride of length
s′ that precedes y) and choose the document dˆi
such that
ˆi =a r gm a x
i∈[k]
pφ(y′|
[
di;x≤(s·j−s′)
]
). (6)
The main motivation is that since BM25 is a lex-
ical retriever, we want to incorporate a semantic
signal induced by the LM. Also, this reranking
shares conceptual similarities with the reranking
framework of Sachan et al. (2022) for open-
domain question answering, where y′ (i.e.,t h e
last prefix tokens) can be thought of as their
‘‘question’’.
1323
Downloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00605/2178834/tacl_a_00605.pdf by guest on 29 January 2025
Model Reranking Model WikiText-103 RealNews
word ppl token ppl
GPT-2 345M (M) GPT-2 110M (S) 20.8 12.1
GPT-2 345M (M) 20.8 12.0
GPT-2 762M (L) GPT-2 110M (S) 17.7 10.7
GPT-2 762M (L) 17.6 10.6
GPT-2 1.5B (XL) GPT-2 110M (S) 16.2 9.9
GPT-2 1.5B (XL) 16.1 9.8
Table 3: Perplexity for zero-shot reranking (§6.1) where the reranking models is smaller than the LM,
or the LM itself. Reranking is performed on the top 16 documents retrieved by BM25. Using a GPT-2
110M (S) instead of a larger language model as a reranker leads to only a minor degradation.
Note that our zero-shot reranking does not re-
quire that the LM used for reranking is the same
model as the LM used for generation (i.e.,t h eL M
in Eq. (6), parameterized byφ, does not need to
be the LM in Eq. (2), parameterized byθ). This
observation unlocks the possibility of reranking
with smaller (and thus faster) models, which is
important for two main reasons: (i) Rerankingk
documents requires k forward passes; and (ii) it
allows our methods to be used in cases where
the actual LM’s log probabilities are not available
(for example, when the LM is accessed through
an API).6
Results A minimal hyper-parameter search on
the development set of WikiText-103 revealed that
the optimal query length iss′=1 6,7 so we proceed
with this value going forward. Table 1 shows
the results of letting the LM perform zero-shot
reranking on the top-16 documents retrieved by
BM25 (third row for each of the models). It is
evident that reranking yielded consistently better
results than simply taking the first result returned
by the retriever.
Table 3 shows that a small LM (GPT-2 117M)
can be used to rerank the documents for all larger
GPT-2 models, with roughly the same perfor-
mance as having each LM perform reranking for
itself, supporting the applicability of this method
for LMs that are only accessible via an API.
6.2 Training LM-dedicated Rerankers
Next, we trained a reranker to choose one of the
top-k documents retrieved by BM25. We refer
to this approach as Predictive Reranking,s i n c e
6Note we do not require that the two models share the
same vocabulary.
7We experimented withs′∈{ 4, 8, 16, 32}.
the reranker learns to choose which document
will help in ‘‘predicting’’ the upcoming text. For
this process, we assume availability of training
data from the target corpus. Our reranker is a
classifier that gets a prefixx≤s·j and a document
di (for i ∈ [k]), and produces a scalarf(x≤s·j,di)
that should resemble the relevance of di for the
continuation of x≤s·j.
We then normalize these relevance scores:
prank(di|x≤s·j)= exp(f(x≤s·j,di))
∑ k
i′=1 exp(f(x≤s·j,di′))
, (7)
and choose the documentdˆi such that
ˆi =a r gm a x
i∈[k]
prank(di|x≤s·j). (8)
Collecting Training Examples To train our
predictive reranker, we collected training exam-
ples as follows. Let x≤s·j be a prefix we sample
from the training data, andy := xs·j+1,...,x s·j+s
be the text for generation upcoming in its next
stride. We run BM25 on the queryqs,ℓ
j de-
rived from x≤s·j (see §3.2) and get k documents
{d1,...,d k}. For each documentdi, we then run
the LM to compute pθ(y|[di;x≤s·j]) similar to
Eq. (4).
Training Our reranker was a fine-tuned
RoBERTa-base (Liu et al., 2019) that trained for
10,000 steps with a peak learning rate of10−5 and
a batch size of 32. Overall, we created 300,000
examples from the training set of WikiText-103
as explained above. The loss function we use to
train the reranker follows previous work (Guu
et al., 2020; Lewis et al., 2020):
−log
k∑
i=1
prank(di|x≤s·j)·pθ(y|[di;x≤s·j]). (9)
1324
Downloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00605/2178834/tacl_a_00605.pdf by guest on 29 January 2025
Figure 8: Zero-shot performance of In-Context RALM
on the development set of Natural Questions and
TriviaQA, when varying the number of documents
(retrieved by DPR) shown in-context.
Note that unlike those works, we train only the
reranker (prank), keeping the LM weightsθfrozen.
Results Table 1 shows the result of our
predictive reranker, trained on WikiText-103.
Specifically, we trained it with data produced
by GPT-2 110M (S), and tested its effectiveness
for all GPT-2 models. We observed significant
gains obtained from Predictive Reranking. For
example, the perplexity of GPT-2 110M (S) im-
proved from 29.6 to 26.8, and that of GPT-2
1.5B (XL) improved from 16.6 to 15.4. This trend
held for the other two models as well. Overall,
these results demonstrate that training a reranker
with domain-specific data was more effective than
zero-shot reranking (Section 6.1). Note that these
results—while impressive—still leave room for
further improvements, compared to the top-16
BM25 oracle results (see Figure 7). Moreover, the
oracle results themselves can be improved by re-
trieving k> 16 documents via a BM25 retriever,
or by training stronger retrievers dedicated to the
RALM task. We leave this direction for future
work.
7 In-Context RALM for Open-Domain
Question Answering
So far, we evaluated our framework on language
modeling benchmarks. To test its efficacy in ad-
ditional scenarios, and specifically downstream
tasks, we now turn to evaluate In-Context RALM
on open-domain question answering (ODQA;
Chen et al., 2017). This experiment is intended
to verify, in a controlled environment, that LMs
can leverage retrieved documents without fur-
Model Retrieval NQ TriviaQA
LLaMA-7B – 10.3 47.5
DPR 28.0 56.0
LLaMA-13B – 12.0 54.8
DPR 31.0 60.1
LLaMA-33B – 13.7 58.3
DPR 32.3 62.7
Table 4: Zero-shot results of In-Context RALM
on the test set of Natural Questions and TriviaQA
measured by exact match. In the open-book set-
ting, we include the top two documents returned
by DPR.
ther training and without any training examples.
Specifically, we use the LLaMA family (Touvron
et al., 2023) with and without In-Context RALM
(often referred to in ODQA literature as open-book
and closed-book settings, respectively). In con-
trast to most prior work on ODQA (e.g., Izacard
and Grave, 2021; Fajcik et al., 2021; Izacard
et al., 2022b; Levine et al., 2022b), our ‘‘reader’’
(i.e., the model that gets the question along with
its corresponding retrieved documents, and re-
turns the answer) is simply a frozen large LM:
not pretrained, fine-tuned, or prompted to be
retrieval-augmented. For the closed-book setting,
we utilize the prompt of Touvron et al. (2023). For
the open-book setting, we extend this prompt to in-
clude retrieved documents (see Appendix C). We
use DPR (Karpukhin et al., 2020) as our retriever.
Varying the Number of DocumentsTo inves-
tigate the the effect of the number of documents
shown to the model, we performed a minimal anal-
ysis on the development set of NQ and TriviaQA.
Figure 8 demonstrates that showing documents
in-context significantly improves the model’s per-
formance. In addition, most of the gain can be
obtained by using only two documents (or even a
single one in some cases).
Results Table 4 gives the results of In-Context
RALM on the test set of Natural Questions
and TriviaQA. Motivated by our previous find-
ings, we used two retrieved documents. It is
evident that showing the model relevant docu-
ments significantly boosted its performance. For
example, adding retrieved documents improved
LLaMA-13B in the zero-shot setting by more than
1325
Downloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00605/2178834/tacl_a_00605.pdf by guest on 29 January 2025
18 points on NQ (from 12.0% to 31.0%) and more
than 5 points on TriviaQA (from 54.8% to 60.1%).
8 Discussion
Retrieval from external sources has become a com-
mon practice in knowledge-intensive tasks (such
as factual question answering, fact checking, and
more; Petroni et al. 2021). In parallel, recent break-
throughs in LM generation capabilities has led to
LMs that can generate useful long texts. How-
ever, factual inaccuracies remain a common way
in which machine-generated text can fall short,
and lack of direct provenance makes it hard to
trust machine generated text. This makes language
modeling both a promising and an urgent new
application area for knowledge grounding, and
motivates promoting RALM approaches. Prior re-
search has already investigated RALM, of course,
but it is not yet widely deployed. One likely reason
is that existing approaches rely upon fine-tuning
the LM, which is typically difficult and costly,
and is even impossible for LMs accessible only
via an API.
This paper presented the framework of
In-Context RALM, enabling frozen, off-the-shelf
LMs to benefit from retrieval. We demonstrated
that substantial performance gains can be
achieved by using general purpose retrievers, and
showed that additional gains can be achieved
by tailoring the document selection to the LM
setting. A recent work by Muhlgay et al. (2023)
demonstrates that In-Context RALM is indeed
able to improve the factuality of large LMs.
Several directions for further improvement re-
main for future work. First, this paper considers
only the case of prepending a single external doc-
ument to the context; adding more documents
could drive further gains (for example, using the
framework of Ratner et al., 2022). Second, we re-
trieved documents every fixed interval ofstokens,
but see potential for large latency and cost gains
by retrieving more sparsely, such as only when a
specialized model predicts that retrieval is needed.
We release the code used in this work, for the
community to use and improve over. We hope it
will drive further research of RALM, which will
enable its wider adoption.
Acknowledgments
We would like to thank the reviewers and the
Action Editor for their valuable feedback."
"A Survey on Retrieval-Augmented Text Generation
Huayang Li♥,∗ Yixuan Su♠,∗ Deng Cai♦,∗ Yan Wang♣,∗ Lemao Liu♣,∗
♥Nara Institute of Science and Technology ♠University of Cambridge
♦The Chinese University of Hong Kong ♣Tencent AI Lab
li.huayang.lh6@is.naist.jp, ys484@cam.ac.uk
thisisjcykcd@gmail.com, brandenwang@tencent.com
lemaoliu@gmail.com
Abstract
Recently, retrieval-augmented text generation
attracted increasing attention of the compu-
tational linguistics community. Compared
with conventional generation models, retrieval-
augmented text generation has remarkable ad-
vantages and particularly has achieved state-of-
the-art performance in many NLP tasks. This
paper aims to conduct a survey about retrieval-
augmented text generation. It ﬁrstly highlights
the generic paradigm of retrieval-augmented
generation, and then it reviews notable ap-
proaches according to different tasks including
dialogue response generation, machine trans-
lation, and other generation tasks. Finally, it
points out some promising directions on top of
recent methods to facilitate future research.
1 Introduction
Retrieval-augmented text generation, as a new
text generation paradigm that fuses emerging deep
learning technology and traditional retrieval tech-
nology, has achieved state-of-the-art (SOTA) per-
formance in many NLP tasks and attracted the at-
tention of the computational linguistics community
(Weston et al., 2018; Dinan et al., 2018; Cai et al.,
2021). Compared with generation-based counter-
part, this new paradigm has some remarkable ad-
vantages: 1) The knowledge is not necessary to be
implicitly stored in model parameters, but is explic-
itly acquired in a plug-and-play manner, leading
to great scalibility; 2) Instead of generating from
scratch, the paradigm generating text from some re-
trieved human-written reference, which potentially
alleviates the difﬁculty of text generation.
This paper aims to review many representative
approaches for retrieval-augmented text generation
tasks including dialogue response generation (We-
ston et al., 2018), machine translation (Gu et al.,
2018) and others (Hashimoto et al., 2018). We
∗All authors contributed equally.
ﬁrstly present the generic paradigm of retrieval-
augmented generation as well as three key com-
ponents under this paradigm, which are retrieval
sources, retrieval metrics and generation models.
Then, we introduce notable methods about
retrieval-augmented generation, which are orga-
nized with respect to different tasks. Speciﬁcally,
on the dialogue response generation task, exem-
plar/template retrieval as an intermediate step has
been shown beneﬁcial to informative response gen-
eration (Weston et al., 2018; Wu et al., 2019; Cai
et al., 2019a,b). In addition, there has been growing
interest in knowledge-grounded generation explor-
ing different forms of knowledge such as knowl-
edge bases and external documents (Dinan et al.,
2018; Zhou et al., 2018; Lian et al., 2019; Li et al.,
2019; Qin et al., 2019; Wu et al., 2021; Zhang et al.,
2021). On the machine translation task, we summa-
rize the early work on how the retrieved sentences
(called translation memory) are used to improve
statistical machine translation (SMT) (Koehn et al.,
2003) models (Simard and Isabelle, 2009; Koehn
and Senellart, 2010) and in particular, we inten-
sively highlight several popular methods to inte-
grating translation memory to NMT models (Gu
et al., 2018; Zhang et al., 2018; Xu et al., 2020;
He et al., 2021). We also review the applications
of retrieval-augmented generation in other genera-
tion tasks such as abstractive summarization (Peng
et al., 2019), code generation (Hashimoto et al.,
2018), paraphrase (Kazemnejad et al., 2020; Su
et al., 2021b), and knowledge-intensive generation
(Lewis et al., 2020b). Finally, we also point out
some promising directions on retrieval-augmented
generation to push forward the future research.
2 Retrieval-Augmented Paradigm
In this section, we ﬁrst give a general formulation
of retrieval-augmented text generation. Then, we
discuss three major components of the retrieval-
augmented generation paradigm, including the re-
arXiv:2202.01110v2  [cs.CL]  13 Feb 2022Input
Sources (Sec. 2.2):Training CorpusExternal DataUnsupervised DataMetrics(Sec. 2.3):Sparse-vector RetrievalDense-vector RetrievalTask-specific Retrieval
Retrieval MemoryGeneration Model
Sec. 4: Machine TranslationSec. 5: Other TasksData AugmentationAttention MechanismSkeleton & Templates
Information RetrievalTasks:Sec. 3: Dialogue GenerationModels (Sec 2.4):
Output
Figure 1: The overview of this survey.
trieval source, retrieval metric and integration meth-
ods.
2.1 Formulation
Most text generation tasks can be formulated as a
mapping from input sequence x to output sequence
y : y = f(x). For instance, x and y could be the
dialogue history and the corresponding response
for dialogue response generation, the text in the
source language and the translation in the target
language for machine translation, and so on.
Recently, some researchers suggest to endow
models the capability to access external memory
via some information retrieval techniques, so that
they can acquire more information in the generation
process (Gu et al., 2018; Weston et al., 2018; Cai
et al., 2019b). The retrieval-augmented generation
can be further formulated as:
y = f(x, z) (1)
where z = {⟨xr, yr⟩}is a set of relevant instances
retrieved from the original training set or external
datasets. The main idea of this paradigm is that yr
may beneﬁt the response generation, if xr (or yr)
is similar (or relevant) to the input x. It is worth
noting that xr = ∅when unsupervised retrieval
sources are used. In general, the retrieval mem-
ory can be retrieved from three kinds of sources:
the training corpus, external datasets in the same
format with the training corpus, and large-scale
unsupervised corpus (§2.2). Metrics that evaluate
the relevance between text are varied as well, in
§2.3 we divided them into three categories: sparse-
vector retrieval, dense-vector retrieval, and training-
based retrieval. Finally, how to integrate the re-
trieval memory to the generation model is also sig-
niﬁcant, we also introduce some popular integra-
tion approaches in §2.4.
2.2 Retrieval Sources
Training Corpus Most previous studies search
the external memory from itstraining corpus (Song
et al., 2016; Gu et al., 2018; Weston et al., 2018).
In the inference time, retrieved examples with high
relevant scores could be regarded as extra refer-
ences and reduce model’s uncertainty in generation.
The main motivation of those works is to to store
knowledge not only in the model parameters but
also in an explicit and accessible form, making the
model be able to re-access it during inference.
External Data Some researchers also propose to
retrieval relevant samples from external datasets
(Su et al., 2021c; Xiao et al., 2021). In these stud-
ies, the retrieval pool is different with the training
corpus, which can further provide additional infor-
mation that are not contained in the training corpus.
This is especially beneﬁcial for applications such
as domain adaptation and knowledge update. For
example, Khandelwal et al. (2020a); Zheng et al.
(2021a) employ the in-domain dataset as the exter-
nal memory to achieve fast domain adaptation for
machine translation.
Unsupervised Data One limitation for previous
two sources is that the datasets have to be super-
vised datasets consisting of aligned input-output
pairs. For machine translation, Cai et al. (2021) pro-
pose a cross-lingual retriever to directly retrieve tar-
get sentence from unsupervised corpus (i.e., mono-
lingual corpus in the target language). The main
idea is aligning source-side sentences and the corre-
sponding target-side translations in a dense vector
space, i.e., aligning x and yr when xr is absent.
As a result, the retriever directly connects the dots
between the source-side input and target-side trans-
lations, enabling monolingual data in the targetlanguage to be used alone as memories.
2.3 Retrieval Metrics
Sparse-vector Retrieval Given an input se-
quence x and a retrieval corpus, retrieval model
aims to retrieve a set of relevant examples z =
{⟨xr, yr⟩}from the corpus. When a supervised
corpus is used, {⟨xr, yr⟩}is retrieved by measur-
ing the similarity between x and xr. For simi-
larity measurement, sparse-vector retrieval meth-
ods such as TF-IDF and BM25 (Robertson and
Zaragoza, 2009) are widely used. They match key-
words efﬁciently with an inverted index.
Dense-vector Retrieval However, these meth-
ods prefer examples with similar surfaces, and may
fail to retrieve examples that are only semantically
relevant. To alleviate above problem, some stud-
ies (Cao and Xiong, 2018) attempt to retrieve in
dense-vector space instead of the lexical overlap.
Recent work (Lee et al., 2019) makes use of pre-
trained language models, which encodes the text to
low-dimensional dense vectors via BERT-based en-
coders. The retrieval score are computed via inner
products between vectors.
Task-speciﬁc Retrieval Similarity-based re-
trieval is based on a simple heuristic. That is, the
more xr resembles with x, the more likely xr
and yr will help the generation. However, the
most similar one by universal textual similarity
does not necessarily serve the best for downstream
models. Ideally, the retrieval metric would be
learned from the data in a task-dependent way: we
wish to consider a memory only if it can indeed
boost the quality of ﬁnal generation. To this end,
Cai et al. (2021) propose to unify the memory
retriever and its downstream generation model
into a learnable whole. Such memory retrieval is
end-to-end optimized for task-speciﬁc objectives.
2.4 Integration
Data Augmentation There are several ways to
integrate the retrieved external memory in gener-
ation. One straightforward way is data augmen-
tation, which constructs some augmented inputs
by concatenating spans from {⟨xr, yr⟩}with the
original input x. By training on the augmented
inputs, a generation model implicitly leans how
to integrate the retrieved information. Despite the
simplicity, this kind of methods works efﬁciently
in lots of tasks (Song et al., 2016; Weston et al.,
2018; Bulte and Tezcan, 2019).
Attention Mechanisms Another integration
method is based on attention mechanisms
(Bahdanau et al., 2014). The main idea of this
fashion is adopting additional encoders (in various
architectures) to encode retrieved target sentences,
and integrate them through attention (Cao and
Xiong, 2018; Gu et al., 2018; Bapna and Firat,
2019). Since the attention mechanism is becoming
(Bahdanau et al., 2014; Vaswani et al., 2017) a
key module in lots of NLP models, integrating
retrieved memory through attention becomes a
very nature and efﬁcient way.
Skeleton Extraction In the previous two meth-
ods, the downstream generation model learns how
to ﬁlter out irrelevant or even harmful informa-
tion from the retrieved examples implicitly. There
also exist some works that try to explicitly extract
useful information, i.e., skeleton extraction, from
the retrieved memory (Cai et al., 2019a; Wu et al.,
2019; Cai et al., 2019b). For example, one skeleton
should be a part of a whole utterance with irrelevant
content masked, and the generation model only in-
tegrate this skeleton in the generation process.
3 Dialogue Response Generation
Background Dialogue systems can be grouped
into two categories: chit-chat systems and task-
oriented systems. While task-oriented dialogue
systems are designed to accomplish speciﬁc user
tasks such as air tickets booking, chit-chat dialogue
systems aim at giving a meaningful and ﬂuent re-
sponse for any dialogue history in the open domain.
Dialogue response generation in chit-chat dialogue
system is challenging partly due to the diversity
of possible responses to a single dialogue history
(i.e., the one-to-many problem). The dialogue his-
tory alone cannot decide a meaningful and speciﬁc
response. Also, external knowledge that is not
present in the dialogue history are often necessary
for avoiding safe but boring responses. We focus
on recent efforts tackling the challenges to develop
chit-chat dialogue systems.
Most modern chit-chat dialogue systems can
be categorized into two classes, namely, retrieval-
based models and generation-based models. The
retrieval-based models (Ji et al., 2014; Hu et al.,
2014) directly copy an existing response from cu-
rated dialogue corpora (i.e., the retrieval pool)
when receiving a response request. The retrieved
responses are often informative and grammatical
as they are collected from real-world conversa-tions and possibly post-edited by a human. How-
ever, such systems perform poorly when a given
dialogue history is substantially different from
those in the retrieval pool. On the other hand,
the generation-based models (Shang et al., 2015;
Vinyals and Le, 2015; Li et al., 2016a) generate
a new utterance from scratch. Those generation-
based models have better generalization capacity
when handling unseen dialogue contexts. Never-
theless, the generated utterances are inclined to be
dull and non-informative (e.g., “I don’t know”, “I
think so”, “Me too” etc.) (Li et al., 2016a).
Shallow Integration As discussed, retrieval-
based models may give informative but inappro-
priate responses while generation-based models
often do the opposite. It is desirable to combine the
best of both worlds. Early work (Qiu et al., 2017)
attempts to re-rank the output from both models.
For a deep integration, Song et al. (2016) and Yang
et al. (2019) extend the standardSEQ2SEQ encoder-
decoder model (Bahdanau et al., 2014) with an ex-
tra encoder for encoding the retrieval result. The
output of the extra encoder, along with the output
from the original encoder for dialogue history, is
used to feed the decoder. Weston et al. (2018) use
a single encoder that takes the concatenation of
the original dialogue history and the retrieved as
input. Wu et al. (2019) note that the retrieved infor-
mation should be used in awareness of the context
difference, and further proposed to construct an
edit vector by explicitly encoding the lexical differ-
ences between the input dialogue history and the
retrieved dialogue history. Pandey et al. (2018) fur-
ther propose to weight different training instances
by context similarity.
Deep Integration To prevent the inﬂow of er-
roneous information, Cai et al. (2019a) propose
a general framework that ﬁrst extracts a skeleton
from the retrieved response and then generates the
response based on the extracted skeleton. This
framework is also adopted for stylistic response
generation (Su et al., 2021c). Gupta et al. (2021)
suggest to use the semantic structure of an exem-
plar response, instead of the tokens of the exem-
plar response, to guide generation. Despite their
differences, a common issue is that the genera-
tion model easily learns to ignore the retrieved re-
sponse entirely and collapses to a vanilla seq2seq
model. This happens with improper training in-
stances. Due to the one-to-many nature, it hap-
pens frequently that a retrieved response (extracted
skeleton) is suitable for responding to the query,
but inconsistent with the current target response.
Earlier studies (Weston et al., 2018; Wu et al.,
2019; Cai et al., 2019a) alleviate the above prob-
lems by putting hard constraints on the data (e.g.,
discarding data with low similarity of the retrieved
response and the target response), which, however,
greatly reduces the amount of usable data. Cai
et al. (2019b) employ a random mechanism for
generating the skeletons used for training, which
extract skeletons from the corresponding responses
with some deliberate disturbance. Paranjape et al.
(2021) propose to model the retriever after the pos-
terior distribution of retrieval given the input and
the target output and train it jointly with the stan-
dard retriever and the generator by maximizing the
evidence lower bound (ELBo) in expectation over
retrieval.
Knowledge-Enhanced Generation The afore-
mentioned work demonstrates that retrieval-based
dialogue systems can be used for building bet-
ter generation-based models. In general, this is
done by conditioning the generation on some re-
trieved responses. More traditionally, to infuse
the response with external knowledge, the retrieval
pool is not necessarily a dialogue corpus. In fact,
knowledge-grounded dialogue response generation
exploring different forms of knowledge such as
knowledge bases and external documents (Dinan
et al., 2018; Zhou et al., 2018; Lian et al., 2019;
Li et al., 2019; Qin et al., 2019; Wu et al., 2021;
Zhang et al., 2021; Komeili et al., 2021) has been
actively explored.
Limitations We note that there are three major
limitations in existing work for dialogue response
generation. First, current methods only use one
retrieved response for generation. It can be more
beneﬁcial to combine multiple retrieval responses.
However, this can be difﬁcult due to the one-to-
many nature of dialogue response generation. Sec-
ond, current methods use universal relevance score
for retrieval. It can be more effective if we can
use more customized retrieval metric especially
for controlled dialogue response generation (e.g.,
persona, emotion, etc). Third, the retrieval pool
of existing methods is limited to dialogue corpora
(context-response pairs) or documents. It might
be useful to enlarge the retrieval pool by including
more corpora in other domains or in other modali-ties. As discussed, there leaves plenty of possible
directions to explore in the future.
4 Machine Translation
Retrieval augmented translation originates from hu-
man translation scenarios (Somers, 2003). When
translating ˆy from an input source sentencex, a hu-
man translator typically involves a search engine to
retrieve similar sentences {⟨xr, yr⟩}from a bilin-
gual database. Such a technique called translation
memory is helpful to improve the translation qual-
ity and efﬁciency for human translators (Dillon
and Fraser, 2006). As the development of ma-
chine translation techniques, there is a surge of
interests in improving machine translation models
with translation memory. In the rest of this section,
we will review translation memory for both statisti-
cal machine translation (SMT) and neural machine
translation (NMT).
4.1 Translation Memory in SMT
Generally, SMT includes three key components in
a pipeline manner such as phrase table extraction,
parameter tuning and decoding (Koehn et al., 2003;
Chiang, 2007). As a result, many efforts have been
made to make use of translation memory (TM) on
top of each component.
Constrained Decoding with TM Constrained
decoding is the most straightforward way to in-
tegrating TM into SMT (Smith and Clark, 2009;
Koehn and Senellart, 2010; Zhechev and Van Gen-
abith, 2010; Ma et al., 2011). Its basic idea is
to reuse the useful segments in yr while trans-
late other segments by SMT. Speciﬁcally, the ap-
proach consists of three steps: 1) identify the un-
matched segments in both xr and x through the
edit-distance algorithm; 2) identify the unmatched
segments in yr, each of which is aligned to one
unmatched segment in xr by a word alignment
algorithm; 3) decode each unmatched segment in
x by SMT and then use the result to replace its
corresponding unmatched segment in yr. Li et al.
(2016b) further extend this approach from sentence
level to phrase level. The advantage in constrained
decoding is that it does not require to change the
translation model (including phrase table and pa-
rameters) and can be applied in a plug-and-play
way. This approach is successful when x is highly
similar to xr; otherwise its performance is de-
graded largely, because it explicitly isolates TM
matching and SMT decoding and reuses the results
in xr or not in a deterministic way.
Phrase Table Aggregation with TM There are
also notable efforts to augment the phrase table
for SMT by extracting translation rules from the
retrieved bilingual sentences {⟨xr, yr⟩}. Then
they re-tune the parameters for the SMT model
which makes use of translation knowledge from
{⟨xr, yr⟩}in a implicit way when translating x.
For example, Biçici and Dymetman (2008); Simard
and Isabelle (2009) directly combine the extracted
translation rules into the phrase table in a shallow
combination way. They introduce an additional fea-
ture to indicate that whether translation rule is from
{⟨xr, yr⟩}or not and then train all feature weights
with MERT (Och, 2003). One characteristic of
these work is that a translation rule extracted from
{⟨xr, yr⟩}which can not exactly match any seg-
ments in x is useless even if it may contain some
useful words in its target side. To remedy this ob-
servation, Wang et al. (2013, 2014) resort to a deep
combination way to using the extracted translation
rules. For each rule in the phrase table, it designs
a generative model to reward the rules which are
similar to those extracted from {⟨xr, yr⟩}. Then
this generative model is used as a feature in the log-
linear based SMT model whose weight is tuned
together with other features by MERT. In addition,
Li et al. (2014) employ a similar way to reward
the rules but it relies on a discriminative model
which is easy to integrate potential features from
{⟨xr, yr⟩}.
Parameter Tuning with TM Unlike the above
two research lines, Liu et al. (2012, 2014) make use
of translation memory only in tuning parameters.
To be speciﬁc, when translating an input sentence
x, they ﬁrstly retrieve many similar bilingual sen-
tences {⟨xr, yr⟩}, and then tune the parameters on
top of the retrieved sentences as well as a given de-
velopment dataset in a sentence-wise manner, i.e.,
it performs an independent tuning for each input
sentence. To improve the efﬁciency of each tuning
step, it propose a local update on top of {⟨xr, yr⟩}
from a baseline model.
Despite the successes of translation memory in
SMT, there are still some limitations for the above
three kinds of methods. Firstly, all these methods
employ fuzzy score for retrieval which is highly de-
pendent on word matching and thus can not recall
such examples which are similar in word seman-tics but different in surface form. Secondly, these
methods integrate the retrieved examples into a
module of SMT in the ways which can not make
full use of the knowledge in retrieved examples.
For example, the integration ways in the ﬁrst two
kinds (constrained decoding and phrase table ag-
gregation) are heuristic and not optimized towards
translation quality; the parameter tuning method
ﬁne-tunes few parameters for log-linear based SMT
which are not enough to preserve sufﬁcient knowl-
edge from retrieved examples. Thirdly, since SMT
performs in a pipeline manner, it is intractable to
jointly optimize retrieval metrics as well as SMT
models. Consequently, all these methods adopt an
off-the-shelf metric for retrieval, leading to sub-
optimal performance.
4.2 Translation Memory in NMT
Translation memory has been widely explored in
Neural Machine Translation (NMT). Depending
on when retrieval is involved, we can categorize
previous works into two classes: 1) an NMT model
leans how to cooperate with the retrieval model in
the training phase; 2) an NMT model is only aware
of the retrieved data in the inference phase.
Inference Phase The key point of literature in
this line is to reward some target words based on
words in yr in the inference process. Thus, a de-
cision can be made based on both the distribution
of generation model and the additional reward of
retrieval model. Some previous works propose to
reward target words based on the sentence-level
similarity between x and xr, and the word align-
ment between xr and yr. Given the input sentence
x, Zhang et al. (2018) try to assign target words
in ˆy with higher rewards, when they appear in yr
and the aligned source words are in both xr and
x. He et al. (2019) follow a similar framework
and consider the position information of those tar-
get words when rewarding. Those works reward
the target words in an explicit way, however, the
one-sentence-one-model approach (Li et al., 2016c;
Turchi et al., 2017) propose to reward target word
implicitly. For each testing input x, their approach
will ﬁrst ﬁnetune the translation model on retrieved
memory {⟨xr, yr⟩}and then translate x.
Others try to reward target words based on token-
level similarity score. Most works in this line are
based on the dense retriever (Khandelwal et al.,
2020a), e.g., faiss. Khandelwal et al. (2020a) build
a key-value datastore, where key h(xr, yr
<t) is the
hidden state at each time step when translating yr
from xr, and value is its golden-truth target word
yr
t . Therefore, in the inference time, they can use
the h(x, ˆy<t) as query and reward target words
with similar hidden representations in the datas-
tore. Although this method achieves signiﬁcant
performance gain, one drawback of it is the high la-
tency. To address this issue, Meng et al. (2021) use
some heuristics, e.g., pre-ﬁltering, to avoid search-
ing on the entire datastore. The reward score of
previous works is got from some non-parametric
approaches, however, Zheng et al. (2021a) propose
a light-weight network to learn the reward score.
Since dense retrieval has the potential of cross-
lingual retrieval, Zheng et al. (2021b) use a similar
approach to achieve unsupervised domain adapta-
tion, where a main change is to create the datastore
based on synthetic sources sentence and the real
target sentences.
Training Phase Different from those model-
agnostic approaches, previous works in this line
aim to train the generation model to learn how
to cooperate with the retrieval model. It is also
worth noting that most works in this line adopt
the sentence-level retrieval, when integrating the
retrieval information in the training process. To
achieve its goal, Bulte and Tezcan (2019) and
Hossain et al. (2020) propose a data augmenta-
tion method to integrate the retrieved information,
where x is concatenated with yr before feeding
into the model . Following the data augmentation
approach, Xu et al. (2020) propose more matching
methods to determine including which retrieved
example in the source is better.
There also exist some works that propose new
architectures to integrate the retrieval information.
Under the RNN-based framework, Cao and Xiong
(2018) and Gu et al. (2018) use the gating and at-
tention mechanism to incorporate the retrieved tar-
get sentences. When Transformer (Vaswani et al.,
2017) becomes the backbone of NMT, some works
also use additional transformer encoders to en-
code retrieved target sentences, and integrate them
through attention mechanism (Bapna and Firat,
2019; Cao et al., 2019). Xia et al. (2019) repre-
sent the retrieved target sentences in a different
data structure, i.e., a graph structure, and integrate
it through attention mechanism. He et al. (2021)
propose a light-weight method to encode the re-
trieved target sentences and leverage the alignment
information to ﬁlter out irrelevant information. Dif-ferent from previous works that rely on bilingual
memories, Cai et al. (2021) propose a framework
that can retrieve the most similar target sentence in
a monolingual dataset, using a source sentence as
query.
Limitations In the section of SMT, we have
showed some limitations of the retrieval augmented
approaches. There also exist some limitations in
the line of NMT. First, the information used for
deriving reward scores is limited. The similarity
between an input and retrieved examples is the
primary feature to derive reward scores. How-
ever, some information, e.g., frequencies of words
and context, may also be beneﬁcial for integrating
the translation memory. Second, it remains to be
an open question that when should we use the re-
trieved information and when not. In the inference
phase, approaches tend to integrate the translation
memory excessively, e.g., at each time step, which
not only reduces the translation efﬁciency but may
also dampen the ﬂuency of generated results.
5 Other Tasks
In addition to dialogue system and machine trans-
lation, retrieval-augmented generation techniques
have shown to be beneﬁcial in many other tasks. In
the following, we highlight several key tasks that
apply retrieval-augmented generation approaches.1
Language Modelling It has been shown that
properly leveraging information from retrieval
memory could improve the performance of large
pre-trained language model. To build a more accu-
rate language model, Khandelwal et al. (2020b) pro-
pose to incorporate a soft memory module into the
system. Speciﬁcally, an index is built by caching
the hidden states of the training corpus. Then, the
language model accesses the index via k-NN search
and displays a greatly improved performance. As
another example, Guu et al. (2020) propose a new
paradigm that applies retrieval-augmented tech-
nique into the pre-training of generative language
model. During learning, they train a neural se-
lector that dynamically samples a relevant text to
guide the reconstruction of a corrupted input se-
quence. In this way, the pre-trained model deliv-
ers better results by explicitly grounding on the
retrieval memory. Lewis et al. (2020a) combine
language model pre-training with a paraphrasing
1Here, we focus on tasks other than question answering.
We refer readers interested in QA to Chen and Yih (2020).
approach. During learning, an input sequence to
the model is ﬁrst corrupted. In the meantime, a set
of multi-lingual texts are retrieved based on which
the model learns to reconstruct the original input
sequence. Recently, Borgeaud et al. (2021) pro-
pose RETRO , a large pre-trained language model
enhanced with retrieved documents, and obtained
comparable performances with GPT-3 using 25×
fewer parameters.
Summarization Text summarization is another
research area that beneﬁts from retrieval-
augmented text generation. Peng et al. (2019)
propose an adaptive decoding framework which
ﬁrst retrieves an exemplar document given the
source document. Then, the summarization of the
source document is derived through an adaptive
generation process based on the retrieved template.
Different from Peng et al. (2019), Cao et al.
(2018) and Hossain et al. (2020) introduce an
intermediate re-ranking stage into the generation
pipeline. Speciﬁcally, before generating the
document summary, the retrieval documents are
ﬁrst re-ranked based on their similarity scores
with respect to the source document. Then, the
document summarization is produced by re-writing
the selected templates.
Paraphrase Generation To address the lack of
quality as well as diversity in the generation of para-
phrases, Kazemnejad et al. (2020) propose a gen-
eration framework which ﬁrst retrieves a sentence
that is similar to input sentence. Then, based on
the retrieved sentence, a neural editor produces the
resulting paraphrased sentence. Chen et al. (2019)
investigate a different aspect of paraphrasing, i.e.
how to control the linguistic syntax displayed in
the generated text. To achieve this goal, Chen et al.
(2019) propose to ﬁrst extract a sentential exem-
plar that serves as the syntax template. A neural
model then generates the paraphrase with desired
linguistic syntax following the retrieved exemplar.
Text Style Transfer To improve the quality of
generated text, Li et al. (2018) propose a retrieval-
augmented framework which ﬁrst retrieves texts
that are similar to the input based on lexical-level
similarity. Then, the retrieved tokens that are irrel-
evant to the source are deleted, and the output is
derived from the edited template. Xiao et al. (2021)
also adopte this framework by incorporating re-
trieval information from two sources (i.e. sparse
and dense memories) and obtained an improvedmodel performance.
Data-to-Text Generation Recently, retrieval-
augmented generation has been adapted to the task
of data-to-text generation. To bridge the gap be-
tween the structured data and natural language
text, Su et al. (2021a) propose a novel retrieval-
augmented framework. Speciﬁcally, given the
source data, a set of candidate texts are ﬁrst re-
trieved from a large unlabelled corpus. Then, a
neural selector is applied to measure the similari-
ties between the source data and candidate texts,
and extract a set of more ﬁne-grained prototypes
from the candidates. Lastly, a generation model
takes the prototypes as input to produce the text
that describes the given structured data.
While retrieval-augmented generation has been
widely explored in the NLP community, we sug-
gest that future research could extend this approach
to tasks that involve data from multiple modali-
ties. For instance, with recent advancements in
image-text retrieval (Jia et al., 2021; Radford et al.,
2021), the structural gap between images and texts
is largely bridged. Some early studies (Zhang et al.,
2020) have shown that information retrieved from
images could improve the performance of neural
machine translation model. Naturally, such meth-
ods could be extended to other multi-modal tasks,
such as image captioning (Karpathy and Li, 2015).
A similar idea could also be applied to tasks be-
yond images, such as speech-to-text transcription
(Gales and Young, 2007).
6 Future Directions
Despite the current success of retrieval augmented
text generation, there is still a long way to go as
discussed in previous sections. We highlight some
directions to facilitate the future research as fol-
lows:
Retrieval Sensitivity The performance of re-
trieval augmented text generation is very sensitive
to the retrieval quality, i.e., the similarity between
the query and the retrieved examples. Currently, re-
trieval augmented text generation models perform
well when the retrieved examples are very simi-
lar to the query. However, they are even worse
than the generation models without retrieval when
the retrieval examples are less similar. Therefore,
it would be important to exploit new methods to
address such an issue on similarity.
Retrieval Efﬁciency Generally, if one enlarges
the retrieval memory to some extent, it would be
possible to retrieve an example which is very simi-
lar to the query.Unfortunately, the downside is that
the overall inference for the retrieval augmented
generation models is less efﬁcient due the consid-
erable retrieval overhead. In this sense, it is urgent
to consider some methods to trade off the retrieval
memory size and retrieval efﬁciency, for example,
data compression for the retrieval memory.
Local vs. Global Optimization Theoretically, it
seems promising to jointly learn retrieval metrics
and generation models. However, in practice, there
is an essential gap about the retrieval metric be-
tween the training and inference phrases. In the
training phase, the loss is locally back-propagated
to only a few retrieved examples while in the infer-
ence phase the metric is globally conducted among
all examples in the memory. It would be interesting
to narrow such a gap when learning a better metric
for generation tasks.
Multi-Modalities With recent advancement in
image-text retrieval, directly associating images
with relevant text becomes possible. This urges
researchers to investigate the possibility of retrieval-
based text generation in tasks that involve data from
different modalities. One typical task is image
captioning. Beyond images, other tasks like speech-
to-text transcription could potentially beneﬁt from
retrieval-based generation methods as well.
Diverse & Controllable Retrieval Most of the
existing approaches adopt a universal metric for
retrieval, such as lexical similarities of sentences.
Future work should explore how to use customized
metrics for retrieval. This can be beneﬁcial for
more controlled text generation. For example, in-
stances with emotions and styles may be more de-
sirable in the personalized dialogue generation, par-
allel data that contains speciﬁc terminologies is
more helpful in machine translation, and so on. On
the other hand, using a universal metric for retrieval
may lead to the lack of diversity of the retrieval re-
sults. Collecting a diverse set of retrieval results
can improve the coverage of useful information.
Thus, considering multiple different metrics for re-
trieval may lead to generation with higher quality
in the future.7 Conclusion
In this paper, we surveyed recent approaches for
retrieval-augmented text generation. We reviewed
and summarized the development of different com-
ponents of retrieval-augmented text generation in-
cluding retrieval metrics, retrieval sources, and in-
tegration paradigms. We gave in-depth discussions
when retrieval-augmented text generation comes to
different applications including dialogue response
generation, machine translation, and other genera-
tion tasks. We also pointed out some future direc-
tions for retrieval-augmented text generation."
"Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics
Volume 1: Long Papers, pages 10014–10037
July 9-14, 2023 ©2023 Association for Computational Linguistics
Interleaving Retrieval with Chain-of-Thought Reasoning
for Knowledge-Intensive Multi-Step Questions
Harsh Trivedi† Niranjan Balasubramanian†
†Stony Brook University
Stony Brook, U.S.A.
{hjtrivedi,niranjan}@cs.stonybrook.edu
Tushar Khot‡ Ashish Sabharwal‡
‡Allen Institute for AI
Seattle, U.S.A.
{tushark,ashishs}@allenai.org
Abstract
Prompting-based large language models
(LLMs) are surprisingly powerful at gener-
ating natural language reasoning steps or
Chains-of-Thoughts (CoT) for multi-step
question answering (QA). They struggle,
however, when the necessary knowledge is
either unavailable to the LLM or not up-to-date
within its parameters. While using the question
to retrieve relevant text from an external
knowledge source helps LLMs, we observe
that this one-step retrieve-and-read approach
is insufficient for multi-step QA. Here, what
to retrieve depends on what has already
been derived, which in turn may depend on
what was previously retrieved . To address
this, we propose IRCoT, a new approach
for multi-step QA that interleaves retrieval
with steps (sentences) in a CoT, guiding the
retrieval with CoT and in turn using retrieved
results to improve CoT. Using IRCoT with
GPT3 substantially improves retrieval (up to
21 points) as well as downstream QA (up
to 15 points) on four datasets: HotpotQA,
2WikiMultihopQA, MuSiQue, and IIRC. We
observe similar substantial gains in out-of-
distribution (OOD) settings as well as with
much smaller models such as Flan-T5-large
without additional training. IRCoT reduces
model hallucination, resulting in factually
more accurate CoT reasoning.1.
1 Introduction
Large language models are capable of answer-
ing complex questions by generating step-by-
step natural language reasoning steps—so called
chains of thoughts (CoT)—when prompted appro-
priately (Wei et al., 2022). This approach has been
successful when all information needed to answer
the question is either provided as context (e.g., al-
gebra questions) or assumed to be present in the
model’s parameters (e.g., commonsense reasoning).
1Code, data, and prompts are available at https://
github.com/stonybrooknlp/ircot
In what country was  
Lost Gravity manufactured?
The Lost Gravity was  
manufactured by Mack Rides.
Mack Rides is a company  
from Germany .
The answer is Germany .
cumulate docs
cumulate docs
cumulate docs
Figure 1: IRCoT interleaves chain-of-thought (CoT)
generation and knowledge retrieval steps in order to
guide the retrieval by CoT and vice-versa. This inter-
leaving allows retrieving more relevant information for
later reasoning steps, compared to standard retrieval us-
ing solely the question as the query.
However, for many open-domain questions, all re-
quired knowledge is not always available or up-to-
date in models’ parameters and it’s beneficial to
retrieve knowledge from external sources (Lazari-
dou et al., 2022; Kasai et al., 2022).
How can we augment chain-of-thought prompt-
ing for open-domain, knowledge-intensive tasks
that require complex, multi-step reasoning?
While a one-shot retrieval from a knowledge
source based solely on the question can success-
fully augment LMs with relevant knowledge for
many factoid-based tasks (Lewis et al., 2020; Guu
et al., 2020; Borgeaud et al., 2022; Izacard et al.,
2022), this strategy has clear limitations for more
complex multi-step reasoning questions. For such
questions, one often must retrieve partial knowl-
edge, perform partial reasoning, retrieve additional
information based on the outcome of the partial
10014reasoning done so far, and iterate. As an example,
consider the question illustrated in Fig. 1, “In what
country was Lost Gravity manufactured?” . The
Wikipedia document retrieved using the question
(in particular, the roller coaster Lost Gravity) as the
query does not mention where Lost Gravity was
manufactured. Instead, one must first infer that
it was manufactured by a company called Mack
Rides, and then perform further retrieval, guided
by the inferred company name, to obtain evidence
pointing to the manufacturing country.
Thus, the retrieval and reasoning steps must in-
form each other. Without retrieval, a model is likely
to generate an incorrect reasoning step due to hallu-
cination. Additionally, without generating the first
reasoning step, the text supporting the second step
can’t be identified easily given the lack of lexical or
even semantic overlap with the question. In other
words, we need retrieved facts in order to generate
factually correct reasoning steps and the reasoning
steps to retrieve relevant facts.
Based on this intuition, we propose an interleav-
ing approach to this problem, where the idea is to
use retrieval to guide the chain-of-thought (CoT)
reasoning steps and use CoT reasoning to guide the
retrieval. Fig. 1 shows an overview of our retrieval
method, which we call IRCoT.2 We begin by re-
trieving a base set of paragraphs using the question
as a query. Subsequently, we alternate between the
following two steps: (i) extend CoT: use the ques-
tion, the paragraphs collected thus far, and the CoT
sentences generated thus far to generate the next
CoT sentence; (ii) expand retrieved information:
use the last CoT sentence as a query to retrieve
additional paragraphs to add to the collected set.
We repeat these steps till the CoT reports an an-
swer or we reach the maximum allowed number
of reasoning steps. Upon termination, all collected
paragraphs are returned as the retrieval outcome.
Finally, we use these as the context for answering
the question via direct QA prompting (Brown et al.,
2020) or CoT prompting (Wei et al., 2022).
We evaluate the efficacy of our system
on 4 multi-step reasoning datasets under an
open-domain setting: HotpotQA (Yang et al.,
2018), 2WikiMultihopQA (Ho et al., 2020),
MuSiQue (Trivedi et al., 2022), and IIRC (Fer-
guson et al., 2020). Our experiments using OpenAI
GPT3 (code-davinci-002) (Brown et al., 2020;
Ouyang et al., 2022; Chen et al., 2021) demon-
2Interleaved Retrieval guided by Chain-of-Thought.
strate that retrieval using IRCoT is substantially
more effective than the baseline, one-step, question-
based retrieval by 11-21 recall points under a fixed-
budget optimal recall setup.3 When IRCoT is used
in conjunction with a prompting-based reader, it
also leads to substantial improvement (up to 15 F1
points) in downstream few-shot QA performance
and reduces factual errors in generated CoT by
up to 50%. Our approach also works on much
smaller Flan-T5 models (11B, 3B, and 0.7B) show-
ing similar trends. In particular, we find QA using
Flan-T5-XL (3B) with IRCoT even outperforms
the 58X larger GPT3 with a one-step question-
based retrieval. Furthermore, these improvements
also hold up in an out-of-distribution (OOD) setting
where the demonstrations from one dataset are used
when testing on another dataset. Lastly, we note
that our QA scores exceed those reported by recent
works on few-shot prompting for open-domain QA
(ODQA) (Khot et al., 2023; Press et al., 2022; Yao
et al., 2022), although a fair apples-to-apples com-
parison with them isn’t possible (cf. Appendix C).
In summary, our maincontribution is a novel re-
trieval method, IRCoT, that leverages LMs’ chain-
of-thought generation capabilities to guide retrieval
and uses retrieval in turn to improve CoT reasoning.
We demonstrate that IRCoT:
1. improves both retrieval and few-shot QA per-
formance on several multi-step open-domain
QA datasets, in both IID and OOD settings;
2. reduces factual errors in generated CoTs; and
3. improves performance with both large-scale
(175B models) as well as smaller-scale mod-
els (Flan-T5-*, ≤11B) without any training.
2 Related Work
Prompting for Open-Domain QA.LLMs can
learn various tasks by simply using a few exam-
ples as prompts (Brown et al., 2020). They’ve
also been shown to answer complex questions
by producing step-by-step reasoning (chain-of-
thoughts, or CoT) when prompted with a few or
zero demonstrations (Wei et al., 2022; Kojima et al.,
2022). Prompting has been applied to open-domain
QA (Lazaridou et al., 2022; Sun et al., 2022; Yu
et al., 2023) but its value in improving retrieval and
QA for multi-step open-domain questions remains
relatively underexplored.
3We explain later (in the Metric section and Footnote 7)
the appropriateness of this metric in our setting as opposed to
more mainstream information recall metrics.
10015Recently three approaches have been proposed
for multi-step open-domain QA. SelfAsk (Press
et al., 2022) prompts LLMs to decompose a ques-
tion into subquestions and answers subquestions by
a call to Google Search API. DecomP (Khot et al.,
2023) is a general framework that decomposes a
task and delegates sub-tasks to appropriate sub-
models. They also decompose questions but dele-
gate retrieval to a BM25-based retriever. Both of
these approaches are not developed for CoT reason-
ing, do not focus on the retrieval problem, and re-
quire a single-hop QA model to answer the decom-
posed questions. Recently proposed ReAct (Yao
et al., 2022) system frames the problem as generat-
ing a sequence of reasoning and action steps. These
steps are much more complex, rely on much larger
models (PaLM-540B), and require fine-tuning to
outperform CoT for multi-step ODQA. Further-
more, none of these works have been shown to be
effective for smaller models without any training.
While a direct comparison with these approaches is
not straightforward (difference in knowledge cor-
pus, LLMs, examples), we find that our ODQA
performance is much higher than all their reported
numbers where available (§5).
Supervised Multi-Step Open-Domain QA.
Prior work has explored iterative retrieval for
open-domain QA in a fully supervised setting. Das
et al. (2019) proposes an iterative retrieval model
that retrieves using a neural query representation
and then updates it based on a reading compre-
hension model’s output. Feldman and El-Yaniv
(2019) apply similar neural query reformulation
idea for multihop open-domain QA. Xiong et al.
(2021) extends the widely-used Dense Passage
Retrieval (DPR) (Karpukhin et al., 2020) to
multihop setting, which has since been improved
by Khattab et al. (2021). Asai et al. (2020)
leverages the graph structure induced by the entity
links present in Wikipedia paragraphs to perform
iterative multi-step retrieval. GoldEn (Gold Entity)
retriever (Qi et al., 2019) iteratively generates
text queries based on paragraphs retrieved from
an off-the-shelf retriever but requires training
data for this next query generator. Nakano et al.
(2021) used GPT3 to answer long-form questions
by interacting with the browser but relied on
human annotations of these interactions. All of
these methods rely on supervised training on a
large-scale dataset and can not be easily extended
to a few-shot setting.
3 Chain-of-Thought-Guided Retrieval
and Open-Domain QA
Our goal is to answer a knowledge-intensive multi-
step reasoning question Q in a few-shot setting
by using a knowledge source containing a large
number of documents. To do this we follow a
retrieve-and-read paradigm (Zhu et al., 2021),
where the retriever first retrieves documents from
the knowledge source and the QA model reads the
retrieved documents and the question to generate
the final answer. Our contribution is mainly in the
retrieve step (§3.1), and we use standard prompt-
ing strategies for the read step (§3.2).
As noted earlier, for multi-step reasoning, re-
trieval can help guide the next reasoning step,
which in turn can inform what to retrieve next. This
motivates our interleaving strategy, discussed next.
3.1 Interleaving Retrieval with
Chain-of-Thought Reasoning
Our proposed retriever method, IRCoT, can be
instantiated from the following three ingredients:
(i) a base retriever that can take a query and re-
turn a given number of paragraphs from a corpus
or knowledge source; (ii) a language model with
zero/few-shot Chain-of-Thought (CoT) generation
capabilities; and (iii) a small number of annotated
questions with reasoning steps explaining how to
arrive at the answer in natural language (chain of
thoughts) and a set of paragraphs from the knowl-
edge source that collectively support the reasoning
chain and the answer.
The overview of IRCoT is given in Fig. 2. We
first gather a base set of paragraphs by retrievingK
paragraphs using the questionQ as the query. Then,
we interleave two steps ( reason and retrieve)
iteratively until the termination criterion is met.
The retrieval-guided reasoning step (“Rea-
son”) generates the next CoT sentence using the
question, the paragraphs collected thus far, and
the CoT sentences generated thus far. The prompt
template for the task looks as follows:
Wikipedia"
"89
Dense Text Retrieval Based on Pretrained Language Models:
A Survey
WAYNE XIN ZHAO,Renmin University of China, China
JING LIU,Baidu Inc., China
RUIYANG RENand JI-RONG WEN,Renmin University of China, China
Text retrieval is a long-standing research topic on information seeking, where a system is required to return
relevantinformationresourcestouser’squeriesinnaturallanguage.Fromheuristic-basedretrievalmethods
tolearning-basedrankingfunctions,theunderlyingretrievalmodelshavebeencontinuallyevolvedwiththe
ever-lasting technical innovation. To design effective retrieval models, a key point lies in how to learn text
representations and model the relevance matching. The recent success of pretrained language models (PLM)
sheds light on developing more capable text-retrieval approaches by leveraging the excellent modeling ca-
pacity of PLMs. With powerful PLMs, we can effectively learn the semantic representations of queries and
texts in the latent representation space, and further construct the semantic matching function between the
dense vectors for relevance modeling. Such a retrieval approach is calleddense retrieval, since it employs
dense vectors to represent the texts. Considering the rapid progress on dense retrieval, this survey systemat-
ically reviews the recent progress on PLM-based dense retrieval. Different from previous surveys on dense
retrieval, we take a new perspective to organize the related studies by four major aspects, including archi-
tecture, training, indexing and integration, and thoroughly summarize the mainstream techniques for each
aspect. We extensively collect the recent advances on this topic, and include 300+ reference papers. To sup-
port our survey, we create a website for providing useful resources, and release a code repository for dense
retrieval.Thissurveyaimstoprovideacomprehensive,practicalreferencefocusedonthemajorprogressfor
dense text retrieval.
CCS Concepts: •Information systems→Information retrieval;
Additional Key Words and Phrases: Text retrieval, dense retrieval, pretrained language models
ACM Reference format:
Wayne Xin Zhao, Jing Liu, Ruiyang Ren, and Ji-Rong Wen. 2024. Dense Text Retrieval Based on Pretrained
Language Models: A Survey.ACM Trans. Inf. Syst.42, 4, Article 89 (February 2024), 60 pages.
https://doi.org/10.1145/3637870
This work was partially supported by National Natural Science Foundation of China under Grants No. 62222215 and
No. U2001212, and Beijing Natural Science Foundation under Grant No. 4222027.
Authors’ addresses: W. Xin Zhao and R. Ren, Gaoling School of Artificial Intelligence, Renmin University of China, No. 59
Zhongguancun Street, Haidian District, Beijing, China, 100872; e-mails: batmanfly@gmail.com, reyon. ren@ruc.edu.cn; J.
Liu (Corresponding author), Baidu Inc., Beijing, China, 100193; e-mail: liujing46@baidu.com; J.-R. Wen, Gaoling School of
Artificial Intelligence, School of Information, Renmin University of China, No. 59 Zhongguancun Street, Haidian District,
Beijing, China, 100872; e-mail: jrwen@ruc.edu.cn.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee
provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and
the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be
honored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,
requires prior specific permission and/or a fee. Request permissions frompermissions@acm.org.
© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
1046-8188/2024/02-ART89 $15.00
https://doi.org/10.1145/3637870
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.
89:2 W. Xin et al.
1 INTRODUCTION
Text retrieval aims at finding relevant information resources (e.g., documents or passages) in re-
sponsetouser’squeries.Itreferstoaspecificinformation-seekingscenariowherethequeriesand
resourcesarepresentintheformofnaturallanguagetext.Asoneofthemostessentialtechniques
to overcome information overload, text-retrieval systems have been widely employed to support
many downstream applications, including question answering [111, 270], dialog system [30, 107],
entity linking [73,286], and Web search [183].
Theideaofdevelopingtext-retrievalsystemshasalonghistoryintheresearchliterature.Early
inthe1950s,pioneeringresearchershavestartedtostudyhowtoindexthetextsbyselectingrepre-
sentativetermsforinformationretrieval[ 110].Amongtheearlyefforts,asignificantachievement
is thevector space model[231, 233] based on the “bag-of-words” assumption, representing both
documents and queries as sparse term-based vectors. To construct sparse vector representations,
various term weighting methods have been designed and implemented, including the classictf-
idf method [1, 225, 232]. Based on this scheme, the relevance can be estimated according to the
lexical similarity between sparse query and text vectors. Such a representation scheme is further
supported by the well-known data structure ofinverted index[336,337], which organizes the text
content as term-oriented posting lists, for efficient text retrieval. To better understand the under-
lying retrieval mechanism, probabilistic relevance frameworks have been proposed for relevance
modeling,exemplifiedbytheclassicBM25model[ 226,227].Furthermore,statisticallanguagemod-
eling approaches [304] have been widely explored for text ranking. These early contributions lay
thefoundationofmoderninformation-retrievalsystems,whiletheproposedretrievalmethodsare
usually based on heuristic strategies or simplified probabilistic principles.
With the development of machine learning discipline,learning to rank[125, 149] introduces
supervised learningfor text ranking. The basic idea is to design feature-based ranking functions
takingasinputhand-craftedfeatures(notonlylimitedtolexicalfeatures)andthentraintherank-
ingfunctionwithrelevancejudgements(binaryorgradedrelevanceannotationsoverdocuments).
Despitetheflexibility,learningtorankmethodsstillrelyonhumaneffortsforfeatureengineering.
Further,there-surge of neuralnetworksshedslights on developing more capabletext-retrieval
systems, which no longer require hand-crafted text features. As an important progress in infor-
mation retrieval, deep learning approaches [96] can learn query and document representations
from labeled data in an automatic way, where both queries and documents are mapped into low-
dimensional vectors (calleddense vectorsorembeddings) in the latent semantic space. In this man-
ner,therelevancecanbemeasuredaccordingtothesemanticsimilaritybetweenthedensevectors.
In contrast to sparse vectors in the classic vector space model, embeddings do not correspond to
explicit term dimensions, but instead aim at capturing latent semantic characteristics for match-
ing. Such a retrieval paradigm is calledNeural Information Retrieval (Neural IR) [75,77,182],
which can be considered as initial explorations for dense retrieval techniques. Following the con-
vention in"
"DEMONSTRATE –SEARCH –PREDICT :
Composing retrieval and language models for knowledge-intensive NLP
Omar Khattab 1 Keshav Santhanam1 Xiang Lisa Li 1 David Hall 1
Percy Liang 1 Christopher Potts 1 Matei Zaharia 1
Abstract
Retrieval-augmented in-context learning has
emerged as a powerful approach for addressing
knowledge-intensive tasks using frozen language
models (LM) and retrieval models (RM). Exist-
ing work has combined these in simple “retrieve-
then-read” pipelines in which the RM retrieves
passages that are inserted into the LM prompt.
To begin to fully realize the potential of frozen
LMs and RMs, we propose DEMONSTRATE –
SEARCH –PREDICT (DSP ), a framework that re-
lies on passing natural language texts in sophisti-
cated pipelines between an LM and an RM. DSP
can express high-level programs that bootstrap
pipeline-aware demonstrations, search for rele-
vant passages, and generate grounded predictions,
systematically breaking down problems into small
transformations that the LM and RM can handle
more reliably. We have written novel DSP pro-
grams for answering questions in open-domain,
multi-hop, and conversational settings, establish-
ing in early evaluations new state-of-the-art in-
context learning results and delivering 37–120%,
8–39%, and 80–290% relative gains against the
vanilla LM (GPT-3.5), a standard retrieve-then-
read pipeline, and a contemporaneous self-ask
pipeline, respectively. We release DSP at https:
//github.com/stanfordnlp/dsp.
1. Introduction
In-context learning adapts a frozen language model (LM) to
tasks by conditioning the LM on a textual prompt including
task instructions and a few demonstrating examples (Mc-
Cann et al., 2018; Radford et al., 2019; Brown et al., 2020).
For knowledge-intensive tasks such as question answering,
fact checking, and information-seeking dialogue, retrieval
models (RM) are increasingly used to augment prompts
1Stanford University. Correspondence to:
Omar Khattab <okhattab@cs.stanford.edu>.
Preprint.
How many storeys are in the castle David Gregory inherited?LM:Castle Gregory has three storeys.❌Hallucinates a fictitious castleRM: “St. Gregory Hotel is a nine-floor boutique hotel in D.C...”LM: St. Gregory Hotel has nine storeys.❌Retrieves a different buildingLM: “Which castle did David Gregory inherit?”RM: “David Gregory inherited Kinnairdy Castle in 1664...”LM: “How many storyes does Kinnairdy Castle have?”RM: “Kinnairdy Castle is a tower house, having five storeys…”LM: Kinnairdy Castlehas fivestoreys.Vanilla LMRetrieve-then-ReadMulti-HopDSP Program
Figure 1.A comparison between three systems based on GPT-
3.5 (text-davinci-002). On its own, the LM often makes false
assertions. An increasingly popular retrieve-then-read pipeline
fails when simple search can’t ﬁnd an answer. In contrast, a task-
aware DSP program successfully decomposes the problem and
produces a correct response. Texts edited for presentation.
with relevant information from a large corpus (Lazaridou
et al., 2022; Press et al., 2022; Khot et al., 2022).
Recent work has shown suchretrieval-augmented in-context
learning to be effective in simple “retrieve-then-read”
pipelines: a query is fed to the RM and the retrieved pas-
sages become part of a prompt that provides context for
the LM to use in its response. In this work, we argue that
the fact that both LMs and RMs consume (and generate or
retrieve) natural language texts creates an opportunity for
much more sophisticated interactions between them. Fully
realizing this would be transformative: frozen LMs and
RMs could serve as infrastructure across tasks, enabling
ML- and domain-experts alike to rapidly build grounded
AI systems at a high level of abstraction and with lower
deployment overheads and annotation costs.
Figure 1 begins to illustrate the power of retrieval-
augmented in-context learning, but also the limitations of
“retrieve-then-read” (Lazaridou et al., 2022; Izacard et al.,
2022). Our query is “How many storeys are in the castle
David Gregory inherited?” When prompted to answer this,
GPT-3.5 (text-davinci-002; Ouyang et al. 2022) makes
up a ﬁctitious castle with incorrect attributes, highlighting
the common observation that knowledge stored in LM pa-
rameters is often unreliable (Shuster et al., 2021; Ishii et al.,
2022). Introducing an RM component helps, as the LM
can ground its responses in retrieved passages, but a rigid
arXiv:2212.14024v2  [cs.CL]  23 Jan 2023DEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models
QHow many storeys are in...Q In which city did Akeem Ellis play in 2017?A Ellesmere PortQ When was the discoverer of Palomar 4 born?A 1889TrainDemonstratedefdemonstrate(x:Example) -> Example:x.demos = annotate(x.train, attempt)returnxdefattempt(d:Example):d= search(d)d= predict(d)if d.pred ==d.answer: returnd1QHow many storeys are in the castle...Q When was the discoverer of Palomar 4 born?A 1889Hop1Who discovered Palomar 4?Psg1Edwin Hubble discovered Palomar 4...Hop2When was Edwin Powell born?Psg2Edwin Powell Hubble (1889–1953) was...Pred1889x : ExampleQ In which city did Akeem Ellis play...A Ellesmere Port... ...PredWaterloo❌Demos“How many storeys are in the castle David Gregory inherited?”QHow many storeys are in the...Demos. . .Hop1Which castle did David Gregory inherit?Psg1David Gregory inherited Kinnairdy Castle...Hop2How many storeys are in Kinnairdy Castle?Psg2Kinnairdy Castle […] having five storeys...QHow many storeys does the.... . .. . .PredFive storeysSearchdefsearch(x:Example) -> Example:x.hop1 =generate(hop_template)(x).predx.psg1 =retrieve(x.hop1, k=1)[0]x.hop2 =generate(hop_template)(x).predx.psg2 =retrieve(x.hop2, k=1)[0]returnx2 Predictdefpredict(x:Example) -> Example:x.context = [x.psg1, x.psg2]x.pred=generate(qa_template)(x).predreturnx3“Five storeys”
Figure 2.A toy example of a DSP program for multi-hop question answering. Given an input question and a 2-shot training set, the
DEMONSTRATE stage programmatically annotates intermediate transformations on the training examples using a form of weak supervision.
Learning from a resulting demonstration, the SEARCH stage decomposes the complex input question and retrieves supporting information
over two retrieval hops. Finally, the PREDICT stage uses the demonstration and retrieved passages to answer the question.
retrieve-then-read strategy fails because the RM cannot ﬁnd
passages that directly answer the question.
We introduce the DEMONSTRATE –SEARCH –PREDICT
(DSP ) framework for in-context learning, which relies en-
tirely on passing natural language text (and scores) be-
tween a frozen RM and LM. DSP introduces a num-
ber of composable functions that bootstrap training exam-
ples (DEMONSTRATE ), gather information from a knowl-
edge corpus ( SEARCH ), and generate grounded outputs
(PREDICT ), using them to systematically unify techniques
from the retrieval-augmented NLP and the in-context learn-
ing literatures (Lee et al., 2019; Khattab et al., 2021a; Anan-
tha et al., 2020; Gao et al., 2022; Izacard et al., 2022; Dohan
et al., 2022; Zelikman et al., 2022; Zhang et al., 2022).
We use DSP to suggest powerful strategies for knowledge-
intensive tasks with compositions of these techniques. This
reveals new conceptual possibilities for in-context learning
in general (§2), and it allows us to present rich programs
that set new state-of-the-art results (§3).
Figure 1 shows the path that a DSP program might take to
arrive at an answer, and Figure 2 illustrates how a deliberate
program achieves this. Instead of asking the LM to answer
this complex question, the program’sSEARCH stage uses the
LM to generate a query “Which castle did David Gregory
inherit?” The RM retrieves a passage saying Gregory inher-
ited the Kinnairdy Castle. After a second search “hop” ﬁnds
the castle’s number of storeys, thePREDICT stage queries
the LM with these passages to answer the original question.
Although this program implements behaviors such as query
generation, it requires no hand-labeled examples of these
intermediate transformations (i.e., of the queries and pas-
sages of both retrieval hops). Instead, the DEMONSTRATE
stage uses labeled question–answer pairs to implement a
form of weak supervision that programmatically annotates
the transformations invoked within SEARCH and PREDICT .
We evaluate severalDSP programs on answering questions
in open-domain, multi-hop, and conversational settings. In
them, we implement novel and reusable transformations
such as bootstrapping annotations for all of our pipelines
with weak supervision (§2.3), reliably rewriting questions to
resolve conversational dependencies and iteratively decom-
pose complex queries with summarization of intermediate
hops (§2.4), and generating grounded responses from mul-
tiple passages with self-consistency (§2.5). We report pre-
liminary results on Open-SQuAD, HotPotQA, and QReCC
using the frozen LM GPT-3.5 and RM ColBERTv2 (Khat-
tab & Zaharia, 2020; Santhanam et al., 2022b) with no
ﬁne-tuning. Our DSP programs deliver 37–120%, 8–39%,
and 80–290% relative gains against corresponding vanilla
LMs, a standard retrieve-then-read pipeline, and a contem-
poraneous self-ask pipeline (Press et al., 2022), respectively.
Future versions of this report will include additional test
tasks and LM choices.
In summary, this work makes the following contributions.
First, we argue that simple task-agnostic pipelines for in-
context learning should give way to deliberate, task-aware
strategies. Second, we show that this shift need not be a
burden: with DSP , such strategies can be easily expressed
as short programs using composable operators. Third, this
composability spawns powerful capacities, like automati-
cally annotating demonstrations for complex pipelines from
end-task labels. Fourth, for three knowledge-intensive tasks,
we implement rich programs that establish state-of-the-art
results for in-context learning.DEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models
2. DEMONSTRATE –SEARCH –PREDICT
We now introduce the DSP framework and show its expres-
sive power by suggesting a number of strategies in which
the LM and RM can come together to tackle complex prob-
lems effectively. We show in §3 that such strategies out-
perform existing in-context learning methods. We begin by
discussing the LM and RM foundation modules on which
DSP is built (§2.1) and then the datatypes and control ﬂow
within DSP (§2.2). Subsequently, we discuss each of the
three inference stages: DEMONSTRATE (§2.3), SEARCH
(§2.4), and PREDICT (§2.5).
2.1. Pretrained Modules: LM and RM
A DSP program deﬁnes the communication between the
language model LM and the retrieval model RM.
Language Model We invoke a frozen language model
LM to conditionally generate (or score) text. For each
invocation, the program prepares a prompt that adapts the
LM to a speciﬁc function (e.g., answering questions or
generating queries). A prompt often includes instructions,
a few demonstrations of the desired behavior, and an input
query to be answered.
As in Figure 2, the LM generates not only: (i) the ﬁnal
answer to the input question (in thePREDICT stage), but also
(ii) intermediate “hop” queries to ﬁnd useful information
for the input question (SEARCH ) as well as (iii) exemplar
queries that illustrate how to produce queries for questions
in the training set (DEMONSTRATE ). This systematic use of
the LM is a hallmark of DSP programs.
Retrieval Model DSP programs also invoke a frozen re-
trieval model RM to retrieve the top-k most “relevant”
text sequences for a given query. The RM can index a
massive set of pre-deﬁned passages for scalable search, and
those passages can be updated without changing the retrieval
parameters. The RM accepts free-form textual inputs and
specializes in estimating the relevance (or similarity) of a
text sequence to a query.
As in Figure 2, the RM is responsible for retrieving (i)
passages for each query generated by the LM (during the
SEARCH stage), but also (ii) passages that are used within
demonstrations (DEMONSTRATE ). In the latter case, the
RM’s contributions are less about providing directly rel-
evant information to the input question and more about
helping the LM adapt to the domain and task.
Though not utilized in this example, the RM is also used in
DSP for functions like retrieving “nearest-neighbor” demon-
strations from task training data (DEMONSTRATE ) and se-
lecting well-grounded generated sequences from the LM
(PREDICT ).
2.2. Datatypes and Control Flow
We have implemented the DSP framework in Python. The
present section introduces the core data types and compos-
able functions provided by the framework. We use illustra-
tive code snippets to ground the examples, and to convey
the power that comes from being able to express complex
interactions between the LM and RM in simple programs.
The Example Datatype To conduct a task, a DSP pro-
gram manipulates one or more instances of the Example
datatype. An Example behaves like a Python dictionary
with multiple ﬁelds. The program is typically provided with
a few training examples. The code snippet below illustrates
this for multi-hop question answering.
1 from dsp import Example
2
3 train = [ Example ( question ="" When was the discoverer
of Palomar 4 born ?"", answer ="" 1889 ""),
4 Example ( question =""In which city did Akeem
Ellis play in 2017? "", answer ="" Ellesmere Port "")]
This snippet contains two labeled examples, each with a
multi-hop question (e.g., “In which city did Akeem Ellis
play in 2017?”) and its short answer (“Ellesmere Port”).
Arbitrary keys and values are allowed within an Example,
though typical values are strings or lists of strings.
In this task, we are unlikely to ﬁnd an individual passage
that provides the answer to any question. For example, the
ﬁrst training example can probably be resolved only by ﬁrst
answering the question of who discovered Palomar (“Edwin
Hubble”) and then addressing the question of Hubble’s birth
date using different evidence passages. We typically assume
that the human-labeled training data do not include labels
for intermediate transformations (e.g., queries for individual
hops) that would be useful for following these steps, and so
it is the job of the DSP program to discover these strategies
via in-context learning.
A DSP Program The following code snippet is a com-
plete program for resolving multi-hop questions like those
in Figure 1, with help from train examples like those above.
1 def multihop_program ( question : str ) -> str :
2 x = Example ( question = question , train = train )
3 x = multihop_demonstrate (x)
4 x = multihop_search (x)
5 x = multihop_predict (x)
6 return x. answer
7
8 multihop_program ("" How many storeys does the castle
David Gregory inherited have ?"")
9 # => "" five storeys ""
The program takes the input (here, a question) and outputs
the system output (its short answer). It starts by creating
an Example for the input question and assigning the train
ﬁeld to the training set from the previous snippet. ProgramsDEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models
invoke and compose DSP primitives (i.e., built-in functions)
to build the DEMONSTRATE , SEARCH , and PREDICT trans-
formations that deﬁne the program.
Transformations A transformation is a function that
takes an Example as input and returns an Example, pop-
ulating new ﬁelds (or modifying existing ﬁelds) in it. This
program invokes three developer-deﬁned transformations,
namely, multihop_demonstrate, multihop_search, and
multihop_predict. Transformations may themselves in-
voke other transformations, and they act analogously to
layers in standard deep neural network (DNN) program-
ming frameworks such as PyTorch, except that they pass
text data instead of tensors between each other and do not
involve backpropagation.
We categorize transformations according to their behavior
(or purpose) under one of the DEMONSTRATE , SEARCH ,
and PREDICT stages. That said, DSP does not impose this
categorization and allows us to deﬁne functions that may
blend these stages. We will discuss each of the three stages
next.
2.3. DEMONSTRATE
It is known that including examples of the desired behavior
from the LM in its prompt typically leads to better perfor-
mance (Brown et al., 2020). In DSP , a demonstration is a
training example that has been prepared to illustrate speciﬁc
desired behaviors from the LM. A DEMONSTRATE transfor-
mation takes as input x of type Example and prepares a list
of demonstrations in x.demos, typically by selecting a sub-
set of the training examples in x.train and bootstrapping
new ﬁelds in them.
Bootstrapping Demonstrations Examples in the train-
ing set typically consist of the input text and the target
output of the task. The DEMONSTRATE stage can aug-
ment a training example by programmatically bootstrapping
annotations for intermediate transformations. In our run-
ning “multi-hop” example, the demonstrations illustrate
three LM-based transformations: (i) how to break down the
input question in order to gather information for answer-
ing it (i.e., ﬁrst-hop retrieval), (ii) how to use information
gathered in an earlier “hop” to ask follow-up questions, and
(iii) how to use the information gathered to answer complex
questions.
1 Examples = list [ Example ]
2 Transformation = Callable [[ Example ],
3 Optional [ Example ]]
4
5 annotate ( train : Examples , fn: Transformation )
6 -> Examples
Akin to a specialized map, the annotate primitive accepts
a user-deﬁned transformation fn and applies it over a list
of training examples. Whenever fn returns an example
(rather than None), annotate caches the intermediate pre-
dictions (i.e., the generated queries and retrieved passages).
These predictions serve as successful demonstrations for the
pipeline transformations. In simple uses, fn may attempt
to answer the example “zero-shot” one or more times. This
is typically done by invoking the SEARCH and PREDICT
stages of the program. When an answer is produced, if
fn assesses it as correct, it returns a populated example in
which the intermediate predictions are present.
Case Study The snippet below deﬁnes the func-
tion multihop_demonstrate, called in Line 3 of
multihop_program, and illustrates the usage of annotate.
1 from dsp import sample , annotate
2
3 def attempt_example (d: Example ):
4 d = d. copy ( demos =[])
5 d = multihop_search (d)
6 d = multihop_predict (d)
7 return d if d. pred == d. answer else None
8
9 def multihop_demonstrate (x: Example ):
10 demos = annotate (x. train , attempt_example )
11 return Example (x, demos = demos )
In Line 10, multihop_demonstrate invokes annotate,
which bootstraps missing ﬁelds in training examples by
caching annotations from attempt_example. The transfor-
mation attempt_example takes a training example d and
attempts to answer it in a zero-shot fashion: it creates a copy
of d with no demonstrations (Line 4; i.e., zero-shot) and
invokes the multi-hop search and predict pipeline (Lines 5
and 6). Each transformation returns an updated version of
d with additional ﬁelds populated. If the pipeline answers
correctly (Line 7), the updated d is returned.
Figure 2 illustrates this behavior. DEMONSTRATE trans-
forms a training question–answer pair to a fully-populated
demonstration, including ﬁelds such as hop1 and hop2 (i.e.,
queries for multi-hop search) as well as psg1 and psg2.
When the LM is later invoked to conduct a transformation,
say, generating a “second-hop” query duringSEARCH , the
psg1 ﬁeld serves as context and the hop2 ﬁeld serves as a
label for this particular training example.
Discussion This simple case study illustrates the power of
composition in the DSP abstraction. Because the pipeline
is a well-deﬁned program in which transformations com-
municate by passing text attached to Examples, a simple
map-and-ﬁlter strategy can leverage the LM and RM to
bootstrap annotations for a full pipeline from end-task la-
bels. This is an extensible strategy, but even in its simplest
form it generalizes the approaches explored recently by Ze-
likman et al. (2022), Wei et al. (2022), Zhang et al. (2022),
and Huang et al. (2022) in which an LM self-generates
chain-of-thought rationales for an individual prompt.DEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models
By bootstrapping pipelines, DEMONSTRATE makes it easy
to explore complex strategies in SEARCH and PREDICT
without writing examples for every transformation. This
includes strategies that are challenging to explore without
custom annotations in traditional retrieval-augmented NLP.
For instance, Khattab et al. (2021a) introduces a pipeline
for multi-hop reasoning that is trained with weak supervi-
sion, extending work by Lee et al. (2019) and Khattab et al.
(2021b). In it, the target 3 or 4 passages that need to re-
trieved must be labeled but the system discovers the best
order of “hops” automatically.
In contrast, DSP allows us to build complex pipelines with-
out labels for intermediate steps, because we can compose
programs out of small transformations. If LM and RM can
accurately process such transformations “zero-shot” (i.e.,
without demonstrations) on at least one or two examples,
these examples can be discovered with end-task labels and
used as demonstrations.
To draw on our earlier analogy with DNN frameworks like
PyTorch, DEMONSTRATE aims to replace the function of
backpropagation in extensible ways by simulating the be-
havior of the program (corresponding to a “forward” pass)
and programmatically learning from errors. In doing this
with frozen models and with only end-task labels, DEMON -
STRATE introduces a high degree of modularity. In partic-
ular, without hand-labeling intermediate transformations,
developers may swap the training domain, update the train-
ing examples, or modify the program’s strategy, and use
annotate to automatically populate all of the intermediate
ﬁelds for demonstrations.
Selecting Demonstrations It is not always possible to ﬁt
all of the training examples in the context window of the
LM. DSP provides three primitives for selecting a subset
of training examples, namely, sample, knn, and crossval.
1 sample ( train : Examples , k: int )
2 -> Examples
3
4 knn ( train : Examples , cast : Callable [[ Example ], str ])
5 -> fn( example : Example , k: int ) # currying
6 -> Examples
7
8 crossval ( train : Examples , n: int , k: int )
9 -> fn( evaluate : Transformation )
10 -> Examples
As a baseline choice, k demonstrations can be randomly
sampled from train using the sample primitive, an ap-
proach used by Brown et al. (2020) and much subsequent
work. We can also leverage theRM’s representations and se-
lect from the training set thek nearest neighbors to the input
text, a strategy explored by Liu et al. (2021). Another strat-
egy is to apply cross-validation to select among a number of
sampled sets of demonstrations (Perez et al., 2021). For ex-
ample, given |train| = 100training examples, crossval
would select n subsets of k = 5examples each, and return
the set with which a transformationevaluate performs best
on the remaining 95 examples.
Compositions & Extensions By manipulating demon-
strations and higher-order transformations, these simple
selection and bootstrapping primitives can be combined to
conduct larger novel strategies. If the training set is very
large (e.g., |train| = 100, 000), we can conduct knn to
ﬁnd the nearest k = 16examples and only annotate these,
arriving at a system that learns incrementally in real-time. If
the training set is moderately large (e.g., |train| = 1000),
we can conduct crossval and cache the performance of all
prompts it evaluates on each training example. At test time,
we can use knn to ﬁnd k = 50similar examples to the test
input and select the prompt that performs best on these k
examples, producing an adaptive system that is informed by
the quality of its pipeline on different types of examples.
2.4. SEARCH
The SEARCH stage gathers passages to support transforma-
tions conducted by the LM. We assume a large knowledge
corpus—e.g., a snippet of Web, Wikipedia, or arXiv—that
is divided into text passages. Providing passages to the LM
facilitates factual responses, enables updating the knowl-
edge store without retraining, and presents a transparency
contract: when in doubt, users can check whether the system
has faithfully used a reliable source in making a prediction.
In the simplest scenarios, SEARCH can directly query the
RM, requesting the top-k passages (from a pre-deﬁned in-
dex) that match an input question. This baseline instantia-
tion of SEARCH simulates retrieval in most open-domain
question answering systems, which implement a “retrieve-
then-read” pipeline, like Lee et al. (2019), Khattab et al.
(2021b), Lazaridou et al. (2022), and many others.
1 from dsp import retrieve
2
3 def simple_search (x):
4 passages = retrieve ( query =x. question , k =2)
5 return passages
SEARCH Strategies In many scenarios, the complexity
of the task demands more sophisticated SEARCH strategies
that empower the RM to ﬁnd relevant passages. Our run-
ning example (Figure 2) is one such scenario, in which we
suspect examples are likely to require multi-hop reasoning
in particular. Other settings, for instance, pose conversa-
tional challenges, in which the information need expressed
by a user can only be resolved by taking into account pre-
vious turns in the conversation, or demand more extensive
planning (Zhong et al., 2022).
In the retrieval-augmented NLP literature, multi-hop
search (Xiong et al., 2020; Khattab et al., 2021a) and con-DEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models
versational search (Del Tredici et al., 2021; Raposo et al.,
2022) pipelines have received much attention. These sys-
tems are typically ﬁne-tuned with many hand-labeled query
“rewrites” (Anantha et al., 2020), “decompositions” (Geva
et al., 2021; Min et al., 2019), or target hops (Yang et al.,
2018; Jiang et al., 2020). Supported with automatic anno-
tations from DEMONSTRATE , the SEARCH stage allows us
to simulate many such strategies and many others in terms
of passing queries, passages, and demonstrations between
the RM and LM. More importantly, SEARCH facilitates our
vision of advanced strategies in which the LM and RM co-
operate to incrementally plan a research path for which the
RM gathers information and the LM identiﬁes next steps.
Case Study Let us build on our running multi-hop exam-
ple as a case study. We can deﬁne multihop_search_v2
(Line 4 in our core program), a slightly more advanced ver-
sion of the SEARCH transformation from Figure 2. This
transformation simulates the iterative retrieval component
of ﬁne-tuned retrieval-augmented systems like IRRR (Qi
et al., 2020), which reads a retrieved passage in every hop
and generates a search query (or a termination condition to
stop hopping), and Baleen (Khattab et al., 2021a), which
summarizes the information from many passages in each
hop for inclusion in subsequent hops.
1 from dsp import generate
2
3 def multihop_search_v2 (x, max_hops =3) :
4 x. hops = []
5
6 for hop in range ( max_hops ):
7 summary , query = generate ( hop_template )(x)
8 x. hops . append (( summary , query ))
9
10 if query == /quotesingle.VarN/A /quotesingle.Var: break
11
12 passages = retrieve ( query , k =5)
13 x. context = [ summary ] + passages
14
15 return x
In multihop_search_v2, Line 7 calls the generate prim-
itive, which invokes the LM to produce a query for each
retrieval hop. The LM is conditioned on a prompt that is
prepared using the hop_template template. (We discuss
prompt templates and thegenerate primitive in §2.5.) Here,
this template may be designed to generate a prompt that has
the following format (e.g., for the second hop).
1 My task is to write a simple query that gathers
information for answering a complex question . I
write N/A if the context contains all
information required .
2
3 { Task demonstrations from x. demos , if any }
4
5 Context : {x. context }
6 Question : {x. question }
7 Summary : Let /quotesingle.Vars summarize the above context .
__{ summary }__
8 Search Query : __{ query }__
As shown, the LM is instructed to read the context re-
trieved in earlier hops and a complex question. It is then
prompted to write: (i) a summary of the supplied con-
text and (ii) a search query that gathers information for
answering that question. The generated text will be ex-
tracted and assigned to the summary and query variables in
(multihop_search_v2; Line 7). On Line 10, we terminate
the hops if the query is “N/A”. Otherwise, Line 12 retrieves
k = 5 passages using the query and Line 13 assigns the
context for the subsequent hop (or for PREDICT ), setting
that to include the summary of all previous hops as well as
the passages retrieved in the ﬁnal hop so far.
Comparison with self-ask It may be instructive to con-
trast this multi-hop DSP program with the recent “self-
ask” (Press et al., 2022) prompting technique, which we
compare against in §3. Self-ask can be thought of as a sim-
ple instantiation of DSP’sSEARCH stage. In it, the LM asks
one or more “follow-up questions”, which are intercepted
and sent to a search engine. The search engine’s answers
are concatenated into the prompt and are used to answer
the question. This is essentially a simpliﬁed simulation of
IRRR (Qi et al., 2020).
As a general framework,DSP can express ideas like self-ask
and many other, more sophisticated pipelines as we discuss
in the present section. More importantly, DSP offers a num-
ber of intrinsic advantages that lead to large empirical gains:
80%–290% over self-ask. For instance, DSP programs are
deeply modular, which among other things means that DSP
programs will annotate and construct their own demonstra-
tions. Thus, they can be developed without labeling any
of the intermediate transformations (e.g., the queries gener-
ated). In addition, the LM prompts constructed by DSP get
automatically updated to align with the training data and re-
trieval corpus provided. In contrast, approaches like self-ask
rely on a hand-written prompt with hard-coded examples.
Moreover, DSP assigns the control ﬂow to an explicit pro-
gram and facilitates design patterns that invoke the LM (or
RM) to conduct small transformations. This allows us to
build steps that are dedicated to generating one or more re-
trieval queries, summarizing multiple passages per hop, and
answering questions. These steps are individually simpler
than the self-ask prompt, yet our multi-hop DSP program
deliberately composes them to build richer pipelines that are
thus more reliable. In contrast, self-ask delegates the con-
trol ﬂow to the LM completions, maintaining state within
the prompt itself and intercepting follow-up questions to
conduct search. We ﬁnd that this paradigm leads to a “self-
distraction” problem (§3.5) that DSP programs avoid.
Fusing Retrieval Results For improved recall and robust-
ness, we can also fuse the retrieval across multiple gen-
erated queries. Fusion has a long history in informationDEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models
retrieval (Fox & Shaw, 1994; Xue & Croft, 2013; Kur-
land & Culpepper, 2018) and sequentially processing multi-
ple queries was explored recently by Gao et al. (2022) for
retroactively attributing text generated by LMs to citations.
Inspired by these, we include afused_retrieval primitive
to DSP to offer a versatile mechanism for interacting with
frozen retrievers. It accepts an optional fusion function that
maps multiple retrieval lists into one. By default, DSP uses
a variant of CombSUM (Fox & Shaw, 1994), assigning each
passage the sum of its probabilities across retrieval lists.
To illustrate, the modiﬁcation below generates n = 10
queries for the transformation multihop_search_v2.
c = generate ( hop_template , n =10) (x)
passages = fused_retrieval (c. queries , k =5)
summary = c. summaries [0] # highest - scoring summary
Compositions & Extensions To illustrate a simple com-
position, we can equip a chatbot with the capacity for con-
versational multi-hop search by combining a query rewriting
step, which produces a query that encompasses all of the
relevant conversational context, with the multi-hop transfor-
mation, as follows.
1 def conversational_multihop_search (x):
2 x. question = generate ( conv_rewriting_template )(x)
3 return multihop_search_v2 (x)
Similar approaches can be used for correcting spelling mis-
takes or implementing pseudo-relevance feedback (Cao
et al., 2008; Wang et al., 2022a), in which retrieved passages
are used to inform a better search query, though this has not
been attempted with pretrained LMs to our knowledge.
2.5. PREDICT
The PREDICT stage generates the system output using
demonstrations (e.g., in x.demos) and passages (e.g., in
x.context). PREDICT tackles the challenges of reliably
solving the downstream task, which integrates much of the
work on in-context learning in general. Within DSP, it also
has the more specialized function of systematically aggre-
gating information across a large number of demonstrations,
passages, and candidate predictions.
Generating Candidates Generally, PREDICT has to pro-
duce one or more candidate predictions for the end-task.
To this end, the basic primitive in PREDICT is generate,
which accepts a Template and (via currying) an Example
and queries the LM to produce one or more completions,
as explored earlier in §2.4. A corresponding primitive that
uses the RM in this stage is rank, which accepts a query
and one or more passages and returns their relevance scores.
1 Template # template : an object that can produce
prompts and parse completions
2
3 generate ( template : Template )
4 -> fn( example : Example )
5 -> Completions # object with keys to access
extracted preds and scores
6
7 rank ( query : str , passages : List [ str ])
8 -> List [ float ] # object with keys to access
passage texts and scores
A Template is an object that can produce prompts, that is,
map an Example to a string, and extract ﬁelds out of com-
pletions. For instance, we can map an example x that has a
question and retrieved passages to the following prompt:
1 My task is to answer questions using Web documents .
2
3 { Task demonstrations from x. demos , if any }
4
5 Context : {x. passage }
6 Question : {x. question }
7 Rationale : Let /quotesingle.Vars think step by step . __{ rationale }__
8 Answer : __{ answer }__
As this illustrates, the LM will be asked to generate a chain-
of-thought rationale (CoT; Wei et al. 2022; Kojima et al.
2022) and an answer, and the generated text will be ex-
tracted back into the rationale and answer keys of each
completion.
Each invocation to the LM can sample multiple candidate
predictions. Selecting a “best” prediction is the subject of
much work on decoding (Wiher et al., 2022; Li et al., 2022),
but a frozen and general-purpose LM may not support cus-
tom modiﬁcations to decoding. Within these constraints, we
present several high-level strategies for selecting predictions
and aggregating information in DSP via the LM and RM.
Selecting Predictions Among multiple candidates, we
can simply extract the most popular prediction. When a CoT
is used to arrive at the answer, this is the self-consistency
method of Wang et al. (2022c), which seeks to identify
predictions at which multiple distinct rationales arrive.
1 from dsp import generate , majority
2
3 def multihop_predict (x):
4 candidates = generate ( template_qa )(x)
5 return x. copy ( answer = majority ( candidates ). answer )
DSP generalizes this in two ways. First, we can sample
multiple “pipelines of transformations” (PoT) within the pro-
gram, rather than locally with “chains of thought” (CoT) in
one transformation. These chains may even invoke different
paths in the program, as illustrated below.DEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models
1 from dsp import branch
2
3 def pipeline (x):
4 return multihop_predict ( multihop_search_v2 (x))
5
6 def PoT_program ( question : str ) -> str :
7 x = Example ( question = question , train = train )
8 x = multihop_demonstrate (x)
9
10 candidates = branch ( pipeline , n=5 , t =0.7) (x)
11 return x. copy ( answer = majority ( candidates ). answer )
In the snippet above, Line 10 invokes the primitivebranch
which samples n different PoTs with a high temperature
(e.g., t = 0 .7) and accumulates their intermediate and
ﬁnal predictions. In this example, our pipeline invokes
multihop_search_v2 (§2.4), which applies a variable num-
ber of retrieval hops depending on the questions generated,
before doing PREDICT . That is, PoT_program potentially
invokes multiple distinct paths in the program (i.e., with dif-
ferent multi-hop queries and number of hops in each) across
branches. It then selects the majority answer overall.
DSP generalizes self-consistency in a second way. When
sampling our CoTs or PoTs provides multiple candidates,
we can select the top- k (e.g., top-4) predictions and then
compare them directly. For instance, we may prompt the
LM to compare these choices as MCQ candidates, a trans-
formation for which DEMONSTRATE can automatically pre-
pare exemplars. This effectively simulates the LM recursion
of Levine et al. (2022), though unlike their approach it does
not require a large training set or updating any (prompt-
tuning) weights. One such implementation is illustrated in
openqa_predict below.
1 def openqa_predict (x):
2 preds = generate ( template_qa , n =20) (x). answers
3 x. choices = most_common ( preds , k =4)
4
5 queries = [f""{x. question } {c}""
6 for c in x. choices ]
7
8 x. passages = fused_retrieval ( queries )
9 x. answer = generate ( TemplateMCQ )(x). answer
10 return x
As an alternative comparison approach, we can invoke the
RM via rank to ﬁnd the prediction that is most grounded in
a retrieved contexts (i.e., most similar to the concatenation
of the retrieved passages) or, given an RM that can score
completions (Krishna et al., 2022), simply the prediction
that has the highest score given the prompt.
Aggregating Information When only a few demonstra-
tions or passages are selected, we can simply concate-
nate them all into the prompt. For instance, GPT-3.5
text-davinci-002 has a context window of 4097 tokens,
which we ﬁnd to be reasonably large for accommodating
several (e.g., 3–5) demonstrations, which individually in-
clude their own passages and rationales.
To deal with a larger number of demonstrations or passages,
we can branch in parallel to process individual subsets
of the passages or demonstrations and then aggregate the
individual answers using one of the scoring methods pre-
sented earlier. Indeed, Lewis et al. (2020) and Lazaridou
et al. (2022) have explored marginalization as a way to com-
bine scores across passages and Le et al. (2022) ensemble
prompts across demonstrations, which can be expressed in
this way.
An alternative aggregation strategy is to accumulate informa-
tion across passages sequentially, rather than independently.
This is effectively how our multi-hop approach works (§2.4).
Such a strategy has also been employed recently by Gao
et al. (2022) for retroactively attributing text generated by
LMs to citations. They generate many queries but instead
of fusion (§2.4), they run their pipeline on each query and
use its outputs to alter the input to subsequent queries.1
3. Evaluation
We now consider how to implement DSP programs for three
diverse knowledge-intensive NLP tasks: open-domain ques-
tion answering (QA), multi-hop QA, and conversational QA.
All of these tasks are “open-domain”, in the sense that sys-
tems are given a short question or participate in a multi-turn
conversation without being granted access to context that
answers these questions.
We build and evaluate intuitive compositions of the func-
tions explored in §2 for each task. We show that, despite
low development effort, the resulting DSP programs exhibit
strong quality and deliver considerable empirical gains over
vanilla in-context learning and a standard retrieve-then-read
pipeline with in-context learning.
3.1. Evaluation Methodology
In this report, we consider one development datasetfor each
of the tasks we consider, namely, the open-domain version
of SQuAD (Rajpurkar et al., 2016; Lee et al., 2019), the
multi-hop HotPotQA (Yang et al., 2018) dataset in the open-
domain “fullwiki” setting, and the conversational question
answering QReCC (Anantha et al., 2020; Vakulenko et al.,
2022) dataset, which we used for developing the DSP ab-
stractions. We report the validation set accuracy on all three
datasets and discuss them in detail §3.5.
Unless otherwise stated, systems are given access to 16-
shot training examples, that is, each DSP program can use
(up to) 16 questions—or conversations, where applicable—
randomly sampled from the respective training set. We
1Though most of the functionality in this section is imple-
mented, the primitives branch, knn, and crossval are currently
work-in-progress.DEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models
subsample the validation and test sets to 1000 questions
(or 400 conversations, where applicable) and report average
quality across ﬁve seeds where each seed ﬁxes a single k-
shot training set of examples. To control the language model
API spending budget, each seed processes one ﬁfth of the
evaluation examples (e.g., 200 questions per seed, for a total
of 1000 unique questions).
We also dedicate held-out test datasets (e.g., Open-
NaturalQuestions; Kwiatkowski et al. 2019) and test tasks
(e.g., claim veriﬁcation, as in FEVER; Thorne et al. 2018)
that we only use for evaluating pre-deﬁned DSP programs
rather than development. We will include these results in a
future version of this report.
3.2. Pretrained Modules
RM We use ColBERTv2 (Santhanam et al., 2022b), a
state-of-the-art retriever based on late interaction (Khattab
& Zaharia, 2020). We choose ColBERTv2 for its highly
effective zero-shot search quality and efﬁcient search (San-
thanam et al., 2022a). However, our DSP programs are
agnostic to how the retriever represents examples or scores
passages, so essentially any retriever can be used.
In addition, by making retrieval a ﬁrst-class construct, DSP
allows us to change or update the search index over time.
We simulate this in our experiments by aligning each of our
datasets with the nearest Wikipedia corpus among the Dec
2016 Wikipedia dump from Chen et al. 2017, the Nov 2017
Wikipedia “abstracts” dump from Yang et al. 2018, and the
Dec 2018 Wikipedia dump from Karpukhin et al. 2020.
LM We use the GPT-3.5 ( text-davinci-002; Brown
et al. 2020; Ouyang et al. 2022) language model. Unless
otherwise stated, we use greedy decoding when generating
n = 1 prediction. We sample with temperature t = 0.7
when n >1, like related work (Wang et al., 2022c).
3.3. Baselines
Vanilla LM The vanilla LM baselines represent the few-
shot in-context learning paradigm used by Brown et al.
(2020). The open-domain QA and multi-hop QA base-
lines randomly sample 16 demonstrations (i.e., all of the
examples available to each program in our evaluation) from
the training set and do not augment these demonstrations
with evidence. Similarly, the conversational QA baseline
samples four conversations. The vanilla baselines do not
search for passages relevant to the input query.
1 def vanilla_LM_QA ( question : str ) -> str :
2 demos = sample ( train , k =16)
3 x = Example ( question = question , demos = demos )
4 return generate ( qa_template )(x). pred
Retrieve-then-Read The “retrieve-then-read” baselines
use the RM to support each example with a potentially rele-
vant passage before submitting the prompt to the LM. This
emulates the pipelines used by state-of-the-art open-domain
question answering systems (Khattab et al., 2021b; Izacard
& Grave, 2020; Hofstätter et al., 2022). In conversational
QA, we concatenate the ﬁrst turn and the ﬁnal question, an
approach that we found to perform much better than simply
using the ﬁnal turn. For multi-hop QA, we retrieve and
concatenate two passages per question.
1 def retrieve_then_read_QA ( question : str ) -> str :
2 demos = sample ( train , k =16)
3 passages = retrieve ( question , k =1)
4 x = Example ( question = question ,
5 passages = passages ,
6 demos = demos )
7 return generate ( qa_template )(x). pred
Self-ask We also compare against self-ask (Press et al.,
2022), a contemporaneous pipeline that can be thought of
as a speciﬁc instantiation of DSP’sSEARCH stage followed
by a simple PREDICT step. For direct comparison with
our methods, we modify the self-ask control ﬂow to query
the same ColBERTv2 index used in our DSP experiments
instead of Google Search. We evaluate two conﬁgurations of
self-ask. The ﬁrst uses the original self-ask prompt template,
which contains four hand-written demonstrations. In the
second conﬁguration, we modify the prompt template to
apply a number of changes that we ﬁnd are empirically
useful for HotPotQA.2
3.4. Proposed DSP Programs
We build on transformations presented in §2. Our programs
for all three tasks have the following structure, illustrated
for open-domain QA.
1 def openqa_program ( question : str ) -> str :
2 x = Example ( question = question , train = train )
3 x = openqa_demonstrate (x)
4 x = openqa_search (x)
5 x = openqa_predict (x)
6 return x. answer
The exception is that the conversational QA program,
2In particular: (i) use ColBERTv2-style passages in the hand-
crafted demonstrations of self-ask (i.e., instead of the original
Google-style snippets), (ii) concatenate 16-shot training examples
from the task (i.e., question–answer pairs) as a preﬁx of the prompt,
(iii) ask the model to generate a short intermediate answer per
retrieval step, and(iv) explicitly ask the model to generate a follow-
up “search query” at each step. We found the ﬁnal item to be
important because self-ask’s default prompt often produces follow-
up questions that are not self-contained (e.g., “what is the name of
the national park?”, which is not an informative search query). We
also ﬁx the casing in the prompt to be consistent.DEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models
Table 1.Development results comparing a task-aware DSP program against baseline vanilla LM and retrieve-then-read LM as well as
recent and contemporaneous in-context learning approaches with and without retrieval. All of our runs use GPT-3.5 and our retrieval-based
rows use ColBERTv2. The results marked with ¶ are collected from related work as of mid-December 2022, and attributed to their
individual sources in the main text. As we discuss in the main text, the marked results are not generally apples-to-apples comparisons,
since they span a variety of evaluation settings. Nonetheless, we report them here as qualitative reference points.
Open-SQuAD HotPotQA QReCC
EM F1 EM F1 F1 nF1
Vanilla LM 16.2 25.6 28.3 36.4 29.8 18.4
No-retrieval LM SoTA 20.2¶ – 33.8 ¶ 44.6¶ – –
Retrieve-then-Read 33.8 46.1 36.9 46.1 31.6 22.2
Self-ask w/ ColBERTv2 Search 9.3 17.2 25.2 33.2 – –
+ Reﬁned Prompt 9.0 15.7 28.6 37.3 – –
Retrieval-augmented LM SoTA 34.0¶ – 35.1 ¶ – – –
Task-aware DSP Program 36.6 49.0 51.4 62.9 35.0 25.3
convqa_program, accepts turns (i.e., a list of strings, rep-
resenting the conversational history) instead of a single
question. Unless otherwise stated, our programs default to
greedy decoding during the DEMONSTRATE stage.
For SEARCH , our open-domain QA program uses the ques-
tion directly for retrieving k = 7 passages and concate-
nates these passages into our QA prompt with CoT. For
PREDICT , it generates n = 20 reasoning chains and uses
self-consistency (SC; Wang et al. 2022c) to select its ﬁnal
prediction. For DEMONSTRATE , our open-domain QA pro-
gram uses the following approach, slightly simpliﬁed for
presentation. In it, the parameter k = 3passed to annotate
requests annotating only three demonstrations, which will
then be used in the prompts.
1 def openqa_demonstrate (x: Example ) -> Example :
2 demos = sample (x. train , k =16)
3
4 def openqa_attempt (d: Example ) -> Example :
5 d. demos = all_but ( demos , d) # all ( raw )
examples different from d
6
7 d = openqa_search (d, k =2)
8 if not passage_match (d): return None # skip
examples where search fails
9
10 d = openqa_predict (d, sc= False )
11 if not answer_match (d): return None # skip
examples where predict fails
12
13 return d
14
15 x. demos = annotate ( demos , openqa_attempt , k =3)
16 return x
Our multi-hop program adopts a very similar approach for
DEMONSTRATE and PREDICT . For SEARCH , it uses the
approach described in §2.4, with the following adjustments.
It uses result fusion across n = 10 queries per hop and,
among the n predictions, uses the summary corresponding
to the largest average log-probability. It uses a ﬁxed number
of hops for HotPotQA, i.e., two hops. In each prompt (i.e.,
each hop and QA), it concatenates the summaries of all
previous hops (i.e., hop 1 onwards) and a total of k = 5
passages divided between the hops (i.e., ﬁve passages from
the ﬁrst hop or two passages from the ﬁrst and three from
the second).
For conversational QA, we use a simple PREDICT which
generates a response with greedy decoding, conditioned
on all of the previous turns of the conversation and ﬁve
retrieved passages. For SEARCH , our conversational QA
pipeline generates n = 10re-written queries (and also uses
the simple query as the retrieve-and-read baseline; §3.3) and
fuses them as in §2.4. We implement DEMONSTRATE simi-
lar to openqa_demonstrate, but sample only four examples
(i.e., four conversational turns; instead of 16 questions as in
open-domain QA) for demonstrating the task for the higher-
order transformation convqa_attempt, which is passed to
annotate (not shown for brevity).
1 def convqa_attempt (d: Example ) -> Example :
2 d. demos = all_but ( demos , d) # all ( raw )
examples that don /quotesingle.Vart intersect with the
conversation of d
3
4 d = convqa_search (d, k =2)
5 if max ( precision (d. answer , p) for p in
d. passages ) < .8: return None # skip examples
where search fails
6
7 d = convqa_predict (d, n =20)
8 if max (F1(c.pred , d. answer ) for c in
d. candidates ) < .75: return None # skip
examples where predict fails out of n =20
attempts
9
10 return d
3.5. Development Datasets & Results
Open-SQuAD We conduct the open-domain version of
SQuAD over the Wikipedia 2016 corpus from Chen et al.
(2017), as processed by Khattab et al. (2021b). We use theDEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models
same train/validation/test splits as in Karpukhin et al. (2020)
and Khattab et al. (2021b).
Table 1 reports the answer EM and F1. The task-awareDSP
program achieves 36.6% EM, outperforming the vanilla LM
baseline by 126% EM relative gains. This indicates the im-
portance of grounding the LM’s predictions in retrieval, and
it shows that state-of-the-art retrievers like ColBERTv2 have
the capacity to do so off-the-shelf. The proposed DSP pro-
gram also achieves relative gains of 8% in EM and 6% in F1
over the retrieve-then-read pipeline, highlighting that non-
trivial gains are possible by aggregating information across
several retrieved passages as we do with self-consistency.
These in-context learning results are competitive with a
number of popular ﬁne-tuned systems. For instance, on
the Open-SQuAD test set, DPR achieves 29.8% EM, well
below our 16-shot DSP program. On the Open-SQuAD
dev set, the powerful Fusion-in-Decoder (Izacard & Grave,
2020) “base” approach achieves approximately 36% (i.e.,
very similar quality to our system) when invoked with ﬁve
retrieved passages. Nonetheless, with the default setting
of reading 100 passages, their system reaches 48% EM in
this evaluation. This may indicate that similar gains are
possible for our DSP program if the PREDICT stage is made
to aggregate information across many more passages.
For comparison, we also evaluate the self-ask pipeline,
which achieves 9.3% EM, suggesting that its ﬁxed pipeline
is ineffective outside its default multi-hop setting. Study-
ing a few examples of its errors reveals that it often de-
composes questions in tangential ways and answers these
questions instead. We refer to this behavior of the LMas
“self-distraction”, and we believe it adds evidence in favor of
our design decisions in DSP . To illustrate self-distraction,
when self-ask is prompted with “When does The Kidnap-
ping of Edgardo Mortara take place?”, it asks “What is The
Kidnapping of Edgardo Mortara“ and then asks when it was
published, a tangential question. Thus, self-ask answers
“1997”, instead of the time The Kidnapping of Edgardo
Mortara takes place (1858).
For reference, Table 1 also reports (as No-retrieval LM
SoTA) the concurrent in-context learning results from Si
et al. (2022) using code-davinci-002, who achieve 20.2%
EM without retrieval and 34.0% EM with retrieval, albeit
on a different sample and split of the SQuAD data. Overall,
their approaches are very similar to the baselines we im-
plement (vanilla LM and retrieve-then-read), though their
retrieval-augmented approach retrieves (and concatenates
into the prompt) 10 passages from a Wikipedia dump.
HotPotQA We use the open-domain “fullwiki” setting
of HotPotQA using its ofﬁcial Wikipedia 2017 “abstracts”
corpus. The HotPotQA test set is hidden, so we reserve
the ofﬁcial validation set for our testing. We sub-divide
the training set into 90%/10% train/validation splits. In the
training (and thus validation) split, we keep only examples
marked as “hard” in the original dataset, which matches the
designation of the ofﬁcial validation and test sets.
We report the ﬁnal answer EM and F1 in Table 1. On
HotPotQA, the task-aware DSP program outperforms the
baselines and existing work by very wide margins, exceed-
ing the vanilla LM, the retrieve-then-read baseline, and the
self-ask pipeline by 82%, 39%, and 80%, respectively, in
EM. This highlights the effectiveness of building up more
sophisticated programs that coordinate the LM and RM for
the SEARCH step.
These results may be pegged against the evaluation on Hot-
PotQA in a number of concurrent papers. We ﬁrst compare
with non-retrieval approaches, though our comparisons must
be tentative due to variation in evaluation methodologies. Si
et al. (2022) achieve 25.2% EM with CoT prompting. With
a “recite-and-answer” technique for PaLM-62B (Chowdh-
ery et al., 2022), Sun et al. (2022) achieve 26.5% EM. Wang
et al. (2022b) achieve 33.8% EM and 44.6 F1 when apply-
ing a self-consistency prompt for PaLM-540B. Next, we
compare with a contemporaneous retrieval-based approach:
Yao et al. (2022) achieve 35.1% EM using a system capable
of searching using a Wikipedia API. All of these approaches
trail our task-aware DSP program, which achieves 51.4%
EM, by large margins.
QReCC We use QReCC (Anantha et al., 2020) in an open-
domain setting over Wikipedia 2018. QReCC does not have
an ofﬁcial development set, so we sub-divide the training
set into 90%/10% train/validation splits. For the ﬁrst ques-
tion in every conversation, we use the rewritten question
as the original question often assumes access to a ground-
truth document. We also ﬁlter low-quality examples from
QReCC.3
We conduct the QReCC conversations in an auto-regressive
manner. At turn t > 1 of a particular conversation, the
system sees its own responses (i.e., not the ground truth
responses) to previous turns of the conversation. We report
the novel-F1 metric (nF1; Paranjape et al. 2022), which
computes the F1 overlap between the system response and
the ground truth while discounting common stopwords and
terms present in the question (or earlier questions). The
results are shown in Table 1, and follow the same general
pattern as SQuAD and HotPotQA.
3We remove conversations that have one or more empty ground-
truth answers and conversations that have only one or two ques-
tions. We also ﬁnd many conversations that include “what other
interesting facts are in this article?”, which conﬂict with the open-
domain formulation and have no well-deﬁned answer. Hence, we
remove any conversation that includes the keywords “other inter-
esting” or “else”, which we found to be markers of low quality.DEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models
4. Conclusion
For a long time, the dominant paradigm for building models
in AI has centered around multiplication of tensor repre-
sentations, and in the deep learning era this has given rise
to highly modular (layer-wise) designs that allow for fast
development and wide exploration. However, these design
paradigms require extensive domain expertise, and even
experts face substantial challenges when it comes to com-
bining different pretrained components into larger systems.
The promise of in-context learning is that we can build com-
plex systems from pretrained components using only natural
language as the medium for giving systems instructions and,
as we argue for, allowing components to communicate with
each other. In this new paradigm, the building blocks are
pretrained models and the core operations are natural lan-
guage instructions and operations on natural language texts.
If we can realize this potential, then we can broaden partici-
pation in AI system development, rapidly prototype systems
for new domains, and maximize the value of specialized
pretrained components.
In the current paper, we introduced the DEMONSTRATE –
SEARCH –PREDICT (DSP ) framework for retrieval aug-
mented in-context learning. DSP consists of a number of
simple, composable functions for implementing in-context
learning systems as deliberate programs—instead of end-
task prompts—for solving knowledge intensive tasks. We
implemented DSP as a Python library and used it to write
programs for Open-SQuAD, HotPotQA, and QReCC. These
programs deliver substantial gains over previous in-context
learning approaches. However, beyond any particular per-
formance number, we argue that the central contribution of
DSP is in helping to reveal a very large space of conceptual
possibilities for in-context learning in general."
"Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 881–893
November 12-16, 2024 ©2024 Association for Computational Linguistics
Retrieval Augmented Generation or Long-Context LLMs?
A Comprehensive Study and Hybrid Approach
Zhuowan Li1 Cheng Li1 Mingyang Zhang1
Qiaozhu Mei2* Michael Bendersky1
1 Google DeepMind 2 University of Michigan
1 {zhuowan,chgli,mingyang,bemike}@google.com 2 qmei@umich.edu
Abstract
Retrieval Augmented Generation (RAG) has
been a powerful tool for Large Language
Models (LLMs) to efﬁciently process overly
lengthy contexts. However, recent LLMs like
Gemini-1.5 and GPT-4 show exceptional ca-
pabilities to understand long contexts directly.
We conduct a comprehensive comparison be-
tween RAG and long-context(LC) LLMs, aim-
ing to leverage the strengths of both. We
benchmark RAG and LC across various public
datasets using three latest LLMs. Results re-
veal that when resourced sufﬁciently, LC con-
sistently outperforms RAG in terms of aver-
age performance. However, RAG’s signiﬁ-
cantly lower cost remains a distinct advantage.
Based on this observation, we propose S ELF -
ROUTE , a simple yet effective method that
routes queries to RAG or LC based on model
self-reﬂection. S ELF -ROUTE signiﬁcantly re-
duces the computation cost while maintaining
a comparable performance to LC. Our ﬁndings
provide a guideline for long-context applica-
tions of LLMs using RAG and LC.
1 Introduction
Retrieval augmented generation (RAG)has been
shown to be a both effective and efﬁcient approach
for large language models (LLMs) to leverage ex-
ternal knowledge. RAG retrieves relevant informa-
tion based on the query and then prompts an LLM
to generate a response in the context of the retrieved
information. This approach signiﬁcantly expands
LLM’s access to vast amounts of information at a
minimal cost.
However, recent LLMs like Gemini and GPT-4
have demonstrated exceptional capabilities in un-
derstanding long contexts directly. For example,
Gemini 1.5 can process up to 1 million tokens (Reid
et al., 2024). This prompts the need for a system-
atic comparison between long-context (LC) LLMs
*Visiting researcher to Google DeepMind.
performance
(a)cost
(b)
Figure 1: While long-context LLMs (LC) surpass RAG
in long-context understanding, RAG is signiﬁcantly
more cost-efﬁcient. Our approach, SELF -ROUTE , com-
bining RAG and LC, achieves comparable performance
to LC at a much lower cost.
and RAG: on one hand, RAG conceptually acts as
a prior, regularizing the attention of LLMs onto
retrieved segments, thus avoiding the distraction of
the irrelevant information and saving unnecessary
attention computations; on the other hand, large-
scale pretraining may enable LLMs to develop even
stronger long-context capabilities. Therefore, we
are motivated to compare RAG and LC, evaluating
both their performance and efﬁciency.
In this work, we systematically benchmark RAG
and LC on various public datasets, gaining a com-
prehensive understanding of their pros and cons,
and ultimately combining them to get the best of
both worlds. Different from ﬁndings in previous
work (Xu et al., 2023), we ﬁnd that LC consistently
outperform RAG in almost all settings (when re-
sourced sufﬁciently). This demonstrates the su-
perior progress of recent LLMs in long-context
understanding.
Despite the suboptimal performance, RAG re-
mains relevant due to its signiﬁcantly lower compu-
tational cost. In contrast to LC, RAG signiﬁcantly
decreases the input length to LLMs, leading to re-
881duced costs, as LLM API pricing is typically based
on the number of input tokens. (Google, 2024; Ope-
nAI, 2024b)1. Moreover, our analysis reveals that
the predictions from LC and RAG are identical for
over 60% of queries. For these queries, RAG can
reduce cost without sacriﬁcing performance.
Based on this observation, we propose SELF -
ROUTE , a simple yet effective method that routes
various queries to RAG or LC based on model self-
reﬂection. With SELF -ROUTE , we signiﬁcantly re-
duce the cost while achieving overall performance
comparable to LC. For example, the cost is reduced
by 65% for Gemini-1.5-Pro and 39% for GPT-4O.
Fig. 1 shows the comparisons of LC, RAG and
SELF -ROUTE using three recent LLMs: GPT-4O,
GPT-3.5-Turbo and Gemini-1.5-Pro. In addition to
quantitative evaluation, we provide a comprehen-
sive analysis comparing RAG and LC, including
common failure patterns of RAG, the trade-offs
between cost and performance, and the results on
additional synthetic datasets. Our analysis serves
as a starting point, inspiring future improvements
of RAG, and as a empirical guide for building long-
context applications using RAG and LC.
2 Related Work
Long-context LLMs. There has long been ef-
forts for enabling LLMs to handle long contexts
(Guo et al., 2022; Beltagy et al., 2020; Chen et al.,
2023b). While recent LLMs like Gemini-1.5 (Reid
et al., 2024), GPT-4 (Achiam et al., 2023), Claude-
3 (Anthropic, 2024) achieve signiﬁcantly larger
context window size, long-context prompting is
still expensive due to the quadratic computation
cost of transformers regarding to the input token
numbers. Recent work proposes methods to reduce
cost by prompt compression (Jiang et al., 2023),
model distillation (Hsieh et al., 2023), or LLM cas-
cading (Chen et al., 2023a).
Retrieval-augmented generation. Augmenting
LLMs with relevant information retrieved from var-
ious sources (Lewis et al., 2020) has been success-
ful in complementing LLMs with external knowl-
edge. RAG achieves good performance on tasks
like language modeling (Khandelwal et al., 2019;
Shi et al., 2023) and QA (Guu et al., 2020; Izacard
and Grave, 2020), with a signiﬁcantly lower compu-
tation cost (Borgeaud et al., 2022). Related to but
different from our work, recently works augment
1While retrieval may introduce extra cost, retrieval system
is much easier to set up and can be hosted on customer side.
RAG with correction (Yan et al., 2024), critique
(Asai et al., 2023), veriﬁcation (Li et al., 2023), or
adaptive search (Wang et al., 2023; Cheng et al.,
2024; Jeong et al., 2024) to improve retrieval qual-
ity on knowledge-intensive tasks.
Long-context evaluation. Evaluating long-
context models is challenging due to the difﬁculty
in collecting and analyzing long texts. Recent re-
searchers propose both synthetic tests like needle-
in-a-haystack (Greg Kamradt, 2023), Ruler (Hsieh
et al., 2024), or Counting Stars (Song et al., 2024),
and real datasets including LongBench (Bai et al.,
2023), ∞Bench (Zhang et al., 2024), L-Eval (An
et al., 2023), and others (Shaham et al., 2022; Yuan
et al., 2024; Maharana et al., 2024). Evaluating
on these datasets, recent works study the perfor-
mance degradation over various context lengths
(Levy et al., 2024; Hsieh et al., 2024), the lost-
in-the-middle phenomenon (Liu et al., 2024), and
explore solutions (Kuratov et al., 2024). Related
to our work, Xu et al. (2023) compare RAG and
long-context prompting and ﬁnd that long-context
models still lags behind RAG. This is different
from our ﬁndings, possibly due to consideration of
stronger LLMs and longer contexts in our work.
3 Benchmarking RAG versus LC
3.1 Datasets and metrics
We evaluate on a subset of datasets from Long-
Bench (Bai et al., 2023) and ∞Bench (Zhang et al.,
2024), which are recent benchmarks containing a
collection of new and existing datasets for LLM
evaluation, covering both synthetic and real texts in
multiple languages. LongBench contains a collec-
tion of 21 datasets, with an average context length
of 7k words. ∞Bench consists of even longer con-
texts with an average length of 100k tokens.
Among the datasets, we mainly focus on tasks
that are (a) in English, (b) real, and (c) query-based
(e.g. summarization tasks do not contain queries
for retrieving relevant information). This results in
7 datasets from LongBench including NarrativeQA
(Koˇcisk`y et al., 2018), Qasper (Dasigi et al., 2021),
MultiFieldQA (Bai et al., 2023), HotpotQA (Yang
et al., 2018), 2WikiMultihopQA (Ho et al., 2020),
MuSiQue (Trivedi et al., 2022), QMSum (Zhong
et al., 2021); and 2 datasets from ∞Bench includ-
ing En.QA and EN.MC. Please refer to Appendix A
for more details. Additionally, in Sec. 5.4, we will
provide an ablation a synthetic datasets PassKey
from ∞Bench.
882For evaluation metrics, we report F1 scores for
the open-ended QA tasks, accuracy for the multi-
choice QA tasks, and ROUGE score for the sum-
marization tasks.
3.2 Models and Retrievers
Three latest LLMs are evaluated, including Gemini-
1.5-Pro (Reid et al., 2024), GPT-4O (OpenAI,
2024a), and GPT-3.5-Turbo (OpenAI, 2023) 2.
Gemini-1.5-Pro is a recent long-context LLM from
Google, supporting up to 1 million tokens. GPT-
4O, the newest lightweight yet strong LLM from
OpenAI, supports 128k tokens. GPT-3.5-Turbo
supports 16k tokens.
Two retrievers are used in our study: Contriever
(Izacard et al., 2021), which is a contrastively
trained dense retriever outperforming BM25 on
BEIR datasets, and Dragon (Lin et al., 2023), which
is a recent generalizable dense retriever achieving
high performance in both supervised and zero-shot
settings without complex late interaction. Follow-
ing (Xu et al., 2023), we divide long contexts into
chunks of 300 words, and select the top k chunks
(default k = 5) based on the cosine similarity of
the query embedding and the chunk embeddings.
The chunks are ordered by the similarity scores,
with the chunk index prepended at the beginning.
Since black-box LLMs are pretrained on un-
known datasets, the leakage of evaluation datasets
may occur. Especially, some of the evaluation
datasets are based on Wikipedia, which has likely
been seen by LLMs during during. In some cases,
we ﬁnd that model may predict the correct answer
using exactly the same words as the groundtruth
(e.g. “meticulously”), even when they do not appear
in the provided context. In our experiment, we try
mitigating this issue by prompting the model to an-
swer “based only on the provided passage”
for both RAG and LC. It remains an open ques-
tion how to address the data leakage issue in LLM
evaluation.
3.3 Benchmarking results
We benchmark the performance of LC and RAG
across the nine datasets, using three recent LLMs:
Gemini-1.5-Pro, GPT-4O and GPT-3.5-Turbo.
Tab. 1 presents the results using the Contriever
retriever, where rows *-1 and rows *-2 present the
benchmarking results for LC and RAG respectively.
Results using the Dragon retriever will be discussed
2gpt-3.5-turbo-0125, gpt-4o-2024-05-13
in Sec. 5.3 and Tab. 2.
As shown in Tab. 1, LC consistently outperforms
RAG for all the three models, with a signiﬁcant
margin. On average, LC surpasses RAG by 7.6%
for Gemini-1.5-Pro, 13.1% for GPT-4O, and 3.6%
for GPT-3.5-Turbo. Noticeably, the performance
gap is more signiﬁcant for the more recent mod-
els (GPT-4O and Gemini-1.5-Pro) compared to
GPT-3.5-Turbo, highlighting the exceptional long-
context understanding capacity of the latest LLMs.
However, there is an exception observed on the
two longer datasets from ∞Bench (i.e., En.QA and
En.MC), where RAG achieves higher performance
than LC for GPT-3.5-Turbo. This result deviates
from the overall trend, likely due to the signiﬁcantly
longer context in these datasets (147k words on av-
erage) compared with the limited context window
(16k) of GPT-3.5-Turbo. This ﬁnding highlights
the effectiveness of RAG when the input text con-
siderably exceeds the model’s context window size,
emphasizing a speciﬁc use case of RAG.
4 Self-Route
4.1 Motivation
As demonstrated in Sec. 3, RAG lags behind long-
context LLMs in terms of performance. However,
despite this performance gap, we surprisingly ﬁnd
a high degree of overlap in their predictions, as
illustrated in Fig. 2.
𝑆!""#	−	𝑆$%
Figure 2: Distribution of the difference of predic-
tion scores between RAG and LC (computed w.r.t.
groundtruth labels). RAG and LC predictions are
highly identical, for both correct and incorrect ones.
Fig. 2 displays the distribution of the differences
between RAG prediction scores SRAG and LC pre-
diction scores SLC, speciﬁcally SRAG −SLC (the
scores are multiplied by 100 to be scaled to 1-100).
These scores S represent the evaluation of model
predictions against the groundtruth. Notably, for
883Avg Narr Qasp Mult Hotp 2Wiki Musi Sum En.QA En.MC
1-1 LC 49.70 32.76 47.83 52.33 61.85 62.9640.22 20.73 43.08 85.57
Gemini-1.5-Pro
1-2 RAG 37.33 22.54 44.68 49.53 48.36 54.24 26.56 19.51 19.46 51.09
1-3 S ELF-ROUTE 46.41 28.32 45.23 51.47 55.18 62.68 40.66 19.77 37.51 76.86
1-4 answerable %76.78 73.00 85.00 96.67 84.50 81.00 58.50 93.50 56.41 62.45
1-5 token % 38.39 23.07 49.93 36.88 32.97 53.49 56.14 17.96 42.25 32.84
GPT-4O
2-1 LC 48.67 32.78 44.54 55.28 62.42 70.69 41.65 21.92 32.36 76.42
2-2 RAG 32.60 18.05 46.02 50.74 36.86 50.21 16.09 19.97 14.43 41.05
2-3 S ELF-ROUTE 48.89 31.36 47.99 53.17 62.14 70.14 41.69 21.31 34.95 77.29
2-4 answerable %57.36 44.00 67.50 94.00 52.50 62.00 30.00 92.00 27.07 47.16
2-5 token % 61.40 66.40 72.25 39.65 65.79 77.05 85.00 20.26 73.01 53.21
GPT-3.5-Turbo
3-1 LC 32.07 23.34 42.96 49.19 45.33 41.04 17.92 19.61 14.73 34.50
3-2 RAG 30.33 18.22 38.15 49.21 37.84 35.16 16.41 18.94 15.39 43.67
3-3 S ELF-ROUTE 35.32 24.06 38.65 52.07 47.28 44.62 34.44 19.88 22.03 44.54
3-4 answerable %74.10 71.50 80.00 91.33 68.50 69.00 47.00 93.50 50.43 95.63
3-5 token % 38.85 20.56 55.08 35.29 48.70 65.91 65.08 16.40 38.17 4.50
Table 1: Results of Gemini-1.5-Pro, GPT-3.5-Turbo, and GPT-4O using the Contriever retriever. LC consistently
outperforms RAG, while SELF -ROUTE achieves performance comparable to LC using much less tokens.
most queries, RAG scores and LC scores are highly
similar. In fact, for 63% queries, the model pre-
dictions are exactly identical; and for 70% queries,
the score difference is less than 10 (absolute value).
Interestingly, the identical predictions are not nec-
essarily correct, as shown by the varying colors rep-
resenting the average score, i.e., (SRAG + SLC)/2.
This observation suggests that RAG and LC tend
to make not only the same correct predictions but
also similar errors.
This ﬁnding motivates us to leverage RAG for
the majority of queries, reserving computationally
more expensive LC for a small subset of queries
where it truly excels. By doing so, RAG can signif-
icantly reduce computational costs without sacriﬁc-
ing overall performance.
4.2 Self-Route
Based on the above motivation, we propose SELF -
ROUTE , a simple yet effective method combining
RAG and LC to reduce cost while maintaining a
performance comparable to LC. SELF -ROUTE uti-
lizes LLM itself to route queries based on self-
reﬂection, under the assumption that LLMs are
well-calibrated in predicting whether a query is
answerable given provided context.
Concretely, our method consists of two steps: a
RAG-and-Route step and a long-context prediction
step. In the ﬁrst step, we provide the query and
the retrieved chunks to the LLM, and prompt it to
predict whether the query is answerable and, if so,
generate the answer. This is similar to standard
RAG, with one key difference: the LLM is given
the option to decline answering with the prompt
“Write unanswerable if the query can not
be answered based on the provided text” .
For the queries deemed answerable, we accept the
RAG prediction as the ﬁnal answer. For the queries
deemed unanswerable, we proceed to the second
step, providing the full context to the long-context
LLMs to obtain the ﬁnal prediction (i.e., LC).
As our results will demonstrate, most queries can
be solved by the ﬁrst RAG-and-Route step ( e.g.,
82% for Gemini-1.5-Pro), with only a small por-
tion requiring the following long-context prediction
step. Since the RAG-and-Route step only needs
the retrieved chunks ( e.g., 1.5k tokens) as input,
which is signiﬁcantly shorter than the full contexts
(e.g., 10k - 100k tokens), the overall computation
cost is substantially reduced. Detailed token count
analysis will be provided in the results.
4.3 Results
Rows *-3 to *-5 in Tab. 1 present the results of our
method, utilizing the three LLMs. Rows *-3 report
the performance. Rows *-4 show the percentage
of answerable queries, as predicted in the RAG-
and-Route step. Rows *-5 display the percentage
of tokens used by our method, compared to that
of LC. In terms of performance (rows *-3), SELF -
ROUTE signiﬁcantly outperforms RAG, achieving
results comparable to LC. Across all three models,
SELF -ROUTE surpasses RAG (rows *-2) by over
5%. Compared to LC (rows *-1), there is a slight
performance drop for GPT-4O (-0.2%) and Gemini-
1.5-Pro (-2.2%), but an improvement for GPT-3.5-
Turbo (+1.7%).
All three LLMs consistently route more than half
884of queries towards RAG, as shown in rows *-4. For
Gemini-1.5-Pro, the answerable percentage even
reaches 81.74% (row 1-4). This indicates that RAG
may answer most queries without the need for LC,
conﬁrming our initial motivation.
Due to the high answerable rate, the number of
tokens required is signiﬁcantly reduced (rows *-
5). For example, GPT-4O uses only 61% tokens
while achieving comparable performance (46.83)
with LC (47.04), Gemini-1.5-Pro uses 38.6% of
the tokens. Since the computation cost of the
transformer-based LLMs is quadratic to token
count, and most LLM APIs charge based on token
count (OpenAI, 2024b; Google, 2024), this lower
token count translates to substantial cost savings.
On longer datasets, the advantage of our method
is more pronounced for OpenAI models, but less
signiﬁcant for Gemini. For instance, for GPT-4O,
SELF -ROUTE outperforms LC by 2.3% and 7.4%
respectively on EN.QA and EN.MC, which contain
longer contexts. For GPT-3.5-Turbo, the advantage
margins are even larger. However, for Gemini-
1.5-Pro, the performance is lower than LC. These
different behaviors are possibly due to the differ-
ence in LLM alignments, i.e., OpenAI models are
more likely to reject answering using RAG, leading
to a lower answerable percentage but higher accu-
racy, which results in a different performance-cost
trade-off compared with Gemini-1.5-Pro.
5 Analysis
5.1 Ablations of k
Both RAG and SELF -ROUTE relies on the top- k
retrieved text chunks. The larger k is, the longer
context are fed into LLMs for RAG prediction as
well as routing, resulting in different costs versus
performances. To study the inﬂuence ofk, in Fig. 3,
we plot the performance and cost (i.e. input token
percentage) curves when different ks are used.
In terms of performance, for both RAG and
SELF -ROUTE , a larger k leads to better perfor-
mance. While k increases, more and more chunks
are fed into the LLMs, thus the performance grad-
ually improves to approach LC. As can be seen in
from the curves, the advantage of SELF -ROUTE
is the most signiﬁcant for smaller k. For example,
when k = 1, RAG gets from 20.24% while SELF -
ROUTE gets 37.9%, while when k is larger than 50,
all three methods get similar performance.
However, the trend of cost is not monotonous
for SELF -ROUTE . As seen, the cost reaches its
performance
top-k
token percentage
(a)
(b)
top-k
Figure 3: Trade-off curves between (a) model perfor-
mance and (b) token percentage as a function of k.
minimum at k = 5. This is because when k in-
creases, the cost of RAG (and routing) increases,
but more queries are routed to RAG from LC, thus
the overall cost may decrease. The sweet point ofk
might be different for each dataset, e.g. on average,
k = 5has the lowest cost as shown in the curves,
but on some datasets, especially ones that contain
extractive questions which does not need multi-hop
reasoning (like NarrativeQA and QMSum), k = 1
leads to the lowest cost. This indicates that the opti-
mal k depends on the nature of the task, as well as
the performance requirement. We encourage future
researchers to look for different ks when applying
our method to various applications.
5.2 Why does RAG fail?
To gain a better understanding of why RAG lags
behind LC, we analyze the failure reasons for the
examples that cannot be answered by RAG. We
ﬁrst manually check some examples for which our
RAG-and-Route step predicts “unanswerable” and
summarize four typical failure reasons, then prompt
LLM to classify all the examples.
The four reasons include: (A) The query requires
multi-step reasoning so the results of previous steps
are needed to retrieve information for later steps,
e.g. “What nationality is the performer of
song XXX”. (B) The query is general, e.g. “What
does the group think about XXX” , which is
challenging for the retriever to formulate a good
query. (C) The query is long and complex, which
is challenging for the retriever to understand. How-
ever, answering this kind of questions is arguably,
885Avg Narr Qasp Mult Hotp 2Wiki Musi Sum En.QA En.MC
1 LC 49.70 32.76 47.83 52.33 61.85 62.96 40.22 20.73 43.08 85.57
Dragon
2 RAG 38.09 21.91 44.33 53.08 51.61 50.05 30.47 19.93 21.25 50.22
3 combine 46.81 28.50 43.82 54.62 56.58 60.62 40.66 20.07 37.79 78.60
4 RAG ratio 77.88 74.00 84.00 97.33 86.00 77.00 66.00 95.50 61.25 59.83
5 Token ratio 37.87 19.31 54.15 34.78 32.64 55.65 48.16 16.64 38.71 40.83
Table 2: Results for Gemini-1.5-Pro using Dragon retriever.
an advantage of LLMs. (D) The query is implicit,
demanding a thorough understanding of the en-
tire context. For instance, in a lengthy conversa-
tional narrative about a space voyage, a question
like “What caused the shadow behind the
spaceship?” requires readers to connect the dots
and deduce the answer, as there is no explicit men-
tion of the shadow when the cause is revealed.
number of queries
Figure 4: Distribution of typical RAG failure reasons.
Using these reasons, we prompt Gemini-1.5-Pro
with few-shot in-context examples that we man-
ually annotated, to classify all the unanswerable
examples into these four categories, plus an “other""
option. Fig. 4 shows the distribution of failure rea-
sons on the seven datasets in LongBench. Each
dataset may contain different number of RAG fail-
ure cases, resulting in various bar heights. The
distribution patterns are consistent with the nature
of the datasets. For example, the three Wikipedia-
based multi-hop reasoning datasets (HotpotQA,
2WikiMQA, MuSiQue) are challenging for RAG
because of multi-step retrieval as shown in blue.
For NarrativeQA, which are long stories containing
a lot of dialogues, most failure cases are due to im-
plicit queries that requires understanding the whole
context (shown in green). For QMSum, which is a
summarization dataset contains open-ended ques-
tions, failures are mostly due to general queries
(shown in red). We manually checked the exam-
ples classiﬁed as “others” and ﬁnd that most of
them are actually multi-step questions, often with
ambiguities, which poses challenges for answering.
We hope this failure analysis inspires future im-
provements of RAG. For example, engaging chain-
of-thought (Wei et al., 2022) into RAG may help ad-
dress the multi-step questions, and revisiting query
understanding techniques like query expansion (Lv
and Zhai, 2009; Zhai and Lafferty, 2001) may help
with the general queries and complex queries. We
are also glad to see recent efforts towards the direc-
tion (Chan et al., 2024; Ma et al., 2023).
5.3 Different retrievers
The results using a retriever, Dragon, is shown in
Tab. 2 based on Gemini-1.5-Pro. As can be seen,
the results are consistent with Contriever, for all
of LC, RAG, and SELF -ROUTE , showing that our
ﬁndings are generalizable across retrievers.
5.4 Results on synthetic data
In this study, we mainly focus on real datasets, with
a consideration that results on synthetic data, which
are artiﬁcially created by researchers, may subject
to dataset artifacts. We notice some methods that
researchers adopted to create synthetic long context
datasets may unconsciously, but largely, inﬂuence
the performance comparison between RAG and LC.
For example, here we describe the results on the
“PassKey” dataset in∞Bench and its variations.
This “PassKey” dataset presents a needle-in-a-
haystack test, where a sentence with a passkey
(e.g. “the passkey is 123456”) is hidden within
chunks of irrelevant text, and the model is asked
to answer the question “What is the passkey”.
The task requires strong retrieval capability. On
this dataset, RAG achieves 80.34% accuracy, out-
performing LC, which gets 65.25% using Gemini-
1.5-Pro. However, if the query is slightly modi-
ﬁed as “What is the special token hidden
inside the texts”, RAG accuracy sharply drops
to only 4.58%, while LC keeps roughly the same
(69.32%). Another example: if the chunks contain
two passkeys and the query is “Which passkey
is larger? First or second?” , then RAG
(47.63%) under-performs LC (64.24%) as well.
886RAG LC
Original 80.34 65.25
Variant-1: “special token” 4.58 69.32
Variant-2: “which is larger” 47.63 64.24
Table 3: Synthetic dataset may unconsciously contain
artifacts that inﬂuence the comparison results.
Tab. 3 summarizes the results, which demonstrates
that the evaluation highly subjects to artifacts in
dataset construction, showing limitation of syn-
thetic testing.
5.5 Exclusion of LLM’s internal knowledge
Ideally, the comparison in this paper should ex-
clude the model’s internal knowledge (i.e., para-
metric knowledge) so that the model’s performance
are solely based on its capability to understand long
contexts. In our study, this internal knowledge is
excluded by utilizing the prompt “based only on
the provided passage”, which we empirically ﬁnd is
a simple yet effective method. Here we discuss the
effectiveness of this method, as well as alternative
methods to exclude external knowledge.
First, we validate the effectiveness of the sim-
ple prompt “based only on the provided passage”.
Tab. 4 compares the performance (long-context) of
Gemini-1.5-Pro with and without this prompt. As
shown, using this prompt consistently limits the
model’s performance (average performance drops
from 50.57 to 45.53), which indicates that using
this simple instruction can already effectively limit
the usage of the model’s parametric knowledge.
without
""based only on ...""
with
""based only on ...""
NarrativeQA 36.35 32.76
Qasper 50.69 47.83
MultiFieldQA 56.07 52.33
HotpotQA 66.47 61.85
2WikiMQA 68.97 62.96
Musique 54.56 40.22
QMSum 20.87 20.73
En.QA 49.20 43.08
En.MC 90.83 85.57
Avg 50.57 45.53
Table 4: Comparison of the long-context performance
of Gemini-1.5-Pro, using the prompt with and without
“based only on the provided passage”.
Second, as an alternative method to exclude
internal knowledge, we remove the questions
where the model can correctly answer without
any contexts (i.e., commonsense questions), and
report the model’s performance only on the non-
commonsense questions. Tab. 5 shows the perfor-
mance of Gemini-1.5-Pro and GPT-3.5-Turbo on
all the questions from the MuSiQue dataset, as well
as their performance on the non-commonsense sub-
set3. As shown, after excluding the commonsense
questions, the trend remains the same.
all questions w/o commonsense
Gemini GPT-3.5 Gemini GPT-3.5
# questions 200 200 133 150
LC 40.22 17.92 31.76 13.00
RAG 26.56 16.41 15.51 13.05
Self-Route 40.66 34.44 31.32 19.76
answerable % 58.50 47.00 52.63 45.33
token % 56.14 65.08 48.46 53.43
Table 5: Results on MuSiQue on all questions, and on
the subset of non-commonsense questions (i.e., exclud-
ing questions that can be answered without contexts).
That said, a more thorough study to explore vari-
ous methods for controlling the usage of model’s
internal knowledge, and to study the source of in-
ternal knowledge (e.g. LLM""s world knowledge
or dataset leakage), will be valuable future work,
which we hope can be further investigated.
6 conclusion
This paper presents a comprehensive comparison of
RAG and LC, highlighting the trade-offs between
performance and computational cost. While LC
demonstrate superior performance in long-context
understanding, RAG remains a viable option due
to its lower cost and advantages when the input
considerably exceeds the model’s context window
size. Our proposed method, which dynamically
routes queries based on model self-reﬂection, ef-
fectively combines the strengths of both RAG and
LC, achieving comparable performance to LC at a
signiﬁcantly reduced cost. We believe our ﬁndings
contribute valuable insights for the practical appli-
cation of long-context LLMs and pave the way for
future research in optimizing RAG techniques.
3Different models may learn different internal knowledge,
resulting in different numbers of non-commonsense ques-
tions. For example, GPT-3.5-Turbo gets 14.53 performance on
MuSiQue while Gemini-1.5-Pro gets 23.58 using only internal
knowledge,
887"
"Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 7969–7992
December 6-10, 2023 ©2023 Association for Computational Linguistics
Active Retrieval Augmented Generation
Zhengbao Jiang1∗ Frank F. Xu1∗ Luyu Gao1∗ Zhiqing Sun1∗ Qian Liu2
Jane Dwivedi-Yu3 Yiming Yang1 Jamie Callan1 Graham Neubig1
1Language Technologies Institute, Carnegie Mellon University
2Sea AI Lab 3FAIR, Meta
{zhengbaj,fangzhex,luyug,zhiqings,gneubig}@cs.cmu.edu
Abstract
Despite the remarkable ability of large lan-
guage models (LMs) to comprehend and gen-
erate language, they have a tendency to hal-
lucinate and create factually inaccurate out-
put. Augmenting LMs by retrieving informa-
tion from external knowledge resources is one
promising solution. Most existing retrieval aug-
mented LMs employ a retrieve-and-generate
setup that only retrieves information once based
on the input. This is limiting, however, in
more general scenarios involving generation
of long texts, where continually gathering in-
formation throughout generation is essential. In
this work, we provide a generalized view of ac-
tive retrieval augmented generation, methods
that actively decide when and what to retrieve
across the course of the generation. We propose
Forward-Looking Active REtrieval augmented
generation (FLARE), a generic method which
iteratively uses a prediction of the upcoming
sentence to anticipate future content, which is
then utilized as a query to retrieve relevant doc-
uments to regenerate the sentence if it contains
low-confidence tokens. We test FLARE along
with baselines comprehensively over 4 long-
form knowledge-intensive generation tasks/-
datasets. FLARE achieves superior or compet-
itive performance on all tasks, demonstrating
the effectiveness of our method.1
1 Introduction
Generative language models (LMs) (Brown et al.,
2020; Ouyang et al., 2022; OpenAI, 2023; Chowd-
hery et al., 2022; Zhang et al., 2022; Touvron et al.,
2023; Zhao et al., 2023) have become a founda-
tional component in natural language processing
(NLP) systems with their remarkable abilities. Al-
though LMs have memorized some world knowl-
edge during training (Petroni et al., 2019; Roberts
et al., 2020; Jiang et al., 2020), they still tend to
∗Lead contributors.
1Code and datasets are available athttps://github.com/
jzbjyb/FLARE.
hallucinate and create imaginary content (Maynez
et al., 2020; Zhou et al., 2021). Augmenting LMs
with retrieval components that look up relevant in-
formation from external knowledge resources is a
promising direction to address hallucination (Khan-
delwal et al., 2020; Izacard et al., 2022).
Retrieval augmented LMs commonly use a
retrieve-and-generate setup where they retrieve doc-
uments based on the user’s input, and then generate
a complete answer conditioning on the retrieved
documents (Chen et al., 2017; Guu et al., 2020;
Lewis et al., 2020; Izacard and Grave, 2021; Sachan
et al., 2021; Lee et al., 2021; Jiang et al., 2022;
Izacard et al., 2022; Nakano et al., 2021; Qian
et al., 2023; Lazaridou et al., 2022; Shi et al., 2023).
These single-time retrieval augmented LMs outper-
form purely parametric LMs, particularly for short-
form knowledge-intensive generation tasks such
as factoid question answering (QA) (Kwiatkowski
et al., 2019; Joshi et al., 2017), where the informa-
tion needs are clear in the user’s input, and it is
sufficient to retrieve relevant knowledge once solely
based on the input.
Increasingly powerful large LMs have also
demonstrated abilities in more complex tasks that
involve generating long-form output, such as long-
form QA (Fan et al., 2019; Stelmakh et al., 2022),
open-domain summarization (Cohen et al., 2021;
Hayashi et al., 2021; Giorgi et al., 2022), and
(chain-of-thought; CoT) reasoning (Wei et al.,
2022; Ho et al., 2020; Geva et al., 2021; Hendrycks
et al., 2020). In contrast to short-form generation,
long-form generation presents complex informa-
tion needs that are not always evident from the in-
put alone. Similar to how humans gradually gather
information as we create content such as papers,
essays, or books, long-form generation with LMs
would require gathering multiple pieces of knowl-
edge throughout the generation process . For ex-
ample, to generate a summary about a particular
topic, the initial retrieval based on the topic name
7969Generate a summary about Joe Biden.
Search results:   !![1]: …[2]: …
Joe Biden (born November 20, 1942) is the 46th president of the United States.Joe Biden (born November 20, 1942) is the 46th president of the United States.
He graduated from the University of Delaware in 1965 with a Bachelor of Arts in history and political science.
Joe Biden attended the University of Pennsylvania, where he earned a law degree.
Retriever
InputStep 1
Search results:   !""![1]: …[2]: …
""####
""#$
#$
$
Step 2
Joe Biden announced his candidacy for the 2020 presidential election on April 25, 2019.
Joe Biden announced his candidacy for the 2020 presidential election on August 18, 2019.""#%
#%
Step 3
Search results:   !""""[1]: …[2]: …Retrieveddocuments
LM
Generation
$
%$
%%
Figure 1: An illustration of forward-looking active retrieval augmented generation (FLARE). Starting with the user
input x and initial retrieval results Dx, FLARE iteratively generates a temporary next sentence (shown in gray
italic) and check whether it contains low-probability tokens (indicated with underline). If so (step 2 and 3), the
system retrieves relevant documents and regenerates the sentence.
(e.g., Joe Biden) may not cover all aspects and de-
tails. It is crucial to retrieve extra information as
needed during generation, such as when generat-
ing a certain aspect (e.g., Joe Biden’s education
history) or a specific detail (e.g., the date of Joe
Biden’s presidential campaign announcement).
Several attempts have been made to retrieve mul-
tiple times throughout generation. These attempts
include methods that passively use the past context
to retrieve additional information at a fixed interval
(Khandelwal et al., 2020; Borgeaud et al., 2022;
Ram et al., 2023; Trivedi et al., 2022) which might
not accurately reflect what LMs intend to gener-
ate in the future or retrieve at inappropriate points.
Some works in multihop QA decompose the full
question into sub-questions, each of which is used
to retrieve extra information (Press et al., 2022; Yao
et al., 2022; Khot et al., 2022; Khattab et al., 2022).
We ask the following question: can we create a
simple and generic retrieval augmented LM thatac-
tively decides when and what to retrievethroughout
the generation process, and are applicable to a va-
riety of long-form generation tasks? We provide a
generalized view of active retrieval augmented gen-
eration. Our hypothesis regarding when to retrieve
is that LMs should retrieve information only when
they lack the required knowledge to avoid unneces-
sary or inappropriate retrieval that occurs in passive
retrieval augmented LMs (Khandelwal et al., 2020;
Borgeaud et al., 2022; Ram et al., 2023; Trivedi
et al., 2022). Given the observation that large LMs
tend to be well-calibrated and low probability/con-
fidence often indicates a lack of knowledge (Ka-
davath et al., 2022), we adopt an active retrieval
strategy that only retrieves when LMs generate low-
probability tokens. When deciding what to retrieve,
it is important to consider what LMs intend to gen-
erate in the future, as the goal of active retrieval is to
benefit future generations. Therefore, we propose
anticipating the future by generating a temporary
next sentence, using it as a query to retrieve rel-
evant documents, and then regenerating the next
sentence conditioning on the retrieved documents.
Combining the two aspects, we propose Forward-
Looking Active REtrieval augmented generation
(FLARE), as illustrated in Figure 1. FLARE iter-
atively generates a temporary next sentence, use
it as the query to retrieve relevant documents if it
contains low-probability tokens and regenerate the
next sentence until reaches the end.
FLARE is applicable to any existing LMs at
inference time without additional training. Con-
7970sidering the impressive performance achieved by
GPT-3.5 (Ouyang et al., 2022) on a variety of
tasks, we examine the effectiveness of our meth-
ods on text-davinci-003. We evaluate FLARE
on 4 diverse tasks/datasets involving generating
long outputs, including multihop QA (2WikiMul-
tihopQA), commonsense reasoning (StrategyQA),
long-form QA (ASQA), and open-domain summa-
rization (WikiAsp) (Ho et al., 2020; Geva et al.,
2021; Stelmakh et al., 2022; Hayashi et al., 2021).
Over all tasks, FLARE achieves superior or com-
petitive performance compared to single-time and
multi-time retrieval baselines, demonstrating the
effectiveness and generalizability of our method.
2 Retrieval Augmented Generation
We formally define single-time retrieval augmented
generation and propose the framework of active
retrieval augmented generation.
2.1 Notations and Definitions
Given a user input x and a document corpus D=
{di}|D|
i=1 (such as all Wikipedia articles), the goal of
retrieval augmented LMs is to generate the answer
y = [s1,s2,..., sm] = [w1,w2,...,w n] containing
m sentences or n tokens leveraging information
retrieved from the corpus.
In retrieval augmented LM, the LM typically
pairs with a retriever that can retrieve a list of
documents Dq = ret(q) for a query q; the LM
conditions on both the user input x and retrieved
documents Dq to generate the answer. Since we
focus on examining various methods of determin-
ing when and what to retrieve, we follow exist-
ing methods (Ram et al., 2023; Trivedi et al.,
2022) to prepend the retrieved documents before
the user input to aid future generation for both
baselines and our method for fair comparisons:
y = LM([Dq,x]), where [·,·] is concatenation fol-
lowing the specified order.
2.2 Single-time Retrieval Augmented
Generation
The most common choice is to directly use the user
input as the query for retrieval and generate the
complete answer at once y = LM([Dx,x]).
2.3 Active Retrieval Augmented Generation
To aid long-form generation with retrieval, we pro-
pose active retrieval augmented generation. It is a
generic framework that actively decides when and
what to retrieve through the generation process,
resulting in the interleaving of retrieval and genera-
tion. Formally, at step t(t≥1), the retrieval query
qt is formulated based on both the user input x and
previously generated output y<t = [y0,..., yt−1]:
qt = qry(x,y<t),
where qry(·) is the query formulation function. At
the beginning (t = 1), the previous generation is
empty (y<1 = ∅), and the user input is used as the
initial query (q1 = x). Given retrieved documents
Dqt, LMs continually generate the answer until the
next retrieval is triggered or reaches the end:
yt = LM([Dqt,x,y<t]),
where yt represents the generated tokens at the cur-
rent step t, and the input to LMs is the concatena-
tion of the retrieved documents Dqt, the user input
x, and the previous generation y<t. We discard
previously retrieved documents ∪t′<tDqt′ and only
use the retrieved documents from the current step
to condition the next generation to prevent reaching
the input length limit of LMs.
3 FLARE: Forward-Looking Active
REtrieval Augmented Generation
Our intuition is that (1) LMs should only retrieve
information when they do not have the necessary
knowledge to avoid unnecessary or inappropriate
retrieval, and (2) the retrieval queries should reflect
the intents of future generations. We propose two
forward-looking active retrieval augmented gener-
ation (FLARE) methods to implement the active
retrieval augmented generation framework. The
first method prompts the LM to generate retrieval
queries when necessary while generating the an-
swer using retrieval-encouraging instructions, de-
noted as FLAREinstruct. The second method directly
uses the LM’s generation as search queries, denoted
as FLAREdirect, which iteratively generates the next
sentence to gain insight into the future topic, and
if uncertain tokens are present, retrieves relevant
documents to regenerate the next sentence.
3.1 FLARE with Retrieval Instructions
Inspired by Toolformer (Schick et al., 2023), a
straightforward way of expressing information
needs for retrieval is to generate “[Search(query)]”
when additional information is needed (Schick
et al., 2023), e.g., “The colors on the flag of
Ghana have the following meanings. Red is for
[Search(Ghana flag red meaning)] the blood of mar-
tyrs, ...” When working with GPT-3.5 models that
7971Search results:   !![1]: …[2]: …
Joe Biden attended
Search results:   !""![1]: …[2]: …Search results:   !""""[1]: …[2]: …
[Search(Joe Biden University)]
[Search(Joe Biden degree)]the University of Pennsylvania, where he earned
a law degree.
Generate a summary about Joe Biden.Input$
&$
&#%$
&%%%Generation
Retriever
$
%$
%%
Figure 2: An illustration of forward-looking active re-
trieval augmented generation with retrieval instructions
(FLAREinstruct). It iteratively generates search queries
(shown in gray italic) to retrieve relevant information to
aid future generations.
offer only API access, we elicit such behavior by
few-shot prompting (Brown et al., 2020).
Specifically, for a downstream task, we place
the search-related instruction and exemplars at the
beginning as skill 1, followed by the instruction and
exemplars of the downstream task as skill 2. Given
a test case, we ask LMs to combine skills 1 and 2 to
generate search queries while performing the task.
The structure of the prompt is shown in Prompt 3.1,
and full details can be found in Prompt D.3.
Prompt 3.1: retrieval instructions
Skill 1. An instruction to guide LMs to generate search
queries.
Several search-related exemplars.
Skill 2. An instruction to guide LMs to perform a
specific downstream task (e.g., multihop QA).
Several task-related exemplars.
An instruction to guide LMs to combine skills 1
and 2 for the test case.
The input of the test case.
As shown in Figure 2, when the LM generates
“[Search(query)]” (shown in gray italic), we stop
the generation and use the query terms to retrieve
relevant documents, which are prepended before
the user input to aid future generation until the
next search query is generated or reaches the end.
Additional implementation details are included in
Appendix A.
3.2 Direct FLARE
Since we cannot fine-tune black-box LMs, we
found queries generated by FLAREinstruct through
retrieval instructions might not be reliable. There-
fore, we propose a more direct way of forward-
looking active retrieval that uses the next sentence
to decide when and what to retrieve.
3.2.1 Confidence-based Active Retrieval
As shown in Figure 1, at step t, we first generate a
temporary next sentence ˆst = LM([x,y<t]) with-
out conditioning on retrieved documents. Then we
decide whether to trigger retrieval and formulate
queries based on ˆst. If the LM is confident aboutˆst,
we accept it without retrieving additional informa-
tion; if not, we use ˆst to formulate search queries
qt to retrieve relevant documents, and then regen-
erate the next sentence st. The reason we utilize
sentences as the basis of our iteration is due to their
significance as semantic units that are neither too
short nor too lengthy like phrases and paragraphs.
However, our approach can also utilize phrases or
paragraphs as the basis.
Since LMs tend to be well-calibrated that low
probability/confidence often indicates a lack of
knowledge (Jiang et al., 2021; Kadavath et al.,
2022; Varshney et al., 2022), we actively trigger
retrieval if any token of ˆst has a probability lower
than a threshold θ ∈[0,1]. θ = 0means retrieval
is never triggered, while θ = 1triggers retrieval
every sentence.
yt =
{
ˆst if all tokens of ˆst have probs ≥θ
st = LM([Dqt,x,y<t]) otherwise
where the query qt is formulated based on ˆst.
3.2.2 Confidence-based Query Formulation
One way to perform retrieval is to directly use the
next sentence ˆst as the query qt. This shares a sim-
ilar spirit with methods that use generated hypo-
thetical"
"Published as a conference paper at ICLR 2024
RETRIEVAL MEETS LONG CONTEXT LARGE LANGUAGE
MODELS
Peng Xu†, Wei Ping†, Xianchao Wu, Lawrence McAfee
Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhturina
Mohammad Shoeybi, Bryan Catanzaro
NVIDIA
†{pengx, wping}@nvidia.com
ABSTRACT
Extending the context window of large language models (LLMs) is getting popular
recently, while the solution of augmenting LLMs with retrieval has existed for years.
The natural questions are: i) Retrieval-augmentation versus long context window,
which one is better for downstream tasks? ii) Can both methods be combined to
get the best of both worlds?In this work, we answer these questions by studying
both solutions using two state-of-the-art pretrained LLMs, i.e., a proprietary 43B
GPT and Llama2-70B. Perhaps surprisingly, we find that LLM with 4K context
window using simple retrieval-augmentation at generation can achieve compa-
rable performance to finetuned LLM with 16K context window via positional
interpolation on long context tasks, while taking much less computation. More
importantly, we demonstrate that retrieval can significantly improve the perfor-
mance of LLMs regardless of their extended context window sizes. Our best model,
retrieval-augmented Llama2-70B with 32K context window, outperforms GPT-3.5-
turbo-16k and Davinci003 in terms of average score on nine long context tasks
including question answering, query-based summarization, and in-context few-shot
learning tasks. It also outperforms its non-retrieval Llama2-70B-32k baseline by a
margin, while being much faster at generation. Our study provides general insights
on the choice of retrieval-augmentation versus long context extension of LLM for
practitioners.
1 I NTRODUCTION
The long context large language models (LLM) have recently received a lot of attention in produc-
tion (e.g., Anthropic, 2023; OpenAI, 2023b), research community (e.g., Chen et al., 2023; Liu et al.,
2023; Tworkowski et al., 2023), and open source community (e.g., Kaiokendev, 2023). Although
the approximate attention methods have been studied for years (e.g., Tay et al., 2022a) (due to the
quadratic time and memory complexities of self-attention mechanism in sequence length), the recent
advance for long context LLMs with exact attention is mainly driven by the development of faster
GPU with more memory and memory-efficient exact attention (Dao et al., 2022; Dao, 2023).
An alternative and long-standing solution for handling long context is retrieval. Specifically, the
LLMs only read relevant context retrieved from a standalone retriever (e.g., Karpukhin et al., 2020;
Wang et al., 2022; Lin et al., 2023), which is much easier to scale1 and runs orders of magnitudes
faster than LLMs for selecting relevant context. Conceptually, the retrieval-augmented decoder-only
LLM can be viewed as applying the sparse attention over its long context window, where the sparsity
pattern is not predefined as Child et al. (2019) but determined by the standalone retriever. In other
words, unretrieved context is treated as irrelevant and has zero-valued attention weights.
Given the surge of interest in long context LLM research and much more required computation
at inference 2, it is still unclear for practitioners whether extending the context window of LLM
1The dense embedding retriever can easily retrieve context from billions of tokens using the fast similarity
search library (Johnson et al., 2019).
2For example, the price of GPT-4 with 32k context length is twice the 8k context model.
1Published as a conference paper at ICLR 2024
provides higher accuracy than the retrieval-augmentation for downstream tasks with informative
queries. Moreover, it would be compelling if we could combine the strength of both methods and
achieve even higher accuracies. In this work, we attempt to answer the above questions through a
comprehensive study.
Specifically, we make the following contributions:
1. We perform comprehensive study using two state-of-the-art LLMs, a proprietary 43B pre-
trained GPT and Llama2-70B (Touvron et al., 2023b) on 9 downstream long context tasks,
including single and multi document question answering (QA), query-based summarization,
and in context few-shot learning tasks.
2. We demonstrate that retrieval-augmentation significantly improves the performance of 4K
context LLMs. Perhaps surprisingly, we find this simple retrieval-augmented baseline can
perform comparable to 16K long context LLMs, i.e., average score 29.32 vs. 29.45 by using
GPT-43B, and 36.02 vs. 36.78 by using Llama2-70B, while using much less computation.
3. Furthermore, we demonstrate that the performance of long context LLM (i.e., 16K or 32K)
can still be improved by retrieval, especially for the larger Llama2-70B. As a result, our best
model, retrieval augmented Llama2-70B-32k-ret with 32K context window (avg. score 43.6),
outperforms GPT-3.5-turbo-16k (avg. score 42.8) and Davinci-003 in terms of average score.
It also largely outperforms its non-retrieval Llama2-70B-32k baseline (avg. score 40.9), while
can be much faster at generation (e.g., 4× faster on NarrativeQA).
We organize the rest of the paper as follows. We discuss related work in Section 2, and present the
experimental setup in Section 3. We report results in Section 4 and conclude the paper in Section 5.
2 R ELATED WORK
In this section, we discuss the related work in long context LLM, efficient attention methods, and
retrieval-augmented language models.
2.1 P ARALLEL WORK
When we are preparing this manuscript, we notice that a concurrent work (Bai et al., 2023) (arXived
on 28 Aug 2023) also studies the impact of retrieval on long context LLM, including black-box model
GPT-3.5-Turbo-16k (OpenAI, 2022), white-box model Llama2-7B-chat-4k (Touvron et al., 2023b),
and ChatGLM2-6B-32k (Zeng et al., 2022). Different from our findings, they find that retrieval is
only helpful for Llama2-7B-chat-4k with 4K context window, but not helpful for long context model,
i.e., GPT-3.5-Turbo-16k and ChatGLM2-6B-32k. We hypothesize the major reasons are: i) it is
challenging to do controlled experiments using black-box APIs, ii) the white-box LLMs used in their
study are relatively small, thus they have limited zero-shot capability of incorporating context through
retrieval. Our conclusions are drawn from much larger LLMs. In particular, our best long context
model Llama2-70B-32k performs as well as Davinci003 and GPT-3.5-turbo-16k, while it can still be
further enhanced by retrieval (see Table 3).
2.2 L ONG CONTEXT LARGE LANGUAGE MODELS
Over the past few years, pretraining large language models (LLMs) with long context window
becomes a viable solution thanks to faster GPU with more memory and memory-efficient exact
attention (e.g., Dao et al., 2022). For example, the context window for pretrained LLM have been
increased from 1024 of GPT-2 (Radford et al., 2019), 2048 of GPT-3 (Brown et al., 2020), 4096 of
Llama 2 (Touvron et al., 2023b), to 8192 of GPT-4 (OpenAI, 2023a). However, further extending the
context window in pretraining can be challenging, because,i) pretraining LLM from scratch with long
context (e.g., >16K tokens) is very expensive due to the quadratic time and memory complexities of
exact attention, and ii) most of documents in pretraining corpus (e.g., Common Crawl) are relatively
short.
Most recently, researchers start to extend the context window of LLMs with continued training or
fine-tuning (e.g., Kaiokendev, 2023; Nijkamp et al., 2023; Chen et al., 2023; Tworkowski et al., 2023;
Mohtashami & Jaggi, 2023). Tworkowski et al. (2023) introduced LongLLaMA by fine-tuning the
2Published as a conference paper at ICLR 2024
3B and 7B OpenLLaMA checkpoints with contrastive training on 8K context length. Landmark
attention (Mohtashami & Jaggi, 2023) extends the context length of LLaMA 7B from 4K to 32K
by introducing “landmark tokens” to represent blocks of the context and fine-tuning the attention
to use landmark tokens for selecting relevant blocks. Chen et al. (2023) and Kaiokendev (2023)
introduced positional interpolationto extend the context window sizes of RoPE-based (Su et al.,
2021) pretrained LLMs. In particular, Chen et al. (2023) demonstrates promising results on LLaMA
7B to 65B (Touvron et al., 2023a) with minimal fine-tuning effort (within 1000 steps). ALiBi (Press
et al., 2021) extrapolates context window length by removing the positional embeddings while simply
biasing the key-query attention scores with a linear penalty that is proportional to their distance, so
one does not need finetuning for context window extrapolation. Ratner et al. (2023) chunks long
context into multiple sub-windows and re-use the positional embeddings across these windows,
thus can handle longer context without any further finetuning. In this work, we apply positional
interpolation method to extend the 4K context window of a proprietary 43B pretrained LLM and
Llama2-70B (Touvron et al., 2023b) to 16K and 32K, as they both use rotary position embedding at
pretraining. In terms of evaluation, we focus on downstream task performance (e.g., Shaham et al.,
2023; Bai et al., 2023) after instruction tuning (Wei et al., 2021).
There are other studies showing the interplay between retrieval-augmentation and long context LLM.
Liu et al. (2023) performs the black-box evaluation for the long context capability of existing LLM
products, including ChatGPT 3.5 (OpenAI, 2022), GPT-4 (OpenAI, 2023a), Claude (Anthropic,
2023), in retrieval-augmented setting, and identify the “lost in the middle” phenomenon in these
models.
2.3 E FFICIENT ATTENTION METHODS
In previous study, many approximate attention methods (Tay et al., 2022a) have been introduced for
dealing with the quadratic complexity of self-attention that becomes a computational bottleneck for
long context. They can be grouped into the following categories: i) Sparse attention mechanisms
with predefined sparsity patterns (e.g., Child et al., 2019; Parmar et al., 2018; Ho et al., 2019; Beltagy
et al., 2020; Zaheer et al., 2020; Zhu et al., 2021), ii) recurrence-based method (Dai et al., 2019;
Bulatov et al., 2022), iii) low-rank projection attention (e.g., Wang et al., 2020; Xiong et al., 2021;
Tay et al., 2021; Zhu et al., 2021), iv) memory-based mechanisms (e.g., Rae et al., 2020; Liu et al.,
2018), v) similarity and clustering based methods (e.g., Kitaev et al., 2020; Tay et al., 2020; Roy
et al., 2021). These approximate methods introduce inductive bias (e.g., predefined sparsity) that can
fit well for specific domain, but may reduce model quality in general LLM training.
Most recently, FlashAttention (Dao et al., 2022; Dao, 2023) is introduced to speed up the exact atten-
tion computation by accounting for reads and writes between levels of GPU memory. FlashAttention
is particularly useful for handling longer sequences.
2.4 R ETRIEVAL -AUGMENTED LANGUAGE MODELS
Retrieval has been integrated into language models for years to improve perplexity (Borgeaud et al.,
2022; Wang et al., 2023), factual accuracy (Nakano et al., 2021), downstream task accuracy (Guu
et al., 2020; Izacard & Grave, 2021; Izacard et al., 2022; Lewis et al., 2020), and in-context learning
capability (Huang et al., 2023). Combined with a standalone retriever (Karpukhin et al., 2020;
Wang et al., 2022; Lin et al., 2023), retrieval-augmented LLM is well established for handling
question answering with long document and in open-domain. In previous study, language models
have been augmented with retrieval at inference (Khandelwal et al., 2019; Yogatama et al., 2021),
fine-tuning (Izacard et al., 2022; Lewis et al., 2020; Guu et al., 2020), and pretraining (Borgeaud
et al., 2022; Izacard et al., 2022; Wang et al., 2023). There are also methods that try to integrate LLM
and retriever in a single model and build the end-to-end solution (e.g., Jiang et al., 2022; Shi et al.,
2023). However, most of previous works mainly study retrieval-augmentation for LLMs that have
around 10 billion parameters, except a few recent ones (e.g., Shi et al., 2023).
In this work, we focus on decoder-only LLMs with 43B and 70B parameters trained on trillions of
tokens, because the LLMs at such scale exhibit strong zero-shot capability to incorporate context
after instruction tuning (Wei et al., 2021; 2022).
3Published as a conference paper at ICLR 2024
3 E XPERIMENTAL SETUP
In this section, we present the details of our experimental setup.
3.1 L ARGE LANGUAGE MODELS
We focus on comparing the zero-shot capability of integrating long context information for generative
QA or summarization tasks via retrieval or LLM’s own self-attention mechanism. In contrast to
most existing works that focus on relatively small models (e.g., 3B or 7B) (Kaiokendev, 2023;
Nijkamp et al., 2023; Tworkowski et al., 2023; Mohtashami & Jaggi, 2023), we gather the insights by
exploring model sizes that are larger than 40B after instruction tuning, as previous study suggests that
instruction tuning becomes effective when the decoder-only LLM has around 50B parameters (Wei
et al., 2021; 2022).
Specifically, we experimented with two pretrained GPT models, a proprietary Nemo GPT-43B and
Llama2-70B. GPT-43B is a 43 billion parameter model that is trained with 1.1T tokens with 70%
English corpus and the other 30% for multilingual and code data. For the English pretraining corpus,
GPT-43B used Common Crawl web archive (W ARC), Wikipedia, Reddit, Books, Gutenberg, ArXiv,
StackExchange, PubMed, etc. It contains 48 layers with the hidden dimension of 8,192. It is trained
with a sequence length of 4,096 and RoPE embeddings (Su et al., 2021). The other Llama2-70B is a
public available 70B GPT model trained on 2T tokens using around 90% English data. It contains 80
layers with the hidden dimension of 8,192. It also has the context window size of 4,096 and trained
with RoPE embeddings.
3.2 D ATASETS AND METRICS
In this study, we include seven datasets ranging from single document QA, multi document QA, to
query-based summarization for our zero shot evaluations. Specifically, we include four datasets from
the validation set of the Scroll benchmark (Shaham et al., 2022).
• QMSum (QM) (Zhong et al., 2021) is a query-based summarization dataset, consisting
of meetings’ transcripts and their corresponding summaries from multiple domains such as
academic, industrial product. In this task, a meeting dialogue transcript is given, and a question
to summarize certain topic is raised about the dialogue, such as “what is agreed between them”.
The answer generally contains a few sentences.
• Qasper (QASP)(Dasigi et al., 2021) is a question answering dataset over NLP papers filtered
from the Semantic Scholar Open Research Corpus (S2ORC) (Lo et al., 2020). Qasper contains
abstractive, extractive, and yes/no questions, as well as unanswerable ones. In this task, one
script is provided together with an information seeking question, such as “which multilingual
approaches do they compare with?”. A model needs to give a short answer by reasoning over
the given context.
• NarrativeQA (NQA) (Koˇciský et al., 2018) is an established question answering dataset over
entire books from Project Gutenberg3 and movie scripts from a list of websites. In this task, the
given passage is transcribed from books and is usually noisy. A model is required to generate a
short phrase by reasoning over the long and noisy text.
• QuALITY (QLTY)(Pang et al., 2022) is a question answering dataset over stories and articles
collected from several resources, such as Project Gutenberg and the Open American National
Corpus4. Different from all the other tasks, this is a multi-choices dataset and a model is required
to select one among four given choices.
We take another three datasets from LongBench (Bai et al., 2023).
• HotpotQA (HQA)(Yang et al., 2018) is a Wikipedia-based question-answer dataset. Different
from above single hot datasets, HQA is a multi-hop dataset where multiple supporting documents
are required to be read for answering and reasoning and the questions are diverse and not
constrained to any pre-existing knowledge bases.
3https://www.gutenberg.org/
4https://anc.org/
4Published as a conference paper at ICLR 2024
QM QASP NQA QLTY MSQ HQA MFQA
# of samples 200 1,726 2,000 2,000 200 200 150
avg doc length 14,140 4,912 84,770 6,592 16,198 13,319 7,185
avg top-5 chunks 2,066 2,071 2,549 2,172 2,352 2,322 2,385
avg top-10 chunks 4,137 3,716 5,125 4,018 4,644 4,554 4,305
avg top-20 chunks 8,160 4,658 10,251 5,890 9,133 8,635 6,570
Table 1: Statistics of seven datasets used for zero-shot evaluation. All lengths are counted by the
number of tokens using Llama2-70B tokenizer, and “avg top N chunks"" denotes the average number
of tokens from the top N retrieved chunks. Figure 2 gives more details.
• MuSiQue (MSQ)(Trivedi et al., 2022) is another multi-hop question answering dataset. Com-
pared to HQA, MSQ requires connected reasoning by reducing potential reasoning shortcuts,
minimizing train-test leakage, and including harder distractor contexts. Thus, MSQ is much
harder task than HQA and significantly less cheatable.
• MultiFieldQA-en (MFQA)(Bai et al., 2023) was manually curated to better test the model’s
long context understanding ability across diverse fields. The evidences from multiple sources,
including legal documents, government reports, encyclopedias, and academic papers, are fairly
randomly located in the documents to avoid biases that might occur at the beginning or ending
of the documents.
The full details of the dataset can be found in Table 1. We can see that our evaluation datasets have a
wide range of average document length from 4.9k (QASP) to 84k (NQA). Therefore, for the baseline
model without retrieval, we truncate the document accordingly to fit into the input sequence length.
Following the official metrics, we report the geometric mean of ROUGE scores (i.e., ROUGE-
1/2/L) (Lin, 2004) for QM, the exact matching (EM) score for QLTY , and F1 scores for the remaining
five datasets QASP, NQA, MSQ, HQA and MFQA.
3.3 C ONTEXT WINDOW EXTENSION
We extend the context window length with position interpolation method (Chen et al., 2023), as it is
simple and effective for RoPE embeddings. We extend the 4K context window to 16K for GPT-43B.
For Llama2, we extend its 4K context window to 32k for Llama2-7B and both 16K and 32K for
Llama2-70B. We follow Chen et al. (2023) and finetune both LLMs on the Pile dataset (Gao et al.,
2021) with batch size as 128, constant learning rate of 5e-6 to adapt the position embeddings.
3.4 R ETRIEVAL
For the retriever, we experimented with three retrievers: 1) Dragon (Lin et al., 2023) as it achieves
state-of-the-art results on both supervised and zero-shot information retrieval benchmarks (Thakur
et al., 2021). Dragon is a dual encoder model that consists of a query encoder and a context encoder.
2) a widely used Contriever model (Izacard et al., 2021). Following the MoCo technique (He et al.,
2020), Contriever used a simple contrastive learning framework to pre-train models for information
retrieval. It was trained without supervision and achieved competitive results with BM25 for R@100
on the BEIR benchmark (Thakur et al., 2021), and 3)OpenAI embedding5. For the OpenAI embedding
model, we use the latest “text-embedding-ada-002” as recommended by OpenAI. It accepts 8,191
maximum input tokens for one sequence with an output vector of 1,536 dimensions. The cosine
similarities are then computed between the questions and the list of contexts for retrieval ranking.
To use these retrievers, we first chunk each context document with 300 words, and then we encode
both the questions and all chunks independently with corresponding encoders. The most relevant
N chunks, ranked by the dot product of the question embedding and chunk embedding, are then
concatenated together (following the left to right order from the most relevant to least relevant) as the
context of the prompt for generation. Table 1 shows the statistics of the top N retrieved chunks while
Figure 2 in the Appendix gives more details of the token length distribution of all seven datasets. We
5https://platform.openai.com/docs/guides/embeddings
5Published as a conference paper at ICLR 2024
Model Seq len. Avg. QM QASP NQA QLTY MSQ HQA MFQA
GPT-43B 4k 26.44 15.56 23.66 15.64 49.35 11.08 28.91 40.90
+ ret 4k 29.32 16.60 23.45 19.81 51.55 14.95 34.26 44.63
GPT-43B 16k 29.45 16.09 25.75 16.94 50.05 14.74 37.48 45.08
+ ret 16k 29.65 15.69 23.82 21.11 47.90 15.52 36.14 47.39
Llama2-70B 4k 31.61 16.34 27.70 19.07 63.55 15.40 34.64 44.55
+ ret 4k 36.02 17.41 28.74 23.41 70.15 21.39 42.06 48.96
Llama2-70B 16k 36.78 16.72 30.92 22.32 76.10 18.78 43.97 48.63
+ ret 16k 37.23 18.70 29.54 23.12 70.90 23.28 44.81 50.24
Llama2-70B 32k 37.36 15.37 31.88 23.59 73.80 19.07 49.49 48.35
+ ret 32k 39.60 18.34 31.27 24.53 69.55 26.72 53.89 52.91
Llama2-7B 4k 22.65 14.25 22.07 14.38 40.90 8.66 23.13 35.20
+ ret 4k 26.04 16.45 22.97 18.18 43.25 14.68 26.62 40.10
Llama2-7B 32k 28.20 16.09 23.66 19.07 44.50 15.74 31.63 46.71
+ ret 32k 27.63 17.11 23.25 19.12 43.70 15.67 29.55 45.03
Table 2: Comparison of model variants (GPT-43B, Llama2-7B, Llama2-70B) with sequence length
ranging from 4k to 32k under seven datasets. “ret” denotes using the best retriever (Dragon or
Contriever or OpenAI embeddings) and here we used top-5 for the retriever.
can see that top-5 chunks can all fit into 4k sequence length (except few outliers) while top-10 and
top-20 chunks can fit into 16k sequence length.
3.5 I NSTRUCTION TUNING
To train the pretrained LLMs to follow instructions for question answering or text summarization, we
also performed instruction tuning. We first construct a blend of instruction tuning datasets consisting
of 102K training samples from the Soda dataset (Kim et al., 2022), ELI5 dataset (Fan et al., 2019),
FLAN dataset (Wei et al., 2021) , Open Assistatant dataset (Köpf et al., 2023), Dolly (Conover et al.,
2023) and a proprietary sourced conversational dataset, to adapt all foundation models to follow
instructions. In terms of the template, we use ""System: {System}\n\nUser: {Question}\n\nAssistant:
{Answer}"" as the format to support multi-turn dialogue training. As all of the tasks contain the
context information for reasoning over at inference time, we add the context before the dialogue, i.e.
""System: {System}\n\n{Context}\n\nUser: {Question}\n\nAssistant: {Answer}"".
We finetune the LLM by taking the loss only on the {Answer} part with batch size 128 and learning
rate of 5e-6 for 1000 steps. For the rest of the paper, results are all reported using the instruction
tuned chat model on top of the foundational GPT-43B, Llama2-7B, and Llama2-70B.
4 R ESULTS
In this section, we report the results and provide detailed analysis.
4.1 M AIN RESULTS
In Table 2, we compare different model variants with context lengths ranging from 4K to as long
as 32K using GPT-43B and Llama2-70B. First, we find that baseline models without retrieval of
4k sequence length achieve the worst results for both GPT-43B and Llama2-70B. This is because
the minimum average sequence length of all seven tasks exceeds 4096, the context window of
the foundation models and therefore valuable texts get truncated randomly. As a result, retrieval
is especially helpful for 4K LLMs e.g., Llama2-70B-4K is improved from 31.61 to 35.73 while
GPT-43B-4K is improved from 26.44 to 29.32. Second, we observe that HotpotQA (HQA) especially
favors long sequence models as the score improves from 34.64 to 43.97 for Llama2-70B and from
28.91 to 37.48 for GPT-43B when the sequence length increases from 4k to 16k. This is because
Hotpot QA is a multi-hop dataset where the questions are not hard to answer but all intermediate
hops are necessary to get correct answer. Therefore, long context are beneficial to increase the recall
of incorporating all intermediate hops.
6Published as a conference paper at ICLR 2024
0 4 9 14 19
Position
40
42
44
46
48
50
52
54EM
EM scores (Llama2 70b)
Llama2-70b-4k
Llama2-70b-32k
Figure 1: Llama2-70B also displays lost-in-the-middle phenomenon
Model Avg-7 Avg-4* QM* QASP* NQA* QLTY* MSQ HQA MFQA
Davinci003 (175B) 39.2 40.8* 16.9* 52.7* 24.6* 69.0* 22.1 41.2 47.8
GPT-3.5-turbo (4k) 38.4 39.2* 15.6* 49.3* 25.1* 66.6* 21.2 40.9 49.2
+ret 24.4 49.5 49.5
GPT-3.5-turbo-16k 42.8 42.4 17.6 50.5 28.8 72.6 26.9 51.6 52.3
+ret 30.4 46.6 52.8
Llama2-70B-32k 40.9 42.4 15.6 45.9 28.4 79.6 19.1 49.5 48.4
Llama2-70B-32k-ret 43.6 43.0 18.5 46.3 31.5 75.6 26.7 53.9 52.9
Table 3: Comparison of our best retrieval-augmented Llama2-70B-32k-ret with GPT-3.5-turbo-16k
and Davinci-003 (175B parameters). For QMSum (QM), Qasper (QASP), NarrativeQA (NQA),
QuALITY (QLTY), we used the test set from the ZeroSCROLLS leaderboard as the organizers have
prepared the scores of GPT-3.5-turbo (4k) and Davinci-003 (highlighted with *). Avg-7 refers to the
average score of all 7 datasets, and Avg-4* refers to the average of 4 datasets from ZeroSCROLLS.
It is quite interesting that the retrieval-augmented long context LLM (e.g., 16K and 32K) can obtain
better results than retrieval-augmented 4K context LLM, even they are feed with the same top 5
chunks of evidence. We hypothesize this interesting observation is related to the “lost in the middle”
phenomenon (Liu et al., 2023), where the LLMs has such “U-shaped” performance curve. Specifically,
LLMs are better at utilizing relevant information that occurs at the beginning or end of its input
context window. To further verify the hypothesis, we conduct the “lost-in-the-middle” study following
Liu et al. (2023) for Llama2-70B-4k and Llama2-70B-32k. As show in Figure 1, we confirm that the
phenomenon also exists in Llama2-70B with different context lengths. In particular, the comparison
of the curves from Llama2-70B-4k and Llama2-70B-32k suggests that the long context model has
better accuracy for incorporating top-5 retrieved context.
Note that, we have very different observation from the conclusion drawn from LongBench work (Bai
et al., 2023): “Retrieval brings improvement for model with weak ability on long contexts, but the
performance still lags behind models that have strong long context understanding capability”. Here,
we demonstrate retrieval can significantly improve the performance of both GPT-43B and Llama2-70B
regardless their context window size. For example, our best retrieval-augmented Llama2-70B-32k-ret
outperforms its baseline w/o retrieval by a margin, i.e., 39.60 vs. 37.36. We think the major reason for
such different conclusion is that Bai et al. (2023) uses much smaller LLM with 6B and 7B parameters,
which usually has relatively worse zero-shot capability to incorporate the retrieved chunked context.
To further validate the hypothesis, we also report the results using Llama2-7B in Table 5. One can
actually draw similar conclusions to Bai et al. (2023) . We think the underlying reasons are: i) For
Llama2-7B-chat-4k, its short context length is the bottleneck for long context tasks. Thus, retrieval-
augmentation largely improves the results. ii) For Llama2-7B-chat-32 and ChatGLM2-6B-32k, the
context length bottleneck has been mostly removed. However, their retrieval-augmented models have
limited zero-shot capability of incorporating retrieved chunks of context, due to the smaller size. As
a result, retrieval is not helpful for both Llama2-7B-32k and ChatGLM2-6B-32k, which is different
from large LLMs like Llama2-70B-32k in our case.
7Published as a conference paper at ICLR 2024
Seq len Setting Avg. QM QASP NQA QLTY MSQ HQA MFQA
4k baseline (w/o ret) 31.61 16.34 27.70 19.07 63.55 15.40 34.64 44.55
Dragon 35.73 18.14 29.20 23.39 70.30 20.09 41.54 47.45
Contriever 36.02 17.41 28.74 23.41 70.15 21.39 42.06 48.96
OpenAI-embedding 35.79 17.76 28.85 23.57 70.70 19.92 41.76 47.99
32k baseline (w/o ret) 37.36 15.37 31.88 23.59 73.80 19.07 49.49 48.35
Dragon 39.60 18.34 31.27 24.53 69.55 26.72 53.89 52.91
Contriever 38.85 17.60 31.56 23.88 69.00 26.61 49.65 53.66
OpenAI-embedding 39.34 18.24 32.07 24.36 69.45 24.90 51.64 54.75
Table 4: Comparisons of adding top 5 retrieved chunks from different retrievers to the context under
Llama2-70B.
Seq len Setting Avg. QM QASP NQA QLTY MSQ HQA MFQA
4k base 31.61 16.34 27.70 19.07 63.55 15.40 34.64 44.55
top-5 35.73 18.14 29.20 23.39 70.30 20.09 41.54 47.45
top-10 34.62 16.54 28.67 24.38 68.70 19.00 42.18 42.84
top-20 34.61 16.52 28.67 24.38 68.70 19.00 42.18 42.84
16k base 36.78 16.72 30.92 22.32 76.10 18.78 43.97 48.63
top-5 37.23 18.70 29.54 23.12 70.90 23.28 44.81 50.24
top-10 38.31 18.41 30.20 25.53 73.60 22.78 47.72 49.91
top-20 36.61 17.26 29.60 25.81 72.30 22.69 41.36 47.23
32k base 37.36 15.37 31.88 23.59 73.80 19.07 49.49 48.35
top-5 39.60 18.34 31.27 24.53 69.55 26.72 53.89 52.91
top-10 38.98 17.71 30.34 25.94 70.45 22.80 55.73 49.88
top-20 38.38 16.36 30.42 24.42 69.60 24.51 54.67 48.65
Table 5: Comparisons of adding top-5/10/20 retrieved chunks to the context under 4k, 16k, and 32k
input sequence lengths using Llama2-70B. More context does not always give better results.
In contrast, the larger instruction tuned LLMs like Llama2-70B has much stronger zero-shot capability
to incorporate retrieved evidence. This observation is becoming more clear when one compares the
gain of retrieval-augmentation between GPT-43B and Llama2-70B, where Llama2-70B enjoys larger
benefit of incorporating context through retrieval.
4.2 C OMPARING TO OPEN AI MODELS
To further understand how good is our best model, i.e., augmenting Llama2-70B-32k with retrieval, we
also compare it to GPT-3.5-turbo(4k), GPT-3.5-turbo-16k and Davinci-003 on those seven datasets.6
We found that Llama2-70B-32k-ret achieves better results than GPT-3.5-turbo-16k in terms of the
average accuracy over seven datasets, while better than Davinci-003 (w/ 175B parameters) on the
average over 4 tasks. This indicates Llama2-70B-32k with retrieval is a strong model for these long
context tasks, and our conclusion is built on the state-of-the-art results.
We also report the retrieval augmented results for GPT3.5-turbo on MSQ, HQA and MFQA. For
GPT3.5-turbo-4k, retrieval significantly improves the performance (avg from 37.08 to 41.15). For
GPT3.5-turbo-16k, the average scores for retrieval (43.27) and non-retrieval (43.60) scores are close
to each other which are both lower than our Llam2-70B-32k-ret results (44.51). Note that GPT3.5-
turbo-16k is a blackbox API, we don’t know how it is implemented, the model size as well as any
preprocessing steps.
6For QMSum (QM), Qasper (QASP), NarrativeQA (NQA), QuALITY (QLTY), we used the test set from the
ZeroSCROLLS leaderboard as the organizers have prepared the scores of GPT-3.5-turbo (4k) and Davinci-003
there.
8Published as a conference paper at ICLR 2024
Model Trec SAMSum
GPT-3.5-turbo-16k 68 41.7
Llama2-70B 73 46.5
Llama2-70B-ret 76 47.3
Table 6: Comparison of Llama2-70B to GPT-3.5-turbo-16k with two few-shot learning tasks from
LongBench. Retrieval is helpful for few-shot learning as well.
4.3 A BLATION ON DIFFERENT RETRIEVERS
To investigate the impacts of different retrievers on top of Llama2-70B, we compare Dragon, Con-
triever, and OpenAI embeddings on top of Llama2-70B-4k and Llama2-70B-32k. The results in
Table 4 confirms that our finding, i.e., retrieval can boost the performance of both short context and
long context LLMs, is consistent across different retrievers.
4.4 I NCREASING THE NUMBER OF RETRIEVED CHUNKS
To study the impact of adding more retrieved chunks to the context, we increase the number of
retrieved chunks from 5 to 20 using Dragon retriever and the results can be found in Table 5. We
observe that for different sequence lengths, the best averaged results are obtained either from top 5 or
top 10. Even if 20 chunks can still fit into the 16K and 32K context window (as shown in Figure 2),
adding more chunks up to 20 is not helpful and will sometime hurt the performance. We believe this
is related to the “lost in the middle” phenomenon (Liu et al., 2023) or the model is getting distracted
by irrelevant information and therefore needs further research.
4.5 R ETRIEVAL FOR FEW-SHOT TASKS
In addition to the zero-shot tasks of query-based summarization tasks and question answering tasks
mentioned above, we further investigate the benefits of long context models for few-shot tasks using
two additional datasets (Trec and SAMSum) from LongBench. We take the question from each
dataset as the query and use it to search relevant QA pairs provided in the given few-shot examples.
Table 6 shows that our best model Llama2-70B-32k-ret outperforms its non-retrieval Llama2-70B-32k
baseline as well as GPT-3.5-turbo-16k by a large margin. It again confirms the benefits on using
retrieval together with long context models.
5 C ONCLUSION
In this work, we systematically study the retrieval-augmentation versus long context extension
using the state-of-the-art LLMs after instruction tuning for various long context QA and query-
based summarization tasks. After study, we have the following interesting findings: i) Retrieval
largely boosts the performance of both 4K short context LLM and 16K/32K long context LLMs.
ii) The 4K context LLMs with simple retrieval-augmentation can perform comparable to 16K long
context LLMs, while being more efficient at inference. iii) After context window extension and
retrieval-augmentation, the best model Llama2-70B-32k-ret can outperform GPT-3.5-turbo-16k and
Davinci003 in terms of average score on a suit of downstream tasks with informative queries. Our
study shed light on the promising direction of combining retrieval and long context techniques
together to build better LLM.
6 F UTURE DIRECTIONS
There are many potential research directions that can be extended from this work. One direction is
to develop advanced methods (e.g. memory or hierarchical attention) for existing pretrained large
language models e.g. Llama2-70B, which is itself non-trivial. Also, further extending the context
window to 64k and even longer would be a very interesting study for large 70B parameter models
even though pre-training longer sequence requires much more computation. Lastly, how to mitigate
the “lost-in-the-middle” phenomenon is an open research topic and continue pretraining with UL2
loss (Tay et al., 2022b) could be one potential solution.
9Published as a conference paper at ICLR 2024"
"Lift Yourself Up: Retrieval-augmented Text
Generation with Self-Memory
Xin Cheng1 Di Luo2 Xiuying Chen3 Lemao Liu4 Dongyan Zhao1 Rui Yan2
1 Peking University 2 Remin University of China
3 KAUST 4 Tencent AI Lab
chengxin1998@stu.pku.edu.cn
Abstract
With direct access to human-written reference as memory, retrieval-augmented
generation has achieved much progress in a wide range of text generation tasks.
Since better memory would typically prompt better generation (we define this as
primal problem). The traditional approach for memory retrieval involves selecting
memory that exhibits the highest similarity to the input. However, this method
is constrained by the quality of the fixed corpus from which memory is retrieved.
In this paper, by exploring the duality of the primal problem: better generation
also prompts better memory, we propose a novel framework, Selfmem, which
addresses this limitation by iteratively employing a retrieval-augmented generator
to create an unbounded memory pool and using a memory selector to choose one
output as memory for the subsequent generation round. This enables the model
to leverage its own output, referred to as self-memory, for improved generation.
We evaluate the effectiveness ofSelfmem on three distinct text generation tasks:
neural machine translation, abstractive text summarization, and dialogue generation,
under two generation paradigms: fine-tuned small model and few-shot LLM.
Our approach achieves state-of-the-art results in four directions in JRC-Acquis
translation dataset, 50.3 ROUGE-1 in XSum, and 62.9 ROUGE-1 in BigPatent,
demonstrating the potential of self-memory in enhancing retrieval-augmented
generation models. Furthermore, we conduct thorough analyses of each component
in the Selfmem framework to identify current system bottlenecks and provide
insights for future research1.
1 Introduction
In recent years, retrieval-augmented text generation has attracted growing interest across various
fields, including neural machine translation[28, 17, 2], dialogue response generation[81, 6, 46], and
language modeling[36, 77, 19]. This innovative generation paradigm initially equips a fine-tuned
small model or a large language model (LLM) with access to an external database (typically the
training corpus) using information retrieval techniques. Subsequently, the generation process is
conducted based on both the input text and the retrieved memory.
In this paradigm, the guiding principle for memory retrieval is to find the memory that exhibits
the highest similarity to the current input [36, 96, 49]. This aligns with the human intuition that a
more similar demonstration sample typically offers more hints. As demonstrated in Figure 1, for a
retrieval-augmented translation model, the memory similarity alone exhibits a strong correlation with
the final translation quality, regardless of other factors that may influence translation quality (e.g.,
1Code and data available at: https://github.com/Hannibal046/SelfMemory
37th Conference on Neural Information Processing Systems (NeurIPS 2023).polysemy, morphology, and coreference). We define this as the primal problem: better memory
prompts better generation. Consequently, numerous studies have focused on how to retrieve better
memory, ranging from sparse retrieval to dense retrieval [10, 63], from a fixed retriever to a learnable
retriever [41, 8], and from sentence-level memory to more fine-grained token-level memory [36, 35].
0.0 0.2 0.4 0.6 0.8 1.0
Memory Similarity
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9Hypothesis BLEU
Figure 1: Relation between memory and hy-
pothesis on JRC-Acquis En→De dataset.
The hypothesis is generated by a retrieval-
augmented translator whose memory is re-
trieved from the training set. The X-axis
represents the similarity between memory
and the reference.
However, a fundamental limitation exists in all previous
works: the memory is retrieved from a fixed corpus
and is constrained by the corpus’s quality. Due to the
finite retrieval space, bounded memory significantly
restricts the potential of memory-augmented generation
models [97]. In this paper, we explore the duality of the
primal problem, which posits that better generation
also prompts better memory. We propose a novel
framework called Selfmem, which iteratively employs
a retrieval-augmented generator to create an unbounded
memory pool and uses a memory selector to choose one
output as memory for the subsequent generation round.
By combining the primal and dual problem, a retrieval-
augmented generation model can elevate itself using
its own output, referred to as self-memory. The key
insight behind Selfmem is that the text more closely
resembling the data distribution during inference is not
the training data [87], but the model’s own output.
Selfmem consists of two complementary components:
a retrieval-augmented generator and a memory selector. The generator operates under two distinct
paradigms: fine-tuning a small model or few-shot prompting an LLM. For the former, we train the
generator with labeled data and retrieved memory, while for the latter, we employ a fixed black-box
LLM exclusively for inference alongside retrieved in-context learning samples. We then use the
generator’s output to train a memory selector based on a specific performance metric. By simply
replacing the retrieved memory with unbounded generated memory, we achieve higher-quality
generation output (primal problem), which subsequently serves as memory for the next round after
being refined by the memory selector (dual problem).
To evaluate the efficacy of theSelfmem, we carry out comprehensive experiments in three distinct text
generation tasks: neural machine translation, abstractive text summarization, and dialogue generation.
We witness substantial enhancements over robust baselines, attaining state-of-the-art outcomes in
JRC-Acquis (four directions), XSum (50.3 ROUGE-1), and BigPatent (62.9 ROUGE-1). To gain
deeper insights into the Selfmem, we meticulously investigate each crucial component and pinpoint
the existing system bottleneck to guide future research endeavors.
2 Related Work
2.1 Retrieval-augmented Text Generation
Since the world is not a snapshot once the training corpus is collected, we can never expect an
ever-large model to capture everything in its parameters, even for LLMs like GPT-4 [62]. Therefore,
it is crucial to equip these models with an external memory bank to store additional knowledge or
useful demonstration examples for solving various NLP tasks[41, 78, 95].
In the translation domain, retrieval techniques have long been employed by the localization industry
to enhance human translators’ productivity and consistency even before the advent of machine
translation [94]. Early works on machine translation primarily focused on utilizing memory for
statistical machine translation (SMT) systems [ 80, 50]. For neural machine translation (NMT),
[28] were the first to use search engines to retrieve memory from the training set and incorporate
it with an external memory network. Subsequent research explored various aspects of retrieval-
augmented NMT, such as memory encoding methods [92, 93, 31], joint training of retrievers and
generators with monolingual data [ 8], memory granularity [ 35], and memory diversity [ 17]. For
few-shot LLM generation, strategies for in-context example selection have been proposed to improve
translation quality [2]. Furthermore, in-context machine translation has been shown to be effective
for on-the-fly adaptation [79]. For dialogue response generation tasks, employing exemplar/template
2retrieval as an intermediate step has proven advantageous for generating informative responses [89,
91, 6, 7]. In-context learning example retrieval also aids in controllable dialogue [ 46]. Other
applications include abstractive summarization [64, 14, 18, 15], code generation [30], paraphrase
generation [34, 83], language modeling [36, 105], counterfactual data generation [24], open domain
question answering [12, 33] and semantic parsing [99].
2.2 Neural Text Reranking
By alleviating the discrepancy between training and inference (i.e., exposure bias) and directly
optimizing desired metrics, two-stage reranking methods have facilitated significant progress in
various text generation tasks. In machine translation, pioneering works by [75] and [61] introduced
and popularized discriminative reranking for SMT. In the context of NMT, research has focused on
two primary reranking approaches: generative reranking [56, 32, 88] and discriminative reranking [39,
71, 23]. For syntactic parsing, [21] were the first to employ a two-stage reranking method to select
outputs from a base parser, while [11] introduced a maximum entropy reranker. In text summarization,
RefSum [53] proposed a second-stage summarization framework to address train-test distribution
mismatches. SimCLS [ 54] used pairwise Learning To Rank (LTR) to select candidates with the
highest matching scores. SummaReranker [68] adopted a multi-task mixture-of-experts framework
to leverage different metrics capturing various aspects of generated candidates. BRIO [55] reused
the base model for a second round of fine-tuning with both cross-entropy loss and a candidate-level
ranking loss. JGR [76] employed an alternate training paradigm to train the generator and reranker.
A key limitation of these reranking methods is that they only represent a one-way process, wherein the
selected candidates become the system’s final output. In contrast, our framework innovatively utilizes
the chosen candidates as memory for the subsequent generation round of a retrieval-augmented
generator, which can produce better candidates with enhanced memory.
3 Methods
In this section, we begin with a motivating experiment on generation as memory(§ 3.1). Then, we
introduce Selfmem, a framework comprising a retrieval-augmented generator(§ 3.2) and a memory
selector (§ 3.3). The complete framework and algorithm are illustrated in Figure 2 and Algorithm 1.
3.1 Generation as Memory
The primary motivation behind our framework stems from the observation that the memory, which is
more similar in distribution to the data during inference, is not the training data (38.89 BLEU, as
shown in the first row of Table 1). Instead, it is the model’s own output (58.58 BLEU) within the
unbounded generation space. One interesting exploration involves directly utilizing the generated
output as memory in relation to the primal problem: better memory prompts better generation.
Table 1: Experiments on the relation between mem-
ory quality and the final hypothesis quality, measured
by the BLEU score with ground truth translation. The
retrieval-augmented translator keeps fixed while the
memory is obtained from different sources.
Memory Source Memory Quality Hypothesis Quality
Retrieval 38.89 58.58
Beam 58.58 58.43
Reference 100 90.43
Random 1.14 49.08
We conduct experiments on the JRC-Acquis
En→De dataset. The first row in Table 1
represents conventional retrieval-augmented
training with retrieved memory and achieves
a 58.58 BLEU score. However, directly in-
corporating beam output of this trained model
as memory (Beam) back into the generation
model does not yield any improvements (row
2), despite its higher similarity to the reference
compared to the retrieved ones. We hypoth-
esize two potential reasons for this: (1) the
retrieval-augmented generator may not gen-
eralize effectively in this context due to the
memory distribution shift (from 38.89 to 58.58), and (2) the beam memory does not offer any
information gain compared to the retrieved one, even it exhibits more overlap with the"
"1
Retrieval-Augmented Generation for
AI-Generated Content: A Survey
Penghao Zhao∗, Hailin Zhang ∗, Qinhan Yu, Zhengren Wang, Yunteng Geng,
Fangcheng Fu†, Ling Yang, Wentao Zhang †, Jie Jiang, Bin Cui †
Abstract—Advancements in model algorithms, the growth of
foundational models, and access to high-quality datasets have
propelled the evolution of Artificial Intelligence Generated Con-
tent (AIGC). Despite its notable successes, AIGC still faces
hurdles such as updating knowledge, handling long-tail data,
mitigating data leakage, and managing high training and infer-
ence costs. Retrieval-Augmented Generation (RAG) has recently
emerged as a paradigm to address such challenges. In partic-
ular, RAG introduces the information retrieval process, which
enhances the generation process by retrieving relevant objects
from available data stores, leading to higher accuracy and better
robustness. In this paper, we comprehensively review existing
efforts that integrate RAG techniques into AIGC scenarios. We
first classify RAG foundations according to how the retriever
augments the generator, distilling the fundamental abstrac-
tions of the augmentation methodologies for various retrievers
and generators. This unified perspective encompasses all RAG
scenarios, illuminating advancements and pivotal technologies
that help with potential future progress. We also summarize
additional enhancements methods for RAG, facilitating effective
engineering and implementation of RAG systems. Then from
another view, we survey on practical applications of RAG across
different modalities and tasks, offering valuable"
"The Power of Noise: Redefining Retrieval for RAG Systems
Florin Cuconasu∗
cuconasu@diag.uniroma1.it
Sapienza University of Rome
Rome, Italy
Giovanni Trappolini∗
trappolini@diag.uniroma1.it
Sapienza University of Rome
Rome, Italy
Federico Siciliano
siciliano@diag.uniroma1.it
Sapienza University of Rome
Rome, Italy
Simone Filice
filice.simone@gmail.com
Technology Innovation Institute
Haifa, Israel
Cesare Campagnano
campagnano@di.uniroma1.it
Sapienza University of Rome
Rome, Italy
Yoelle Maarek
yoelle@yahoo.com
Technology Innovation Institute
Haifa, Israel
Nicola Tonellotto
nicola.tonellotto@unipi.it
University of Pisa
Pisa, Italy
Fabrizio Silvestri
fsilvestri@diag.uniroma1.it
Sapienza University of Rome
Rome, Italy
ABSTRACT
Retrieval-Augmented Generation (RAG) has recently emerged as
a method to extend beyond the pre-trained knowledge of Large
Language Models by augmenting the original prompt with relevant
passages or documents retrieved by an Information Retrieval (IR)
system. RAG has become increasingly important for Generative
AI solutions, especially in enterprise settings or in any domain in
which knowledge is constantly refreshed and cannot be memorized
in the LLM. We argue here that the retrieval component of RAG
systems, be it dense or sparse, deserves increased attention from
the research community, and accordingly, we conduct the first com-
prehensive and systematic examination of the retrieval strategy
of RAG systems. We focus, in particular, on the type of passages
IR systems within a RAG solution should retrieve. Our analysis
considers multiple factors, such as the relevance of the passages in-
cluded in the prompt context, their position, and their number. One
counter-intuitive finding of this work is that the retriever’s highest-
scoring documents that are not directly relevant to the query (e.g.,
do not contain the answer) negatively impact the effectiveness of
the LLM. Even more surprising, we discovered that adding random
documents in the prompt improves the LLM accuracy by up to
35%. These results highlight the need to investigate the appropriate
strategies when integrating retrieval with LLMs, thereby laying the
groundwork for future research in this area.1
CCS CONCEPTS
• Information systems →Novelty in information retrieval.
1The code and data are available at github.com/florin-git/The-Power-of-Noise
*These authors contributed equally to this work.
This work is licensed under a Creative Commons Attribution
International 4.0 License.
SIGIR ’24, July 14–18, 2024, Washington, DC, USA
© 2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0431-4/24/07
https://doi.org/10.1145/3626772.3657834
KEYWORDS
RAG, LLM, Information Retrieval
ACM Reference Format:
Florin Cuconasu∗, Giovanni Trappolini ∗, Federico Siciliano, Simone Fil-
ice, Cesare Campagnano, Yoelle Maarek, Nicola Tonellotto, and Fabrizio
Silvestri. 2024. The Power of Noise: Redefining Retrieval for RAG Sys-
tems. InProceedings of the 47th International ACM SIGIR Conference on
Research and Development in Information Retrieval (SIGIR ’24), July 14–18,
2024, Washington, DC, USA. ACM, New York, NY, USA, 11 pages. https:
//doi.org/10.1145/3626772.3657834
1 INTRODUCTION
Large Language Models (LLMs) [ 9] have demonstrated unprece-
dented proficiency in various tasks, ranging from text generation
and complex question answering [6], to information retrieval (IR)
tasks [22, 57]. However, LLMs have limitations in the handling of
long contexts [52], a constraint that leads to an increased reliance
on their pre-trained knowledge. This limitation not only confines
their ability to effectively manage extended discourse, such as in
books or long conversations, but also increases the probability of
generating hallucinations, instances for which the model produces
factually incorrect or nonsensical information [41]. To improve the
accuracy of responses generated by LLMs, Retrieval-Augmented
Generation (RAG) has emerged as a promising solution [28]. RAG
is primarily designed to improve factual accuracy by providing
the model access to auxiliary information, thereby augmenting the
original prompt with information not necessarily memorized in
the LLM. A key benefit of this approach is that it helps ground the
prompt with relevant information that might help the LLM gener-
ate more accurate answers at inference time. At their core, RAG
systems consist of two fundamental components: a retriever and a
generator. The retriever is responsible for invoking an external IR
system (dense and/or sparse) and feeding the selected results to a
generator component.
This study focuses on the IR aspect of RAG, posing the following
research question: “What characteristics are desirable in a retriever
to optimize prompt construction for RAG systems? Are current re-
trievers ideal?"". We focus on the three main types of documents
719
SIGIR ’24, July 14–18, 2024, Washington, DC, USA Florin Cuconasu et al.
(or passages2) that a retriever can return: relevant, distracting, and
random. Relevant documents contain pertinent information that
either directly answers or might inform the query. Distracting doc-
uments, while not directly answering the query, are semantically
or contextually linked to the topic. For instance, if one asks for
the color of Napoléon’s horse, a passage describing the color of
Joséphine de Beauharnais’ (Napoléon’s first wife) horse, while not
containing the right information, would be highly related. Random
documents have no relation whatsoever to the query and can be
seen as a kind of informational noise within the retrieval process.
One of the key goals of our study is to determine the role of each
type of document and the relative value they bring to the LLM
effectiveness. In particular, we verify whether there is a need to
revisit some of the commonly accepted assumptions in IR systems
when used in the context of LLMs. The main contributions of our
work are the following:
(1) We conduct the first comprehensive study examining the
impact of the type of retrieved documents in RAG on the
LLM effectiveness.
(2) We propose retrieval RAG heuristics that leverage the unex-
pected results of this study.
(3) We release all associated code and data to the community to
encourage further research.
2 RELATED WORKS
2.1 Generative Language Models
The inception of the modern LLM era can be traced back to the
seminal paper"
"LeanDojo: Theorem Proving with
Retrieval-Augmented Language Models
Kaiyu Yang1, Aidan M. Swope2, Alex Gu3, Rahul Chalamala1, Peiyang Song4,
Shixing Yu5, Saad Godil∗, Ryan Prenger2, Anima Anandkumar1,2
1Caltech, 2NVIDIA, 3MIT, 4UC Santa Barbara, 5UT Austin
https://leandojo.org
Abstract
Large language models (LLMs) have shown promise in proving formal theorems
using proof assistants such as Lean. However, existing methods are difficult to
reproduce or build on, due to private code, data, and large compute requirements.
This has created substantial barriers to research on machine learning methods for
theorem proving. This paper removes these barriers by introducing LeanDojo:
an open-source Lean playground consisting of toolkits, data, models, and bench-
marks. LeanDojo extracts data from Lean and enables interaction with the proof
environment programmatically. It contains fine-grained annotations of premises in
proofs, providing valuable data for premise selection—a key bottleneck in theorem
proving. Using this data, we develop ReProver (Retrieval-Augmented Prover): an
LLM-based prover augmented with retrieval for selecting premises from a vast
math library. It is inexpensive and needs only one GPU week of training. Our
retriever leverages LeanDojo’s program analysis capability to identify accessible
premises and hard negative examples, which makes retrieval much more effec-
tive. Furthermore, we construct a new benchmark consisting of 98,734 theorems
and proofs extracted from Lean’s math library. It features challenging data split
requiring the prover to generalize to theorems relying on novel premises that are
never used in training. We use this benchmark for training and evaluation, and
experimental results demonstrate the effectiveness of ReProver over non-retrieval
baselines and GPT-4. We thus provide the first set of open-source LLM-based
theorem provers without any proprietary datasets and release it under a permissive
MIT license to facilitate further research.
1 Introduction
Reasoning is a cornerstone of human intelligence and a fundamental goal of AI [3]. One prominent
task is automated theorem proving (ATP): automatically generating proofs for theorems expressed
in formal logic. ATP is useful for formal mathematics, producing mathematical proofs that can be
checked rigorously [4]. Furthermore, it underpins formal verification, which is essential for proving
the correctness and safety of high-stakes applications [5, 6].
ATP is challenging since the search space is prohibitively large. In many applications, it is impractical
to generate proofs fully automatically. Therefore, interactive theorem proving (ITP) has emerged as
an alternative paradigm. In ITP, proofs are constructed by human experts interacting with software
tools called proof assistants, such as Coq [ 7], Isabelle [ 8], and Lean [ 1]. Machine learning can
automate such interactive theorem proving, opening up a new avenue for theorem proving [9]. The
model can learn to interact with proof assistants, given data containing human-written proofs.
∗Research conducted while Saad Godil was at NVIDIA.
37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks.prelude
import init.data.nat.lemmas init.meta.well_founded_tactics
namespace nat
/-
t h e o r e m m o d _ s e l f ( n : n a t ) : n % n = 0 : =
b e g i n
r w [ m o d _ e q _ s u b _ m o d ( l e _ r e f l _ ) , n a t . s u b _ s e l f , z e r o _ m o d ]
e n d
-/
def gcd : nat → nat → nat
| 0 y := y
| ( x + 1 ) y := have y % ( x + 1 ) < x + 1 , from mod_lt _ $ succ_pos _ ,
gcd ( y % ( x + 1 ) ) ( x + 1 )
theorem gcd_zero_left ( x : nat ) : gcd 0 x = x := begin simp [ gcd ] end
theorem gcd_self ( n : nat ) : gcd n n = n :=
begin
cases n ,
{ unfold gcd } ,
unfold gcd ,
rewrite mod_self ,
apply gcd_zero_left
end
end nat
1
prelude
import init.data.nat.lemmas init.meta.well_founded_tactics
namespace nat
theorem mod_self ( n : nat ) : n % n = 0 :=
begin
rw [ mod_eq_sub_mod ( le_refl _ ) , nat.sub_self , zero_mod ]
end
def gcd : nat → nat → nat
| 0 y := y
| ( x + 1 ) y := have y % ( x + 1 ) < x + 1 , from mod_lt _ $ succ_pos _ ,
gcd ( y % ( x + 1 ) ) ( x + 1 )
theorem gcd_zero_left ( x : nat ) : gcd 0 x = x := begin simp [ gcd ] end
theorem gcd_self ( n : nat ) : gcd n n = n :=
begin
cases n ,
{ unfold gcd } ,
unfold gcd ,
rw mod_self ,
apply gcd_zero_left
end
end nat
1
Lean 
LeanDojoBenchmark•98,734 theorems and proofs•217,776 tactics•129,243 premises
k : ℕ⊢gcd((k + 1) % (k + 1)) (k + 1) = k + 1All accessible premises in the math library
Maximum cosine similarity
Encoder theorem mod_lt ( x : nat ) { y : nat } ( h : 0 < y ) : x % y < y
theorem mod_self ( n : nat ) : n % n = 0
theorem mod_eq_of_lt { ab : nat } ( h : a < b ) : a % b = a
theorem zero_mod ( b : nat ) : 0 % b = 0
1
rewrite mod_self
... Encoder...Encoder
EncoderState ConcatEncoder-decoderTactic
Retrieved premises
Machine learning model
Data extractionTraining
Prove theorems by Interactionn : ℕ⊢gcdn n = n
⊢gcd0 0 = 0k : ℕ⊢gcd(k + 1) (k + 1) = k + 1
k : ℕ⊢gcd((k + 1) % (k + 1)) (k + 1) = k + 1
k : ℕ⊢gcd0 (k + 1) = k + 1
Tacticcases n
unfold gcdunfold gcd
rewrite mod_self
apply gcd_zero_left
Local context⊢GoalProof tree
prelude
import init.data.nat.lemmas init.meta.well_founded_tactics
namespace nat
/-
t h e o r e m m o d _ s e l f ( n : n a t ) : n % n = 0 : =
b e g i n
r w [ m o d _ e q _ s u b _ m o d ( l e _ r e f l _ ) , n a t . s u b _ s e l f , z e r o _ m o d ]
e n d
-/
def gcd : nat → nat → nat -- gcd z y
| 0 y := y -- Case 1: z == 0
| ( x + 1 ) y := gcd ( y % ( x + 1 ) ) ( x + 1 ) -- Case 2: z > 0
theorem gcd_zero_left ( x : nat ) : gcd 0 x = x := begin simp [ gcd ] end
theorem gcd_self ( n : nat ) : gcd n n = n :=
begin
cases n ,
{ unfold gcd } ,
unfold gcd ,
rewrite mod_self ,
apply gcd_zero_left
end
end nat
1
... 33K on average…
Figure 1: Top right: LeanDojo extracts proofs in Lean [ 1] into datasets for training machine
learning models. It also enables the trained model to prove theorems by interacting with Lean’s proof
environment. Top left: The proof tree of a Lean theorem ∀n ∈ N, gcd n n = n, where gcdis the
greatest common divisor (details in Sec. 3). When proving the theorem, we start from the original
theorem as the initial state (the root) and repeatedly apply tactics (the edges) to decompose states
into simpler sub-states, until all states are solved (the leaf nodes). Tactics may rely on premises such
as mod_self and gcd_zero_left defined in a large math library. E.g., mod_self is an existing
theorem ∀n ∈ N, n % n = 0used in the proof to simplify the goal. Bottom: Our ReProver model
(Sec. 5). Given a state, it retrieves premises from the math library, which are concatenated with the
state and fed into an encoder-decoder Transformer [2] to generate the next tactic.
Formal theorem proving serves as an important challenge for machine learning. From a computer
science perspective, formal proofs can be treated as programs [10]. But unlike conventional programs
in C++ or Python, the correctness of proofs can be verified using proof assistants. Therefore, theorem
proving may be considered a special form of code generation, with rigorous evaluation and no room
for the model to hallucinate. This can be consequential to current large language models (LLMs), as
they have demonstrated exceptional capability in code generation [11] but have flaws in factuality
and hallucination [12]. In addition, augmenting LLMs with external tools, such as proof assistants,
has shown promise in improving their various capabilities, including multi-step reasoning [13].
Current research on LLMs for theorem proving is facing many barriers. To our knowledge, none
of the existing LLM-based provers are open-source [14–21]. They all use private pretraining data,
and the compute requirements can reach thousands of GPU days [ 17]. Furthermore, some rely
on tailored infrastructure for distributed training and interaction with the proof assistant—both are
not possible to fully reproduce without open-source code [ 17, 19]. We change the status quo by
introducing LeanDojo: open-source toolkits, models, and benchmarks that give researchers access to
state-of-the-art LLM-based provers with modest computational costs.
Tools for Data Extraction and Interaction. We focus on Lean, a proof assistant popular among
mathematicians.2 Our framework LeanDojo provides two essential functions for learning-based
theorem proving (Fig. 1): extracting data and enabling models to interact with Lean programmatically.
For data extraction, LeanDojo extracts training data not directly visible in the raw Lean code (Fig. 2),
e.g., proof trees consisting of intermediate states between proof steps (Fig. 1 Top left). In addition,
LeanDojo is the first tool to locate premises in Lean proofs, enabling training machine learning
models for premise selection. For interaction, LeanDojo turns Lean into a gym-like interactive
environment [22]. Using LeanDojo, the model can observe proof states, change the state by executing
2“Lean” in our paper refers to Lean 3 by default. Lean 4 is not backward-compatible but is also supported by
LeanDojo. Our Lean 4 results are in Appendix D.
2proof steps (referred to as “tactics” in proof assistants), and receive feedback from Lean. LeanDojo
is the first tool capable of interacting with Lean reliably, reducing proof-checking errors in existing
tools [19] (correct proofs misjudged as incorrect) from 21.1% to 1.4%.
Retrieval-Augmented LLMs for Theorem Proving. LeanDojo addresses a key bottleneck in
theorem proving: premise selection [23, 24]. Existing LLM-based provers generate the next proof
step (tactic), taking only the current state as input. However, proving theorems depends critically on
the premises, such as lemmas and definitions, from a math library.
For example, Fig. 1 (Top left) illustrates the proof of “∀n ∈ N, gcd n n = n”, where gcd stands for
greatest common divisor. The proof starts from the original theorem as the initial state and repeatedly
applies tactics to decompose states into simpler sub-states, until all states are solved. Tactics may rely
on premises such as mod_selfand gcd_zero_leftdefined in a large math library. E.g., mod_self
is an existing theorem “∀n ∈ N, n % n = 0” useful for simplifying the goal.
Incorporating all possible premises is too large to fit into LLMs’ input, given the limited context
window. Existing methods must learn to memorize the association between the proof state and the
name mod_self. It works if the premise has been used in the training data to solve similar goals, but
does not generalize to truly novel scenarios, e.g., theorems requiring lemmas unseen in training.
One potential solution is to complement memorization with explicit premise selection. LeanDojo
extracts premise data from Lean, including where they are defined and used. It enables us to tackle
premise selection by augmenting LLMs with retrieval. We introduceReProver (Retrieval-Augmented
Prover) (Fig. 1 Bottom): Given the current state, it generates a tactic conditioning on a small number
of premises retrieved from Lean’s math library,mathlib[25].
We need to limit retrieval to a small number of premises for it to be effective, and ideally, they should
contain the ground truth premise. Our retriever builds upon Dense Passage Retriever (DPR) [ 26]
but incorporates two algorithmic innovations: First, not all premises are accessible when proving
a theorem (Sec. 3). LeanDojo can perform program analysis on Lean code to determine accessible
premises. On our data, that reduces the average number of premises from 128K to 33K, significantly
simplifying the retriever’s task. Second, DPR needs negative examples in training and benefits from
hard negatives, i.e., irrelevant premises that are hard to distinguish from ground truth ones. We
propose in-file negatives: a simple mechanism to find hard negatives in premise selection, which
samples negative premises defined in the same Lean source file as the ground truth premise.
LeanDojo Benchmark. Using LeanDojo, we construct a benchmark containing 98,734 theorem-
s/proofs extracted from mathlib. Our benchmark is one of the largest math-focused theorem-proving
datasets. We find that the common practice of splitting theorems randomly into training/testing has
led to an overestimated performance in the previous papers. LLMs can prove seemingly difficult
theorems simply by memorizing the proofs of similar theorems during training. In LeanDojo Bench-
mark, we mitigate this issue by designing challenging data split requiring the model to generalize to
theorems relying on novel premises that are never used in training.
We use LeanDojo Benchmark to train and evaluate ReProver. Training takes only five days on a single
GPU. In evaluation, ReProver can prove 51.2% theorems, outperforming a baseline that generates
tactics directly without retrieval (47.6%) and another baseline using GPT-4 [27] to generate tactics
in a zero-shot manner (29.0%). We also test ReProver on two existing datasets, MiniF2F [28] and
ProofNet [29]. It can prove 26.5% theorems in MiniF2F and 13.8% in ProofNet, which is competitive
with state-of-the-art methods without reinforcement learning [ 19], even though trained using far
fewer resources. Moreover, it can prove 65 theorems that currently do not have proofs in Lean. Thus,
our tool can also serve as an effective tool for augmenting existing math libraries in Lean.
Contributions. In summary, we make four main contributions: First, we introduce tools for
extracting data from and interacting with Lean. Second, we develop ReProver, the first retrieval-
augmented language model for theorem proving. Third, we construct a challenging benchmark for
learning-based theorem proving and use it to validate the effectiveness of ReProver. Finally, we
facilitate open research on LLMs for theorem proving by releasing our data, model, and code. Our
method does not rely on private datasets and can be trained on a single GPU within a week. We
believe this will significantly lower the barriers to academic research in this area and establish the first
accessible baselines for future work to build upon. Further, our method can be used to automatically
generate new Lean proofs without requiring human effort.
32 Related Work
Theorem Proving. Classical provers express theorems in first-order logic and search for proofs
automatically in a large space [30, 31]. Even with data-driven search heuristics [32, 33], they fail to
scale to large formalization projects. Therefore, recent work on learning-based theorem proving has
focused on an alternative paradigm: automating the interaction with proof assistants.
The architecture of learning-based provers progressed from classical machine learning algorithms such
as KNN [34], to graph neural networks explicitly encoding the syntax of formal expressions [9, 35],
and now Transformer-based LLMs treating expressions as plain strings [ 14]. Besides the model
architecture, researchers have explored several complementary dimensions: proof search algorithms
for assembling model-generated steps into complete proofs [ 17, 21]; overcoming data scarcity
through reinforcement learning (RL) [17, 19, 36, 37] or synthetic/auxiliary data [16, 38–40]; as well
as outsourcing some proof goals to classical provers [18, 41–43]. Our base model without retrieval is
a combination of straightforward design choices. It generates tactics by finetuning an encoder-decoder
Transformer, ByT5 [44], via supervised learning without RL or auxiliary data. Then it searches for
proofs using best-first search. Our model’s algorithmic novelty lies in the retrieval.
Premise Selection. Selecting useful premises is recognized as a key challenge in theorem prov-
ing [23, 24, 45, 46]. Machine learning methods for premise selection have also progressed from
classical models [41, 47, 48], recurrent neural networks [24], graph neural networks [38], to Trans-
formers [49, 50]. However, existing methods either tackle premise selection in isolation without
theorem proving [24, 38, 48] or feed the premises to a symbolic prover [41, 47, 49]. To our knowl-
edge, we are the first to augment a learning-based formal theorem prover with retrieved premises so
that the prover can learn how to use them effectively. For example, it can decide whether to use an
explicitly retrieved premise or an implicitly memorized one.
Data and Tools for Theorem Proving. Tools for data extraction and interacting with proof
assistants have been crucial drivers of learning-based theorem proving. Existing tools and datasets
can be divided by proof assistants: Coq has GamePad [51], CoqGym [9], and PRISM [52]; Isabelle
has IsarStep [ 53] and PISA [ 15]; HOL Light has HOList [ 54] and HoLStep [ 55], and Lean has
LeanStep [16] and lean-gym[19]. MiniF2F [28] is the only cross-system dataset, with 488 theorems
for evaluation. However, it does not have training theorems and is restricted to the domain of math
olympiads.
Among available tools extracting data from proof assistants, LeanDojo is the only one that can extract
premises for retrieval-augmented theorem proving. A few existing datasets also have premises [49,
54], but their data extraction tools are not public, making it difficult to construct new datasets. In
addition, LeanDojo is the only tool that can interact with Lean robustly (Sec. 4) and can extract data
from Lean 4. See Appendix A.3 for a detailed comparison between LeanDojo and alternatives.
Mathematical Reasoning in Natural Language. We focus on proving theorems expressed in
formal logic, whereas researchers have also produced a plethora of work on mathematical reasoning
in natural language [56–63]. A particularly relevant task is autoformalization, translating natural
language texts into formal theorems and proofs [29, 64–72].
Retrieval-Augmented Language Models. Our ReProver is the first retrieval-augmented language
model for formal theorem proving, though similar architectures have been studied extensively in
NLP [73–81]. In addition, there have been many retrieval-augmented methods for code generation [82–
88]. Most of them retrieve from a corpus not directly related to the current file, e.g., GitHub or Stack
Overflow. In contrast, our retrieval corpus consists of premises accessible to the current file, which is
determined by program analysis using LeanDojo. This is similar to what CoCoMIC [ 88] does for
Python. However, their retrieval is based on heuristics, whereas ours is learned.
3 Background: Theorem Proving in Lean
At a high level, Lean is a programming language that allows you to write not only conventional
programs but also theorems and proofs. To that end, it provides two pieces of machinery: First,
it provides a unified language for defining programs, mathematical objects, theorems, and proofs,
based on functional programming with dependent types [89]. Second, it provides a tactic system for
constructing machine-checkable proofs semi-automatically.
4data/nat/gcd.lean
data/nat/lemmas.lean
prelude
import init.data.nat.lemmas init.meta.well_founded_tactics
namespace nat
theorem mod_self ( n : nat ) : n % n = 0 :=
begin
rw [ mod_eq_sub_mod ( le_refl _ ) , nat.sub_self , zero_mod ]
end
def gcd : nat → nat → nat
| 0 y := y
| ( x + 1 ) y := have y % ( x + 1 ) < x + 1 , from mod_lt _ $ succ_pos _ ,
gcd ( y % ( x + 1 ) ) ( x + 1 )
theorem gcd_zero_left ( x : nat ) : gcd 0 x = x := begin simp [ gcd ] end
theorem gcd_self ( n : nat ) : gcd n n = n :=
begin
cases n ,
{ unfold gcd } ,
unfold gcd ,
rw mod_self ,
apply gcd_zero_left
end
end nat
1
Math library
prelude
import init.data.nat.lemmas init.meta.well_founded_tactics
namespace nat
/-
t h e o r e m m o d _ s e l f ( n : n a t ) : n % n = 0 : =
b e g i n
r w [ m o d _ e q _ s u b _ m o d ( l e _ r e f l _ ) , n a t . s u b _ s e l f , z e r o _ m o d ]
e n d
-/
def gcd : nat → nat → nat
| 0 y := y
| ( x + 1 ) y := have y % ( x + 1 ) < x + 1 , from mod_lt _ $ succ_pos _ ,
gcd ( y % ( x + 1 ) ) ( x + 1 )
theorem gcd_zero_left ( x : nat ) : gcd 0 x = x := begin simp [ gcd ] end
theorem gcd_self ( n : nat ) : gcd n n = n :=
begin
cases n ,
{ unfold gcd } ,
unfold gcd ,
rewrite mod_self ,
apply gcd_zero_left
end
end nat
1
prelude
import init.data.nat.lemmas init.meta.well_founded_tactics
namespace nat
/-
t h e o r e m m o d _ s e l f ( n : n a t ) : n % n = 0 : =
b e g i n
r w [ m o d _ e q _ s u b _ m o d ( l e _ r e f l _ ) , n a t . s u b _ s e l f , z e r o _ m o d ]
e n d
-/
def gcd : nat → nat → nat -- gcd z y
| 0 y := y -- Case 1: z == 0
| ( x + 1 ) y := gcd ( y % ( x + 1 ) ) ( x + 1 ) -- Case 2: z > 0
theorem gcd_zero_left ( x : nat ) : gcd 0 x = x := begin simp [ gcd ] end
theorem gcd_self ( n : nat ) : gcd n n = n :=
begin
cases n ,
{ unfold gcd } ,
unfold gcd ,
rewrite mod_self ,
apply gcd_zero_left
end
end nat
1
Import
Figure 2: Definition of greatest common divisor (gcd) in Lean and two related theorems. The proof
of gcd_self(between “begin” and “end”) relies on a premise mod_selfimported from another
file in the math library. Lean can run this proof to produce the proof tree in Fig.1 (Top left).
We use a simple example in Fig. 2 to illustrate how theorems are formalized and proved in Lean.3
Here we want to formalize the greatest common divisor (gcd) of two natural numbers. First, we define
gcdas a recursive function, taking two natural numbers as parameters and returning their gcdvia the
Euclidean algorithm. Then, we state a lemma named gcd_zero_leftthat ∀x ∈ N, gcd 0 x = x,
which can be proved simply by the definition of gcd. Finally, we state our main theorem gcd_self
that ∀n ∈ N, gcd n n = n, followed by its proof consisting of five tactics. In theorem proving, we
are only concerned with generating the proof, i.e., the part between “begin” and “end”; everything
before “begin” is known, including other files imported.
The syntax of tactics is quite expressive. They can take arguments and can be combined into
compound tactics. You can think of tactics as programs in a domain-specific language (DSL). Users
can extend the DSL by defining new tactics. This discrete, combinatorial, and unbounded action
space makes theorem proving challenging for machine learning.
Another challenge is premise selection. Premises are existing lemmas or definitions useful for proving
a theorem. They are used as arguments in tactics. For example, in Fig. 2 and Fig. 1 ( Top left), the
tactic “rewrite mod_self” rewrites the goal using the premise mod_self, which is defined in
another file imported by the current file. Proofs cannot use premises that haven’t been defined. For
example, gcd_selfcannot be used to prove gcd_zero_left. In addition, they cannot use premises
not imported to the current file. Still, premises come from a large math library containing hundreds
of thousands of existing definitions and theorems, making it hard, for humans and machines alike, to
select the right premises when generating a tactic. This is a key bottleneck in theorem proving and is
what we aim to address through retrieval-augmented LLMs.
4 LeanDojo: Toolkit and Benchmark
LeanDojo serves two essential needs of learning-based theorem proving in Lean. First, it extracts
training data from Lean, and we use this capability to construct a challenging theorem proving
benchmark. Second, it enables the model to interact with Lean programmatically.
Data Extraction. Lean repos (e.g., mathlib or lean-liquid) contain source code of human-
written theorems/proofs. However, the raw code is unsuitable for training the prover. It lacks runtime
information that humans can access when using Lean, such as intermediate states between proof
steps. Therefore, LeanDojo extracts the following information not directly visible in the code:
3The process is similar in many other proof assistants, though they may have different logical foundations.
5• File dependencies and abstract syntax trees (ASTs): LeanDojo processes the repo to produce
a directed acyclic graph whose nodes are files and edges are import relations between files. In
addition, LeanDojo produces the AST of each file. File dependencies and ASTs are useful for
program analysis, e.g., collecting theorems defined in a file or premises accessible to a theorem.
• States and tactics: LeanDojo extracts all tactics in proofs. For each tactic, it also extracts the states
before/after the tactic, which allows us to reconstruct the proof tree in Fig. 1 (Top left).
• Premises: For each premise, such as mod_self in Fig. 2, LeanDojo records where it is defined
(location in data/nat/lemma.lean) and where it is used (locations across many files). In addition,
premises have unique fully qualified names (e.g.,nat.mod_self) but are often used by ambiguous
short names (mod_self), relying on Lean to perform name resolution. LeanDojo is capable of
recording their full names.
Lean has basic support for exporting dependencies, ASTs, states, and tactics. However, it cannot
resolve the premises’ full names and locate their definitions. Therefore, we modify Lean to record
this information (details in Appendix A.1). The modified Lean is used only for data extraction but
not for evaluation, so we do not risk accidentally breaking Lean’s logical soundness.
LeanDojo Benchmark. We construct a benchmark for premise selection and theorem proving,
named LeanDojo Benchmark. The data is extracted from mathlib,4 Lean’s centralized math library
covering diverse topics such as analysis, algebra, and geometry.5 LeanDojo Benchmark is one of
the largest math-focused theorem proving datasets, consisting of 98,734 theorems from 3,384 Lean
files. Unlike existing datasets in Lean [ 16], LeanDojo Benchmark also contains the definitions of
130,262 premises, including not only theorems but also other definitions that can be used as premises
(e.g., gcd in Fig. 2. Furthermore, the dataset has 217,776 tactics, 129,243 of them with at least one
premise. The average number of premises is 2.13 among tactics with premises. Appendix B contains
additional information on data format, datasheet [90], hosting, and licensing.
lemma conj_mul : ( a * b ) . conj = b.conj * a.conj := begin
ext ; simp ; ring_exp
end
lemma conj_conj_mul : ( a.conj * b ) . conj = b.conj * a := begin
rw [ conj_mul , conj_conj ]
end
lemma conj_mul_conj : ( a * b.conj ) . conj = b * a.conj := begin
rw [ conj_mul , conj_conj ]
end
1
src/algebra/quaternion.lean
Figure 3: Similar theorems/proofs are common. If splitting them randomly into training/testing, the
model can prove testing theorems by memorization.
LeanDojo Benchmark has 94,734/2,000/2,000 theorems for training/validation/testing. It features
a challenging data split for testing the prover’s generalization in more realistic scenarios. Splitting
theorems randomly can overestimate the prover’s performance, by allowing it to prove many theorems
through memorization. In human-written Lean code, a common idiom is to have a block of similar
theorems/proofs for slightly different properties of the same math concept. For example, in Fig. 3,
the last two theorems not only look similar but have identical proofs. If one of them is in training,
the model can easily prove the other one by memorization. This shortcut enables the model to prove
seemingly nontrivial theorems, including those requiring premises to prove.
To mitigate this issue, besides the random split, we create a challenging data split named
novel_premises. It requires testing proofs to use at least one premise that has never been used in
training. For example, the last two theorems in Fig. 3 both use the premise conj_mul. If one theorem
is in the training set of the novel_premisessplit, the other one must also be in training.
4We use the commit 19c869efa56bbb8b500f2724c0b77261edbfa28c released on October 11, 2023.
5More details, statistics, and visualizations ofmathlib can be found athttps://leanprover-community.
github.io/mathlib_stats.html.
6Interacting with Lean. Another important function of LeanDojo is to interact with Lean program-
matically. It turns Lean into a gym-like environment [22], in which the prover can observe the proof
state, run tactics to change the state, and receive feedback on errors or on proof completion. This
environment is indispensable for evaluating/deploying the prover or training it through RL.
Below is LeanDojo’s main interface for interacting with Lean through tactics. Lean also supports
other proof styles not based on tactics. Although we only support tactic-style proofs, they are
sufficiently general since any proof can be converted to a tactic-style proof.6
• initialize(theorem): Given the theorem to prove, LeanDojo returns the initial state. A valid
state is a string representing current proof goals and local contexts (see the nodes in Fig. 1 Top left).
When there are multiple goals, their strings are concatenated.
• run_tac(state, tactic): Run a tactic on a given state and return the next state. The returned
state will be an error state if the tactic execution is not successful, e.g., due to timeout or inapplicable
tactic. If the input state is an error, the result can only be an error.
Building this environment is technically challenging, as Lean is designed for human users, not
machines. LeanDojo is the first tool that can interact with Lean reliably. Existing tool [19] is limited:
21.1% of the ground truth proofs are misjudged as incorrect, due to issues with how they construct
the proof environment, which distorts the reported performance and produces unreliable feedback
when used in reinforcement learning. In contrast, LeanDojo reduces the number of misjudgments to
1.4%. Details are in Appendix A.2.
5 ReProver: Retrieval-Augmented Theorem Prover
We develop the ReProver model that uses retrieval to select premises explicitly. At its core is a
retrieval-augmented tactic generator (Fig. 1 Bottom). Given the current proof state, it retrieves a
handful of potentially useful premises and generates a tactic conditioning on the concatenation of the
state and retrieved premises. When proving theorems, the model generates multiple tactic candidates
at each step, which are used in a standard best-first search algorithm to find proofs [16, 18, 19, 28].
Premise Retrieval. Our retriever is based on Dense Passage Retriever [ 26]. Given a state s as
the query and a library of candidate premises P = {pi}N
i=1, it retrieves a ranked list of m premises
{p′
i}m
i=1 from P. In DPR, s and pi are both raw texts but are embedded in a vector space, and we
retrieve the top m premises maximizing the cosine similarity between the state and the premise.
More formally, we have a function f parameterized by θ for embedding both the state and the
premises into a h-dimensional vector space: f(s, θ), f(pi, θ) ∈ Rh. We retrieve premises maximizing
f(s, θ)T f(pi, θ)/(∥f(s, θ)∥2∥f(pi, θ)∥2). We choose f to be a Transformer encoder [2] followed
by average pooling: f(·, θ) = AvgPool(Enc(·, θ)).
The retrieval is efficient. The premise embeddings f(pi, θ) can be pre-computed, and we only
need one forward pass to compute f(s, θ). We do not rerank the retrieved premises as in Mag-
nushammer [49], which is more costly since it requires a separate forward pass for each retrieved
premise.
Similar to DPR, we train the retriever by minimizing a contrastive loss between positive premises
and in-batch negative premises. Specifically, suppose we have a batch of b states. For each state, we
sample a positive premise from the ground truth and n negative premises from P.7 They are called
“in-batch” negatives because they are shared by all states in the batch—Every state is associated with
all b · (n + 1) premises; at least 1 of them is positive. Let lij ∈ {0, 1} denote whether a state-premise
pair (si, pj) is positive. We minimize the mean squared loss:
L(θ) =
bX
i=1
b·(n+1)X
j=1
lij − f(si, θ)T f(pj, θ)
∥f(si, θ)∥2∥f(pj, θ)∥2

2
. (1)
6Another common type of proofs is “term-style proofs”. Any term-style proof “X” can always be converted
into an equivalent tactic-style proof “exact X”, though such conversion may lead to unidiomatic proofs.
7When training the retriever, we ignore proof states followed by tactics without using any premise.
7Retrieving from Accessible Premises. We incorporate into DPR two insights tailored to premise
selection. First, instead of retrieving from all premises in the math library, we restrict to premises
accessible to the current theorem. They include premises defined in the same file before the theorem,
as well as those imported from other files. We compute accessible premises for each theorem, relying
on LeanDojo’s capability in program analysis (Sec. 4). Focusing on accessible premises makes P
much smaller. LeanDojo Benchmark contains 130,262 premises in total, but the average number of
accessible premises is only 33,160.
In-file Negative Examples. DPR’s performance depends critically on the quality of negative
examples [91, 92]. In early experiments, we sampled all n negative premises randomly, and the
model often mistakenly retrieved other premises from the same file as the positive one. Therefore, we
propose a scheme that samples k in-file negatives and n − k random negatives for training.
Tactic Generation. As in Fig. 1 ( Bottom), retrieved premises are concatenated with the state. 8
Then an encoder-decoder Transformer, ByT5 [44], takes them as input and generates the tactic. The
model is trained to minimize the cross entropy loss w.r.t. human-written tactics.
Training ReProver takes substantially less compute than prior methods (120 GPU hours vs. more
than 1000 hours [16, 17]). All existing LLM-based provers pretrain on datasets specific to math and
coding [14–20]. The pretraining is computationally expensive, and the datasets are kept private. In
contrast, we choose to avoid domain-specific pretraining and build upon google/byt5-small—a
model checkpoint that is generic, publicly available, and relatively small (299M parameters vs.
837M [16] or 600M [ 17]). We could see further benefits from domain-specific pretraining, as in
Minerva [57], or stronger LLMs like LLaMA [93] or StarCoder [94], but that is beyond our scope. In
addition, our model is finetuned on human-written tactics only, without auxiliary data [16] or data
collected through online interaction with Lean [17, 19]. These orthogonal directions are valuable but
will significantly increase the method’s complexity and compute requirements.
6 Experiments
We evaluate ReProver on LeanDojo Benchmark. It outperforms baselines on premise selection and
theorem proving, demonstrating the promise of theorem proving with retrieval-augmented language
models. Experimental details and hyperparameters are in Appendix C.1.
Premise Selection. For premise selection, we only use tactics in LeanDojo Benchmark that have at
least one premise. The model, based on a ByT5 encoder, uses the state before a tactic as the query to
retrieve 100 premises. Then, we calculate standard metrics in information retrieval: R@k (recall for
the top k retrieved premises) and MRR (mean reciprocal rank).
Our first baseline is a classical BM25 retriever [ 95] without machine learning. Results in Table 1
show that our method outperforms BM25 significantly across the board. However, it exhibits a large
performance degradation on the challenging data split (comparing novel_premises to random).
This is consistent with the general observation that machine learning can be brittle in the presence
of distribution shifts. In addition, we compare with two ablations: one retrieving from all premises
(instead of accessible premises only) and the other without in-file negatives. They perform worse
than our method, demonstrating the effectiveness of our two improvements upon DPR.
Theorem Proving Experimental Setup. Then we evaluate ReProver on theorem proving. The
training has two stages: First, we train the retriever and use it to retrieve 100 premises for all
proof states in LeanDojo Benchmark. Second, we train the tactic generator, taking as input the
concatenation of the state and retrieved premises (truncated to a length limit). During evaluation, the
tactic generator is combined with best-first search to prove theorems. We evaluate thePass@1 metric:
The prover is given only one attempt and must find the proof within a wall time limit of 10 minutes.
Training takes five days on a single NVIDIA A100 GPU with 80GB memory, and evaluation takes
two days on eight V100 GPUs. Please see Appendix C.1 for details.
Baselines. Following prior work [16, 28], we include tidyas a baseline. It is a tactic in mathlib
that tries to complete the proof using heuristics (without machine learning). We apply tidydirectly
8We retrieve 100 premises, concatenate them with the state, and truncate the concatenation to a fixed length.
8Table 1: Premise selection testing performance. For each method, we train and evaluate two models
independently using different data splits (random and novel_premises; see Sec. 4). R@k is the
recall for the top k retrieved premises, and MRR is the mean reciprocal rank metric (higher is better).
Our retriever outperforms BM25 and ablations. Results for Lean 4 are in Appendix D.
Method random novel_premises
R@1 R@10 MRR R@1 R@10 MRR
BM25 6.7 17.2 0.15 5.9 15.5 0.14
w/ all premises 1.9 11.9 0.08 2.1 12.4 0.08
Ours 13.5 38.4 0.31 9.1 27.6 0.24
w/ all premises 11.7 36.2 0.27 7.1 23.1 0.20
w/o in-file negatives 10.8 33.1 0.25 7.9 25.7 0.22
to the original theorem and see if it can succeed within the wall time limit. Another baseline uses
GPT-4 as the tactic generator. Given a state, it queries GPT-4 to generate 35 tactics in zero-shot. After
removing invalid ones, the remaining tactics are combined with best-first search to find proofs. Data
contamination is possible: Many proofs had been publicly available on GitHub before GPT-4’s data
cutoff date (September 2021). See Appendix C.2 for details.
Unfortunately, it is not feasible to compare with existing LLM-based provers in Lean [16, 17, 19].
None of them are open-source or can be reproduced with reasonable effort. Furthermore, we cannot
compare directly with the numbers reported in their papers, due to differences in data, infrastructure,
and training procedures (details in Appendix C.3). Many difficulties are due to the private nature
of existing methods. By releasing our code and models, we hope to create accessible baselines for
future work to build upon.
Table 2: Theorem proving Pass@1 (%) on the testing data of LeanDojo Benchmark. Our ReProver
model outperforms tidy, GPT-4, and a baseline that generates tactics directly without retrieval.
Results for Lean 4 are in Appendix D.
Method random novel_premises
tidy 23.8 5.3
GPT-4 29.0 7.4
ReProver (ours) 51.2 26.3
w/o retrieval 47.6 23.2
Results. Table 2 shows the results on the testing data of LeanDojo Benchmark. ReProver outper-
forms all baselines on two different data splits, demonstrating the effectiveness of retrieval-augmented
theorem proving. GPT-4 performs substantially worse than our method, even though it may have
seen the ground truth proofs due to data contamination. The task cannot be solved out of the box by
state-of-the-art LLMs, calling for algorithmic innovations to make further progress.
Testing theorems in novel_premisesare indeed much more challenging. All methods in Table 2
perform substantially worse on novel_premisesthan the randomsplit. We argue that performance
on challenging splits is more indicative of the prover’s capability and should be emphasized in the
future development of theorem proving.
Evaluation on MiniF2F and ProofNet. We run ReProver to prove theorems in MiniF2F [28] and
ProofNet [29]. These two datasets are for testing only and do not have training theorems, which makes
them challenging since the distribution of theorems is quite different from mathlib used to train
ReProver. MiniF2F focuses on math olympiads, and ProofNet focuses on exercises in undergraduate
math textbooks. On MiniF2F’s test set in Lean, ReProver achieves a Pass@1 of 26.5%, which is
competitive with state-of-the-art methods without RL (25.9% in Polu et al. [19]). On ProofNet, our
Pass@1 is 13.8%, which is the first reported theorem proving result on this dataset. Further, many
theorems do not have ground truth proofs in Lean. Our prover discovers 33 proofs in MiniF2F and
39 proofs in ProofNet that currently do not have Lean proofs. Please see Appendix C.4 for details,
examples, and caveats.
97 Conclusion
We have introduced LeanDojo: an open-source playground for learning-based theorem proving in
Lean, consisting of toolkits, models, and benchmarks. It extracts data from Lean and enables the
model to interact with Lean programmatically. We have developed ReProver, the first retrieval-
augmented LLM for theorem proving. Limitations and future work are discussed in Appendix F.
We have released our code, data, models, and documentation to facilitate future research:
• LeanDojo’s codebase for data extraction and interaction with Lean: https://github.
com/lean-dojo/LeanDojo
• LeanDojo’s documentation: https://leandojo.readthedocs.io
• Datasets: (1) LeanDojo Benchmark: https://doi.org/10.5281/zenodo.8016385
with DOI 10.5281/zenodo.8016385. (2) LeanDojo Benchmark 4 (Appendix D): https:
//doi.org/10.5281/zenodo.8040109with DOI 10.5281/zenodo.8040109.
• ReProver’s code and models: https://github.com/lean-dojo/ReProver
• ChatGPT plugin (Appendix E): https://github.com/lean-dojo/LeanDojoChatGPT
• LeanDojo Website: https://leandojo.org
Acknowledgments and Disclosure of Funding
This work is partially supported by Caltech’s Center for Autonomous Systems and Technologies.
Kaiyu Yang is supported by the Computing, Data, and Society Postdoctoral Fellowship at Caltech.
Alex Gu is supported by the National Science Foundation (NSF) Graduate Research Fellowship. Rahul
Chalamala and Peiyang Song are supported by the Summer Undergraduate Research Fellowships
(SURF) program at Caltech. Anima Anandkumar is partially supported by the Bren endowed chair.
We appreciate the valuable feedback from Logan Murphy and members of the Anima AI+Science
Lab on an initial version of this paper. We thank Junyan Xu for manually inspecting the proofs
generated by our model on ProofNet. We also thank Jeremy Avigad and Mario Carneiro for insightful
discussions on supporting Lean 4 in LeanDojo.
10"
"Retrieval-Augmented Generation for
Knowledge-Intensive NLP Tasks
Patrick Lewis†‡ , Ethan Perez?,
Aleksandra Piktus† , Fabio Petroni† , Vladimir Karpukhin† , Naman Goyal† , Heinrich Küttler† ,
Mike Lewis† , Wen-tau Yih† , Tim Rocktäschel†‡ , Sebastian Riedel†‡ , Douwe Kiela†
† Facebook AI Research;‡ University College London;?New York University;
plewis@fb.com
Abstract
Large pre-trained language models have been shown to store factual knowledge
in their parameters, and achieve state-of-the-art results when ﬁne-tuned on down-
stream NLP tasks. However, their ability to access and precisely manipulate
knowledge is still limited, and hence on knowledge-intensive tasks, their perfor-
mance lags behind task-speciﬁc architectures. Additionally, providing provenance
for their decisions and updating their world knowledge remain open research prob-
lems. Pre-trained models with a differentiable access mechanism to explicit non-
parametric memory can overcome this issue, but have so far been only investigated
for extractive downstream tasks. We explore a general-purpose ﬁne-tuning recipe
for retrieval-augmented generation (RAG) — models which combine pre-trained
parametric and non-parametric memory for language generation. We introduce
RAG models where the parametric memory is a pre-trained seq2seq model and
the non-parametric memory is a dense vector index of Wikipedia, accessed with
a pre-trained neural retriever. We compare two RAG formulations, one which
conditions on the same retrieved passages across the whole generated sequence,
and another which can use different passages per token. We ﬁne-tune and evaluate
our models on a wide range of knowledge-intensive NLP tasks and set the state of
the art on three open domain QA tasks, outperforming parametric seq2seq models
and task-speciﬁc retrieve-and-extract architectures. For language generation tasks,
we ﬁnd that RAG models generate more speciﬁc, diverse and factual language than
a state-of-the-art parametric-only seq2seq baseline.
1 Introduction
Pre-trained neural language models have been shown to learn a substantial amount of in-depth knowl-
edge from data [47]. They can do so without any access to an external memory, as a parameterized
implicit knowledge base [51, 52]. While this development is exciting, such models do have down-
sides: They cannot easily expand or revise their memory, can’t straightforwardly provide insight into
their predictions, and may produce “hallucinations” [38]. Hybrid models that combine parametric
memory with non-parametric (i.e., retrieval-based) memories [20, 26, 48] can address some of these
issues because knowledge can be directly revised and expanded, and accessed knowledge can be
inspected and interpreted. REALM [20] and ORQA [31], two recently introduced models that
combine masked language models [8] with a differentiable retriever, have shown promising results,
34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.TheDiYineComed\(x) T
QXeU\EQcRdeU
T([)
MIPS pθ
GeneUaWoU¬Sѡ(PaUaPeWULc)
MaUgin-ali]eThiV14WhcenWXU\ZoUkiVdiYidedinWo3VecWionV:""InfeUno"",""PXUgaWoUio""&""PaUadiVo""(y)
End-to-End Backprop through T and¬pθ
BaUackObamaZaVboUninHaZaii.(x)FacW VeUiÀcaWiRQ: FacW QXeU\
VXppoUWV(y)
QXeVWiRQ GeQeUaWiRQ
FacW VeUiÀcaWiRQ:LabeO GeQeUaWiRQ
DRcXmeQWIQde[
Define""middleeaU""(x)
QXeVWiRQ AQVZeUiQg:QXeVWiRQ QXeU\
ThemiddleeaUinclXdeVWheW\mpaniccaYiW\andWheWhUeeoVVicleV.(y)QXeVWiRQ AQVZeUiQg:AQVZeU GeQeUaWiRQReWUieYeU Sη(NRQ-PaUaPeWULc) z4z3z2z1
d(])
JeRSaUd\ QXeVWiRQGeQeUaWiRQ:AQVZeU QXeU\
Figure 1: Overview of our approach. We combine a pre-trained retriever (Query Encoder+ Document
Index) with a pre-trained seq2seq model (Generator) and ﬁne-tune end-to-end. For queryx, we use
Maximum Inner Product Search (MIPS) to ﬁnd the top-K documentszi. For ﬁnal predictiony, we
treat z as a latent variable and marginalize over seq2seq predictions given different documents.
but have only explored open-domain extractive question answering. Here, we bring hybrid parametric
and non-parametric memory to the “workhorse of NLP,” i.e. sequence-to-sequence (seq2seq) models.
We endow pre-trained, parametric-memory generation models with a non-parametric memory through
a general-purpose ﬁne-tuning approach which we refer to as retrieval-augmented generation (RAG).
We build RAG models where the parametric memory is a pre-trained seq2seq transformer, and the
non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural
retriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The
retriever (Dense Passage Retriever [26], henceforth DPR) provides latent documents conditioned on
the input, and the seq2seq model (BART [32]) then conditions on these latent documents together with
the input to generate the output. We marginalize the latent documents with a top-K approximation,
either on a per-output basis (assuming the same document is responsible for all tokens) or a per-token
basis (where different documents are responsible for different tokens). Like T5 [51] or BART, RAG
can be ﬁne-tuned on any seq2seq task, whereby both the generator and retriever are jointly learned.
There has been extensive previous work proposing architectures to enrich systems with non-parametric
memory which are trained from scratch for speciﬁc tasks, e.g. memory networks [64, 55], stack-
augmented networks [25] and memory layers [30]. In contrast, we explore a setting where both
parametric and non-parametric memory components are pre-trained and pre-loaded with extensive
knowledge. Crucially, by using pre-trained access mechanisms, the ability to access knowledge is
present without additional training.
Our results highlight the beneﬁts of combining parametric and non-parametric memory with genera-
tion forknowledge-intensive tasks—tasks that humans could not reasonably be expected to perform
without access to an external knowledge source. Our RAG models achieve state-of-the-art results
on open Natural Questions [29], WebQuestions [3] and CuratedTrec [2] and strongly outperform
recent approaches that use specialised pre-training objectives on TriviaQA [24]. Despite these being
extractive tasks, we ﬁnd that unconstrained generation outperforms previous extractive approaches.
For knowledge-intensive generation, we experiment with MS-MARCO [1] and Jeopardy question
generation, and we ﬁnd that our models generate responses that are more factual, speciﬁc, and
diverse than a BART baseline. For FEVER [56] fact veriﬁcation, we achieve results within 4.3% of
state-of-the-art pipeline models which use strong retrieval supervision. Finally, we demonstrate that
the non-parametric memory can be replaced to update the models’ knowledge as the world changes.1
2 Methods
We explore RAG models, which use the input sequencex to retrieve text documentsz and use them
as additional context when generating the target sequencey. As shown in Figure 1, our models
leverage two components: (i) a retrieverp⌘(z|x) with parameters⌘ that returns (top-K truncated)
distributions over text passages given a queryx and (ii) a generatorp✓(yi|x, z, y1:i 1) parametrized
1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform-
ers Library [66] and can be found athttps://github.com/huggingface/transformers/blob/master/
examples/rag/. An interactive demo of RAG models can be found athttps://huggingface.co/rag/
2by ✓ that generates a current token based on a context of the previousi   1 tokens y1:i 1, the original
input x and a retrieved passagez.
To train the retriever and generator end-to-end, we treat the retrieved document as a latent variable.
We propose two models that marginalize over the latent documents in different ways to produce a
distribution over generated text. In one approach,RAG-Sequence, the model uses the same document
to predict each target token. The second approach,RAG-Token, can predict each target token based
on a different document. In the following, we formally introduce both models and then describe the
p⌘ and p✓ components, as well as the training and decoding procedure.
2.1 Models
RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate
the completesequence. Technically, it treats the retrieved document as a single latent variable that
is marginalized to get the seq2seq probabilityp(y|x) via a top-K approximation. Concretely, the
top K documents are retrieved using the retriever, and the generator produces the output sequence
probability for each document, which are then marginalized,
pRAG-Sequence(y|x) ⇡
X
z2top-k(p(·| x))
p⌘(z|x)p✓(y|x, z)=
X
z2top-k(p(·| x))
p⌘(z|x)
NY
i
p✓(yi|x, z, y1:i 1)
RAG-Token Model In the RAG-Token model we can draw a different latent document for each
target token and marginalize accordingly. This allows the generator to choose content from several
documents when producing an answer. Concretely, the top K documents are retrieved using the
retriever, and then the generator produces a distribution for the next output token for each document,
before marginalizing, and repeating the process with the following output token, Formally, we deﬁne:
pRAG-Token(y|x) ⇡
NY
i
X
z2top-k(p(·| x))
p⌘(z|x)p✓(yi|x, zi,y 1:i 1)
Finally, we note that RAG can be used for sequence classiﬁcation tasks by considering the target class
as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent.
2.2 Retriever: DPR
The retrieval componentp⌘(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture:
p⌘(z|x) / exp
 
d(z)>q(x)
 
d(z)= BERTd(z), q(x)= BERTq(x)
where d(z) is a dense representation of a document produced by a BERTBASE document encoder[8],
and q(x) a query representation produced by aquery encoder, also based on BERTBASE. Calculating
top-k(p⌘(·| x)), the list ofk documents z with highest prior probabilityp⌘(z|x), is a Maximum Inner
Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use
a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This
retriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and
Natural Questions [29]. We refer to the document index as thenon-parametric memory.
2.3 Generator: BART
The generator componentp✓(yi|x, z, y1:i 1) could be modelled using any encoder-decoder. We use
BART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input
x with the retrieved contentz when generating from BART, we simply concatenate them. BART was
pre-trained using a denoising objective and a variety of different noising functions. It has obtained
state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5
models [32]. We refer to the BART generator parameters✓ as theparametric memoryhenceforth.
2.4 Training
We jointly train the retriever and generator components without any direct supervision on what
document should be retrieved. Given a ﬁne-tuning training corpus of input/output pairs(xj,y j), we
3minimize the negative marginal log-likelihood of each target,P
j  log p(yj|xj) using stochastic
gradient descent with Adam [28]. Updating the document encoderBERTd during training is costly as
it requires the document index to be periodically updated as REALM does during pre-training [20].
We do not ﬁnd this step necessary for strong performance, and keep the document encoder (and
index) ﬁxed, only ﬁne-tuning the query encoder BERTq and the BART generator.
2.5 Decoding
At test time, RAG-Sequence and RAG-Token require different ways to approximatearg maxy p(y|x).
RAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera-
tor with transition probability:p0
✓(yi|x, y1:i 1)= P
z2top-k(p(·| x)) p⌘(zi|x)p✓(yi|x, zi,y 1:i 1) To
decode, we can plugp0
✓(yi|x, y1:i 1) into a standard beam decoder.
RAG-Sequence For RAG-Sequence, the likelihoodp(y|x) does not break into a conventional per-
token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for
each documentz, scoring each hypothesis usingp✓(yi|x, z, y1:i 1). This yields a set of hypotheses
Y , some of which may not have appeared in the beams of all documents. To estimate the probability
of an hypothesisy we run an additional forward pass for each documentz for whichy does not
appear in the beam, multiply generator probability withp⌘(z|x) and then sum the probabilities across
beams for the marginals. We refer to this decoding procedure as “Thorough Decoding.” For longer
output sequences,|Y | can become large, requiring many forward passes. For more efﬁcient decoding,
we can make a further approximation thatp✓(y|x, zi) ⇡ 0 where y was not generated during beam
search fromx, zi. This avoids the need to run additional forward passes once the candidate setY has
been generated. We refer to this decoding procedure as “Fast Decoding.”
3 Experiments
We experiment with RAG in a wide range of knowledge-intensive tasks. For all experiments, we use
a single Wikipedia dump for our non-parametric knowledge source. Following Lee et al.[31] and
Karpukhin et al.[26], we use the December 2018 dump. Each Wikipedia article is split into disjoint
100-word chunks, to make a total of 21M documents. We use the document encoder to compute an
embedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical
Navigable Small World approximation for fast retrieval [37]. During training, we retrieve the top
k documents for each query. We considerk 2{ 5, 10} for training and setk for test time using dev
data. We now discuss experimental details for each task.
3.1 Open-domain Question Answering
Open-domain question answering (QA) is an important real-world application and common testbed
for knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs(x, y)
and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to
the popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved
documents, relying primarily on non-parametric knowledge. We also compare to “Closed-Book
QA” approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead
relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural
Questions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As
CT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG
model. We use the same train/dev/test splits as prior work [31, 26] and report Exact Match (EM)
scores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set.
3.2 Abstractive Question Answering
RAG models can go beyond simple extractive QA and answer questions with free-form, abstractive
text generation. To test RAG’s natural language generation (NLG) in a knowledge-intensive setting,
we use the MSMARCO NLG task v2.1 [43]. The task consists of questions, ten gold passages
retrieved from a search engine for each question, and a full sentence answer annotated from the
retrieved passages. We do not use the supplied passages, only the questions and answers, to treat
4MSMARCO as an open-domain abstractive QA task. MSMARCO has some questions that cannot be
answered in a way that matches the reference answer without access to the gold passages, such as
“What is the weather in V olcano, CA?” so performance will be lower without using gold passages.
We also note that some MSMARCO questions cannot be answered using Wikipedia alone. Here,
RAG can rely on parametric knowledge to generate reasonable responses.
3.3 Jeopardy Question Generation
To evaluate RAG’s generation abilities in a non-QA setting, we study open-domain question gen-
eration. Rather than use questions from standard open-domain QA tasks, which typically consist
of short, simple questions, we propose the more demanding task of generating Jeopardy questions.
Jeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity.
For example, “The World Cup” is the answer to the question “In 1986 Mexico scored as the ﬁrst
country to host this international sports competition twice.” As Jeopardy questions are precise,
factual statements, generating Jeopardy questions conditioned on their answer entities constitutes a
challenging knowledge-intensive generation task.
We use the splits from SearchQA [10], with 100K train, 14K dev, and 27K test examples. As
this is a new task, we train a BART model for comparison. Following [67], we evaluate using the
SQuAD-tuned Q-BLEU-1 metric [42]. Q-BLEU is a variant of BLEU with a higher weight for
matching entities and has higher correlation with human judgment for question generation than
standard metrics. We also perform two human evaluations, one to assess generation factuality, and
one for speciﬁcity. We deﬁne factuality as whether a statement can be corroborated by trusted external
sources, and speciﬁcity as high mutual dependence between the input and output [33]. We follow
best practice and use pairwise comparative evaluation [34]. Evaluators are shown an answer and two
generated questions, one from BART and one from RAG. They are then asked to pick one of four
options—quuestion A is better, question B is better, both are good, or neither is good.
3.4 Fact Veriﬁcation
FEVER [56] requires classifying whether a natural language claim is supported or refuted by
Wikipedia, or whether there is not enough information to decide. The task requires retrieving
evidence from Wikipedia relating to the claim and then reasoning over this evidence to classify
whether the claim is true, false, or unveriﬁable from Wikipedia alone. FEVER is a retrieval problem
coupled with an challenging entailment reasoning task. It also provides an appropriate testbed for
exploring the RAG models’ ability to handle classiﬁcation rather than generation. We map FEVER
class labels (supports, refutes, or not enough info) to single output tokens and directly train with
claim-class pairs. Crucially, unlike most other approaches to FEVER, we do not use supervision on
retrieved evidence. In many real-world applications, retrieval supervision signals aren’t available, and
models that do not require such supervision will be applicable to a wider range of tasks. We explore
two variants: the standard 3-way classiﬁcation task (supports/refutes/not enough info) and the 2-way
(supports/refutes) task studied in Thorne and Vlachos [57]. In both cases we report label accuracy.
4 Results
4.1 Open-domain Question Answering
Table 1 shows results for RAG along with state-of-the-art models. On all four open-domain QA
tasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). RAG combines
the generation ﬂexibility of the “closed-book” (parametric only) approaches and the performance of
""open-book"" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results
without expensive, specialized “salient span masking” pre-training [20]. It is worth noting that RAG’s
retriever is initialized using DPR’s retriever, which uses retrieval supervision on Natural Questions
and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based “cross-
encoder” to re-rank documents, along with an extractive reader. RAG demonstrates that neither a
re-ranker nor extractive reader is necessary for state-of-the-art performance.
There are several advantages to generating answers even when it is possible to extract them. Docu-
ments with clues about the answer but do not contain the answer verbatim can still contribute towards
a correct answer being generated, which is not possible with standard extractive approaches, leading
5Table 1: Open-Domain QA Test Scores. For TQA,
left column uses the standard test set for Open-
Domain QA, right column uses the TQA-Wiki
test set. See Appendix D for further details.
Model NQ TQA WQ CT
Closed
Book
T5-11B [52] 34.5 - /50.1 37.4 -
T5-11B+SSM[52] 36.6 - /60.5 44.7 -
Open
Book
REALM [20] 40.4 - / - 40.7 46.8
DPR [26] 41.5 57.9/ - 41.1 50.6
RAG-Token 44.1 55.2/66.1 45.5 50.0
RAG-Seq. 44.5 56.8/68.0 45.2 52.2
Table 2: Generation and classiﬁcation Test Scores.
MS-MARCO SotA is [4], FEVER-3 is [68] and
FEVER-2 is [57] *Uses gold context/evidence.
Best model without gold access underlined.
Model Jeopardy MSMARCO FVR3 FVR2
B-1 QB-1 R-L B-1 Label Acc.
SotA - - 49.8* 49.9* 76.8 92.2 *
BART 15.1 19.7 38.2 41.6 64.0 81.1
RAG-Tok. 17.3 22.2 40.1 41.5 72.5 89.5RAG-Seq. 14.7 21.4 40.8 44.2
to more effective marginalization over documents. Furthermore, RAG can generate correct answers
even when the correct answer is not in any retrieved document, achieving 11.8% accuracy in such
cases for NQ, where an extractive model would score 0%.
4.2 Abstractive Question Answering
As shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu
points and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is
impressive given that (i) those models access gold passages with speciﬁc information required to
generate the reference answer, (ii) many questions are unanswerable without the gold passages, and
(iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers
from our models. Qualitatively, we ﬁnd that RAG models hallucinate less and generate factually
correct text more often than BART. Later, we also show that RAG generations are more diverse than
BART generations (see §4.5).
4.3 Jeopardy Question Generation
Table 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation,
with both models outperforming BART on Q-BLEU-1. Table 4 shows human evaluation results, over
452 pairs of generations from BART and RAG-Token. Evaluators indicated that BART was more
factual than RAG in only 7.1% of cases, while RAG was more factual in 42.7% of cases, and both
RAG and BART were factual in a further 17% of cases, clearly demonstrating the effectiveness of
RAG on the task over a state-of-the-art generation model. Evaluators also ﬁnd RAG generations to
be more speciﬁc by a large margin. Table 3 shows typical generations from each model.
Jeopardy questions often contain two separate pieces of information, and RAG-Token may perform
best because it can generate responses that combine content from several documents. Figure 2 shows
an example. When generating “Sun”, the posterior is high for document 2 which mentions “The
Sun Also Rises”. Similarly, document 1 dominates the posterior when “A Farewell to Arms” is
generated. Intriguingly, after the ﬁrst token of each book is generated, the document posterior ﬂattens.
This observation suggests that the generator can complete the"
"Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics
System Demonstrations, pages 150–158
March 17-22, 2024c⃝2024 Association for Computational Linguistics
RAGA S: Automated Evaluation of Retrieval Augmented Generation
Shahul Es†, Jithin James†, Luis Espinosa-Anke∗♢, Steven Schockaert∗
†Exploding Gradients
∗CardiffNLP, Cardiff University, United Kingdom
♢AMPLYFI, United Kingdom
{shahules786,jamesjithin97}@gmail.com
{espinosa-ankel,schockaerts1}@cardiff.ac.uk
Abstract
We introduce RAGA S1 (Retrieval Augmented
Generation Assessment), a framework for
reference-free evaluation of Retrieval Aug-
mented Generation (RAG) pipelines. RAG sys-
tems are composed of a retrieval module and an
LLM based generation module. They provide
LLMs with knowledge from a reference corpus,
which can help to keep LLM based systems
up-to-date and can reduce the risk of halluci-
nations, among others. However, evaluating
RAG architectures is challenging because there
are several dimensions to consider: the abil-
ity of the retrieval system to identify relevant
and focused context passages, the ability of the
LLM to exploit such passages in a faithful way,
and the quality of the generation itself. With
RAGA S, we put forward a suite of metrics
which can be used to evaluate these different
dimensions without having to rely on ground
truth human annotations. We posit that such
a framework can crucially contribute to faster
evaluation cycles of RAG architectures, which
is especially important given the fast adoption
of LLMs.
1 Introduction
Language Models (LMs) capture a vast amount
of knowledge about the world, which allows them
to answer questions without accessing any exter-
nal sources. This idea of LMs as repositories of
knowledge emerged shortly after the introduction
of BERT (Devlin et al., 2019) and became more
firmly established with the introduction of ever
larger LMs (Roberts et al., 2020). While the most
recent Large Language Models (LLMs) capture
enough knowledge to rival human performance
across a wide variety of question answering bench-
marks (Bubeck et al., 2023), the idea of using
LLMs as knowledge bases still has two fundamen-
tal limitations. First, LLMs are not able to answer
1RAGA S is available at https://github.com/
explodinggradients/ragas.
questions about events that have happened after
they were trained. Second, even the largest models
struggle to memorise knowledge that is only rarely
mentioned in the training corpus (Kandpal et al.,
2022; Mallen et al., 2023). The standard solution
to these issues is to rely on Retrieval Augmented
Generation (RAG) (Lee et al., 2019; Lewis et al.,
2020; Guu et al., 2020). Answering a question
then essentially involves retrieving relevant pas-
sages from a corpus and feeding these passages,
along with the original question, to the LM. While
initial approaches relied on specialised LMs for
retrieval-augmented language modelling (Khandel-
wal et al., 2020; Borgeaud et al., 2022), recent work
has suggested that simply adding retrieved docu-
ments to the input of a standard LM can also work
well (Khattab et al., 2022; Ram et al., 2023; Shi
et al., 2023), thus making it possible to use retrieval-
augmented strategies in combination with LLMs
that are only available through APIs.
While the usefulness of retrieval-augmented
strategies is clear, their implementation requires
a significant amount of tuning, as the overall per-
formance will be affected by the retrieval model,
the considered corpus, the LM and the prompt for-
mulation, among others. Automated evaluation of
retrieval-augmented systems is thus paramount. In
practice, RAG systems are often evaluated in terms
of the language modelling task itself, i.e. by mea-
suring perplexity on some reference corpus. How-
ever, such evaluations are not always predictive
of downstream performance (Wang et al., 2023c).
Moreover, this evaluation strategy relies on the LM
probabilities, which are not accessible for some
closed models (e.g. ChatGPT and GPT-4). Ques-
tion answering is another common evaluation task,
but usually only datasets with short extractive an-
swers are considered, which may not be represen-
tative of how the system will be used.
To address these issues, in this paper we present
RAGA S, a framework for the automated assess-
150ment of retrieval augmented generation systems.
We focus on settings where reference answers may
not be available, and where we want to estimate
different proxies for correctness, in addition to the
usefulness of the retrieved passages. The RAGA S
framework provides an integration with both llama-
index and Langchain, the most widely used frame-
works for building RAG solutions, thus enabling
developers to easily integrate RAGA S into their
standard workflow.
2 Related Work
Estimating faithfulness using LLMsThe prob-
lem of detecting hallucinations in LLM-generated
responses has been extensively studied (Ji et al.,
2023). Some authors have suggested the idea of
predicting factuality using a few-shot prompting
strategy (Zhang et al., 2023). Recent analyses, how-
ever, suggest that existing models struggle with de-
tecting hallucination when using standard prompt-
ing strategies (Li et al., 2023; Azaria and Mitchell,
2023). Other approaches rely on linking the gener-
ated responses to facts from an external knowledge
base (Min et al., 2023), but this is not always possi-
ble.
Yet another strategy is to inspect the probabili-
ties assigned to individual tokens, where we would
expect the model to be less confident in halluci-
nated answers than in factual ones. For instance,
BARTScore (Yuan et al., 2021) estimates factuality
by looking at the conditional probability of the gen-
erated text given the input. Kadavath et al. (2022)
use a variation of this idea. Starting from the ob-
servation that LLMs provide well-calibrated proba-
bilities when answering multiple-choice questions,
they essentially convert the problem of validating
model generated answers into a multiple-choice
question which asks whether the answer is true or
false. Rather than looking at the output probabil-
ities, Azaria and Mitchell (2023) propose to train
a supervised classifier on the weights from one of
the hidden layers of the LLM, to predict whether a
given statement is true or not. While the approach
performs well, the need to access the hidden states
of the model makes it unsuitable for systems that
access LLMs through an API.
For models that do not provide access to token
probabilities, such as ChatGPT and GPT-4, differ-
ent methods are needed. SelfCheckGPT (Manakul
et al., 2023) addresses this problem by instead sam-
pling multiple answers. Their core idea is that
factual answers are more stable: when an answer is
factual, we can expect that different samples will
tend to be semantically similar, whereas this is less
likely to be the case for hallucinated answers.
Automated evaluation of text generation systems
LLMs have also been leveraged to automatically
evaluate other aspects of generated text fragments,
beyond factuality. For instance, GPTScore (Fu
et al., 2023) uses a prompt that specifies the consid-
ered aspect (e.g. fluency) and then scores passages
based on the average probability of the generated
tokens, according to a given autoregressive LM.
This idea of using prompts was previously also
considered by Yuan et al. (2021), although they
used a smaller fine-tuned LM (i.e. BART) and did
not observe a clear benefit from using prompts. An-
other approach directly asks ChatGPT to evaluate
a particular aspect of the given answer by provid-
ing a score between 0 and 100, or by providing a
rating on a 5-star scale (Wang et al., 2023a). Re-
markably, strong results can be obtained in this
way, although it comes with the limitation of being
sensitive to the design of the prompt. Rather than
scoring individual answers, some authors have also
focused on using an LLM to select the best answer
among a number of candidates (Wang et al., 2023b),
typically to compare the performance of different
LLMs. However, care is needed with this approach,
as the order in which the answers are presented can
influence the result (Wang et al., 2023b).
More generally, however, most approaches have
relied on the availability of one or more refer-
ence answers for evaluating text generation sys-
tems. For instance, BERTScore (Zhang et al., 2020)
and MoverScore (Zhao et al., 2019) use contex-
tualised embeddings, produced by a pre-trained
BERT model, to compare the similarity between
the generated answer and the reference answers.
BARTScore (Yuan et al., 2021) similarly uses refer-
ence answers to compute aspects such as precision
(estimated as the probability of generating the gen-
erated answer given the reference) and recall (esti-
mated as the probability of generating the reference
given the generated answer).
3 Evaluation Strategies
We consider a standard RAG setting, where given a
question q, the system first retrieves some context
c(q) and then uses the retrieved context to generate
an answer as(q). When building a RAG system,
we usually do not have access to human-annotated
151datasets or reference answers. We therefore fo-
cus on metrics that are fully self-contained and
reference-free. We focus in particular three quality
aspects, which we argue are of central importance.
First, Faithfulness refers to the idea that the
answer should be grounded in the given context.
This is important to avoid hallucinations, and to
ensure that the retrieved context can act as a jus-
tification for the generated answer. Indeed, RAG
systems are often used in applications where the
factual consistency of the generated text w.r.t. the
grounded sources is highly important, e.g. in do-
mains such as law, where information is constantly
evolving. Second, Answer Relevancerefers to the
idea that the generated answer should address the
actual question that was provided. Finally, Con-
text Relevancerefers to the idea that the retrieved
context should be focused, containing as little ir-
relevant information as possible. This is important
given the cost associated with feeding long context
passages to LLMs. Moreover, when context pas-
sages are too long, LLMs are often less effective
in exploiting that context, especially for informa-
tion that is provided in the middle of the context
passage (Liu et al., 2023).
We now explain how these three quality aspects
can be measured in a fully automated way, by
prompting an LLM. In our implementation and
experiments, all prompts are evaluated using the
gpt-3.5-turbo-16k model, which is available
through the OpenAI API2.
Faithfulness We say that the answer as(q) is
faithful to the context c(q) if the claims that are
made in the answer can be inferred from the con-
text. To estimate faithfulness, we first use an LLM
to extract a set of statements, S(as(q)). The aim
of this step is to decompose longer sentences into
shorter and more focused assertions. We use the
following prompt for this step3:
Given a question and answer, create one
or more statements from each sentence
in the given answer.
question: [question]
answer: [answer]
where [question] and [answer] refer to the
given question and answer. For each statementsi in
2https://platform.openai.com
3To help clarify the task, we include a demonstration as
part of the prompt. This demonstration is not explicitly shown
in the listing of the prompts throughout this paper.
S(as(q)), the LLM determines if si can be inferred
from c(q) using a verification function v(si, c(q)).
This verification step is carried out using the fol-
lowing prompt:
Consider the given context and following
statements, then determine whether they
are supported by the information present
in the context. Provide a brief explana-
tion for each statement before arriving
at the verdict (Yes/No). Provide a final
verdict for each statement in order at the
end in the given format. Do not deviate
from the specified format.
statement: [statement 1]
...
statement: [statement n]
The final faithfulness score, F, is then computed
as F = |V |
|S|, where |V | is the number of statements
that were supported according to the LLM and |S|
is the total number of statements in S(as(q)).
Answer relevance We say that the answer as(q)
is relevant if it directly addresses the question in
an appropriate way. In particular, our assessment
of answer relevance does not take into account fac-
tuality, but penalises cases where the answer is
incomplete or where it contains redundant informa-
tion. To estimate answer relevance, for the given
answer as(q), we prompt the LLM to generate n
potential questions qi based on as(q), as follows:
Generate a question for the given an-
swer.
answer: [answer]
We then obtain embeddings for all questions us-
ing the text-embedding-ada-002 model, avail-
able from the OpenAI API. For each qi, we cal-
culate the similarity sim(q, qi) with the original
question q, as the cosine between the correspond-
ing embeddings. The answer relevance score, AR,
for question q is then computed as:
AR = 1
n
n∑
i=1
sim(q, qi) (1)
This metric evaluates how closely the generated
answer aligns with the initial question or instruc-
tion.
152Context relevance The context c(q) is consid-
ered relevant to the extent that it exclusively con-
tains information that is needed to answer the ques-
tion. In particular, this metric aims to penalise the
inclusion of redundant information. To estimate
context relevance, given a question q and its con-
text c(q), the LLM extracts a subset of sentences,
Sext, from c(q) that are crucial to answer q, using
the following prompt:
Please extract relevant sentences from
the provided context that can potentially
help answer the following question. If no
relevant sentences are found, or if you
believe the question cannot be answered
from the given context, return the phrase
""Insufficient Information"". While extract-
ing candidate sentences you’re not al-
lowed to make any changes to sentences
from given context.
The context relevance score is then computed as:
CR = number of extracted sentences
total number of sentences in c(q) (2)
4 The WikiEval Dataset
To evaluate the proposed framework, we ideally
need examples of question-context-answer triples
which are annotated with human judgments. We
can then verify to what extent our metrics agree
with human assessments of faithfulness, answer
relevance and context relevance. Since we are not
aware of any publicly available datasets that could
be used for this purpose, we created a new dataset,
which we refer to as WikiEval4. To construct the
dataset, we first selected 50 Wikipedia pages cov-
ering events that have happened since the start of
20225. In selecting these pages, we prioritised
those with recent edits. For each of the 50 pages,
we then asked ChatGPT to suggest a question that
can be answered based on the introductory section
of the page, using the following prompt:
Your task is to formulate a question from
given context satisfying the rules given
below:
1. The question should be fully answered
from the given context.
4https://huggingface.co/datasets/
explodinggradients/WikiEval.
5That is, beyond the reported training cutoff of the model
we used in our experiments.
2. The question should be framed from
a part that contains non-trivial informa-
tion.
3. The answer should not contain any
links.
4. The question should be of moderate
difficulty.
5. The question must be reasonable and
must be understood and responded to by
humans.
6. Do not use phrases that ‘provided con-
text’, etc in the question
context:
We also used ChatGPT to answer the generated
question, when given the corresponding introduc-
tory section as context, using the following prompt:
Answer the question using the informa-
tion from the given context.
question: [question]
context: [context]
All questions were annotated along the three con-
sidered quality dimensions by two annotators. Both
annotators were fluent in English and were given
clear instructions about the meaning of the three
considered quality dimensions. For faithfulness
and context relevance, the two annotators agreed in
around 95% of cases. For answer relevance, they
agreed in around 90% of the cases. Disagreements
were resolved after a discussion between the anno-
tators. Tables 1, 2 and 3 show examples from the
WikiEval dataset, focusing in particular on answers
with high and low faithfulness (Table 1), high and
low answer relevance (Table 2), and high and low
context relevance (Table 3).
Faithfulness To obtain human judgements about
faithfulness, we first used ChatGPT to answer the
question without access to any additional context.
We then asked the annotators to judge which of the
two answers was the most faithful (i.e. the standard
one or the one generated without context), given
the question and corresponding Wikipedia page.
Answer relevance We first used ChatGPT to
obtain candidate answers with lower answer rel-
evance, using the following prompt:
Answer the given question in an incom-
plete manner.
question: [question]
153Question Context Answer
Who directed the film Op-
penheimer and who stars
as J. Robert Oppenheimer
in the film?
Oppenheimer is a 2023 biographical thriller film written
and directed by Christopher Nolan. Based on the 2005
biography American Prometheus by Kai Bird and Mar-
tin J. Sherwin, the film chronicles the life of J. Robert
Oppenheimer, a theoretical physicist who was pivotal in
developing the first nuclear weapons as part of the Man-
hattan Project, and thereby ushering in the Atomic Age.
Cillian Murphy stars as Oppenheimer, with Emily Blunt
as Oppenheimer’s wife Katherine ""Kitty"" Oppenheimer.
High Faithfulness : Christopher
Nolan directed the film Oppen-
heimer. Cillian Murphy stars as J.
Robert Oppenheimer in the film.
Low Faithfulness : James
Cameron directed the film Op-
penheimer. Tom Cruise stars as J.
Robert Oppenheimer in the film.
Table 1: Example from WikiEval, showing answers with high and low faithfulness.
Question Answer
When is the scheduled
launch date and time for
the PSLV-C56 mission,
and where will it be
launched from?
High answer relevance: The PSLV-C56 mission is scheduled to be launched on Sunday, 30
July 2023 at 06:30 IST / 01:00 UTC. It will be launched from the Satish Dhawan Space
Centre, Sriharikota, Andhra Pradesh, India.
Low answer relevance: The scheduled launch date and time for the PSLV-C56 mission have
not been provided.The PSLV-C56 mission is an important space mission for India. It aims to
launch a satellite into orbit to study weather patterns.
Table 2: Example from WikiEval, showing answers with high and low answer relevance.
We then asked human annotators to compare this
answer, and indicate which of the two answers had
the highest answer relevance.
Context relevance To measure this aspect, we
first added additional sentences to the context by
scraping back-links to the corresponding Wikipedia
page. In this way, we were able to add information
to the context that was related but less relevant for
answering the question. For the few pages with-
out any back-links, we instead used ChatGPT to
complete the given context.
5 Experiments
Table 4 analyses the agreement between the met-
rics proposed in Section 3 and the human assess-
ments from the proposed WikiEval dataset. Each
WikiEval instance requires the model to compare
two answers or two context fragments. We count
how often the answer/context preferred by the
model (i.e. with highest estimated faithfulness, an-
swer relevance, or context relevance) coincides
with the answer/context preferred by the human
annotators. We report the results in terms of ac-
curacy (i.e. the fraction of instances on which the
model agrees with the annotators).
To put the results in context, we compare our
proposed metrics (shown as RAGA S in Table 4)
with two baseline methods. For the first method,
shown as GPT Score, we ask ChatGPT to assign a
score between 0 and 10 for the three quality dimen-
sions. To this end, we use a prompt that describes
the meaning of the quality metric and then asks
to score the given answer/context in line with that
definition. For instance, for evaluating faithfulness,
we used the following prompt:
Faithfulness measures the information
consistency of the answer against the
given context. Any claims that are made
in the answer that cannot be deduced
from context should be penalized.
Given an answer and context, assign a
score for faithfulness in the range 0-10.
context: [context]
answer: [answer]
Ties, where the same score is assigned by the LLM
to both answer candidates, were broken randomly.
The second baseline, shown as GPT Ranking, in-
stead asks ChatGPT to select the preferred answer/-
context. In this case, the prompt again includes
a definition of the considered quality metric. For
instance, to evaluate answer relevance, we used the
following prompt:
Answer Relevancy measures the degree
to which a response directly addresses
and is appropriate for a given question.
It penalizes the present of redundant in-
formation or incomplete answers given a
question. Given an question and answer,
rank each answer based on Answer Rele-
vancy.
question: [question]
154Question Context
When was the Chimnabai
Clock Tower completed,
and who was it named af-
ter?
High context relevance: The Chimnabai Clock Tower, also known as the Raopura Tower, is
a clock tower situated in the Raopura area of Vadodara, Gujarat, India. It was completed
in 1896 and named in memory of Chimnabai I (1864–1885), a queen and the first wife of
Sayajirao Gaekwad III of Baroda State.
Low context relevance: The Chimnabai Clock Tower, also known as the Raopura Tower, is
a clock tower situated in the Raopura area of Vadodara, Gujarat, India. It was completed
in 1896 and named in memory of Chimnabai I (1864–1885), a queen and the first wife of
Sayajirao Gaekwad III of Baroda State. It was built in Indo-Saracenic architecture style.
History. Chimnabai Clock Tower was built in 1896. The tower was named after Chimnabai
I (1864–1885), a queen and the first wife of Sayajirao Gaekwad III of Baroda State. It was
inaugurated by Mir Kamaluddin Hussainkhan, the last Nawab of Baroda. During the rule of
Gaekwad, it was a stoppage for horse drawn trams. The clock tower was erected at the cost
of 25,000 (equivalent to 9.2 million or USD 120,000 in 2023).
Table 3: Example from WikiEval, showing answers with high and low context relevance.
Faith. Ans. Rel. Cont. Rel.
RAGA S 0.95 0.78 0.70
GPT Score 0.72 0.52 0.63
GPT Ranking 0.54 0.40 0.52
Table 4: Agreement with human annotators in pairwise
comparisons of faithfulness, answer relevance and con-
text relevance, using the WikEval dataset (accuracy).
answer 1: [answer 1]
answer 2: [answer 2]
The results in Table 4 show that our proposed
metrics are much closer aligned with the human
judgements than the predictions from the two base-
lines. For faithfulness, the RAGA S prediction are
in general highly accurate. For answer relevance,
the agreement is lower, but this is largely due to the
fact that the differences between the two candidate
answers are often very subtle. We found context
relevance to be the hardest quality dimension to
evaluate. In particular, we observed that ChatGPT
often struggles with the task of selecting the sen-
tences from the context that are crucial, especially
for longer contexts.
5.1 Reproducibility
Obtaining reproducible results with (large) lan-
guage models is challenging. For this reason, reli-
able software that uses prompts should account not
only for hallucinations, but for the fact that several
runs of the same experiment under the same config-
uration might yield different results, e.g., because
of an undocumented change in the underlying API,
or because of the inherent randomness in neural
networks. Furthermore, we require the LLM to gen-
erate outputs in structured JSON format. We found
that this largely makes RAGA S compatible with
different LLMs, and ultimately lowers the error rate
when consuming LLM generated text. To measure
the effectiveness of the JSON-formatting, we mea-
sured the correlation between RAGA S scores in
successive runs with and without JSON formatting.
As shown in Figure 1, the scores are clearly more
consistent when JSON formatted outputs are used.
Figure 1: We compare the consistency of RAGA S
scores across two different runs of the model, with
JSON formatting (left) and without (right). The use
of JSON formatting leads to more consistent scores.
6 Python API
RAGA S provides access to metrics and datasets via
an easy-to-use Python API. Its syntax is similar to
other well-known libraries such as transformers
or datasets. As an example, once installed, load-
ing a dataset, evaluating a pipeline with the de-
sired metrics, and exporting the results to a pandas
dataframe can be accomplished with the snippet be-
low. The metrics available at ragas.metrics use
OpenAI’s API by default, which requires having
the appropriate environment variables set up. It is
however possible to experiment with other LLMs
155for evaluation6.
# import required modules
from ragas . metrics import (
answer_relevancy ,
faithfulness ,
context_relevancy ,
)
from ragas import evaluate
from datasets import load_dataset
# loading the eval dataset
amnesty_qa = load_dataset (
' explodinggradients / amnesty_qa ',
'english_v2 '
)
# evaluate
from ragas import evaluate
result = evaluate (
amnesty_qa ["" eval ""],
metrics =[
faithfulness ,
answer_relevancy ,
context_relevancy ,
],
)
# export results to pandas dataframe
df = result . to_pandas ()
7 Conclusions
We have highlighted the need for automated
reference-free evaluation of RAG systems. In par-
ticular, we have argued the need for an evaluation
framework that can assess faithfulness (i.e. is the
answer grounded in the retrieved context), answer
relevance (i.e. does the answer address the ques-
tion) and context relevance (i.e. is the retrieved
context sufficiently focused). To support the devel-
opment of such a framework, we have introduced
WikiEval, a dataset which human judgements of
these three different aspects. Finally, we have also
described RAGA S, our implementation of the three
considered quality aspects. This framework is easy
to use and can provide developers of RAG sys-
tems with valuable insights, even in the absence
of any ground truth. Our evaluation on WikiEval
has shown that the predictions from RAGA S are
closely aligned with human judgments, especially
for faithfulness and answer relevance.
8 Limitations
This paper introduces a toolkit aimed at provid-
ing an end-to-end evaluation framework for RAG
6See https://github.com/explodinggradients/
ragas/blob/main/docs/howtos/customisations/llms.
ipynb.
systems. It relies heavily on the performance of
the LLMs used for evaluating the different compo-
nents. While the current set of experiments demon-
strate high correlation of these metrics with human
judgements, we acknowledge that relying on LLMs
comes with known limitations. Therefore, careful
reviewing of the suitability of LLMs (ideally pri-
oritizing open models over full-fledged products
behind paid APIs) is critical for RAGA S, and other
contributions in this space, to nurture a healthy
environment. We are also aware of the potential
implications of enabling large user bases into using
more accurate RAG systems, and therefore we will
continue to encourage applications of fair systems
on top of RAGA S."
"Evaluating Retrieval Quality in Retrieval-Augmented Generation
Alireza Salemi
University of Massachusetts Amherst
Amherst, MA, United States
asalemi@cs.umass.edu
Hamed Zamani
University of Massachusetts Amherst
Amherst, MA, United States
zamani@cs.umass.edu
ABSTRACT
Evaluating retrieval-augmented generation (RAG) presents chal-
lenges, particularly for retrieval models within these systems. Tra-
ditional end-to-end evaluation methods are computationally expen-
sive. Furthermore, evaluation of the retrieval model’s performance
based on query-document relevance labels shows a small correla-
tion with the RAG system’s downstream performance. We propose
a novel evaluation approach, eRAG, where each document in the
retrieval list is individually utilized by the large language model
within the RAG system. The output generated for each document is
then evaluated based on the downstream task ground truth labels.
In this manner, the downstream performance for each document
serves as its relevance label. We employ various downstream task
metrics to obtain document-level annotations and aggregate them
using set-based or ranking metrics. Extensive experiments on a
wide range of datasets demonstrate that eRAG achieves a higher
correlation with downstream RAG performance compared to base-
line methods, with improvements in Kendall’s𝜏 correlation ranging
from 0.168 to 0.494. Additionally, eRAG offers significant compu-
tational advantages, improving runtime and consuming up to 50
times less GPU memory than end-to-end evaluation.
CCS CONCEPTS
• Computing methodologies →Natural language generation;
• Information systems →Evaluation of retrieval results.
KEYWORDS
Evaluation; retrieval quality; retrieval-augmented generation
ACM Reference Format:
Alireza Salemi and Hamed Zamani. 2024. Evaluating Retrieval Quality in
Retrieval-Augmented Generation. In Proceedings of the 47th International
ACM SIGIR Conference on Research and Development in Information Retrieval
(SIGIR ’24), July 14–18, 2024, Washington, DC, USA. ACM, New York, NY,
USA, 6 pages. https://doi.org/10.1145/3626772.3657957
1 INTRODUCTION
Retrieval-augmented generation (RAG) has emerged as a prominent
approach in natural language processing, combining the strengths
of retrieval and generation models [35], with use cases in decreas-
ing hallucination [ 1, 29], knowledge-grounding [ 9, 16, 34], and
personalization [25, 26]. Evaluating RAG systems is important as
it ensures the effectiveness of integrating retrieval-based methods
This work is licensed under a Creative Commons Attribution
International 4.0 License.
SIGIR ’24, July 14–18, 2024, Washington, DC, USA
© 2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0431-4/24/07.
https://doi.org/10.1145/3626772.3657957
with generative models [10, 23]. Traditionally, RAG evaluation has
primarily relied on end-to-end assessment, which entails compar-
ing the generated output with one or more ground truth"
"Published in Transactions on Machine Learning Research (06/2023)
Augmented Language Models: a Survey
Grégoire Mialon∗ gmialon@meta.com
Roberto Dessì∗† rdessi@meta.com
Maria Lomeli∗ marialomeli@meta.com
Christoforos Nalmpantis∗ christoforos@meta.com
Ram Pasunuru∗ rpasunuru@meta.com
Roberta Raileanu∗ raileanu@meta.com
Baptiste Rozière∗ broz@meta.com
Timo Schick∗ schick@meta.com
Jane Dwivedi-Yu∗ janeyu@meta.com
Asli Celikyilmaz∗ aslic@meta.com
Edouard Grave∗ egrave@meta.com
Yann LeCun∗ yann@meta.com
Thomas Scialom∗ tscialom@meta.com
∗Meta AI †Universitat Pompeu Fabra
Reviewed on OpenReview:https://openreview.net/forum?id=jh7wH2AzKK
Abstract
This survey reviews works in which language models (LMs) are augmented with reasoning
skills and the ability to use tools. The former is defined as decomposing a potentially complex
task into simpler subtasks while the latter consists in calling external modules such as a
code interpreter. LMs can leverage these augmentations separately or in combination via
heuristics, or learn to do so from demonstrations. While adhering to a standard missing
tokens prediction objective, such augmented LMs can use various, possibly non-parametric
external modules to expand their context processing ability, thus departing from the pure
language modeling paradigm. We therefore refer to them as Augmented Language Models
(ALMs). The missing token objective allows ALMs to learn to reason, use tools, and even
act, while still performing standard natural language tasks and even outperforming most
regular LMs on several benchmarks. In this work, after reviewing current advance in ALMs,
we conclude that this new research direction has the potential to address common limitations
of traditional LMs such as interpretability, consistency, and scalability issues.
1 Introduction: motivation for the survey and definitions
1.1 Motivation and Definitions
Large Language Models (LLMs) (Devlin et al., 2019; Brown et al., 2020; Chowdhery et al., 2022) have fueled
dramatic progress in Natural Language Processing (NLP) and are already core in several products with
millions of users, such as the coding assistant Copilot (Chen et al., 2021), Google search engine1 or more
recently ChatGPT2. Memorization (Tirumala et al., 2022) combined with compositionality (Zhou et al., 2022)
capabilities made LLMs able to execute various tasks such as language understanding or conditional and
1See e.g. https://blog.google/products/search/search-language-understanding-bert/
2https://openai.com/blog/chatgpt/
1Published in Transactions on Machine Learning Research (06/2023)
unconditional text generation at an unprecedented level of performance, thus opening a realistic path towards
higher-bandwidth human-computer interactions.
However, LLMs suffer from important limitations hindering a broader deployment. LLMs often provide
non-factual but seemingly plausible predictions, often referred to as hallucinations (Welleck et al., 2020).
This leads to many avoidable mistakes, for example in the context of arithmetics (Qian et al., 2022) or within
a reasoning chain (Wei et al., 2022c). Moreover, many LLMs groundbreaking capabilities seem to emerge
with size, measured by the number of trainable parameters: for example, Wei et al. (2022b) demonstrate
that LLMs become able to perform some BIG-bench tasks3 via few-shot prompting once a certain scale is
attained. Although a recent line of work yielded smaller LMs that retain some capabilities from their largest
counterpart (Hoffmann et al., 2022), the size and need for data of LLMs can be impractical for training but
also maintenance: continual learning for large models remains an open research question (Scialom et al.,
2022). Other limitations of LLMs are discussed by Goldberg (2023) in the context ofChatGPT, a chatbot
built uponGPT3.
We argue these issues stem from a fundamental defect of LLMs: they are generally trained to perform
statistical language modeling given (i) a single parametric model and (ii) a limited context, typically then
previous or surrounding tokens. Whilen has been growing in recent years thanks to software and hardware
innovations, most models still use a relatively small context size compared to the potentially large context
needed to always correctly perform language modeling. Hence, massive scale is required to store knowledge
that is not present in the context but necessary to perform the task at hand.
As a consequence, a growing research trend emerged with the goal to solve these issues, slightly moving
away from the pure statistical language modeling paradigm described above. For example, a line of work
circumvents the limited context size of LLMs by increasing its relevance: this is done by adding information
extracted from relevant external documents. Through equipping LMs with a module that retrieves such
documents from a database given a context, it is possible to match certain capabilities of some of the largest
LMs while having less parameters (Borgeaud et al., 2022; Izacard et al., 2022). Note that the resulting model
is now non-parametric, since it can query external data sources. More generally, LMs can also improve their
context via reasoning strategies (Wei et al. (2022c); Taylor et al. (2022); Yang et al. (2022c)inter alia) so that
a more relevant context is produced in exchange for more computation before generating an answer. Another
strategy is to allow LMs to leverage external tools (Press et al. (2022); Gao et al. (2022); Liu et al. (2022b)
inter alia) to augment the current context with important missing information that was not contained in
the LM’s weights. Although most of these works aim to alleviate the downfalls of LMs mentioned above
separately, it is straightforward to think that more systematically augmenting LMs with both reasoning
and tools may lead to significantly more powerful agents. We will refer to these models asAugmented
Language Models (ALMs). As this trend is accelerating, keeping track and understanding the scope of
the numerous results becomes arduous. This calls for a taxonomy of ALMs works and definitions of technical
terms that are used with sometimes different intents.
Definitions. We now provide definitions for terms that will be used throughout the survey.
• Reasoning. In the context of ALMs, reasoning is decomposing a potentially complex task into
simpler subtasks the LM can solve more easily by itself or using tools. There exist various ways to
decompose into subtasks, such as recursion or iteration. In that sense, reasoning is akin to planning
as defined for example in LeCun (2022). In this survey, reasoning will very often refer to the various
strategies to improve reasoning skills in LMs, such as step-by-step reasoning using few-shot examples.
It is not yet fully understood whether the LM is really reasoning, or simply producing a larger context
that increases the likelihood of correctly predicting the missing tokens. We refer to Huang and Chang
(2022) for a discussion on this topic: although reasoning may currently be an abuse of language given
the current state of the art, the term is already in use within the community. A more pragmatic
definition of reasoning in the context of ALMs is giving more computation steps to the model before
yielding the answer to a prompt.
3https://github.com/google/BIG-bench
2Published in Transactions on Machine Learning Research (06/2023)
• Tool. For ALMs, a tool is an external module that is typically called using a rule or a special token
and whose output is included in the ALM’s context. The tool can gather external information, or
have an effect on the virtual or physical world (generally perceived by the ALM). An example of
a tool fetching external information is a document retriever, while a tool having an external effect
could be a robotic arm. A tool can be called at training or at inference time. More generally, learning
to interact with a tool may consist in learning to call its API.
• Act. For ALMs, calling a tool that modifies a state in a virtual or physical object, and observing
the result, typically by including it in the ALM’s current context. For example, some works from
the survey discuss searching the web, or robotic arm manipulation via LMs. With a slight abuse of
term, we will sometimes denote the call of a tool by an ALM as an action, even if it does not have
an external effect.
Why jointly discussing reasoning and tools?The combination of reasoning and tools within LMs
should allow solving a broad range of complex tasks without heuristics, hence with better generalization
capabilities. Typically, reasoning would foster the LM to decompose a given problem into potentially simpler
subtasks while tools would help getting each step right, for example obtaining the result from a mathematical
operation. Put it differently, reasoning is a way for LMs to combine different tools in order to solve complex
tasks, and tools are a way to not fail a reasoning with valid decomposition. Both should benefit from the
other. Moreover, reasoning and tools can be put under the same hood, as both augment the context of the
LM so that it better predicts the missing tokens, albeit in a different way.
Why jointly discussing tools and actions?Tools that gather additional information and tools that
have an effect on the virtual or physical world can be called in the same fashion by the LM. For example,
there is seemingly no difference between a LM outputting python code for solving a mathematical operation,
and a LM outputting python code to manipulate a robotic arm. A few works discussed in the survey are
already using LMs that have effects on the virtual or physical world: under this view, we can say that the
LM have the potential to act, and expect important advances in the direction of LMs as autonomous agents.
1.2 Our classification
We decompose the works included in the survey under three axes. Section 2 studies works which augment
LM’s reasoning capabilities as defined above. Section 3 focuses on works allowing LMs to interact with
external tools and act. Finally, Section 4 explores whether reasoning and tools usage are implemented via
heuristics or learned,e.g. via supervision or reinforcement. Other axes could naturally have been chosen
for this survey and are discussed in Section 5. For conciseness, the survey focuses on works that combine
reasoning or tools with LMs. However, the reader should keep in mind that many of these techniques were
originally introduced in another context than LMs, and consult the introduction and related work section of
the papers we mention if needed. Finally, although we focus on LLMs, not all works we consider employ
large models, hence we stick to LMs for correctness in the remainder of the survey.
2 Reasoning
In general, reasoning is the ability to make inferences using evidence and logic. Reasoning can be divided
into multiple types of skills such as commonsense reasoning (McCarthy et al., 1960; Levesque et al., 2012),
mathematical reasoning (Cobbe et al., 2021), symbolic reasoning (Wei et al., 2022c), etc. Often, reasoning
involves deductions from inference chains, called as multi-step reasoning. In the context of LMs, we will
use the definition of reasoning provided in Section 1. Previous work has shown that LLMs can solve simple
reasoning problems but fail at complex reasoning (Creswell et al., 2022): hence, this section focuses on various
strategies to augment LM’s reasoning skills. One of the challenges with complex reasoning problems for LMs
is to correctly obtain the solution by composing the correct answers predicted by it to the sub-problems.
For example, a LM may correctly predict the dates of birth and death of a celebrity, but may not correctly
predict the age. Press et al. (2022) call this discrepancy the compositionality gap for LMs. For the rest of
this section, we discuss the works related to three popular paradigms for eliciting reasoning in LMs. Note
3Published in Transactions on Machine Learning Research (06/2023)
that Huang and Chang (2022) propose a survey on reasoning in language models. Qiao et al. (2022) also
propose a survey on reasoning albeit with a focus on prompting. Since our present work focuses on reasoning
combined with tools, we refer the reader to Huang and Chang (2022); Qiao et al. (2022) for a more in-depth
review of works on reasoning for LLMs.
2.1 Eliciting reasoning with prompting
In recent years, prompting LMs to solve various downstream tasks has become a dominant paradigm (Brown
et al., 2020). In prompting, examples from a downstream task are transformed such that they are formulated
as a language modeling problem. Prompting typically takes one of the two forms: zero-shot, where the
model is directly prompted with a test example’s input; and few-shot, where few examples of a task are
prepended along with a test example’s input. This few-shot prompting is also known as in-context learning
or few-shot learning. As opposed to “naive” prompting that requires an input to be directly followed by
the output/answer, elicitive prompts encourage LMs to solve tasks by following intermediate steps before
predicting the output/answer. While Nye et al. (2021) provides the first example of few-shot prompting LLMs
with reasoning examples and Cobbe et al. (2021) generalizes the use of reasoning examples to non-algorithmic
tasks, Wei et al. (2022c) extensively studies how elicitive prompting enables LMs to be better reasoners in a
few-shot setting. Later, Kojima et al. (2022) showed similar ability in a zero-shot setting. We discuss them in
detail in the following paragraphs.
Few-shot setting. Wei et al. (2022c) popularized chain-of-thought (CoT), a few-shot prompting technique
for LMs. The prompt consists of examples of a task, with inputs followed by intermediate reasoning steps
leading to the final output, as depicted in Figure 1. Table 1 shows that CoT outperforms standard prompting
methods. Wei et al. (2022b) observe that the success of the few-shot strategy emerges with scale, while Tay
et al. (2022) add that without fine-tuning, successful use of CoT generally requires 100B+ parameters LMs
such asLaMDA (Thoppilan et al., 2022),PaLM (Chowdhery et al., 2022) orGPT3 (Brown et al., 2020;
Ouyang et al., 2022), before proposingUL2, a 20B open source model that can perform CoT. Using few-shot
CoT prompting,Minerva (Lewkowycz et al., 2022) achieves excellent performance on math benchmarks such
as GSM8K (Cobbe et al., 2021). Wang et al. (2022c) further improve CoT withSelf-consistency: diverse
reasoning paths are sampled from a given language model using CoT, and the most consistent answer is
selected as the final answer. Press et al. (2022) introduceSelf-ask, a prompt in the spirit of CoT. Instead of
providing the model with a continuous chain of thought as in Figure 1,Self-ask explicitly states the follow-up
question before answering it and relies on a scaffold (e.g,“Follow-up question:” or “So the final answer is:”),
so that the answers are more easily parseable. The authors demonstrate an improvement over CoT on their
introduced datasets aiming at measuring the compositionality gap. They observe that this gap does not
narrow when increasing the size of the model. Note that Press et al. (2022) focus on 2-hop questions,i.e.,
questions for which the model only needs to compose two facts to obtain the answer. Interestingly,Self-ask
can easily be augmented with a search engine (see Section 3).ReAct (Yao et al., 2022b) is another few-shot
prompting approach eliciting reasoning that can query three tools throughout the reasoning steps:search
and lookup in Wikipedia, andfinish to return the answer.ReAct will be discussed in more detail in the
next sections.
Zero-shot setting. Kojima et al. (2022) extend the idea of eliciting reasoning in LMs to zero-shot prompting.
Whereas few-shot provides examples of the task at hand, zero-shot conditions the LM on a single prompt that
is not an example. Here, Kojima et al. (2022) simply appendLet’s think step by stepto the input question
before querying the model (see Figure 2), and demonstrate that zero-shot-CoT for large LMs does well on
reasoning tasks such as GSM8K although not as much as few-shot-CoT.
2.2 Recursive prompting
Several works attempt to improve LM’s reasoning by explicitly decomposing problems into sub-problems in
order to solve the problem in a divide and conquer manner. This recursive approach slightly differs from CoT
since the latter does not explicitly formulate sub-problems, and can be especially useful for complex tasks,
given that compositional generalization can be challenging for LMs (Lake and Baroni, 2018; Keysers et al.,
4Published in Transactions on Machine Learning Research (06/2023)
Question: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls.
How many tennis balls does he have now?
Answer: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The
answer is 11.
Question: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many
apples do they have?
Answer:
<LM>
Figure 1: An example of few-shot Chain-of-Thought prompt.<LM> denotes call to the LM with the above
prompt.
Question: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many
apples do they have?
Answer: Let’s think step by step
<LM>
Figure 2: An example of zero-shot Chain-of-Thought prompt.<LM> denotes call to the LM with the above
prompt.
2019; Li et al., 2022a). Methods that employ problem decomposition can either then solve the sub-problems
independently, where these answers are aggregated to generate the final answer (Perez et al., 2020; Min et al.,
2019), or solve the sub-problems sequentially, where the solution to the next sub-problem depends on the
answer to the previous ones (Yang et al., 2022a; Zhou et al., 2022; Drozdov et al., 2022; Dua et al., 2022;
Khot et al., 2022; Wang et al., 2022a; Wu et al., 2022b; Mishra and Nouri, 2022). For instance, in the context
of math problems,Least-to-most prompting (Zhou et al., 2022) allows a language model to solve harder
problems than the demonstration examples by decomposing a complex problem into a list of sub-problems. It
first employs few-shot prompting to decompose the complex problem into sub-problems, before sequentially
solving the extracted sub-problems, using the solution to the previous sub-problems to answer the next one.
Patel et al. (2022) show that modifying existing benchmarks by letting human decompose questions into
subquestions that are relatively easier for models to solve leads to significant improvement forGPT3 and
RoBERTa-SQuADequipped with a symbolic calculator.
While many earlier works include learning to decompose through distant supervision (Perez et al., 2020;
Talmor and Berant, 2018; Min et al., 2019), like Zhou et al. (2022), many recent works employ in-context
learning to do so (Yang et al., 2022a; Khot et al., 2022; Dua et al., 2022; Kazemi et al., 2022). Among these,
there are further differences. For instance, Drozdov et al. (2022) is a follow-up work to Zhou et al. (2022),
but differs by using a series of prompts to perform recursive syntactic parses of the input rather than a linear
decomposition, and also differs by choosing the exemplars automatically through various heuristics. Dua et al.
(2022) is concurrent work with Zhou et al. (2022) but differs by interweaving the question decomposition
and answering stages, i.e., the next sub-question prediction has access to the previous questions and answers
as opposed to generating all sub-questions independently of any previous answers. Yang et al. (2022a), on
the other hand, decomposes using rule-based principles and slot-filling prompting to translate questions into
a series of SQL operations. Khot et al. (2022) also employs prompts to decompose into specific operations,
but then allows each sub-problem to be solved using a library of specialized handlers, where each is devoted
to a particular sub-task (e.g., retrieval). Finally, Kazemi et al. (2022) decompose a given problem in a
backward fashion: starting from the goal and a set of rules, the system decompose the goal into sub-goals
and recursively check whether the sub-goals can be proved. Here again, the modules are implemented by
few-shot prompting a pre-trained LM.
5Published in Transactions on Machine Learning Research (06/2023)
Model Accuracy (%)
OpenAI (text-davinci-002)[1] 15.6
OpenAI (text-davinci-002) + CoT[1] 46.9
OpenAI (text-davinci-002) + CoT + Calculator[1] 46.9
OpenAI (code-davinci-002)[1] 19.7
OpenAI (code-davinci-002) + CoT[1] 63.1
OpenAI (code-davinci-002) + CoT + Calculator[1] 65.4
GPT-3 175B + FT + CoT + Calculator[2] 34.0
GPT-3 175B + FT + CoT + Calculator + Verifier[2] 55.0
PaLM 540B[3] 17.0
PaLM 540B+CoT[3] 54.0
PaLM 540B+CoT+Calculator[3] 58.0
PAL[4] 72.0
Table 1: Evaluation of different reasoning methods on GSM8K, a popular reasoning benchmark. FT denotes
fine-tuning and CoT denotes chain-of-thought. The reported accuracies are based on [1]: (Wei et al., 2022c);
[2]: (Cobbe et al., 2021); [3]: (Chowdhery et al., 2022); and [4]: (Gao et al., 2022).
2.3 Explicitly teaching language models to reason
Despite their spectacular results, prompting approaches have some drawbacks in addition to requiring model
scale. Namely, they require to discover prompts that elicit e.g. step-by-step reasoning, manually providing
examples when it comes to few-shot for a new task. Moreover, prompting is computationally expensive in the
case of long prompts, and it is harder to benefit from a relatively large number of examples due to limited
context size of the model. Recent works suggest to circumvent these issues by training LMs to use, as humans,
a working memory when more than one step are required to solve a task correctly. Nye et al. (2021) introduce
the notion of scratchpad, allowing a LM to better perform on multi-step computation tasks such as addition
or code execution. More precisely, at training time, the LM sees input tasks such as addition along with
associated intermediate steps: the ensemble is called a scratchpad. At test time, the model is required to
predict the steps and the answer from the input task. Scratchpads differ from the above prompting strategies
in that they are fine-tuned on example tasks with associated computation steps. Note however that Nye
et al. (2021) also perform experiments in the few-shot regime. Taylor et al. (2022) use a similar approach in
the context of large LM pre-training:Galactica was trained on a corpus of scientific data including some
documents where step-by-step reasoning is wrapped with a special token<work> and </work> to mimic an
internal working memory. At inference time, the model can be asked explicitly to activate this reasoning mode
via the<work> token. Taylor et al. (2022) argue that one more problem arise when training on reasoning
examples: many intermediate reasoning steps may be missing in the training data curated from the internet,
as humans do not explicitly write all their reasoning steps. To circumvent the issue of missing steps, the
authors created datasets with detailed reasoning process. An example of prompt seen duringGalactica’s
pre-training is presented in Figure 4.
Other recent works improve the reasoning abilities of pre-trained LMs via fine-tuning. Zelikman et al. (2022)
propose a bootstrap approach to generate reasoning steps (also called rationales) for a large set of unlabeled
data and use that data to fine-tune the model. Lengerich et al. (2022) propose a self-supervised method which
extracts reasoning capabilities of a teacher model by asking the question “why” given the student’s initial
responses to various NLP problems. Next, the student model is fine-tuned by using contrastive distillation
via sampling evidence from memory. Yu et al. (2022) show that standard LM fine-tuning on reasoning tasks
lead to better reasoning skills such as textual entailment, abductive reasoning, and analogical reasoning,
compared to pre-trained models. Further, several instruction fine-tuning approaches (Ouyang et al., 2022;
Chung et al., 2022; Iyer et al., 2022; Ho et al., 2022) use chain-of-thought style prompts to achieve remarkable
improvements on popular benchmarks such as BBH (Srivastava et al., 2022) and MMLU (Hendrycks et al.,
2021). Interestingly, all these works also show that small scale instruction-finetuned models can perform
better than un-finetuned large scale models, especially in the tasks where instruction following is important.
6Published in Transactions on Machine Learning Research (06/2023)
Prompt 0
Question: It takes Amy 4 minutes to climb to the top of a slide. It takes her 1 minute to
slide down. The water slide closes in 15 minutes. How many times can she slide before it closes?
<LM>
Answer: To solve “ How many times can she slide before it closes?”, we need to first solve:
“ How long does each trip take? ”
</LM>
Prompt 1
It takes Amy 4 minutes to climb to the top of a slide. It takes her 1 minute to slide down.
The water slide closes in 15 minutes.
Subquestion 1: How long does each trip take?
<LM>
Answer 1: It takes Amy 4 minutes to climb and 1 minute to slide down. 4 + 1 = 5. So each trip
takes 5 minutes.
</LM>
Prompt 2
It takes Amy 4 minutes to climb to the top of a slide. It takes her 1 minute to slide down.
The slide closes in 15 minutes.
Subquestion 1: How long does each trip take?
Answer 1: It takes Amy 4 minutes to climb and 1 minute to slide down. 4 + 1 = 5. So each trip
takes 5 minutes.
Subquestion 2: How many times can she slide before it closes?
<LM>
Answer 2: The water slide closes in 15 minutes. Each trip takes 5 minutes. So Amy can slide 15÷ 5
= 3 times before it closes.
</LM>
Figure 3: Recursive prompting example.<LM> denotes the start of the LM’s output to the prompt, while
</LM> denotes the end. The problem is first decomposed into subproblems inPrompt 0. Then, Answer
2 to Subquestion 2and Answer 1to Subquestion 1are sequentially fed toPrompt 2and Prompt 1.
The few-shot examples for each stage’s prompt are omitted. Inspired from Figure 1 in Zhou et al. (2022).
2.4 Comparison and limitations of abstract reasoning
Overall, reasoning can be seen as decomposing a problem into a sequence of sub-problems either iteratively
or recursively.4 Exploring as many reasoning paths as possible is hard and there is no guarantee that the
intermediate steps are valid. A way to produce faithful reasoning traces is to generate pairs of questions
and their corresponding answers for each reasoning step (Creswell and Shanahan, 2022), but there is still
no guarantee of the correctness of these intermediate steps. Overall, a reasoning LM seeks to improve its
context by itself so that it has more chance to output the correct answer. To what extent LMs actually use
the stated reasoning steps to support the final prediction remains poorly understood (Yu et al., 2022).
4Here, reasoning is described as a sequential operation. However, other reasoning structures such as trees could be considered.
For example, Lample et al. (2022) leverage trees to model the different strategies leading to a proof for a given theorem. A
strategy is a set of intermediate results that must be either true or themselves proved, hence decomposed into another new
subset of intermediate results.
7Published in Transactions on Machine Learning Research (06/2023)
Question: A needle 35 mm long rests on a water surface at 20◦C. What force over and above the
needle’s weight is required to lift the needle from contact with the water surface?σ = 0.0728m.
<work>
σ= 0.0728N/m
σ= F/L
0.0728 =F/(2 ×0.035)
F = 0.0728(2 ×0.035)
calculate.py
“‘
f = 0.0728*(2*0.035)
with open(""output.txt"", ""w"") as file:
file.write(str(round(f, 5)))
”’
«run: calculate.py»
«read: output.txt»
0.0051
</work>
Answer: F = 0.0051N
Figure 4: Working memory example from Taylor et al. (2022). This prompt and its output are seen during
LM pre-training.
In many cases, some reasoning steps may suffer from avoidable mistakes that compromise the correctness of
the output. For example, mistakes on nontrivial mathematical operations in a reasoning step may lead to
the wrong final output. The same goes with known facts such as the identity of a president at a given year.
Some of the works studied above (Yao et al., 2022b; Press et al., 2022) already leverage simple external tools
such as asearch engine or acalculator to validate intermediate steps. More generally, the next section
of the survey focuses on the various tools that can be queried by LMs to increase the chance of outputting a
correct answer.
3 Using Tools and Act
A recent line of LM research allows the model to access knowledge that is not necessarily stored in its
weights, such as a given piece of factual knowledge. More precisely, tasks such as exact computation or
information retrieval for example can be offloaded to external modules such as apython interpreter or a
search engine that are queried by the model which, in that respect, use tools. Additionally, we can say the
LM performs an action when the tool has an effect on the external world. The possibility to easily include
tools and actions in the form of special tokens is a convenient feature of language modeling coupled with
transformers.
3.1 Calling another model
In many cases, the tool can simply be another neural network or the LM itself.
8Published in Transactions on Machine Learning Research (06/2023)
Iteration 0
Text: Brittney Reese (born September 9, 1986 in Gulfport, Mississippi) is an American
long jumper.
<LM>
Plan: Remove incorrect information
Edit: Brittney Reese (born September 9, 1986in Gulfport, Mississippi) is an American long jumper.
</LM>
Iteration 1
Text: Brittney Reese (born September 9, 1986) is an American long jumper.
<LM>
Plan: Add information about her career
Edit: Brittney Reese (born September 9, 1986) is an American long
jumper , who competed at the 2008 Summer Olympics, and is a 4-time World Champion .
</LM>
Iteration 2
Text: Brittney Reese (born September 9, 1986) is an American long jumper, who competed at the
2008 Summer Olympics, and is a 4-time World Champion.
<LM>
Plan: Add her birthplace
Edit: Brittney Reese (born September 9, 1986 in Inglewood, California) is an American long
jumper, who competed at the 2008 Summer Olympics, and is a 4-time World Champion.
</LM>
Figure 5: Iterative prompting example using PEER (Schick et al., 2022), a LM trained to produce a plan of
action and edit to the input text at each step. This process can be repeated until the generated text requires
no further updates.<LM> denotes the start of the LM’s output to the prompt, while</LM> denotes the
end.
Iterative LM calling. As an alternative to improving the LM’s context for better outputs after a single
inference pass, an alternative and intuitive way to get better results from LMs consists of repeatedly calling
the model to iteratively refine its output.Re3 (Yang et al., 2022c) exploits this idea to automatically generate
stories of over two thousand words. More precisely,Re3 first generates a plan, setting, and characters by
prompting GPT3 (Brown et al., 2020) with a premise. Then,Re3 iteratively injects information from both
the plan and current story state into a newGPT3 prompt to generate new story passages. This work is
improved upon in Yang et al. (2022b) with the use of a learned detailed outliner that iteratively expands the
brief initial outline to any desired level of granularity. Other approaches that teach models to iteratively
improve texts in an unsupervised fashion range from applications such as blank filling (Shen et al., 2020;
Donahue et al., 2020) to denoising a sequence of Gaussian vectors into word vectors (Li et al., 2022c).
PEER (Schick et al., 2022), for example, is a model initialized fromLM-Adapted T5(Raffel et al., 2020)
and trained on Wikipedia edits, learning both how to carry out edits and how to plan for the next steps.
Consequently, PEER is able to develop articles by repeatedly planning and editing as in Figure 5. The
iterative approach has the additional benefit of allowing a complex task like story and article generation to be
decomposed into smaller subtasks. Importantly and apart fromPEER, the works mentioned above employ
heuristics to call the LM. A future research direction may consist in allowing the LM to call itself repeatedly
until the output satisfies a certain criterion. Rather than just calling a single model repeatedly, Wu et al.
9Published in Transactions on Machine Learning Research (06/2023)
(2022a) propose an interactive interface for a pipeline allowing chaining of multiple LMs together, where the
output of one step is passed as input to the next. Such contributions allow non-AI-experts to refine solutions
to complex tasks that cannot be appropriately handled by a single LM.
Leveraging other modalities. Prompts under the form of text may not contain enough context to
correctly perform a given task. For example, a question does not call for the same answer if it is asked with a
serious or ironic tone. Including various modalities into the context would probably be useful for LMs such
as chatbots. As recently demonstrated by Hao et al. (2022) and Alayrac et al. (2022), LMs can also be used
as a general-purpose interface with models pre-trained on different modalities. For example, Hao et al. (2022)
take a number of pre-trained encoders that can process diverse modalities such as vision and language, and
connect them to a LM that serves as a universal task layer. The interface and modular encoders are jointly
pre-trained via a semi-causal language modeling objective. This approach combines the benefits of causal
and non-causal language modeling, enabling both in-context learning and open-ended generation, as well
as easy fine-tuning of the encoders. Similarly, Alayrac et al. (2022) introduceFlamingo, a family of Visual
Language Models (VLMs) that can handle any interleaved sequences of visual and textual data.Flamingo
models are trained on large-scale multimodal web corpora containing interleaved text and images, which
enables them to display in-context few-shot learning capabilities of multimodal tasks. With only a handful of
annotated examples,Flamingo can easily adapt to both generation tasks such as visual question-answering
and captioning, as well as classification tasks such as multiple-choice visual question-answering. Zeng et al.
(2022) introduce Socratic Models, a modular framework in which various models pre-trained on different
modalities can be composed zero-shot. This allows models to exchange information with each other and
acquire new multimodal capabilities without additional finetuning. Socratic Models enable new applications
such as robot perception and planning, free-form question-answering about egocentric videos, or multimodal
assistive dialogue by interfacing with external APIs and databases such as search engines. Interestingly,
other modalities such as images can be incorporated to improve reasoning capabilities of moderate size LMs
(1B) (Zhang et al., 2023), and enabling multimodal chain of thought reasoning (Lu et al., 2022a).
3.2 Information retrieval
LMs can be augmented with memory units, for example via a neural cache of recent inputs (Grave et al.,
2017; Merity et al., 2017), to improve their reasoning abilities. Alternatively, knowledge in the form of natural
language can be offloaded completely from the LM by retrieving from an external knowledge source. Memory
augmentation strategies help the language model to avoid producing non-factual and out-of-date information
as well as reducing the number of parameters required to achieve comparable performance to large LMs.
3.2.1 Retrieval-augmented language models
Dense and sparse retrievers. There exist two types of retrievers that can be used to augment a LM:
dense and sparse. Sparse retrievers work with sparse bag-of-words representations of the documents and the
queries (Robertson and Zaragoza, 2009). In contrast, dense neural retrievers use a dense query and dense
document vectors obtained from a neural network (Asai et al., 2021). Both types of retrievers assess the
relevance of a document to an information-seeking query. This can be done by (i) checking for precise term
overlap or (ii) computing the semantic similarity across related concepts. Sparse retrievers excel at the first
sub-problem, while dense retrievers can be better at the second (Luan et al., 2021).
Conditioning LMs on retrieved documents.Various works augment LMs with adense retriever
by adding the retrieved documents to the current context (Chen et al., 2017; Clark and Gardner, 2017; Lee
et al., 2019; Guu et al., 2020; Khandelwal et al., 2020; Lewis et al., 2020; Izacard and Grave, 2020; Zhong
et al., 2022; Borgeaud et al., 2022; Izacard et al., 2022; Shi et al., 2023). Even though the idea of retrieving
documents to perform question answering is not new, retrieval-augmented LMs have recently demonstrated
strong performance in other knowledge intensive tasks besides Q&A. These proposals close the performance
gap compared to larger LMs that use significantly more parameters.REALM (Guu et al., 2020) was the
first method to jointly train end-to-end a retrieval system with an encoder LM.RAG (Lewis et al., 2020)
jointly fine-tunes the retriever with a sequence-to-sequence model. Izacard and Grave (2020) introduced a
10Published in Transactions on Machine Learning Research (06/2023)
modification of the seq2seq architecture to efficiently process many retrieved documents. Borgeaud et al.
(2022) focuses on an auto-regressive LM, calledRETRO, and use a large-scale corpus indexed by frozen
BERT embeddings for the retriever module. Crucially, the authors show that at scale (retrieval corpus and
language model), this approach does not require to train and update the retriever. In particular,RETRO
obtains comparable performance toGPT3 on different downstream tasks. AlthoughRETRO was trained
with retrieval from scratch, their approach allows the integration of retrieval into existing pre-trained LMs.
Atlas (Izacard et al., 2022) jointly trains a retriever with a sequence-to-sequence model to obtain a LM with
strong few-shot learning capabilities in spite of being orders of magnitude smaller than many other large
LMs. Table 2 compares the main characteristics of the models discussed, notably how the retrieval results
are integrated into the LM’s context. In all the aforementioned cases, the query corresponds to the prompt
but this has been relaxed, see the chain-of-thought subsection below.
Model # Retrieval tokens Granularity Retriever training Retrieval integration
REALM (Guu et al., 2020) O(109) Prompt End-to-End Append to prompt
RAG (Lewis et al., 2020) O(109) Prompt Fine-tuning Cross-attention
RETRO (Borgeaud et al., 2022) O(1012) Chunk Frozen Chunked cross-attn.
Atlas (Izacard et al., 2022) O(109) Prompt Fine-tuning Cross-attention
Table 2: Comparison between database retrieval augmented languages models. Inspired by Table 3
from Borgeaud et al. (2022).
Efficient large scale retrieval.Since retrieval-augmented language models store knowledge in an external
data store, it is crucial that the information retrieval step is efficient, especially when dealing with a large
number of document/passage/sentence embeddings and/or a large number (billions) of query vectors. The
literature offers a variety of optimisations to boost performance of retrievers as well as reducing the memory
footprint of the data store.
First, an index structure can be built for the document/passage/sentence embeddings to perform efficient
similarity search, for instance, using the Facebook AI Similarity Search library (faiss) (Johnson et al., 2021).
Leveraging indexing structures enables efficient search operations by partitioning the vector space and enabling
fast pruning of irrelevant vectors.
Second, if there exist memory constraints due to a large number of document/passage/sentence embeddings, a
multi-node, multi-gpu distributed framework can be used to store the index corresponding to only part of the
document/passage/sentence embeddings per process. The query embeddings can also be split in batches and
obtain the top-k results in parallel using gpus for further speed ups for approximate or exact search. When
exact search becomes intractable due to scale, approximate nearest neighbor algorithms can be used. These
algorithms trade off accuracy for speed, allowing significantly faster retrieval while maintaining reasonably
accurate results, see Johnson et al. (2021) for further details.
Finally, in order to further reduce the memory footprint of each of the index shards corresponding to a subset
of the document/passage/sentence embeddings, different compression techniques can be used. For instance,
the Atlas retrieval-augmented language model (Izacard et al., 2022) uses product quantisation (Jégou et al.,
2011) to reduce the memory footprint of the retriever without sacrificing accuracy in downstream tasks in
terms of exact match and recall@50 metrics of Q&A.
Currently, there also exist a variety of vector database frameworks that have some of these optimisations
implemented out of the box and can be leveraged to use retrieval as a service without requiring the user to
implement all optimisations from scratch.
Chain-of-thought prompting and retrievers. Recent works (He et al., 2022; Trivedi et al., 2022)
propose to combine aretriever with reasoning via chain-of-thoughts (CoT) prompting to augment a LM.
He et al. (2022) use the CoT prompt to generate reasoning paths consisting of an explanation and prediction
pair. Then, knowledge is retrieved to support the explanations and the prediction that is mostly supported by
the evidence is selected. This approach does not require any additional training or fine-tuning. Trivedi et al.
(2022) propose an information retrieval chain-of-thought approach (IRCoT) which consists of interleaving
11Published in Transactions on Machine Learning Research (06/2023)
retrieval with CoT for multi-step QA. The idea is to use retrieval to guide the CoT reasoning steps and
conversely, using CoT reasoning to guide the retrieval step.
In all these works, aretriever is systematically called for every query in order to get the corresponding
documents to augment the LM. These approaches also assume that the intent is contained in the query. The
query could be augmented with the user’s intent by providing a natural language description of the search
task (instruction) in order to disambiguate the intent, as proposed by Asai et al. (2022). Also, the LM could
query the retriever only occasionally—when a prompt suggests it to do so—which is discussed in the next
subsection.
3.2.2 Querying search engines
In the previous paragraph, the information retrieval query corresponds to the LM’s context. However, the
LM can also have the ability to generate a query based on the prompt, thus enlarging its action space and
becoming more active.
LaMDAis one example of an agent-like LM designed for dialogue applications. The authors pre-train the
model on dialog data as well as other public web documents. In addition to this, to ensure that the model
is factually grounded as well as enhancing its conversational abilities, it is augmented withretrieval, a
calculator, and atranslator (Thoppilan et al., 2022). Furthermore, to improve the model’s safety,LaMDA
is fine-tuned with annotated data. Another example isBlenderBot (Shuster et al., 2022b), where the LM
decides to generate a query based on a prompt. In this case, the prompt corresponds to the instruction of
calling the search engine tool.BlenderBot is capable of open-domain conversation, it has been deployed on a
public website to further improve the model via continual learning with humans in the loop. Similarly,ReAct
uses few-shot prompting to teach a LM how to use different tools such assearch and lookup in Wikipedia,
and finish to return the answer (Yao et al., 2022b). Similarly, Komeili et al. (2021); Shuster et al. (2022a)
propose a model that learns to generate an internet search query based on the context, and then conditions on
the search results to generate a response.ReAct interleaves reasoning and acting, allowing for greater synergy
between the two and improved performance on both language and decision making tasks.ReAct performs
well on a diverse set of language and decision making tasks such as question answering, fact verification, or
web and home navigation.
In general, reasoning can improve decision making by making better inferences and predictions, while the
ability to use external tools can improve reasoning by gathering additional information from knowledge bases
or environments.
3.2.3 Searching and navigating the web
It is also possible to train agents that can navigate the open-ended internet in pursuit of specified goals
such as searching information or buying items. For example,WebGPT (Nakano et al., 2021) is a LM-based
agent which can interact with a custom text-based web-browsing environment in order to answer long-form
questions. In contrast with other models that only learn how to query retrievers or search engines like
LaMDA(Thoppilan et al., 2022) orBlenderBot (Shuster et al., 2022b),WebGPT learns to interact with a
web-browser, which allows it to further refine the initial query or perform additional actions based on its
interactions with the tool. More specifically,WebGPT can search the internet,navigate webpages, follow
links, and cite sources (see Table 3 for the full list of available actions). By accessing the internet, the
agent is able to enhance its question-answering abilities, even surpassing those of humans as determined
by human evaluators. The best model is obtained by fine-tuningGPT3 on human demonstrations, and
then performing rejection sampling against a reward model trained to predict human p"
