Findings of the Association for Computational Linguistics: EMNLP 2023, pages 9248–9274
December 6-10, 2023 ©2023 Association for Computational Linguistics
Enhancing Retrieval-Augmented Large Language Models with Iterative
Retrieval-Generation Synergy
Zhihong Shao1, Yeyun Gong2, yelong shen3, Minlie Huang1∗, Nan Duan2, Weizhu Chen3
1 The CoAI Group, DCST, Institute for Artificial Intelligence,
1 State Key Lab of Intelligent Technology and Systems,
1 Beijing National Research Center for Information Science and Technology,
1 Tsinghua University, Beijing 100084, China
2 Microsoft Research Asia 3 Microsoft Azure AI
szh19@mails.tsinghua.edu.cn aihuang@tsinghua.edu.cn
Abstract
Retrieval-augmented generation has raise exten-
sive attention as it is promising to address the
limitations of large language models including
outdated knowledge and hallucinations. How-
ever, retrievers struggle to capture relevance,
especially for queries with complex informa-
tion needs. Recent work has proposed to im-
prove relevance modeling by having large lan-
guage models actively involved in retrieval, i.e.,
to guide retrieval with generation. In this pa-
per, we show that strong performance can be
achieved by a method we call ITER -RETGEN,
which synergizes retrieval and generation in an
iterative manner: a model’s response to a task
input shows what might be needed to finish
the task, and thus can serve as an informative
context for retrieving more relevant knowledge
which in turn helps generate a better response
in another iteration. Compared with recent
work which interleaves retrieval with gener-
ation when completing a single output, I TER -
RETGEN processes all retrieved knowledge as
a whole and largely preserves the flexibility in
generation without structural constraints. We
evaluate ITER -RETGEN on multi-hop question
answering, fact verification, and commonsense
reasoning, and show that it can flexibly lever-
age parametric knowledge and non-parametric
knowledge, and is superior to or competitive
with state-of-the-art retrieval-augmented base-
lines while causing fewer overheads of retrieval
and generation. We can further improve per-
formance via generation-augmented retrieval
adaptation.
1 Introduction
Generative Large Language Models (LLMs)
have powered numerous applications, with well-
perceived utility. Despite being powerful, LLMs
lack knowledge that is under-represented in their
training data, and are prone to hallucinations, es-
pecially in open-domain settings (OpenAI, 2023).
∗*Corresponding author: Minlie Huang.
Retrieval-augmented LLMs, therefore, have raised
widespread attention as LLM outputs can be poten-
tially grounded on external knowledge.
Previous retrieval-augmented LMs (Izacard
et al., 2022b; Shi et al., 2023) typically adopted
one-time retrieval, i.e., to retrieve knowledge us-
ing only the task input (e.g., a user question for
open-domain question answering). One-time re-
trieval should suffice to fulfill the information needs
if they are clearly stated in the original input,
which is applicable to factoid question answering
(Kwiatkowski et al., 2019) and single-hop fact ver-
ification (Thorne et al., 2018), but not to tasks with
complex information needs, e.g., multi-hop rea-
soning (Yang et al., 2018) and long-form question
answering (Fan et al., 2019).
To fulfill complex information needs, recent
work proposes to gather required knowledge multi-
ple times throughout the generation process, using
partial generation (Trivedi et al., 2022a; Press et al.,
2022)) or forward-looking sentence(s) (Jiang et al.,
2023) as search queries. However, such structured
workflows of interleaving retrieval with generation
have the following limitations: (1) as intermediate
generation is conditioned on knowledge retrieved
before, with no awareness of knowledge retrieved
afterwards, they fail to process all retrieved knowl-
edge as a whole during the generation process; (2)
they require multi-round retrieval to gather a com-
prehensive set of knowledge, and may frequently
change the prompts by updating newly retrieved
knowledge, thus increasing the overheads of both
retrieval and generation.
In this paper, we find it simple but effective to
enhance retrieval-augmented LLMs through itera-
tive retrieval-generation synergy (ITER -RETGEN,
Fig 1). ITER -RETGEN iterates retrieval-augmented
generation and generation-augmented retrieval:
Retrieval-augmented generation outputs a response
to a task input based on all retrieved knowledge
(initially using the task input as the query). This
9248output shows what might be needed to fulfill the
task, and thus can serve as an informative context to
retrieve more relevant knowledge, i.e., generation-
augmented retrieval. The newly retrieved knowl-
edge can benefit another iteration of retrieval-
augmented generation. We can also leverage model
generations to adapt retrieval, by distilling knowl-
edge from a re-ranker with access to model genera-
tions to a dense retriever with access to task inputs
only, which may be beneficial in scenarios where
user inputs can be easily collected, but relevant
knowledge or desirable outputs are not annotated.
We evaluate our method on three tasks, includ-
ing multi-hop question answering, fact verification,
and commonsense reasoning. Our method prompts
an LLM to produce a chain of reasoning steps fol-
lowed by the final answer under a few-shot set-
ting. For in-context demonstrations, we focus on
problem-solving and follow Wei et al. (2022) to
annotate chains of thoughts, without explicitly con-
sidering how generation-augmented retrieval might
be affected, which makes it conceptually simple
and easy to implement. Our method achieves up
to 8.6% absolute gains over previous state-of-the-
art retrieval-augmented methods on four out of six
datasets while being competitive on the remaining
two. According to our experiments, generation gen-
erally benefits from more iterations, with two itera-
tions giving the most performance gains. One may
customize the performance-cost tradeoffs by choos-
ing an appropriate number of iterations. We can
further improve performance and also reduce itera-
tions via the aforementioned generation-augmented
retrieval adaptation.
We summarize our findings as follows:
• Automatic metrics such as exact match can
significantly underestimate the performance
of LLMs in question answering tasks. More-
over, improvements in exact match do not
always reflect improvements in generations.
Evaluation using LLMs may be more reliable.
• ITER -RETGEN is superior to or competi-
tive with state-of-the-art retrieval-augmented
methods, while being simpler and causing
fewer overheads of retrieval and generation.
With generation-augmented retrieval adapta-
tion, we can further improve performance
and also reduce overheads (by reducing itera-
tions).
• It is desirable for an LLM to leverage both
parametric knowledge and non-parametric
knowledge effectively. I TER -RETGEN con-
sistently outperforms Self-Ask on question
answering tasks, regardless of whether in-
context non-parametric knowledge mentions
the answers or not.
2 Related Work
In recent months, there has been a surge in LLM-
powered applications, such as ChatGPT, Bing Chat,
and CoPilot (Chen et al., 2021). While showing an
unprecedented level of performance, LLMs are sub-
ject to the following limitations: (1) Due to a high
demand for compute and data, it remains an open
research question to continually update LLMs both
efficiently and effectively (Scialom et al., 2022);
(2) LLMs also tend to hallucinate (OpenAI, 2023),
i.e., generating plausible but non-factual texts. To
alleviate these issues, there is a growing trend of
augmenting LLMs with tools (Mialon et al., 2023;
Gou et al., 2023), e.g., a code interpreter (Gao
et al., 2022b; Shao et al., 2023) or a search engine
(Nakano et al., 2021), in an attempt to offload sub-
tasks to more qualified experts, or to enrich the
input context for LLMs by providing more relevant
information.
Retrieval augmentation is a mainstream direc-
tion to connect LLMs to the external world. Previ-
ous retrieval-augmented LMs (Izacard and Grave,
2021; Shao and Huang, 2022) typically receive re-
trieved knowledge in a passive way: knowledge
is retrieved based on the task inputs without LMs’
intervention. As it is difficult for a retriever to cap-
ture relevance, especially in the zero-shot setting,
recent work shows a shift towards having LLMs
actively involved in retrieval to improve relevance
modeling, e.g., to provide a specific context for
retrieval with model generations (e.g., generated
search queries (Nakano et al., 2021; Press et al.,
2022; Yao et al., 2022), partial generation (Trivedi
et al., 2022a), or forward-looking sentences (Jiang
et al., 2023)). Khattab et al. (2022) proposed a
DSP programming framework that supports vari-
ous retrieval-augmented methods.
Recent work interleaves retrieval with generation
when completing a single output. Such a structured
workflow may reduce the flexibility in generation
(Yao et al., 2022). ITER -RETGEN avoids interrupt-
ing generation with retrieval, but iterates retrieval
and generation, i.e., to leverage the complete gen-
eration from the previous iteration to retrieve more
9249Question:
Retrieval:
Retrieval -Augmented Generation: 
What is the height of the player who won the 2015 AFL 
Rising Star award?
Title: 2015 AFL Rising Star Context: The NAB AFL Rising 
Star award is given annually to a stand out young player in 
the Australian Football League. The award was won by Jesse 
Hogan of Melbourne
The 2015 AFL Rising Star award was won by Jesse Hogan of 
Melbourne. Jesse Hogan is a professional Australian rules 
footballer. He is 198 cm tall. So the answer is 198 cm
Title: Jesse Hogan Context: Jesse Hogan … playing for the 
Melbourne Football Club. A key forward, Hogan is 1.95 m 
tall … made his AFL debut in the 2015 season and won the 
Ron Evans Medal as the AFL Rising Star
The 2015 AFL Rising Star award was won by Jesse Hogan of 
Melbourne. Jesse Hogan is 1.95 m tall.
So the answer is 1.95 m
Iteration 1 Iteration 2
Generation-Augmented Retrieval:
Retrieval -Augmented Generation: 
Question:
What is the height of the player who won the 2015 AFL 
Rising Star award?
Figure 1: ITER -RETGEN iterates retrieval and generation. In each iteration, I TER -RETGEN leverages the model
output from the previous iteration as a specific context to help retrieve more relevant knowledge, which may help
improve model generation (e.g., correcting the height of Hesse Hogan in this figure). We only show two iterations
in this figure for brevity. Solid arrows connect queries to the retrieved knowledge, and dashed arrows denote
retrieval-augmented generation.
relevant information which helps improve genera-
tion in the next iteration. ITER -RETGEN also has
the advantage of processing all retrieved knowl-
edge as a whole during the generation process, and
is conceptually simpler and easier-to-implement,
while being empirically strong in multi-hop ques-
tion answering, fact verification, and commonsense
reasoning.
A closely related work called GAR (Mao et al.,
2021) augments queries with generated background
information. HyDE (Gao et al., 2022a) also shares
a similar spirit, but focuses on zero-shot informa-
tion retrieval, and proposes to first prompt an LLM
to produce “hypothetical” paragraphs that cover the
information needed to answer a given question, and
then use the generated paragraphs to retrieve the
real ones. RepoCoder (Zhang et al., 2023) focuses
on repository-level code completion, and proposes
a 2-iteration retrieval-generation paradigm where
the second iteration leverages the intermediate code
completion for retrieval. By contrast, we propose
to synergize retrieval and generation with I TER -
RETGEN on various natural language tasks, and
explore how we can further adapt retrieval with
model generations.
3 Iterative Retrieval-Generation Synergy
3.1 Overview
Given a question q and a retrieval corpus D =
{d}where dis a paragraph, ITER -RETGEN repeats
retrieval-generation for T iterations; in iteration
t, we (1) leverage the generation yt−1 from the
previous iteration, concatenated with q, to retrieve
top-kparagraphs, and then (2) prompt an LLM M
to produce an output yt, with both the retrieved
paragraphs (denoted as Dyt−1||q) and qintegrated
into the prompt. Therefore, each iteration can be
formulated as follows:
yt = M(yt|prompt(Dyt−1||q,q)), ∀1 ≤t≤T (1)
The last output yT will be produced as the final
response.
3.2 Generation-Augmented Retrieval
There are many natural language tasks with com-
plex information needs. For example, in open-
domain multi-hop question answering, specific in-
formation needs may manifest themselves only
after correctly answering some prerequisite sub-
questions. In other words, there may exist semantic
gaps between the original question q and its sup-
porting knowledge, which can not be effectively
addressed by a retriever with a representation bot-
tleneck. In the first iteration, we can retrieve knowl-
edge with only the question q. In later iterations,
the LLM output from the previous iteration, though
having no guarantee of correctness, shows what
might be needed to answer the question, and thus
can be leveraged to bridge the semantic gaps; with
improved retrieval, an LLM can potentially pro-
duce a better output.
3.3 Retrieval-Augmented Generation
In each iteration, we generate an output using
Chain-of-Thought prompting except that we also
prepend retrieved knowledge to the question q.
Though there may exist more advanced prompting
9250variants, e.g., incorporating previous generations
into the prompt to enable direct refinements, we
leave the explorations for future work, and focus
on investigating the synergy between retrieval and
generation in a straightforward manner.
3.4 Generation-Augmented Retrieval
Adaptation
Model generations not only provide specific con-
texts for retrieval, but can also be leveraged to op-
timize the retriever, so that information needs in a
question can be better captured by the retriever.
Dense Retriever We adopted dense retrieval
in our experiments. Given a dense retriever
parametrized by θ= {θq,θd}where θq and θd de-
note parameters of the query encoder and the para-
graph encoder, respectively, the similarity score
between a query and a paragraph is calculated as
the inner product of their encoded vectors:
sθ(q,d) =⟨E(q; θq),E(d; θd)⟩ (2)
Re-ranker A re-ranker, parametrized by ϕ, out-
puts the probability of a paragraph being relevant
to a query; we denote the probability as sϕ(q,d).
Distillation A re-ranker is typically better at cap-
turing relevance between a query and a paragraph
than a retriever. Therefore, we distill knowledge
from a re-ranker to a retriever. To help the retriever
better address the semantic gaps between a ques-
tion and its supporting knowledge, we allow access
to y1 for the re-ranker (where y1 is the LLM out-
put from the first iteration). We optimize only the
query encoder of the retriever using the following
training objective:
θ∗
q = argmin
θq
KL(Pϕ(·|y1,q),Pθ(·|q))
Pϕ(d|y1,q) = exp(sϕ(y1||q,d)/τ)∑
d′∈Dy1||q
exp(sϕ(y1||q,d′)/τ)
Pθ(d|q) = exp(sθ(q,d)/τ)∑
d′∈Dy1||q
exp(sθ(q,d′)/τ)
(3)
where KL(·,·) denotes the KL divergence between
two probabilistic distributions.
4 Experiments
4.1 Datasets
We experimented on six datasets of three reason-
ing tasks: (1) Multi-hop question answering ,
including HotPotQA (Yang et al., 2018), 2Wiki-
MultiHopQA (Ho et al., 2020), MuSiQue (Trivedi
et al., 2022b), and Bamboogle (Press et al., 2022).
On MuSiQue, we followed Press et al. (2022) to
use only 2-hop questions; (2) Fact Verification,
including Feverous (Aly et al., 2021); (3) Com-
monsense reasoning, including StrategyQA (Geva
et al., 2021). Examples are presented in Table 1.
We used the October 2017 (Yang et al., 2018)
and the December 2018 (Karpukhin et al., 2020)
Wikipedia dump as the retrieval corpus for Hot-
PotQA and 2WikiMultiHopQA, respectively, and
used the December 2021 Wikipedia dump (Izacard
et al., 2022b) for the other datasets.
4.2 Evaluation Settings
We conducted evaluations on all 125 questions
from Bamboogle, the first 500 questions from the
train set of StrategyQA, and the first 500 questions
from the development sets of the other datasets.
All methods are evaluated under the 3-shot setting,
sharing the same questions in demonstrations.
Evaluation metrics are exact match (EM) and F1
for multi-hop question answering datasets, and ac-
curacy for both fact verification and commonsense
reasoning datasets. For more robust evaluation,
we also evaluate the correctness of model outputs
using text-davinci-003, the resulting metric de-
noted as Acc†. The prompt used for evaluation is
as follows, where {question}, {model output},
and {answer}are placeholders.
Prompt for Evaluating the Correctness of a
Model Output
In the following task, you are given a
Question, a model Prediction for the
Question, and a Ground-truth Answer to the
Question. You should decide whether the
model Prediction implies the Ground-truth
Answer.
Question
{question}
Prediction
{model output}
Ground-truth Answer
{answer}
Does the Prediction imply the Ground-truth
Answer? Output Yes or No:
9251Datasets Example
HotPotQA What is the name of this American musician, singer, actor, comedian, and songwriter,
who worked with Modern Records and born in December 5, 1932?
2WikiMultiHopQA Which film came out first, Blind Shaft or The Mask Of Fu Manchu?
MuSiQue In which year did the publisher of In Cold Blood form?
Bamboogle When did the first prime minister of the Russian Empire come into office?
Feverous Is it true that Based on the same platform as the Chevrolet Sail, the Baojun 310 was
launched on 2017 Beijing Auto Show where the price ranges from 36.800 yuan to
60.800 yuan?
StrategyQA Is it common to see frost during some college commencements?
Table 1: Example questions from six datasets.
4.3 Baselines
Direct Prompting (Brown et al., 2020) prompts an
LLM to directly generate the final answer without
an explanation. When augmenting Direct prompt-
ing with retrieval, we used the question to retrieve
knowledge which will be placed before the ques-
tion in the prompt.
CoT Prompting (Wei et al., 2022) prompts an
LLM to generate natural language reasoning steps
followed by the final answer.
ReAct (Yao et al., 2022) interleaves reasoning, ac-
tion, and observation steps, until reaching the ac-
tion of finalizing an answer. An action can be
either generating a query to search for informa-
tion or finalizing an answer. An observation is the
concatenation of retrieved paragraphs.
Self-Ask (Press et al., 2022) interleaves (i) follow-
up question generation, (ii) retrieval using the
follow-up, and (iii) answering the follow-up condi-
tioned on the retrieved knowledge, until no more
follow-up questions are generated and the LLM
gives an answer to the original question. We fol-
lowed (Yoran et al., 2023) to prepend newly re-
trieved paragraphs to the original question. On our
evaluated tasks, Self-Ask is conceptually similar
to ReAct, with the main difference being that Self-
Ask accumulates retrieved knowledge before the
original question in the prompt, while ReAct places
retrieved knowledge right after its query. Self-Ask
and IRCoT (Trivedi et al., 2022a) also share the
spirit of synergizing reasoning and retrieval.
DSP (Khattab et al., 2022) comprises a multi-hop
retrieval stage and an answer prediction stage. For
each hop within the retrieval stage, the model is
prompted to generate search queries and to sum-
marize retrieve knowledge for subsequent use. In
the prediction stage, DSP generates the answer us-
ing CoT based on the summarized knowledge and
retrieved documents.
4.4 Implementation Details
We used text-davinci-003 version of Instruct-
GPT (Ouyang et al., 2022) as the backend LLM.
We also present experiments using the open-source
Llama-2 models (Touvron et al., 2023) in Ap-
pendix A. All experiments used greedy decoding.
Contriever-MSMARCO (Izacard et al., 2022a) was
used for retrieval. We retrieved top-5 paragraphs
for each query. We allowed at most 5 interactions
with retrieval for ReAct and Self-Ask. We adapted
the implementation of DSP 1 to use the same gen-
eration model and retrieval systems as the other
methods.
Note that the first iteration of ITER -RETGEN is
CoT prompting with retrieval augmentation. There-
fore, ITER -RETGEN and CoT prompting share the
same annotated in-context demonstrations. All
prompts are presented in the Appendix.
4.5 Main Results
As shown by Table 2, I TER -RETGEN (T ≥ 2)
achieve significantly higher Acc† than retrieval-
augmented baselines on HotPotQA, 2WikiMulti-
HopQA, Bamboogle, and StrategyQA, while being
competitive with the best method (i.e., Self-Ask)
on MuSiQue and Feverous.
When increasing the number of iterations for
ITER -RETGEN, performance generally improves,
with the second iteration giving the greatest boost.
1https://github.com/stanfordnlp/dspy/issues/85
9252Method HotPotQA 2WikiMultiHopQA MuSiQue Bamboogle Feverous StrategyQA
EM F1 Acc † EM F1 Acc † EM F1 Acc † EM F1 Acc † Acc Acc† Acc Acc†
Without Retrieval
Direct 21.9 36.8 44.8 21.3 29.2 33.9 7.0 18.7 15.8 11.2 24.4 28.0 60.1 60.1 66.5 66.7
CoT 30.0 44.1 50.0 30.0 39.6 44.0 19.4 30.9 28.6 43.2 51.1 60.0 59.8 59.8 71.0 71.0
With Retrieval
Direct 31.6 44.7 53.3 27.3 35.4 43.6 13.9 28.2 26.5 17.6 31.8 43.2 69.8 69.8 65.6 65.6
ReAct 24.9 44.7 61.1 28.0 38.5 45.9 23.4 37.0 37.9 21.8 31.0 40.3 66.4 66.4 66.9 66.9
Self-Ask 36.8 55.2 64.8 37.3 48.8 55.9 27.6 41.5 42.9 31.5 41.2 54.8 70.7 70.7 70.2 70.2
DSP 43.8 55.0 60.8 - - - - - - - - - - - - -
ITER-RETGEN 1 39.2 53.9 65.5 33.7 45.2 55.4 24.2 38.6 38.1 36.8 47.7 57.6 67.0 67.0 72.0 72.0
ITER-RETGEN 2 44.1 58.6 71.2 34.9 47.0 58.1 26.4 41.1 41.0 38.4 48.7 59.2 68.8 68.8 73.0 73.0
ITER-RETGEN 3 45.2 59.9 71.4 34.8 47.8 58.3 25.7 41.4 40.8 37.6 47.0 59.2 69.0 69.0 72.3 72.3
ITER-RETGEN 4 45.8 61.1 73.4 36.0 47.4 58.5 26.7 41.8 40.8 38.4 49.6 60.0 71.5 71.5 73.8 73.8
ITER-RETGEN 5 45.2 60.3 72.8 35.5 47.5 58.8 25.7 40.7 39.6 39.2 49.7 60.8 70.3 70.3 73.2 73.2
ITER-RETGEN 6 45.9 61.0 73.3 35.5 48.1 59.4 25.9 40.5 39.8 40.0 50.0 59.2 70.9 70.9 72.4 72.4
ITER-RETGEN 7 45.1 60.4 72.9 35.5 47.4 58.4 26.1 42.0 41.0 40.0 50.7 60.8 70.5 70.5 74.1 74.1
Table 2: Evaluation results on multi-hop question answering, fact verification, and commonsense reasoning datasets.
Acc† is the accuracy of model outputs evaluated with text-davinci-003. For ITER -RETGEN, we evaluated LLM
outputs in different iterations (up to 7 iterations). Underlined metric values are higher than those of Self-Ask.
Method HotPotQA 2WikiMultiHopQA MuSiQue Bamboogle Feverous StrategyQA
# API # Doc # API # Doc # API # Doc # API # Doc # API # Doc # API # Doc
ReAct 2.9 14.3 3.0 15.0 2.9 14.4 2.8 14.1 2.1 10.6 2.8 14.2
Self-Ask 3.2 16.0 3.2 15.9 3.0 14.8 3.0 14.9 2.3 11.3 3.0 15.1
Table 3: Average numbers of API calls to text-davinci-003 and retrieved paragraphs for ReAct and Self-Ask.
Note that ITER -RETGEN (T = 2) achieves significantly higher or competitive Acc† with fewer API calls (i.e., 2)
and fewer retrieved paragraphs (5 per iteration, 10 in total).
It is worth noting that, as shown by Table 3, ITER -
RETGEN (T = 2) is superior to or competitive with
ReAct and Self-Ask using fewer API calls to the
LLM (i.e., 2) and fewer retrieved paragraphs (i.e.,
5 per iteration, 10 in total). ITER -RETGEN is also
conceptually simple, which is to iterate retrieval-
augmented CoT, without complex processing.
We also compared I TER -RETGEN with DSP
which also generates the answer using CoT based
on retrieved knowledge but differs in information
collection and processing. In each iteration, ITER -
RETGEN retrieves knowledge based on (1) the ques-
tion and (2) the previous model output which shows
what may be needed to answer the question. With
the number of iterations increasing, we tend to
obtain a more comprehensive and relevant set of
knowledge. Besides, unlike DSP, we do not summa-
rize the retrieved documents for answer generation,
and thus will not introduce summarization errors.
As shown in Table 2, ITER -RETGEN outperforms
DSP significantly. We manually investigate 10 ran-
dom questions where DSP fails but ITER -RETGEN
provides correct answers. On 40% of them, DSP
fails to retrieve documents that cover the correct
answers, while on 50% of them, the summarized
knowledge is misleading, e.g., for the question
“What occupation do Chris Menges and Aram
Avakian share?”, DSP generates a wrong sum-
mary “Chris Menges and Aram Avakian are
both members of the American and British
Societies of Cinematographers.”, while the
retrieved documents mention that Aram Avakian is
a film editor and director, and only Chris Menges
is with the American and British Societies of Cine-
matographers.
Acc† is a Reliable Metric To investigate how re-
liable Acc† is, we focused on model outputs where
EM and Acc† disagree, and manually checked
which metric gives more correct labels. On each
of the four multi-hop question answering datasets,
9253Dataset HotPotQA Feverous
Retriever Original Distilled w/oy1 Distilled w/y1 Original Distilled w/oy1 Distilled w/y1
ITER-RETGEN1 65.5 67.1 67.7 67.0 67.3 70.7
ITER-RETGEN2 71.2 75.2 75.7 68.8 68.1 69.5
Table 4: Effect of using LLM generation y1 on optimizing a dense retriever. We evaluated I TER -RETGEN on
HotPotQA and Feverous in terms of Acc†.
Subset CoT ✓ CoT/enc-37w/ Answer Retrieved w/o Answer Retrieved
Method Self-Ask I TER-RETGEN2 Self-Ask ITER-RETGEN2 Self-Ask ITER-RETGEN2 Self-Ask ITER-RETGEN2
HotPotQA 77.5 88.0 52.0 54.4 78.1 86.9 29.9 40.8
2WikiMultiHopQA 68.8 78.2 46.2 42.0 73.1 77.2 30.1 42.3
MuSiQue 68.5 66.9 32.6 30.7 72.9 78.9 12.2 22.9
Bamboogle 73.0 77.3 28.0 32.0 76.2 82.2 32.8 46.2
Table 5: Comparisons between Self-Ask and ITER -RETGEN (T = 2) on different subsets, in terms of Acc†. CoT ✓
is the subset of questions which CoT answers correctly without retrieval; CoT /enc-37is the complement. w/ Answer
Retrieved is the subset of questions for which a method (Self-Ask or I TER -RETGEN) successfully retrieves
paragraphs that mention the answers; w/o Answer Retrievedis the complement. ITER -RETGEN tends to be much
better at preserving the LLM’s performance on questions that can be solved using CoT without retrieval, and is
consistently more accurate regardless of whether retrieved knowledge mentions the answers or not.
we randomly sampled 20 model outputs from the
second iteration of ITER -RETGEN, resulting in 80
samples in total. For 98.75% of samples, EM is
0 and Acc† is 1, while Acc† gives the correct la-
bels 97.5% of the time, indicating that EM severely
underestimates model performance. We also car-
ried out the same evaluation for Self-Ask, andAcc†
gives the correct labels 98.75% of the time when it
is inconsistent with EM.
Acc† offers the advantage of identifying model
outputs that are semantically correct, even if
their surface forms differ from the annotated an-
swers. As an illustration, for the question “Which
country Jan Baptist Van Rensselaer’s
father is from?”, the annotated answer isDutch,
while the model prediction is Netherlands, which
is correct in terms of Acc† but is penalized by EM.
Notably, ITER -RETGEN (T ≥2) consistently
demonstrate lower EM but higher Acc† than Self-
Ask on 2WikiMultiHopQA, suggesting that en-
hancements in EM do not necessarily reflect im-
provements in the quality of generated answers.
Iteration 1 2 3 4 5 6 7
HotPotQA 49.5 66.1 65.7 66.5 66.7 66.7 67.1
2WikiMultiHopQA 29.0 45.2 46.2 46.7 45.8 45.8 46.5
MuSiQue 18.6 32.3 32.3 33.7 32.7 33.5 32.9
Bamboogle 20.8 36.0 36.8 36.0 35.2 36.0 36.0
Table 6: Answer recall of retrieved paragraphs in differ-
ent iterations for ITER -RETGEN.
Generation Benefits Retrieval Adaptation To
investigate how LLM outputs can be leveraged
for retrieval adaptation, we experimented on Hot-
PotQA and Feverous. Specifically, on each dataset,
we sampled 9,000 random questions from the train
set for training, and 1,000 for validation. We ap-
plied ITER -RETGEN for one iteration, and used the
model outputs y1 for retrieval adaptation as in Sec-
tion 3.4. We used TART (Asai et al., 2022) as the
re-ranker, and distilled knowledge from TART to
the dense retriever for no more than 1,000 steps.
Batch size was 32 and learning rate was 1e-5. We
used the retriever checkpoint with the lowest distil-
lation loss.
As shown by Table 4, retrieval adaptation en-
ables ITER -RETGEN to achieve significantly higher
Acc† with fewer iterations. We also demonstrated
the benefits of using y1 for adaptation by showing
its improvements over a variant which only dif-
fers in that the re-ranker has no access to y1; the
training objective of this variant can be obtained by
removing all y1 notations in Eq. 3.
4.6 Ablation Study
4.6.1 Generation Augments Retrieval
Table 6 shows the answer recall of retrieval in dif-
ferent iterations. The first iteration uses only the
questions for retrieval and suffers from low an-
swer recall. In the second iteration, retrieval, aug-
mented with the LLM output from the first iteration,
9254HotPotQA
Question: The arena where the Lewiston Maineiacs played their home games can seat how many people?
Answer: 3,677 seated
Iteration 1
Retrieved Knowledge
(1) Title: Lewiston Maineiacs Context: The Lewiston Maineiacs were a junior ice hockey team ... The team played its home
games at the Androscoggin Bank Colisée ...
(2) Title: Bangor Auditorium Context: ... a 5,948 seat multi-purpose arena located in Bangor, Maine ...
Generation
The Lewiston Maineiacs were a junior ice hockey team of the Quebec Major Junior Hockey League based in Lewiston, Maine. The
team played its home games at the Androscoggin Bank Colisée.The Androscoggin Bank Colisée can seat up to 5,948 people.
So the answer is 5,948
Iteration 2
Retrieved Knowledge
(1) Title: Lewiston Maineiacs Context: The Lewiston Maineiacs were a junior ice hockey team ... The team played its home
games at the Androscoggin Bank Colisée ...
(2) Title: Androscoggin Bank Colisée Context: The Androscoggin Bank Colisée (formerly Central Maine Civic Center and
Lewiston Colisee) is a 4,000 capacity (3,677 seated) multi-purpose arena ...
Generation
The Lewiston Maineiacs were a junior ice hockey team of the Quebec Major Junior Hockey League based in Lewiston, Maine.
They played their home games at the Androscoggin Bank Colisée. The Androscoggin Bank Colisée has a capacity of 4,000
(3,677 seated). So the answer is 3,677
StrategyQA
Question: Can you get Raclette in YMCA headquarters city?
Answer: Yes
Iteration 1
Retrieved Knowledge
(1) Title: YMCA Building (Racine, Wisconsin) Context: ... Since the 50’s, headquarters of the publishing house and the
bookstore are located in 11 rue de la Montagne Sainte-Geneviève 75005 Paris ...
Generation
The YMCA headquarters is located in Paris, France.Raclette is a traditional Swiss dish, made with melted cheeseserved with
potatoes, pickles, and other accompaniments. It is not a traditional French dish, so it is unlikely to be found in Paris. So the
answer is No
Iteration 2
Retrieved Knowledge
(1) Title: Raclette Context: Raclette is a Swiss dish, also popular in France, based on heating cheese and scraping off the melted
part ...
(2) Title: YMCA Building (Racine, Wisconsin) Context: ... Since the 50’s, headquarters of the publishing house and the
bookstore are located in 11 rue de la Montagne Sainte-Geneviève 75005 Paris ...
Generation
YMCA headquarters is located in Paris, France. Raclette is a dish native to parts of Switzerland, but it is also popular in France.
So it is likely that Raclette can be found in Paris. So the answer is Yes
Table 7: Two examples demonstrating retrieval-generation synergy. We underline generated phrases that help
retrieve relevant knowledge which successfully corrects factual errors (in red) in the second iteration. Irrelevant
retrieved paragraphs are not shown in the table for brevity.
achieves significantly higher recall, indicating that
LLM generations can help bridge the semantic gaps
between complex questions and their supporting
knowledge. However, performance quickly hits a
plateau afterwards.
4.6.2 I TER -RETGEN Leverages Parametric
and Non-Parametric Knowledge Better
Ideally, an LLM should flexibly utilize non-
parametric knowledge or parametric knowledge
depending on whether in-context non-parametric
knowledge is relevant or not. Table 5 presents per-
formance breakdowns on different subsets of ques-
tions for investigation. We considered the ability
of CoT to answer a question correctly without re-
trieval as a proxy for assessing an LLM’s capability
to answer the question using its parametric knowl-
edge. Compared with Self-Ask, I TER -RETGEN
tends to be significantly better at preserving the
LLM’s performance on questions that the LLM can
solve using CoT without retrieval, while being com-
petitive on the complementary subset. This may
be because the structural constraints from Self-Ask
makes an LLM over-sensitive to the precision and
comprehensiveness of follow-up question genera-
tion and answering, and Self-Ask is also incapable
of processing all retrieved knowledge as a whole,
thus reducing the LLM’s flexibility in solving a
question. Moreover, I TER -RETGEN consistently
outperforms Self-Ask by a large margin, regardless
9255of whether the in-context non-parametric knowl-
edge mentions the answers or not. This indicates
that when the in-context non-parametric knowledge
is irrelevant or incomplete, ITER -RETGEN exploits
parametric knowledge better than Self-Ask.
4.7 Error Analysis
On HotPotQA, we manually analyzed 20 random
cases where ITER -RETGEN (T = 2) fails. 25% of
predictions are false negatives. On 10% of cases,
ITER -RETGEN retrieves all necessary information
but fails to perform correct reasoning. The remain-
ing 65% of error cases are related with retrieval, on
76.9% of which, retrieval is misled by completely
wrong reasoning from the first iteration, while on
the other cases, reasoning in the first iteration is
partially correct, but the retriever fails to retrieve
the missing pieces in the second iteration. We also
observed that, in the first iteration, reasoning can be
negatively affected by noisy and possibly distrac-
tive knowledge retrieved using only the questions
as the queries.
5 Case Study
Table 7 demonstrates retrieval-generation synergy
with two examples from HotPotQA and Strate-
gyQA, respectively. In the first iteration, as both
questions need multi-hop reasoning, the retriever
fails to retrieve all supporting knowledge using only
the questions. Despite being affected by distrac-
tive retrieved knowledge (the capacity of a different
arena in the example from HotPotQA) and show-
ing imperfect parametric knowledge (the generated
statement that Raclette is unlikely to be found in
Paris in the example from StrategyQA) in the first
iteration, the LLM generates phrases that help re-
trieve relevant knowledge in the second iteration,
and successfully corrects its outputs.
6 Conclusion
We demonstrate the effectiveness of ITER -RETGEN
in answering questions with complex information
needs. Despite simple, I TER -RETGEN outper-
forms retrieval-augmented methods that have a
more complex workflow, which we believe could
serve as a strong baseline for future research on
retrieval-augmented generation. We also show that
generation-augmented retrieval adaptation can fur-
ther improve the performance of I TER -RETGEN
while also reducing overheads.
Limitations
In this work, we propose to enhance retrieval-
augmented large language models with I TER -
RETGEN which synergizes retrieval and generation
in an iterative manner, and demonstrates strong
performance compared to more structured prompt-
ing techniques such as Self-Ask. However, it’s
worth noting that our experiments utilized a fixed
black-box large language model, which may not
have been equally optimized for various forms of
prompting. It would be intriguing to investigate the
potential of prompting-specific (gradient-based) op-
timization in pushing the limits further. This could
involve enabling a large language model to leverage
parametric and non-parametric knowledge more
flexibly and effectively. By exploring this avenue,
we may uncover new insights and advancements
in the field. Furthermore, our experiments did not
cover long-form generation which would probably
benefit from more fine-grained retrieval than ITER -
RETGEN does in this work. We acknowledge that
this area warrants further exploration, and we leave
it for future work.
Acknowledgements
Zhihong Shao and Minlie Huang were supported by
the National Science Foundation for Distinguished
Young Scholars (with No. 62125604) and the
NSFC projects (Key project with No. 61936010).
They were also supported by the Guoqiang In-
stitute of Tsinghua University, with Grant No.
2020GQG0005.
9256References
Rami Aly, Zhijiang Guo, Michael Sejr Schlichtkrull,
James Thorne, Andreas Vlachos, Christos
Christodoulopoulos, Oana Cocarascu, and Arpit
Mittal. 2021. FEVEROUS: fact extraction and verifi-
cation over unstructured and structured information.
In Proceedings of the Neural Information Processing
Systems Track on Datasets and Benchmarks 1,
NeurIPS Datasets and Benchmarks 2021, December
2021, virtual.
Akari Asai, Timo Schick, Patrick S. H. Lewis, Xilun
Chen, Gautier Izacard, Sebastian Riedel, Hannaneh
Hajishirzi, and Wen-tau Yih. 2022. Task-aware re-
trieval with instructions. CoRR, abs/2211.09260.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei.
2020. Language models are few-shot learners. CoRR,
abs/2005.14165.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,
Henrique Pondé de Oliveira Pinto, Jared Kaplan,
Harrison Edwards, Yuri Burda, Nicholas Joseph,
Greg Brockman, Alex Ray, Raul Puri, Gretchen
Krueger, Michael Petrov, Heidy Khlaaf, Girish Sas-
try, Pamela Mishkin, Brooke Chan, Scott Gray,
Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz
Kaiser, Mohammad Bavarian, Clemens Winter,
Philippe Tillet, Felipe Petroski Such, Dave Cum-
mings, Matthias Plappert, Fotios Chantzis, Eliza-
beth Barnes, Ariel Herbert-V oss, William Hebgen
Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie
Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,
William Saunders, Christopher Hesse, Andrew N.
Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan
Morikawa, Alec Radford, Matthew Knight, Miles
Brundage, Mira Murati, Katie Mayer, Peter Welinder,
Bob McGrew, Dario Amodei, Sam McCandlish, Ilya
Sutskever, and Wojciech Zaremba. 2021. Evaluat-
ing large language models trained on code. CoRR,
abs/2107.03374.
Angela Fan, Yacine Jernite, Ethan Perez, David Grang-
ier, Jason Weston, and Michael Auli. 2019. ELI5:
long form question answering. In Proceedings of
the 57th Conference of the Association for Compu-
tational Linguistics, ACL 2019, Florence, Italy, July
28- August 2, 2019, Volume 1: Long Papers, pages
3558–3567. Association for Computational Linguis-
tics.
Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan.
2022a. Precise zero-shot dense retrieval without rele-
vance labels. CoRR, abs/2212.10496.
Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,
Pengfei Liu, Yiming Yang, Jamie Callan, and Gra-
ham Neubig. 2022b. PAL: program-aided language
models. CoRR, abs/2211.10435.
Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,
Dan Roth, and Jonathan Berant. 2021. Did aristotle
use a laptop? A question answering benchmark with
implicit reasoning strategies. Trans. Assoc. Comput.
Linguistics, 9:346–361.
Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen,
Yujiu Yang, Nan Duan, and Weizhu Chen. 2023.
Critic: Large language models can self-correct with
tool-interactive critiquing.
Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara,
and Akiko Aizawa. 2020. Constructing A multi-hop
QA dataset for comprehensive evaluation of reason-
ing steps. In Proceedings of the 28th International
Conference on Computational Linguistics, COLING
2020, Barcelona, Spain (Online), December 8-13,
2020, pages 6609–6625. International Committee on
Computational Linguistics.
Gautier Izacard, Mathilde Caron, Lucas Hosseini, Se-
bastian Riedel, Piotr Bojanowski, Armand Joulin,
and Edouard Grave. 2022a. Unsupervised dense in-
formation retrieval with contrastive learning. Trans.
Mach. Learn. Res., 2022.
Gautier Izacard and Edouard Grave. 2021. Leveraging
passage retrieval with generative models for open do-
main question answering. In Proceedings of the 16th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics: Main Volume,
EACL 2021, Online, April 19 - 23, 2021, pages 874–
880. Association for Computational Linguistics.
Gautier Izacard, Patrick S. H. Lewis, Maria Lomeli,
Lucas Hosseini, Fabio Petroni, Timo Schick, Jane
Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and
Edouard Grave. 2022b. Few-shot learning with
retrieval augmented language models. CoRR,
abs/2208.03299.
Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun,
Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie
Callan, and Graham Neubig. 2023. Active retrieval
augmented generation. CoRR, abs/2305.06983.
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick
S. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen,
and Wen-tau Yih. 2020. Dense passage retrieval for
open-domain question answering. In Proceedings of
the 2020 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP 2020, Online,
November 16-20, 2020, pages 6769–6781. Associa-
tion for Computational Linguistics.
Omar Khattab, Keshav Santhanam, Xiang Lisa Li,
David Hall, Percy Liang, Christopher Potts, and
Matei Zaharia. 2022. Demonstrate-search-predict:
Composing retrieval and language models for
knowledge-intensive NLP. CoRR, abs/2212.14024.
9257Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-
field, Michael Collins, Ankur P. Parikh, Chris Alberti,
Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-
ton Lee, Kristina Toutanova, Llion Jones, Matthew
Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob
Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-
ral questions: a benchmark for question answering
research. Trans. Assoc. Comput. Linguistics, 7:452–
466.
Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong
Shen, Jianfeng Gao, Jiawei Han, and Weizhu Chen.
2021. Generation-augmented retrieval for open-
domain question answering. In Proceedings of the
59th Annual Meeting of the Association for Com-
putational Linguistics and the 11th International
Joint Conference on Natural Language Processing,
ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual
Event, August 1-6, 2021, pages 4089–4100. Associa-
tion for Computational Linguistics.
Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christo-
foros Nalmpantis, Ramakanth Pasunuru, Roberta
Raileanu, Baptiste Rozière, Timo Schick, Jane
Dwivedi-Yu, Asli Celikyilmaz, Edouard Grave, Yann
LeCun, and Thomas Scialom. 2023. Augmented
language models: a survey. CoRR, abs/2302.07842.
Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,
Long Ouyang, Christina Kim, Christopher Hesse,
Shantanu Jain, Vineet Kosaraju, William Saunders,
Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen
Krueger, Kevin Button, Matthew Knight, Benjamin
Chess, and John Schulman. 2021. Webgpt: Browser-
assisted question-answering with human feedback.
CoRR, abs/2112.09332.
OpenAI. 2023. GPT-4 technical report. CoRR,
abs/2303.08774.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll L. Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray,
John Schulman, Jacob Hilton, Fraser Kelton, Luke
Miller, Maddie Simens, Amanda Askell, Peter Welin-
der, Paul F. Christiano, Jan Leike, and Ryan Lowe.
2022. Training language models to follow instruc-
tions with human feedback. In NeurIPS.
Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,
Noah A. Smith, and Mike Lewis. 2022. Measuring
and narrowing the compositionality gap in language
models. CoRR, abs/2210.03350.
Thomas Scialom, Tuhin Chakrabarty, and Smaranda
Muresan. 2022. Continual-t0: Progressively instruct-
ing 50+ tasks to language models without forgetting.
CoRR, abs/2205.12393.
Zhihong Shao, Yeyun Gong, Yelong Shen, Min-
lie Huang, Nan Duan, and Weizhu Chen. 2023.
Synthetic prompting: Generating chain-of-thought
demonstrations for large language models. CoRR,
abs/2302.00618.
Zhihong Shao and Minlie Huang. 2022. Answering
open-domain multi-answer questions via a recall-
then-verify framework. In Proceedings of the 60th
Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), ACL 2022,
Dublin, Ireland, May 22-27, 2022, pages 1825–1838.
Association for Computational Linguistics.
Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon
Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and
Wen-tau Yih. 2023. REPLUG: retrieval-augmented
black-box language models. CoRR, abs/2301.12652.
James Thorne, Andreas Vlachos, Christos
Christodoulopoulos, and Arpit Mittal. 2018.
FEVER: a large-scale dataset for fact extraction
and VERification. In Proceedings of the 2018
Conference of the North American Chapter of
the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long
Papers), pages 809–819, New Orleans, Louisiana.
Association for Computational Linguistics.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti
Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-
Ferrer, Moya Chen, Guillem Cucurull, David Esiobu,
Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
Cynthia Gao, Vedanuj Goswami, Naman Goyal, An-
thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan
Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
Isabel Kloumann, Artem Korenev, Punit Singh Koura,
Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-
ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-
tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein, Rashi Rungta, Kalyan Saladi, Alan Schelten,
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,
Melanie Kambadur, Sharan Narang, Aurélien Ro-
driguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023. Llama 2: Open foundation and fine-
tuned chat models. CoRR, abs/2307.09288.
Harsh Trivedi, Niranjan Balasubramanian, Tushar
Khot, and Ashish Sabharwal. 2022a. Interleav-
ing retrieval with chain-of-thought reasoning for
knowledge-intensive multi-step questions. CoRR,
abs/2212.10509.
Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot,
and Ashish Sabharwal. 2022b. MuSiQue: Multi-
hop questions via single-hop question composition.
Trans. Assoc. Comput. Linguistics, 10:539–554.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022.
Chain of thought prompting elicits reasoning in large
language models. CoRR, abs/2201.11903.
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-
gio, William W. Cohen, Ruslan Salakhutdinov, and
9258Christopher D. Manning. 2018. Hotpotqa: A dataset
for diverse, explainable multi-hop question answer-
ing. In Proceedings of the 2018 Conference on Em-
pirical Methods in Natural Language Processing,
Brussels, Belgium, October 31 - November 4, 2018,
pages 2369–2380. Association for Computational
Linguistics.
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak
Shafran, Karthik Narasimhan, and Yuan Cao. 2022.
React: Synergizing reasoning and acting in language
models. CoRR, abs/2210.03629.
Ori Yoran, Tomer Wolfson, Ben Bogin, Uri Katz, Daniel
Deutch, and Jonathan Berant. 2023. Answering
questions by meta-reasoning over multiple chains
of thought. CoRR, abs/2304.13007.
Fengji Zhang, Bei Chen, Yue Zhang, Jin Liu, Daoguang
Zan, Yi Mao, Jian-Guang Lou, and Weizhu Chen.
2023. Repocoder: Repository-level code completion
through iterative retrieval and generation. CoRR,
abs/2303.12570.
9259A Experiments Using Llama-2
To demonstrate the effectiveness of ITER -RETGEN
on open-source models, we replaced the generation
model text-davinci-003 in Table 2 with Llama-
2 models (Touvron et al., 2023), and re-ran the
evaluation. As shown in Table 8, I TER -RETGEN
consistently outperforms all baselines significantly.
B Few-Shot Prompts
In this section, we present all few-shot prompts
used in our experiments. We replace retrieved
paragraphs with the placeholder {Knowledge} for
brevity. CoT prompting shares the same in-context
demonstrations with ITER -RETGEN, except that it
is not augmented with retrieval.
B.1 HotPotQA
Prompts for Direct Prompting, ReAct, Self-Ask,
and ITER -RETGEN are presented in Table 9, Table
10, Table 11, and Table 12, respectively.
B.2 2WikiMultiHopQA
Prompts for Direct Prompting, ReAct, Self-Ask,
and ITER -RETGEN are presented in Table 13, Table
14, Table 15, and Table 16, respectively.
B.3 MuSiQue
Prompts for Direct Prompting, ReAct, Self-Ask,
and ITER -RETGEN are presented in Table 17, Table
18, Table 19, and Table 20, respectively.
B.4 Bamboogle
Prompts for Direct Prompting, ReAct, Self-Ask,
and ITER -RETGEN are presented in Table 21, Table
22, Table 23, and Table 24, respectively.
B.5 Feverous
Prompts for Direct Prompting, ReAct, Self-Ask,
and ITER -RETGEN are presented in Table 25, Table
26, Table 27, and Table 28, respectively.
B.6 StrategyQA
Prompts for Direct Prompting, ReAct, Self-Ask,
and ITER -RETGEN are presented in Table 29, Table
30, Table 31, and Table 32, respectively.
9260Model Llama-2-13B Llama-2-70B
Dataset HotPotQA 2WikiMultiHopQA StrategyQA HotPotQA 2WikiMultiHopQA StrategyQA
Without Retrieval
Direct 36.4 31.6 60.5 47.2 39.0 72.7
CoT 43.0 33.2 63.7 55.2 46.0 72.7
With Retrieval
Direct 51.8 38.6 63.3 58.6 45.1 73.3
ReAct 36.0 27.5 61.5 42.6 36.8 69.5
Self-Ask 45.8 38.5 63.3 58.4 53.2 71.7
ITER-RETGEN 1 53.8 44.6 62.8 64.4 55.1 74.8
ITER-RETGEN 2 57.8 48.0 67.2 67.8 57.9 76.6
Table 8: Experiments using the open-source Llama-2 models. We usedAcc† as the evaluation metric, i.e., to evaluate
the accuracy of model outputs with text-davinci-003.
{Knowledge}
Question: What is the name of this American musician, singer, actor, comedian, and songwriter, who worked with Modern
Records and born in December 5, 1932?
The answer is Little Richard
{Knowledge}
Question: Between Chinua Achebe and Rachel Carson, who had more diverse jobs?
The answer is Chinua Achebe
{Knowledge}
Question: Remember Me Ballin’ is a CD single by Indo G that features an American rapper born in what year?
The answer is 1979
Table 9: 3-Shot Demonstrations for Direct Prompting on HotPotQA.
9261Given the following question, answer it by providing follow up questions and intermediate answers. For each follow up question,
you are given a context which is the top returned Wikipedia snippets for the question. If no follow up questions are necessary,
answer the question directly.
#
Question: What is the name of this American musician, singer, actor, comedian, and songwriter, who worked with Modern
Records and born in December 5, 1932?
Are follow up questions needed here: Yes.
Follow up: Who worked with Modern Records?
{Knowledge}
Intermediate answer: Artists worked with Modern Records include Etta James, Little Richard, Joe Houston, Ike and Tina Turner
and John Lee Hooker.
Follow up: Is Etta James an American musician, singer, actor, comedian, and songwriter, and was born in December 5, 1932?
{Knowledge}
Intermediate answer: Etta James was born in January 25, 1938, not December 5, 1932, so the answer is no.
Follow up: Is Little Richard an American musician, singer, actor, comedian, and songwriter, and was born in December 5, 1932?
{Knowledge}
Intermediate answer: Yes, Little Richard, born in December 5, 1932, is an American musician, singer, actor, comedian and
songwriter.
So the final answer is: Little Richard
#
Question: Between Chinua Achebe and Rachel Carson, who had more diverse jobs?
Are follow up questions needed here: Yes.
Follow up: What jobs did Chinua Achebe have?
{Knowledge}
Intermediate answer: Chinua Achebe was a Nigerian (1) novelist, (2) poet, (3) professor, and (4) critic, so Chinua Achebe had 4
jobs.
Follow up: What jobs did Rachel Carson have?
{Knowledge}
Intermediate answer: Rachel Carson was an American (1) marine biologist, (2) author, and (3) conservationist, so Rachel Carson
had 3 jobs.
Follow up: Did Chinua Achebe have more jobs than Rachel Carson?
{Knowledge}
Intermediate answer: Chinua Achebe had 4 jobs, while Rachel Carson had 3 jobs. 4 is greater than 3, so yes, Chinua Achebe had
more jobs.
So the final answer is: Chinua Achebe
#
Question: Remember Me Ballin’ is a CD single by Indo G that features an American rapper born in what year?
Are follow up questions needed here: Yes.
Follow up: Which American rapper is featured by Remember Me Ballin’, a CD single by Indo G?
{Knowledge}
Intermediate answer: Gangsta Boo
Follow up: In which year was Gangsta Boo born?
{Knowledge}
Intermediate answer: Gangsta Boo was born in August 7, 1979, so the answer is 1979.
So the final answer is: 1979
Table 10: 3-Shot Demonstrations for ReAct on HotPotQA.
9262Given the following question, answer it by providing follow up questions and intermediate answers. For each follow up question,
you are given a context which is the top returned Wikipedia snippets for the question. If no follow up questions are necessary,
answer the question directly.
#
{Knowledge}
Question: What is the name of this American musician, singer, actor, comedian, and songwriter, who worked with Modern
Records and born in December 5, 1932?
Are follow up questions needed here: Yes.
Follow up: Who worked with Modern Records?
Intermediate answer: Artists worked with Modern Records include Etta James, Little Richard, Joe Houston, Ike and Tina Turner
and John Lee Hooker.
Follow up: Is Etta James an American musician, singer, actor, comedian, and songwriter, and was born in December 5, 1932?
Intermediate answer: Etta James was born in January 25, 1938, not December 5, 1932, so the answer is no.
Follow up: Is Little Richard an American musician, singer, actor, comedian, and songwriter, and was born in December 5, 1932?
Intermediate answer: Yes, Little Richard, born in December 5, 1932, is an American musician, singer, actor, comedian and
songwriter.
So the final answer is: Little Richard
#
{Knowledge}
Question: Between Chinua Achebe and Rachel Carson, who had more diverse jobs?
Are follow up questions needed here: Yes.
Follow up: What jobs did Chinua Achebe have?
Intermediate answer: Chinua Achebe was a Nigerian (1) novelist, (2) poet, (3) professor, and (4) critic, so Chinua Achebe had 4
jobs.
Follow up: What jobs did Rachel Carson have?
Intermediate answer: Rachel Carson was an American (1) marine biologist, (2) author, and (3) conservationist, so Rachel Carson
had 3 jobs.
Follow up: Did Chinua Achebe have more jobs than Rachel Carson?
Intermediate answer: Chinua Achebe had 4 jobs, while Rachel Carson had 3 jobs. 4 is greater than 3, so yes, Chinua Achebe had
more jobs.
So the final answer is: Chinua Achebe
#
{Knowledge}
Question: Remember Me Ballin’ is a CD single by Indo G that features an American rapper born in what year?
Are follow up questions needed here: Yes.
Follow up: Which American rapper is featured by Remember Me Ballin’, a CD single by Indo G?
Intermediate answer: Gangsta Boo
Follow up: In which year was Gangsta Boo born?
Intermediate answer: Gangsta Boo was born in August 7, 1979, so the answer is 1979.
So the final answer is: 1979
Table 11: 3-Shot Demonstrations for Self-Ask on HotPotQA.
{Knowledge}
Question: What is the name of this American musician, singer, actor, comedian, and songwriter, who worked with Modern
Records and born in December 5, 1932?
Let’s think step by step.
Artists who worked with Modern Records include Etta James, Joe Houston, Little Richard, Ike and Tina Turner and John Lee
Hooker in the 1950s and 1960s. Of these Little Richard, born in December 5, 1932, was an American musician, singer, actor,
comedian, and songwriter.
So the answer is Little Richard
{Knowledge}
Question: Between Chinua Achebe and Rachel Carson, who had more diverse jobs?
Let’s think step by step.
Chinua Achebe was a Nigerian novelist, poet, professor, and critic. Rachel Carson was an American marine biologist, author,
and conservationist. So Chinua Achebe had 4 jobs, while Rachel Carson had 3 jobs. Chinua Achebe had more diverse jobs than
Rachel Carson.
So the answer is Chinua Achebe
{Knowledge}
Question: Remember Me Ballin’ is a CD single by Indo G that features an American rapper born in what year?
Let’s think step by step.
Remember Me Ballin’ is the CD single by Indo G featuring Gangsta Boo. Gangsta Boo is Lola Mitchell’s stage name, who was
born in August 7, 1979, and is an American rapper.
So the answer is 1979
Table 12: 3-Shot Demonstrations for ITER -RETGEN on HotPotQA.
9263{Knowledge}
Question: Which film came out first, Blind Shaft or The Mask Of Fu Manchu?
The answer is The Mask Of Fu Manchu
{Knowledge}
Question: When did John V , Prince Of Anhalt-Zerbst’s father die?
The answer is 12 June 1516
{Knowledge}
Question: Which film has the director who was born later, El Extrano Viaje or Love In Pawn?
The answer is El Extrano Viaje
Table 13: 3-Shot Demonstrations for Direct Prompting on 2WikiMultiHopQA.
Given the following question, answer it by providing follow up questions and intermediate answers. For each follow up question,
you are given a context which is the top returned Wikipedia snippets for the question. If no follow up questions are necessary,
answer the question directly.
#
Question: Which film came out first, Blind Shaft or The Mask Of Fu Manchu?
Are follow up questions needed here: Yes.
Follow up: When did Blind Shaft come out?
{Knowledge}
Intermediate answer: Blind Shaft came out in 2003.
Follow up: When did The Mask Of Fu Manchu come out?
{Knowledge}
Intermediate answer: The Mask Of Fu Manchu came out in 1932.
So the final answer is: The Mask Of Fu Manchu
#
Question: When did John V , Prince Of Anhalt-Zerbst’s father die?
Are follow up questions needed here: Yes.
Follow up: Who is the father of John V , Prince Of Anhalt-Zerbst?
{Knowledge}
Intermediate answer: The father of John V , Prince Of Anhalt-Zerbst is Ernest I, Prince of Anhalt-Dessau.
Follow up: When did Ernest I, Prince of Anhalt-Dessau die?
{Knowledge}
Intermediate answer: Ernest I, Prince of Anhalt-Dessau died on 12 June 1516.
So the final answer is: 12 June 1516
#
Question: Which film has the director who was born later, El Extrano Viaje or Love In Pawn?
Are follow up questions needed here: Yes.
Follow up: Who is the director of El Extrano Viaje?
{Knowledge}
Intermediate answer: The director of El Extrano Viaje is Fernando Fernan Gomez.
Follow up: Who is the director of Love in Pawn?
{Knowledge}
Intermediate answer: The director of Love in Pawn is Charles Saunders.
Follow up: When was Fernando Fernan Gomez born?
{Knowledge}
Intermediate answer: Fernando Fernan Gomez was born on 28 August 1921.
Follow up: When was Charles Saunders (director) born?
{Knowledge}
Intermediate answer: Charles Saunders was born on 8 April 1904.
So the final answer is: El Extrano Viaje
Table 14: 3-Shot Demonstrations for ReAct on 2WikiMultiHopQA.
9264Given the following question, answer it by providing follow up questions and intermediate answers. For each follow up question,
you are given a context which is the top returned Wikipedia snippets for the question. If no follow up questions are necessary,
answer the question directly.
#
{Knowledge}
Question: Which film came out first, Blind Shaft or The Mask Of Fu Manchu?
Are follow up questions needed here: Yes.
Follow up: When did Blind Shaft come out?
Intermediate answer: Blind Shaft came out in 2003.
Follow up: When did The Mask Of Fu Manchu come out?
Intermediate answer: The Mask Of Fu Manchu came out in 1932.
So the final answer is: The Mask Of Fu Manchu
#
{Knowledge}
Question: When did John V , Prince Of Anhalt-Zerbst’s father die?
Are follow up questions needed here: Yes.
Follow up: Who is the father of John V , Prince Of Anhalt-Zerbst?
Intermediate answer: The father of John V , Prince Of Anhalt-Zerbst is Ernest I, Prince of Anhalt-Dessau.
Follow up: When did Ernest I, Prince of Anhalt-Dessau die?
Intermediate answer: Ernest I, Prince of Anhalt-Dessau died on 12 June 1516.
So the final answer is: 12 June 1516
#
{Knowledge}
Question: Which film has the director who was born later, El Extrano Viaje or Love In Pawn?
Are follow up questions needed here: Yes.
Follow up: Who is the director of El Extrano Viaje?
Intermediate answer: The director of El Extrano Viaje is Fernando Fernan Gomez.
Follow up: Who is the director of Love in Pawn?
Intermediate answer: The director of Love in Pawn is Charles Saunders.
Follow up: When was Fernando Fernan Gomez born?
Intermediate answer: Fernando Fernan Gomez was born on 28 August 1921.
Follow up: When was Charles Saunders (director) born?
Intermediate answer: Charles Saunders was born on 8 April 1904.
So the final answer is: El Extrano Viaje
Table 15: 3-Shot Demonstrations for Self-Ask on 2WikiMultiHopQA.
{Knowledge}
Question: Which film came out first, Blind Shaft or The Mask Of Fu Manchu?
Let’s think step by step.
Blind Shaft is a 2003 film, while The Mask Of Fu Manchu opened in New York on December 2, 1932. 2003 comes after 1932.
Therefore, The Mask Of Fu Manchu came out earlier than Blind Shaft.
So the answer is The Mask Of Fu Manchu
{Knowledge}
Question: When did John V , Prince Of Anhalt-Zerbst’s father die?
Let’s think step by step.
John was the second son of Ernest I, Prince of Anhalt-Dessau. Ernest I, Prince of Anhalt-Dessau died on 12 June 1516.
So the answer is 12 June 1516
{Knowledge}
Question: Which film has the director who was born later, El Extrano Viaje or Love In Pawn?
Let’s think step by step.
The director of El Extrano Viaje is Fernando Fernan Gomez, who was born on 28 August 1921. The director of Love In Pawn
is Charles Saunders, who was born on 8 April 1904. 28 August 1921 comes after 8 April 1904. Therefore, Fernando Fernan
Gomez was born later than Charles Saunders.
So the answer is El Extrano Viaje
Table 16: 3-Shot Demonstrations for ITER -RETGEN on 2WikiMultiHopQA.
9265{Knowledge}
Question: In which year did the publisher of In Cold Blood form?
The answer is 2001
{Knowledge}
Question: Who was in charge of the city where The Killing of a Sacred Deer was filmed?
The answer is John Cranley
{Knowledge}
Question: Where on the Avalon Peninsula is the city that Signal Hill overlooks?
The answer is eastern tip
Table 17: 3-Shot Demonstrations for Direct Prompting on MuSiQue.
Given the following question, answer it by providing follow up questions and intermediate answers. For each follow up question,
you are given a context which is the top returned Wikipedia snippets for the question. If no follow up questions are necessary,
answer the question directly.
#
Question: In which year did the publisher of In Cold Blood form?
Are follow up questions needed here: Yes.
Follow up: What business published In Cold Blood?
{Knowledge}
Intermediate answer: In Cold Blood was published in book form by Random House.
Follow up: Which year witnessed the formation of Random House?
{Knowledge}
Intermediate answer: Random House was form in 2001.
So the final answer is: 2001
#
Question: Who was in charge of the city where The Killing of a Sacred Deer was filmed?
Are follow up questions needed here: Yes.
Follow up: In which city was The Killing of a Sacred Deer filmed
{Knowledge}
Intermediate answer: The Killing of a Sacred Deer was filmed in Cincinnati.
Follow up: Who was in charge of Cincinnati?
{Knowledge}
Intermediate answer: The present Mayor of Cincinnati is John Cranley, so John Cranley is in charge.
So the final answer is: John Cranley
#
Question: Where on the Avalon Peninsula is the city that Signal Hill overlooks?
Are follow up questions needed here: Yes.
Follow up: What city does Signal Hill overlook?
{Knowledge}
Intermediate answer: Signal Hill is a hill which overlooks the city of St. John’s.
Follow up: Where on the Avalon Peninsula is St. John’s located?
{Knowledge}
Intermediate answer: St. John’s is located on the eastern tip of the Avalon Peninsula.
So the final answer is: eastern tip
Table 18: 3-Shot Demonstrations for ReAct on MuSiQue.
9266Given the following question, answer it by providing follow up questions and intermediate answers. For each follow up question,
you are given a context which is the top returned Wikipedia snippets for the question. If no follow up questions are necessary,
answer the question directly.
#
{Knowledge}
Question: In which year did the publisher of In Cold Blood form?
Are follow up questions needed here: Yes.
Follow up: What business published In Cold Blood?
Intermediate answer: In Cold Blood was published in book form by Random House.
Follow up: Which year witnessed the formation of Random House?
Intermediate answer: Random House was form in 2001.
So the final answer is: 2001
#
{Knowledge}
Question: Who was in charge of the city where The Killing of a Sacred Deer was filmed?
Are follow up questions needed here: Yes.
Follow up: In which city was The Killing of a Sacred Deer filmed
Intermediate answer: The Killing of a Sacred Deer was filmed in Cincinnati.
Follow up: Who was in charge of Cincinnati?
Intermediate answer: The present Mayor of Cincinnati is John Cranley, so John Cranley is in charge.
So the final answer is: John Cranley
#
{Knowledge}
Question: Where on the Avalon Peninsula is the city that Signal Hill overlooks?
Are follow up questions needed here: Yes.
Follow up: What city does Signal Hill overlook?
Intermediate answer: Signal Hill is a hill which overlooks the city of St. John’s.
Follow up: Where on the Avalon Peninsula is St. John’s located?
Intermediate answer: St. John’s is located on the eastern tip of the Avalon Peninsula.
So the final answer is: eastern tip
Table 19: 3-Shot Demonstrations for Self-Ask on MuSiQue.
{Knowledge}
Question: In which year did the publisher of In Cold Blood form?
Let’s think step by step.
In Cold Blood was first published in book form by Random House. Random House was form in 2001.
So the answer is 2001
{Knowledge}
Question: Who was in charge of the city where The Killing of a Sacred Deer was filmed?
Let’s think step by step.
The Killing of a Sacred Deer was filmed in Cincinnati. The present Mayor of Cincinnati is John Cranley. Therefore, John
Cranley is in charge of the city.
So the answer is John Cranley
{Knowledge}
Question: Where on the Avalon Peninsula is the city that Signal Hill overlooks?
Let’s think step by step.
Signal Hill is a hill which overlooks the city of St. John’s. St. John’s is located on the eastern tip of the Avalon Peninsula.
So the answer is eastern tip
Table 20: 3-Shot Demonstrations for ITER -RETGEN on MuSiQue.
9267{Knowledge}
Question: When did the first prime minister of the Russian Empire come into office?
The answer is 1905-11-06 00:00:00
{Knowledge}
Question: The most populous city in Punjab is how large (area wise)?
The answer is 310 square kilometers
{Knowledge}
Question: What is the capital of the country where yoga originated?
The answer is New Delhi
Table 21: 3-Shot Demonstrations for Direct Prompting on Bamboogle.
Given the following question, answer it by providing follow up questions and intermediate answers. For each follow up question,
you are given a context which is the top returned Wikipedia snippets for the question. If no follow up questions are necessary,
answer the question directly.
#
Question: When did the first prime minister of the Russian Empire come into office?
Are follow up questions needed here: Yes.
Follow up: Who is the first prime minister of the Russian Empire?
{Knowledge}
Intermediate answer: Sergei Witte
Follow up: When did Sergei Witte come into office?
{Knowledge}
Intermediate answer: Sergei Witte was appointed on 6 November 1905.
So the final answer is: 1905-11-06 00:00:00
#
Question: The most populous city in Punjab is how large (area wise)?
Are follow up questions needed here: Yes.
Follow up: What is the most populous city in Punjab?
{Knowledge}
Intermediate answer: Ludhiana is the most populous and largest city in Punjab.
Follow up: How large is Ludhiana, the most populous city in Punjab?
{Knowledge}
Intermediate answer: The area of Ludhiana is over 310 km2.
So the final answer is: 310 square kilometers
#
Question: What is the capital of the country where yoga originated?
Are follow up questions needed here: Yes.
Follow up: Which country was yoga originated?
{Knowledge}
Intermediate answer: There is no consensus on yoga’s origin. Suggested origins include India.
Follow up: What is the capital of India?
{Knowledge}
Intermediate answer: The current capital of India is New Delhi.
So the final answer is: New Delhi
Table 22: 3-Shot Demonstrations for ReAct on Bamboogle.
9268Given the following question, answer it by providing follow up questions and intermediate answers. For each follow up question,
you are given a context which is the top returned Wikipedia snippets for the question. If no follow up questions are necessary,
answer the question directly.
#
{Knowledge}
Question: When did the first prime minister of the Russian Empire come into office?
Are follow up questions needed here: Yes.
Follow up: Who is the first prime minister of the Russian Empire?
Intermediate answer: Sergei Witte
Follow up: When did Sergei Witte come into office?
Intermediate answer: Sergei Witte was appointed on 6 November 1905.
So the final answer is: 1905-11-06 00:00:00
#
{Knowledge}
Question: The most populous city in Punjab is how large (area wise)?
Are follow up questions needed here: Yes.
Follow up: What is the most populous city in Punjab?
Intermediate answer: Ludhiana is the most populous and largest city in Punjab.
Follow up: How large is Ludhiana, the most populous city in Punjab?
Intermediate answer: The area of Ludhiana is over 310 km2.
So the final answer is: 310 square kilometers
#
{Knowledge}
Question: What is the capital of the country where yoga originated?
Are follow up questions needed here: Yes.
Follow up: Which country was yoga originated?
Intermediate answer: There is no consensus on yoga’s origin. Suggested origins include India.
Follow up: What is the capital of India?
Intermediate answer: The current capital of India is New Delhi.
So the final answer is: New Delhi
Table 23: 3-Shot Demonstrations for Self-Ask on Bamboogle.
{Knowledge}
Question: When did the first prime minister of the Russian Empire come into office?
Let’s think step by step.
The first prime minister of the Russian Empire was Count Sergei Witte. Sergei Witte was appointed on 6 November 1905.
So the answer is 1905-11-06 00:00:00
{Knowledge}
Question: The most populous city in Punjab is how large (area wise)?
Let’s think step by step.
Ludhiana is the most populous and the largest city in the Indian state of Punjab. The city has an area of over 310 km2.
So the answer is 310 square kilometers
{Knowledge}
Question: What is the capital of the country where yoga originated?
Let’s think step by step.
Suggested origins include pre-Vedic Eastern states of India. The current capital of India is New Delhi.
So the answer is New Delhi
Table 24: 3-Shot Demonstrations for ITER -RETGEN on Bamboogle.
9269{Knowledge}
Question: Is it true that Belgrade Race is an annual men’s footrace of around 6 kilometres (5834 metres) that is held in Belgrade,
Serbia through history, past winners includes Brahim Lahlafi (1st edition), Philip Mosima (3rd) and Josphat Menjo (6th)?
The answer is Yes
{Knowledge}
Question: Is it true that Based on the same platform as the Chevrolet Sail, the Baojun 310 was launched on 2017 Beijing Auto
Show where the price ranges from 36.800 yuan to 60.800 yuan?
The answer is No
{Knowledge}
Question: Is it true that Florida International University pedestrian bridge collapse was funded with a $19.4 million Transportation
Investment Generating Economic Recovery grant from the United States Department of Transportation in 2013, along with state
agencies and the bridge cost $14.2 million to construct?
The answer is No
Table 25: 3-Shot Demonstrations for Direct Prompting on Feverous.
Given the following question, answer it by providing follow up questions and intermediate answers. For each follow up question,
you are given a context which is the top returned Wikipedia snippets for the question. If no follow up questions are necessary,
answer the question directly. The final answer should always be either Yes or No, and NOTHING ELSE.
#
Question: Is it true that Belgrade Race is an annual men’s footrace of around 6 kilometres (5834 metres) that is held in Belgrade,
Serbia through history, past winners includes Brahim Lahlafi (1st edition), Philip Mosima (3rd) and Josphat Menjo (6th)?
Are follow up questions needed here: Yes.
Follow up: What is the Belgrade Race?
{Knowledge}
Intermediate answer: The Belgrade Race Through History is an annual men’s footrace of around 6 kilometres (5834 metres) that
is held in Belgrade, Serbia.
Follow up: Has Brahim Lahlafi won Belgrade Race?
{Knowledge}
Intermediate answer: Yes, Brahim Lahlafi was the winner in 1996.
Follow up: Has Philip Mosima won Belgrade Race?
{Knowledge}
Intermediate answer: Yes, Philip Mosima beat Marathon world record and won in 1998
Follow up: Has Josphat Menjo won Belgrade Race?
{Knowledge}
Intermediate answer: Yes, Josphat Menjo broke the meet record and won the competition.
So the final answer is: Yes
#
Question: Is it true that Based on the same platform as the Chevrolet Sail, the Baojun 310 was launched on 2017 Beijing Auto
Show where the price ranges from 36.800 yuan to 60.800 yuan?
Are follow up questions needed here: Yes.
Follow up: When and where was the Baojun 310 launched?
{Knowledge}
Intermediate answer: The Baojun 310 was launched on 2016 Beijing Auto Show, not 2017 Beijing Auto Show.
So the final answer is: No
#
Question: Is it true that Florida International University pedestrian bridge collapse was funded with a $19.4 million Transportation
Investment Generating Economic Recovery grant from the United States Department of Transportation in 2013, along with state
agencies and the bridge cost $14.2 million to construct?
Are follow up questions needed here: Yes.
Follow up: How was Florida International University pedestrian bridge collapse funded?
{Knowledge}
Intermediate answer: Florida International University pedestrian bridge was a $14.2 million project funded with a $19.4 million
Transportation Investment Generating Economic Recovery (TIGER) grant from the United States Department of Transportation
in 2013, along with state agencies, which is consistent with facts in the question.
Follow up: How much did it cost to construct Florida International University pedestrian bridge?
{Knowledge}
Intermediate answer: The bridge cost $9 million to construct, not $14.2 million.
So the final answer is: No
Table 26: 3-Shot Demonstrations for ReAct on Feverous.
9270Given the following question, answer it by providing follow up questions and intermediate answers. For each follow up question,
you are given a context which is the top returned Wikipedia snippets for the question. If no follow up questions are necessary,
answer the question directly. The final answer should always be either Yes or No, and NOTHING ELSE.
#
{Knowledge}
Question: Is it true that Belgrade Race is an annual men’s footrace of around 6 kilometres (5834 metres) that is held in Belgrade,
Serbia through history, past winners includes Brahim Lahlafi (1st edition), Philip Mosima (3rd) and Josphat Menjo (6th)?
Are follow up questions needed here: Yes.
Follow up: What is the Belgrade Race?
Intermediate answer: The Belgrade Race Through History is an annual men’s footrace of around 6 kilometres (5834 metres) that
is held in Belgrade, Serbia.
Follow up: Has Brahim Lahlafi won Belgrade Race?
Intermediate answer: Yes, Brahim Lahlafi was the winner in 1996.
Follow up: Has Philip Mosima won Belgrade Race?
Intermediate answer: Yes, Philip Mosima beat Marathon world record and won in 1998
Follow up: Has Josphat Menjo won Belgrade Race?
Intermediate answer: Yes, Josphat Menjo broke the meet record and won the competition.
So the final answer is: Yes
#
{Knowledge}
Question: Is it true that Based on the same platform as the Chevrolet Sail, the Baojun 310 was launched on 2017 Beijing Auto
Show where the price ranges from 36.800 yuan to 60.800 yuan?
Are follow up questions needed here: Yes.
Follow up: When and where was the Baojun 310 launched?
Intermediate answer: The Baojun 310 was launched on 2016 Beijing Auto Show, not 2017 Beijing Auto Show.
So the final answer is: No
#
{Knowledge}
Question: Is it true that Florida International University pedestrian bridge collapse was funded with a $19.4 million Transportation
Investment Generating Economic Recovery grant from the United States Department of Transportation in 2013, along with state
agencies and the bridge cost $14.2 million to construct?
Are follow up questions needed here: Yes.
Follow up: How was Florida International University pedestrian bridge collapse funded?
Intermediate answer: Florida International University pedestrian bridge was a $14.2 million project funded with a $19.4 million
Transportation Investment Generating Economic Recovery (TIGER) grant from the United States Department of Transportation
in 2013, along with state agencies, which is consistent with facts in the question.
Follow up: How much did it cost to construct Florida International University pedestrian bridge?
Intermediate answer: The bridge cost $9 million to construct, not $14.2 million.
So the final answer is: No
Table 27: 3-Shot Demonstrations for Self-Ask on Feverous.
9271You are required to verify facts in the following questions. The final answer to a question should always be either Yes or No, and
NOTHING ELSE.
{Knowledge}
Question: Is it true that Belgrade Race is an annual men’s footrace of around 6 kilometres (5834 metres) that is held in Belgrade,
Serbia through history, past winners includes Brahim Lahlafi (1st edition), Philip Mosima (3rd) and Josphat Menjo (6th)?
Let’s think step by step.
I need to verify facts in the question. The Belgrade Race Through History is an annual men’s footrace of around 6 kilometres
(5834 metres) that is held in Belgrade, Serbia. In 1996 Brahim Lahlafi was the winner of the competition. Philip Mosima won
the competition in 1998, and beat Marathon world record holder Paul Tergat. Josphat Menjo also won the competition and broke
the meet record. Therefore, past winners include Brahim Lahlafi, Philip Mosima and Josphat Menjo. All facts are verified.
So the answer is Yes
{Knowledge}
Question: Is it true that Based on the same platform as the Chevrolet Sail, the Baojun 310 was launched on 2017 Beijing Auto
Show where the price ranges from 36.800 yuan to 60.800 yuan?
Let’s think step by step.
I need to verify facts in the question. The Baojun 310 was indeed based on the same platform as the Chevrolet Sail. The Baojun
310 was launched on 2016 Beijing Auto Show, not 2017 Beijing Auto Show.
So the answer is No
{Knowledge}
Question: Is it true that Florida International University pedestrian bridge collapse was funded with a $19.4 million Transportation
Investment Generating Economic Recovery grant from the United States Department of Transportation in 2013, along with state
agencies and the bridge cost $14.2 million to construct?
Let’s think step by step.
I need to verify facts in the question. Florida International University pedestrian bridge was a $14.2 million project funded with a
$19.4 million Transportation Investment Generating Economic Recovery (TIGER) grant from the United States Department of
Transportation in 2013, along with state agencies. The Bridge cost $8 million to construct, not $14.2 million.
So the answer is No
Table 28: 3-Shot Demonstrations for ITER -RETGEN on Feverous.
{Knowledge}
Question: Is it common to see frost during some college commencements?
The answer is Yes
{Knowledge}
Question: Could a llama birth twice during War in Vietnam (1945-46)?
The answer is No
{Knowledge}
Question: Would Richard Dawkins hypothetically refuse an offering of the Last rites?
The answer is Yes
Table 29: 3-Shot Demonstrations for Direct Prompting on StrategyQA.
9272Given the following question, answer it by providing follow up questions and intermediate answers. For each follow up question,
you are given a context which is the top returned Wikipedia snippets for the question. If no follow up questions are necessary,
answer the question directly. The final answer should always be either Yes or No, and NOTHING ELSE.
#
Question: Is it common to see frost during some college commencements?
Are follow up questions needed here: Yes.
Follow up: What seasons can you expect to see frost?
{Knowledge}
Intermediate answer: Frost is common during the winter.
Follow up: When is college commencement?
{Knowledge}
Intermediate answer: College commencement ceremonies often happen during the months of December, May, June.
Follow up: Do any of the months December, May, June occur during the Winter?
{Knowledge}
Intermediate answer: December is in the winter.
So the final answer is: Yes
#
Question: Could a llama birth twice during War in Vietnam (1945-46)?
Are follow up questions needed here: Yes.
Follow up: How long did the vietnam war (1945-1946) last?
{Knowledge}
Intermediate answer: The War in Vietnam (1945-46) lasted around 6 months.
Follow up: How long is the llama gestation period?
{Knowledge}
Intermediate answer: The gestation period for a llama is 11.5 months.
Follow up: What is 2 times 11.5?
{Knowledge}
Intermediate answer: 23, which is longer than 6.
So the final answer is: No
#
Question: Would Richard Dawkins hypothetically refuse an offering of the Last rites?
Are follow up questions needed here: Yes.
Follow up: What are the last Rites?
{Knowledge}
Intermediate answer: The Last rites, in Catholicism, are the last prayers and ministrations given to an individual of the faith,
when possible, shortly before death.
Follow up: What are Richard Dawkins religious beliefs?
{Knowledge}
Intermediate answer: Richard Dawkins is known as an outspoken atheist, well known for his criticism of creationism and
intelligent design.
Follow up: Would an atheist participate in Catholics prayers?
{Knowledge}
Intermediate answer: It is unlikely that an atheist would participate in Catholics prayers.
So the final answer is: Yes
Table 30: 3-Shot Demonstrations for ReAct on StrategyQA.
9273Given the following question, answer it by providing follow up questions and intermediate answers. For each follow up question,
you are given a context which is the top returned Wikipedia snippets for the question. If no follow up questions are necessary,
answer the question directly. The final answer should always be either Yes or No, and NOTHING ELSE.
#
{Knowledge}
Question: Is it common to see frost during some college commencements?
Are follow up questions needed here: Yes.
Follow up: What seasons can you expect to see frost?
Intermediate answer: Frost is common during the winter.
Follow up: When is college commencement?
Intermediate answer: College commencement ceremonies often happen during the months of December, May, June.
Follow up: Do any of the months December, May, June occur during the Winter?
Intermediate answer: December is in the winter.
So the final answer is: Yes
#
{Knowledge}
Question: Could a llama birth twice during War in Vietnam (1945-46)?
Are follow up questions needed here: Yes.
Follow up: How long did the vietnam war (1945-1946) last?
Intermediate answer: The War in Vietnam (1945-46) lasted around 6 months.
Follow up: How long is the llama gestation period?
Intermediate answer: The gestation period for a llama is 11.5 months.
Follow up: What is 2 times 11.5?
Intermediate answer: 23, which is longer than 6.
So the final answer is: No
#
{Knowledge}
Question: Would Richard Dawkins hypothetically refuse an offering of the Last rites?
Are follow up questions needed here: Yes.
Follow up: What are the last Rites?
Intermediate answer: The Last rites, in Catholicism, are the last prayers and ministrations given to an individual of the faith,
when possible, shortly before death.
Follow up: What are Richard Dawkins religious beliefs?
Intermediate answer: Richard Dawkins is known as an outspoken atheist, well known for his criticism of creationism and
intelligent design.
Follow up: Would an atheist participate in Catholics prayers?
Intermediate answer: It is unlikely that an atheist would participate in Catholics prayers.
So the final answer is: Yes
Table 31: 3-Shot Demonstrations for Self-Ask on StrategyQA.
You are required to answer the following questions. The final answer to a question should always be either Yes or No, and
NOTHING ELSE.
{Knowledge}
Question: Is it common to see frost during some college commencements?
Let’s think step by step.
College commencement ceremonies often happen during the months of December, May, and sometimes June. Frost isn’t
uncommon to see during the month of December, as it is the winter.
So the answer is Yes
{Knowledge}
Question: Could a llama birth twice during War in Vietnam (1945-46)?
Let’s think step by step.
The War in Vietnam (1945-46) lasted around 6 months. The gestation period for a llama is 11 months. If a llama birth twice, the
minimum time needed is 2 times 11 months, which is 22 months, longer than 6 months.
So the answer is No
{Knowledge}
Question: Would Richard Dawkins hypothetically refuse an offering of the Last rites?
Let’s think step by step.
Richard Dawkins is known as an outspoken atheist, well known for his criticism of creationism and intelligent design. The Last
rites, in Catholicism, are the last prayers and ministrations given to an individual of the faith, when possible, shortly before death.
It is unlikely that an atheist would participate in Catholics prayers.
So the answer is Yes
Table 32: 3-Shot Demonstrations for ITER -RETGEN on StrategyQA.
9274

Published as a conference paper at ICLR 2023
GENERATE RATHER THAN RETRIEVE : L ARGE LANGU -
AGE MODELS ARE STRONG CONTEXT GENERATORS
Wenhao Yu1∗, Dan Iter2, Shuohang Wang2, Yichong Xu2, Mingxuan Ju1,
Soumya Sanyal3∗, Chenguang Zhu2, Michael Zeng2, Meng Jiang1
1University of Notre Dame 2Microsoft Cognitive Service Research
3University of Southern California
1wyu1@nd.edu; 2iterdan@microsoft.com
ABSTRACT
Knowledge-intensive tasks, such as open-domain question answering (QA), require
access to a large amount of world or domain knowledge. A common approach
for knowledge-intensive tasks is to employ a retrieve-then-read pipeline that ﬁrst
retrieves a handful of relevant contextual documents from an external corpus such
as Wikipedia and then predicts an answer conditioned on the retrieved documents.
In this paper, we present a novel perspective for solving knowledge-intensive tasks
by replacing document retrievers with large language model generators. We call
our method generate-then-read (GENREAD ), which ﬁrst prompts a large language
model to generate contextual documents based on a given question, and then
reads the generated documents to produce the ﬁnal answer. Furthermore, we
propose a novel clustering-based prompting method that selects distinct prompts,
in order to generate diverse documents that cover different perspectives, leading to
better recall over acceptable answers. We conduct extensive experiments on three
different knowledge-intensive tasks, including open-domain QA, fact checking, and
dialogue system. Notably, GENREAD achieves 71.6 and 54.4 exact match scores on
TriviaQA and WebQ, signiﬁcantly outperforming the state-of-the-art retrieve-then-
read pipeline DPR-FiD by +4.0 and +3.9, without retrieving any documents from
any external knowledge source. Lastly, we demonstrate the model performance can
be further improved by combining retrieval and generation. Our code and generated
documents can be found at https://github.com/wyu97/GenRead.
1 I NTRODUCTION
Knowledge-intensive tasks, such as open-domain question answering (QA) and fact checking, require
access to a large amount of world or domain knowledge (Petroni et al., 2021). These tasks are
even challenging for humans without access to an external knowledge source such as Wikipedia.
A common thread of existing methods for knowledge-intensive tasks employ a retrieve-then-read
pipeline that ﬁrst retrieves a handful of relevant contextual documents from Wikipedia and then
conditions the prediction of the answer on these documents along with the question (Karpukhin et al.,
2020; Lewis et al., 2020; Izacard & Grave, 2021). Nevertheless, these methods mainly suffer from
three drawbacks. First, candidate documents for retrieval are chunked (e.g., 100 words) and ﬁxed, so
the retrieved documents might contain noisy information that is irrelevant to the question. Second, the
representations of questions and documents are typically obtained independently in modern two-tower
dense retrieval models (Karpukhin et al., 2020), leading to only shallow interactions captured between
them (Khattab et al., 2021). Third, document retrieval over a large corpus requires the retriever model
to ﬁrst encode all candidate documents and store representations for each document. These two
operations limit the parameters of dense retrievers and the size of embedding vectors, and thus cannot
enjoy the world knowledge or deduction capabilities of large language models (Levine et al., 2022).
§ Unless otherwise speciﬁed, we use the text-davinci-002 version of InstructGPT in our experiments.
* Work done during internship at Microsoft Cognitive Service Research group.
1Published as a conference paper at ICLR 2023
In this paper, we propose to leverage large language models, such as InstructGPT (Ouyang et al.,
2022), to directly generate contextual documents for a given question, instead of retrieving relevant
documents from an external corpus, such as Wikipedia. Our approach has two main advantages. First,
we show that generated contextual documents contain the correct answer more often than the top
retrieved documents. We believe this is because large language models generate contextual documents
by performing deep token-level cross-attention between all the question and document contents,
resulting in generated documents that are more speciﬁc to the question than retrieved documents.
Second, we show that our approach signiﬁcantly outperforms directly generating answers from large
language models despite not incorporating any new external information. This is mainly because
the task of generating document-level contexts is close to the objective of causal language modeling
pre-training, so the world knowledge stored in the model parameters can be better utilized.
We show, on multiple datasets, that generated documents are more likely to contain correct answers
than the top retrieved documents. Notably, in dense retrieval methods, as more documents are
retrieved, the recall of documents containing the correct answer increases (Karpukhin et al., 2020).
However, the recall performance does not scale as well with generated documents because even with
sampling methods, generated documents tend to contain duplicate information. In order to improve
the recall performance of generated documents, we propose a novel clustering-based prompt method.
We synthesize a prompt with in-context demonstrations of question-document pairs sampled from
diverse clusters. These prompts result in generated documents that cover different perspectives of the
question and improve the scaling of performance as more documents are generated per question.
In contrast to the retrieve-then-read pipeline, our method is essentially a generate-then-read pipeline.
Speciﬁcally, it ﬁrst prompts a large language model to generate contextual documents based on a
given question, and then reads the generated document to produce the ﬁnal answer. The reader can
still be a large model (e.g., InstructGPT (Ouyang et al., 2022)) used under a zero-shot setting, or a
small one (e.g., FiD (Izacard & Grave, 2021)) ﬁne-tuned with generated documents on the training
split of the target dataset. We evaluate our proposed method on three different knowledge-intensive
tasks and demonstrate its effectiveness on both zero-shot and supervised settings.
Overall, our main contributions can be summarized as follows:
1. We propose a novel generate-then-read pipeline for solving knowledge-intensive tasks, i.e.,
replacing the process of retrieving documents from Wikipedia or searching for related documents on
Google, by prompting a large language model to generate relevant contextual documents.
2. We propose a novel clustering-based prompting approach to generate multiple diverse contextual
documents that increases the likelihood of covering the correct answer. We demonstrate this approach
can signiﬁcantly improve performance on end QA and other downstream tasks.
3. We conduct extensive experiments with three knowledge-intensive NLP tasks under both zero-
shot and supervised settings. Notably, our method can match or even outperform retrieve-then-read
pipeline methods, without retrieving any documents from any external knowledge source.
2 R ELATED WORK
KNOWLEDGE -INTENSIVE NLP VIA RETRIEVE -THEN -READ PIPELINE . Mainstream methods
for solving knowledge-intensive NLP tasks employ a retrieve-then-read model pipeline. Given a
question, this model ﬁrst leverages a retriever over a large evidence corpus (e.g. Wikipedia) to
fetch a set of relevant documents that may contain the answer. A reader is then used to peruse the
retrieved documents and predict an answer. Recent follow-up work has mainly focused on improving
the retriever (Karpukhin et al., 2020; Qu et al., 2021; Sachan et al., 2022) or the reader (Izacard
& Grave, 2021; Cheng et al., 2021; Yu et al., 2022), or training the system end-to-end (Lewis
et al., 2020; Singh et al., 2021). Early retrieval methods mainly employed sparse retrievers, such as
BM25 (Chen et al., 2017). Recently, ORQA (Lee et al., 2019) and DPR (Karpukhin et al., 2020) have
revolutionized the ﬁeld by utilizing dense contextualized vectors for document indexing, leading to
superior performance to traditional approaches. We propose an alternative approach which forgoes
retrieval, instead extracting the knowledge from the model parameters of a large language model.
We show that our approach is can be combine with dense retrievers to outperform both methods
independently. Our method can also be combined with any reader mechanism, allowing generated
context documents to be plugged into any current knowledge-intensive NLP pipelines.
GENERATOR AS RETRIEVER FOR OBTAINING CONTEXTUAL DOCUMENTS . Recent works have
investigated using auto-regressive language models to generate identiﬁer strings for documents, as an
2Published as a conference paper at ICLR 2023
intermediate target for retrievals, such as entity names (De Cao et al., 2020) or distinctive n-grams that
can be mapped to full passages (Bevilacqua et al., 2022). However, one needs to create the identiﬁers,
hence the structure was not thoroughly evaluated on a large-scale benchmark (Bevilacqua et al., 2022).
Other works have demonstrated that the knowledge stored in the parameters of pre-trained language
models could be “retrieved” to some extent by directly generating text (Petroni et al., 2019; Roberts
et al., 2020). However, the previous work only used generation for query expansion (Mao et al., 2021),
which did not exploit the potential of directly generating contextual documents for open-domain
questions. Different from the above approaches that aimed to train a generator model to produce
contextual document identiﬁers (which is still using the original Wikipedia text) or provide data
augmentation to retrievers, our work directly generates contextual documents for given questions.
NLP M ODELS ENHANCED BY LARGE LANGUAGE MODEL OUTPUTS . A line of recent work
has shown that relevant knowledge can be elicited from large language models, especially for those
domains that lack appropriate knowledge bases with sufﬁcient coverage (Liu et al., 2022b; Fang et al.,
2022). For example, Liu et al. (2022b) proposed leveraging GPT-3 to generate relevant contexts, then
providing the contexts as additional input when answering a commonsense question. Another line of
work focused on prompting a large language model to generate a series of intermediate reasoning
steps, often referred to as chain-of-thought (Wei et al., 2022b; Kojima et al., 2022; Li et al., 2022).
The prompt consists of an instruction (e.g., Let’s think step by step!), a few demonstrations that are
ﬁxed for each task, and a new-question placeholder. The demonstrations are human-written, and
each consists of a question in the style of the task and a series of intermediate reasoning steps that is
helpful for answering the question. Our work does not require any human annotation, but adds to this
line of work of leveraging model generated text to guide further generations. In our case, we apply
this approach to knowledge-intensive tasks, which have not been explored by previous work.
3 P ROPOSED METHOD
In this section, we present details of our proposed novel generate-then-read (GENREAD ) pipeline for
solving various knowledge-intensive tasks. Speciﬁcally, it ﬁrst prompts a large language model to
generate contextual documents with respect to a given query, then reads the generated documents
to predict the ﬁnal answer. The reader can either be a large model (e.g., InstructGPT) used for the
zero-shot setting, or a small one (e.g., FiD) ﬁne-tuned with generated documents on the training split
of the target dataset. We introduce the zero-shot setting in §3.1 and supervised setting in §3.2.
3.1 Z ERO -SHOT SETTING
Under the zero-shot setting, there is no training data – neither questions nor contextual documents.
When tested on the open-domain QA task, most existing large language models directly encode the
given question and predict the answer (Brown et al., 2020; Du et al., 2022; Chowdhery et al., 2022).
Speciﬁcally, the question q, associated with some text prompt, is input to the model, which then
generates the answer, denoted as p(a|q,θ), where θrepresents the pre-trained model parameters. In
practice, the maximum a posteriori estimation (MAP) is the ﬁnal answer, i.e.,ˆa= arg maxa p(a|q,θ).
However, this way of directly asking large language models to output answers often leads to poor
performance, as it leaves a considerable amount of additional world knowledge unexploited (Levine
et al., 2022). On the contrary, the zero-shot retrieve-then-read pipeline ﬁrst uses an off-the-shelf
retriever to fetch relevant documents from an external knowledge source such as Wikipedia, then
asks the large language model to read the documents and predict the answer.
In this work, we improve the performance by introducing an additional auxiliary generated document
variable d, and then extend the model to have the form p(a|q) =∑
i p(a|di,q)p(di|q). In practice,
we cannot sum over all possible documents d. Therefore, the most common approach is to compute
the MAP estimate ˆd = arg max ˆp(d) using beam search, and then to approximate the sum over d
with this single value. This two step approach, we label it as a generate-then-read pipeline.
STEP 1: G ENERATE . In this step, we ﬁrst prompt a large language model (e.g., InstructGPT (Ouyang
et al., 2022)) to generate documents based on the given question. For example, the input to the
language model could be “Generate a background document to answer the given question. {question
placeholder}”. We can use any decoding strategy (e.g., greedy decoding, beam search), but we used
greedy decoding throughout the zero-shot experiments for simplicity and reproducibility.
3Published as a conference paper at ICLR 2023
What does Monsanto own? (WebQ) 
QuestionDocumentCluster𝑞 𝑑 …𝑞!" 𝑑!" 𝑐… … …𝑞!# 𝑑!# 𝑐… … …
Step1:Getonedocument𝑑foreachquestion𝑞viaretrievalorgeneration.
Step2:Getembeddings, andcluster them by K-means.
Initial 𝑑:Monsanto is a multinational agrochemical and agricultural biotechnology corporation…It is one of the world’s leading producers of roundup, a gly-phosateherbicide.(63words)Step3:Givenquestion 𝑞fortrainingorinference,foreachcluster𝑐∈1…𝑘:•sample𝑞!#,𝑑!#,𝑗=1…𝑛,whoseclusteridis𝑐;•createprompt𝑝!=“𝑞!";𝑑!";…;…;𝑞!$;𝑑!$”;•generatedocument𝑑!with𝑝!usinga large language model.Usinga reader (e.g., FiD),with𝑞andthediversedocuments{𝑑",𝑑%,…,𝑑&},findanswers𝑎.
•agriculturalchemicals•seed (also correct)
Generated𝑑!:Monsanto Company is an Ame-rican multinational agrochemical and agricultural biotechnology corporation …It is a leading producer of genetically engi-neered seedand … (70 words)
Generated𝑑":Monsanto is a multinational agricultural biotechnology corporation. … The company also manufactures other agricultural chemicals, such as insecticides … (36 words) 
Figure 1: An overall framework of clustering-based prompting method. It leverages distinct question-
document pairs sampled from each embedding cluster as in-context demonstrations to prompt a large
language model to generate diverse documents, then read the documents to predict an answer.
STEP 2: R EAD . In the second step, we use generated sentence ˆdalong with the input question to
produce the ﬁnal answer from the large language model. This is actually the same setting as “zero-
shot” reading comprehension, as widely studied in existing works (Brown et al., 2020; Lazaridou
et al., 2022). We choose appropriate prompts from P3 (Bach et al., 2022), such as “Refer to the
passage below and answer the following question. Passage: {background placeholder}Question:
{question placeholder}”. Finally, the language model is fed the prompted text to generate an answer.
3.2 S UPERVISED SETTING
Although large language models demonstrate impressive performance on zero-shot learning abilities,
their performance still lag behind the supervised setting. Therefore, we also explore how the generated
documents from large language models can beneﬁt the supervised setting. As directly ﬁne-tuning
large language models on downstream datasets could be prohibitively expensive, we leverage a small
reader model such as FiD to peruse the generated documents under the supervised setting.
Under the supervised setting, scaling the size of retrieved documents can lead to better perfor-
mance (Karpukhin et al., 2020; Izacard & Grave, 2021). This is mainly because retrieving more
documents can cover more relevant information and knowledge, i.e., a higher recall score. Nev-
ertheless, asking a large language model to generate multiple high-quality contextual documents
is a challenging task. Dense retrieval methods can fetch multiple documents covering different
perspectives of the answer. Compared to dense retrievers, simply prompting a large language model
to generate multiple contextual documents often leads to low knowledge coverage, since the contents
generated by multiple decoding passes from the same input tend to be similar. Sampling decoding
methods, such as nucleus sampling1 (Holtzman et al., 2020) can diversify the generation process to
some extent, but the knowledge content of generated texts still tends to be highly repetitive when
used to generate documents for a given question. We further propose two novel solutions, including
diverse human prompts and clustering-based prompts, which will be elaborated on in this section.
3.2.1 D IVERSE HUMAN PROMPTS
In order to avoid similar token distributions under a single prompt, we ask human annotators to
provide different prompts, in order to make the generated document diverse. This method is simple,
but can effectively vary the token distribution during generation. In the experiments, we empirically
found this method can bring improvement to the retrieval performance (Figure 2). However, this
method suffers from two drawbacks. On one hand, it requires human annotators to write different
prompts, which cannot be easily generalized to different knowledge-intensive tasks. On the other
hand, different large language models might be sensitive to different prompt words, which might
cause a set of good prompt words not work on a different large language model.
3.2.2 C LUSTERING -BASED PROMPTS
To increase knowledge coverage in generated documents, we propose a novel clustering-based prompt
method. It ﬁrst clusters the representations of a set of documents into Kclasses (K = 2in Figure
1We treated nucleus sampling as a baseline to generate multiple documents, in which we set p = .95.
4Published as a conference paper at ICLR 2023
Models Open-domain QA Fact Checking Dialogue System
NQ TriviaQA WebQ FEVER FM2 WoW (F1 / R-L)
*with retriever, AND directly trained on these datasets
DPR + InstructGPT* 29.1 53.8 20.2 79.8 65.9 15.4 13.7
*with retriever, BUT NOT trained on these datasets
BM25 + InstructGPT 19.7 52.2 15.8 78.7 65.2 15.7 13.7
Contriever + InstructGPT 18.0 51.3 16.6 80.4 66.6 15.5 14.0
Google + InstructGPT 28.8 58.8 20.4 82.9 66.0 14.8 13.2
*without retriever, and not using external documents
Previous SoTA methods 24.71 56.72 19.01 - - - -
InstructGPT (no docs.) 20.9 57.5 18.6 77.6 59.4 15.4 13.8
GENREAD (InstructGPT) 28.0 59.0 24.6 80.4 65.5 15.8 14.2
Table 1: Zero-shot open-domain QA performance. Our proposed GENREAD with the InstructGPT
reader (named GENREAD (InstructGPT)) can signiﬁcantly outperform the original InstructGPT,
achieving new state-of-the-art performance on three open-domain QA benchmarks (previous SoTA:
1GLaM (Du et al., 2022), 2FLAN (Wei et al., 2021)) under this setting without using any external
document. Our GENREAD can achieve comparable or even better performance than zero-shot
retrieve-then-read models that use a retriever or search engine to ﬁrst obtain contextual documents.
To ensure reproducibility, we use greedy search in decoding. All prompts used are shown in the §B.1.
1), where the number of classes is equal to the number of documents that need to be generated
in the end. Next, it randomly selects nquestion-document pairs (n = 5 in Figure 1) from each
cluster. Lastly, a large language model presents the different nquestion-document pairs as in-context
demonstrations for generating documents to a given question. In this way, large language models
are based on different distributions of examples, hence resulting in generated documents covering
different perspectives. We show this in Figure 1 and illustrate the details of each step as follows.
STEP 1: G ET ONE INITIAL DOCUMENT PER QUESTION . Similar to the zero-shot setting, we ﬁrst
ask a large language model to generate one contextual document dfor each question q∈Q, where Q
is the set of questions in the training split. Alternatively, we can use an unsupervised retriever (e.g.,
BM25) to obtain a document from Wikipedia. We now have a question-document pair set{qi,di}|Q|
i=1.
STEP 2: E NCODE EACH DOCUMENT , DO K-MEANS CLUSTERING . We then use a large language
model (i.e., GPT-3) to encode each question-document pair, i.e., ei = GPT-3([qi,di]), resulting in a
12,288-dimensional vector per document. Then, we use K-means to cluster all embedding vectors
{ei}|Q|
i=1 into Ksets, so each question-document pair is assigned a unique cluster id c∈{1,...,K}.
We vary the number of Kin the experiments, which will be illustrated in Figure 2.
STEP 3: S AMPLE AND GENERATE K DOCUMENTS . Lastly, we sample nquestion-document pairs
from each cluster c, denoted as {qc1,dc1; qc2,dc2; ...; qcn,dcn}, in which nis a hyperparameter 2.
Then, the nsampled question-document pairs from the same cluster serve as in-context demonstrations
for the large language model to generate a contextual document. For example, the input to the
large language model could be “ {qc1 placeholder}{dc1 placeholder}... {qcn placeholder}{dcn
placeholder}{input question placeholder}”. By enumerating the sampled documents in these K
clusters, we can ﬁnally get K-generated documents. By conditioning on different sampled in-context
demonstrations collected from different clusters, the large language model has been biased for
different perspectives. Although these different perspectives exist in a latent manner, we empirically
show it works well in practice, by comparing it with sampling methods, diverse human prompts
(Figure 2 and Table 2) and randomly sampling npairs from the entire dataset (Table 11).
4 E XPERIMENTS
In this section, we conduct comprehensive experiments on three knowledge-intensive NLP tasks,
including open-domain QA (NQ (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017) and
WebQ (Berant et al., 2013)), fact checking (FEVER (Thorne et al., 2018) and FM2 (Eisenschlos
et al., 2021)) and open-domain dialogue system (WoW (Dinan et al., 2019)). More detailed dataset
2In the experiments, we set n = 5and found increasing n does not bring extra improvement.
5Published as a conference paper at ICLR 2023
505866748290
1 5 101520
Recall@K 
TriviaQATopK documents
Google searchDPR-MultiOurs (sampling)Ours (human)Ours (clustering)
404856647280
1 5 101520
Recall@K 
WebQ TopK documents
Google searchDPR-MultiOurs (sampling)Ours (human)Ours (clustering)
Figure 2: Recall@K on test sets, measured as the percentage of top-K documents that contain the
answer. Our proposed clustering-based prompting method can outperform DPR and Google search,
also two variants of using LLMs to generate documents. Exact numbers are reported in Table 5.
information can be found in Appendix A.1. To evaluate the model performance, we use exact
match (EM) score for evaluating open-domain QA (Zhu et al., 2021). An answer is considered
correct if and only if its normalized form has a match in the acceptable answer list. We also employ
Recall@K (R@K) as an intermediate evaluation metric, measured as the percentage of top-K retrieved
or generated documents that contain the answer. This metric is commonly used in evaluations of
previous works (Karpukhin et al., 2020; Izacard & Grave, 2020; Sachan et al., 2022). For other
knowledge-intensive tasks, we follow the KILT benchmark (Petroni et al., 2021) to use accuracy
(ACC) for fact checking and F1 / Rouge-L (R-L) score for open-domain dialogue system.
4.1 Z ERO -SHOT SETTING EXPERIMENTS
We ﬁrst compare our proposed GENREAD approach with various large language models proposed
in recent years, including GPT-3 (Brown et al., 2020), Gopher (Rae et al., 2021), FLAN (Wei et al.,
2021), GLaM (Du et al., 2022), Chinchilla (Hoffmann et al., 2022), PaLM (Chowdhery et al., 2022)
and InstructGPT (Ouyang et al., 2022). Due to the space limitation, we only put the best performance
on each dataset in Table 1, in which the line is called previous SoTA methods. In addition, their
corresponding model parameters and performance are listed in Table 9 in Appendix. All of these
baseline methods use the same input formats, i.e., [prompt words; question].
GENREAD is based on InstructGPT with 175B parameters. In order to fully evaluate the effec-
tiveness of our proposed method, we also compare with InstructGPT augmented with retrieved
documents from Wikipedia or Google search. The baseline methods (1) BM25 / Contriever + In-
structGPT; (2) Google + InstructGPT; (3) DPR + InstructGPT have the same input format as our
GENREAD , i.e., [prompt words; contextual document; question]. BM25 is a traditional sparse
retrieval method. Contriever (Izacard et al., 2022a) is a state-of-the-art unsupervised dense retrieval
model. DPR (Karpukhin et al., 2020) is a supervised dense retrieval model directly trained on NQ,
TriviaQA and WebQ datasets. We note that comparing with above three methods is challenging
because our method only relies on the large language model itself, without using any external corpus.
4.1.1 E XPERIMENTAL RESULTS
In the experiments, we use InstructGPT as our backbone model. As shown in Table 1, compared
with state-of-the-art large language models, our proposed GENREAD with the InstructGPT reader
improves its performance by generating contextual documents and conditioning on the generated
documents, even though no new data is introduced, and the generator and reader have the exact
same parameters. Speciﬁcally, GENREAD can improve the EM score by +6.9 on three open-domain
QA benchmarks, compared to the original InstructGPT. We also make a similar observation on fact
checking and open-domain dialogue system. Our proposed GENREAD can consistently outperform
the baseline InstructGPT model without retrieving any contextual documents.
To further validate the effectiveness ofGENREAD , we compare against zero-shot retrieve-then-read
pipeline models, which ﬁrst use a retrieval model or the Google search engine to get a relevant
contextual document, then use InstructGPT to read the texts and produce the ﬁnal answer. As shown
in Table 1, GENREAD can achieve on-par performance with zero-shot retrieve-then-read pipeline
models on the NQ and FM2 datasets, and outperform them on all other benchmarks. The knowledge
6Published as a conference paper at ICLR 2023
Models # reader # docu- TriviaQA WebQ NQ Avg.parameters ments open test open test open test
*baselines with retrieving from Wikipedia; all numbers reported by existing papers
DPR (Karpukhin et al., 2020) 110M 100 56.8 41.1 41.5 46.5
RAG (Lewis et al., 2020) 400M 10 56.1 45.2 44.5 48.6
FiD (Izacard & Grave, 2021) 770M 100 67.6 50.5 51.4 56.5
*baselines with retrieving from Wikipedia or Google; all numbers from our experiments
FiD-l (DPR, Wikipedia) 770M 10 61.9 48.1 46.7 52.2
FiD-xl (DPR, Wikipedia) 3B 10 66.3 50.8 50.1 55.7
FiD-xl (Google search) 3B 10 70.1 53.6 45.0 56.2
*our proposed method by leveraging a large language model to generate documents
GENREAD (FiD-l) (sampling) 770M 10 67.8 51.5 40.3 53.2
GENREAD (FiD-l) (clustering) 770M 10 70.2 53.3 43.5 55.6
GENREAD (FiD-xl) (sampling) 3B 10 69.6 52.6 42.6 54.9
GENREAD (FiD-xl) (clustering) 3B 10 71.6 54.4 45.6 57.1
⊢merge retrieved documents with generated documents 74.3 56.2 54.0 61.5
Table 2: Supervised open-domain QA performance. By only using generated documents from In-
structGPT, our GENREAD with FiD reader (named GENREAD (FiD)) can achieve better performance
than baseline methods on TriviaQA and WebQ. Through our detailed analysis of NQ, we found the
performance gap mainly due to the temporality issue, which will be elaborated in §A.8.
learned by the large language models can be retrieved via autoregressive text generation. Without
seeing any examples from these datasets, GENREAD can outperform using the supervised retrieval
model (i.e., DPR) to recover relevant contextual documents.
4.2 S UPERVISED SETTING EXPERIMENTS
We compare our proposed GENREAD with retrieve-then-read models, including DPR (Karpukhin
et al., 2020), RAG (Lewis et al., 2020), and FiD (Izacard & Grave, 2021). In addition, we compared
with obtaining relevant documents from the internet using the Google search engine.
4.2.1 E XPERIMENTAL SETUP
For our proposed method, we replace the retriever with a large language model to directly generate
contextual documents. In the experiments, we use InstructGPT (Ouyang et al., 2022). After contextual
documents are retrieved or generated, we employ a FiD reader with 770M parameter models (i.e.,
FiD-l) and 3B parameter models (i.e., FiD-xl) that are ﬁne-tuned on the training split of target datasets.
We note that we only use 10 documents during reading for the following reasons.
Why do we choose to use only 10 documents instead of 100 when reading?
As noted in Section 6.2 in DPR (Karpukhin et al., 2020) and Figure 3 in FiD (Izacard & Grave, 2021),
increasing the number of documents can lead to better model performance and achieve state-of-the-art
when using 100 documents. However, there are two major drawbacks to using 100 documents during
the reading step. First, the operation is very expensive, leading to a signiﬁcant increase in memory
consumption and training time. As reported by Izacard & Grave (2021), the training process requires
64 Tesla V100 32GB running for around one day. Second, generating documents by using a large
language model is slow and expensive, so only using 10 documents can be a signiﬁcant cost saving
in our method. Therefore, in our experiments, we choose to use 10 documents during the reading
process. When using FiD-770M (i.e., FiD-large), the training process can be easily performed even
on a single Tesla V100 32GB GPU. Meanwhile, when only using 10 documents, we can also increase
the size of FiD model from 770M to 3B, which takes about the same amount of GPU memory as
using 100 documents on a 770M model, but at the same time signiﬁcantly shortens the training time.
We note that training T5-3B model needs a bigger cluster such as 8 Tesla V100 or A100 GPUs.
4.2.2 E XPERIMENTAL RESULTS ON OPEN -DOMAIN QA
We ﬁrst use Recall@K to compare the retrieval accuracy of different models. As shown in Figure 2,
GENREAD can signiﬁcantly outperform DPR and Google search for under 10 retrieved or generated
7Published as a conference paper at ICLR 2023
4748495051525354
2040 70100
EMscore
NQTopKdocuments
DPR only (K documents)DPR (K-10) + LLM (10)
6062646668707274
2040 70100
EMscore
TriviaQATopKdocuments
DPR only (K documents)DPR (K-10) + LLM (10)
4748495051525354
2040 70100
EMscore
WebQTopKdocuments
DPR only (K documents)DPR (K-10) + LLM (10)
Figure 3: Combining DPR retrieved documents and large language model (LLM) generated docu-
ments can achieve signiﬁcantly better performance than using DPR retrieved documents only. For a
fair comparison, instead of adding LLM generated documents to the model, we replace 10 documents
retrieved by DPR with 10 documents generated by LLM so the total number of documents is the same.
In this experiment, we use FiD-l (i.e., FiD-large) as the reader model because when the documents
scale to more than 20, FiD-xl (i.e., FiD-3B) causes out-of-memory issues on A100 GPUs.
documents. Compared to different GENREAD variants, including nucleus sampling, human written
prompts, and clustering-based prompts, clustering-based prompts achieve the best performance. At
the same time, we notice that the language model inevitably has the problem that the slope of the
curve decreases as the number of generated documents increases. On one hand, this is due to the
similarity of token distributions when large language models generate multiple documents. On the
other hand, due to the shallow interaction characteristics of the dense retrieval model itself, the
retrieved documents might not be completely relevant to the given question, so that the increase in
recall might come from false positive documents, as also mentioned by Sachan et al. (2022).
As shown in Table 2, we can ﬁrst observe the FiD model performs the best among all baseline models.
Using FiD-xl with only 10 documents achieves comparable performance with using FiD-l with 100
documents. The average gap is less than 1% on three benchmarks. Compared with both close-book
models and Wikipedia-based retrieve-then-read pipelines, our proposed GENREAD can achieve
state-of-the-art performance. Furthermore, compared with using sampling methods to generate
documents, the clustering-based prompt method can improve the EM score by +2.2 on average. This
indicates that the clustering-based prompt method is effectively increasing the knowledge coverage
of generated documents, and also leading to better downstream QA performance. We also show
that GENREAD can outperform Google search on all benchmarks. We observe both our method and
Google search perform worse than DPR, mainly due to the signiﬁcant portion of time-dependent
questions in the dataset, which is described in the following analysis.
4.2.3 E XPERIMENTAL RESULTS ON OTHER TASKS
Models FEVER FM2 WoW
Acc. Acc. F1 / R-L
RAG (Lewis et al., 2020) 86.3 71.1 13.1 / 11.6
FiD (Izacard & Grave, 2021) 90.2 77.6 17.5 / 16.1
GENREAD (FiD-xl) (sampling) 89.0 76.3 18.9 / 16.7
GENREAD (FiD-xl) (clustering) 89.6 77.8 19.1 / 16.8
⊢merge two source docs. 91.8 78.9 20.1 / 17.9
Table 3: Supervised performance on fact checking (FEVER
and FM2) and open-domain dialogue system (WoW).
We demonstrate the experimental re-
sults in Table 3. Under the supervised
setting, GENREAD can achieve on
par performance on the fact checking
task and superior performance on the
dialogue system task, indicating that
large language model can be seen as
a strong knowledge generator.
The main reason that GENREAD per-
forms worse than the dense retriever
for fact checking is that the task provides sufﬁcient semantic information to reach strong performance
on this binary decision task. So, there is a smaller semantic gap between the given factual statement
and contextual documents than that of question and document pairs in open-domain QA, which is an
easier retrieval setting for modern dense retrieval methods that are mainly based on vector similarity.
4.3 O BSERVATIONS AND EXPERIMENTAL ANALYSIS
4.3.1 C OMPLEMENTARITY OF GENERATED AND RETRIEVED DOCUMENTS
Generated documents can be combined with retrieved documents to outperform both. Even with a very
large number of retrieved documents, including few samples of generated knowledge leads to large
8Published as a conference paper at ICLR 2023
improvements. As shown in Table 2, merging retrieved documents with generated documents can
achieve state-of-the-art performance compared to all baseline methods listed in the table. Speciﬁcally,
it can improve +5.7 averagely on three open-domain QA benchmarks compared to DPR alone, and
improve +4.4 averagely compared to the large language model alone.
4.3.2 C OVERAGE ANALYSIS OVER ALL POSSIBLE ANSWERS
The improvement in open-domain QA performance is due to the fact that correct answers are
included more frequently in the generated text Recall@K is the most commonly used met-
ric in existing works to measure the retrieval performance, which computes the percentage of
top-K retrieved or generated documents that contain any possible answer at least once. than
in the retrieved documents. However, as many questions contain multiple correct answers,
recall@K cannot fully reﬂect the diversity of generated or retrieved documents. Each ques-
tion in the WebQ has 2.39 correct answers, 1.79 correct answers in NQ and 14.02 (includ-
ing all entity alias) in the TriviaQA. NQ and WebQ do not include alias names in the labels.
Documents obtained by ↓ NQ TriviaQA WebQ
- w. alias w/o alias -
BM25 (Robertson et al., 2009) 48.4 17.1 63.8 41.2
Google search engine3 57.9 18.9 72.0 54.2
DPR (Karpukhin et al., 2020) 67.9 17.9 67.3 58.8
GENREAD (nucleus sampling) 56.6 19.6 74.5 59.8
GENREAD (10 human prompts) 57.4 20.1 74.8 61.1
GENREAD (clustering prompts) 61.7 20.4 76.5 62.1
Table 4: Answer coverage (%) over 10 retrieved or generated docu-
ments. Case studies are provided in Tables 16-19 in Appendix.
In this section, we also
demonstrate the answer
coverage performance of
different models in Table 5.
Answer coverage measures
the percentage of the num-
ber of answers that are con-
tained in the documents
over all possible answers.
Coverage analysis showed
that generated text tends
to have lower coverage
than retrieved documents
because generated documents tends to have little diversity compared to retrieved documents. To
improve coverage, we propose GENREAD with clustering, where we include examples in the prompt
from different clusters of the training data to elicit more diverse generations.
5 E PILOGUE
CONCLUSION . In this paper, we present a novel perspective for solving knowledge-intensive tasks by
replacing the dense retrieval models with large language model generators. We call it generate-then-
read, which ﬁrst prompts a large language model to generate contextual documents, then read the
generated document to infer the ﬁnal answer. Notably, without retrieving any documents, it reaches
71.6 and 54.4 exact match scores on TriviaQA and WebQ, signiﬁcantly outperforming the current
retrieval-reader model DPR-FiD, as well as on other two knowledge-intensive tasks.
LIMITATION AND FUTURE WORK . Despite the strong performance on the presented datasets, our
approach is limited in its ability to update knowledge state and adapt to new domains. A major feature
of retrieve-then-read is the ability to swap in new documents when new information is learned, such
as temporally more recent documents, or adding in documents from a new domain to quickly adapt to
a new downstream task. Our approach relies on a large language model to contain all this knowledge
and adding new knowledge would likely require some retraining. Future work will explore how
to efﬁciently incorporate new knowledge into our generate-then-read method. Besides, generated
documents might suffer from hallucination error, resulting in incorrect predictions. We demonstrated
case study in Table 15. Consideration in combination with recent approaches (Creswell & Shanahan,
2022) to boost generative faithfulness is a also direction worthy of future research.
ACKNOWLEDGEMENTS
This work was supported in part by NSF IIS-2119531, IIS-2137396, IIS-2142827, CCF-1901059, and
ONR N00014-22-1-2507. Wenhao is supported in part by Bloomberg Data Science Ph.D Fellowship.
We are grateful to the reviewers for their insightful suggestions that have improved the quality of our
paper. Their dedication to ensuring the completeness of our research is greatly appreciated.
9Published as a conference paper at ICLR 2023
ETHICS STATEMENT
Large language models have a wide range of beneﬁcial applications for society, but they also have
potentially harmful applications. Previous work has shown various forms of bias, such as racial
and gender bias, in large language models like GPT-3, even after explicit efforts to reduce toxic
language (Chan, 2022). The importance of addressing these societal harms is acknowledged by
OpenAI themselves in their 2020 paper introducing GPT-3 (Brown et al., 2020), which stated “we
focus on two primary issues: the potential for deliberate misuse of language models like GPT-3 ...
and issues of bias, fairness, and representation within models like GPT-3.” on page 34.
The goal of this paper is to utilize knowledge stored in the parameters of large language models
to answer open-domain questions and solve knowledge-intensive tasks. Unlike retrieve-then-read
where an external corpus can be curated to be trustworthy, the use of a model to generate contextual
documents may further permeate existing biases in common models. First, our work shows that
generated documents suffer from challenges of stale information from outdated documents used
for training. Second, we show that generated documents tend to be less diverse, potentially biasing
answers towards more common entities and terms from the training data. Finally, we conducted
experiments on only three large language models. It is possible that some of our conclusions or
observations may not necessarily hold for other models trained with different data or objectives.
Regarding ethical solutions, future work includes (i) further exploring potential bias and intentional
or unintentional harm that may result from using generated contextual documents; (ii) better aligning
language models with user intent to generate less biased contents and fewer fabricated facts.
REFERENCES
Stephen Bach, Victor Sanh, Zheng Xin Yong, Albert Webson, Colin Raffel, Nihal V Nayak, Abheesht
Sharma, Taewoon Kim, M Saiful Bari, Thibault F ´evry, et al. Promptsource: An integrated
development environment and repository for natural language prompts. In Proceedings of the 60th
Annual Meeting of the Association for Computational Linguistics: System Demonstrations , pp.
93–104, 2022.
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on freebase from
question-answer pairs. In EMNLP 2013, pp. 1533–1544, 2013.
Michele Bevilacqua, Giuseppe Ottaviano, Patrick Lewis, Wen-tau Yih, Sebastian Riedel, and Fabio
Petroni. Autoregressive search engines: Generating substrings as document identiﬁers. arXiv
preprint arXiv:2204.10628, 2022.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.
Anastasia Chan. Gpt-3 and instructgpt: technological dystopianism, utopianism, and “contextual”
perspectives in ai ethics and industry. AI and Ethics, pp. 1–12, 2022.
Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading wikipedia to answer open-
domain questions. In Proceedings of the 55th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pp. 1870–1879, 2017.
Hao Cheng, Yelong Shen, Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Unitedqa:
A hybrid approach for open domain question answering. InProceedings of the 59th Annual Meeting
of the Association for Computational Linguistics and the 11th International Joint Conference on
Natural Language Processing (Volume 1: Long Papers), pp. 3080–3090, 2021.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:
Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.
Antonia Creswell and Murray Shanahan. Faithful reasoning using large language models. arXiv
preprint arXiv:2208.14271, 2022.
10Published as a conference paper at ICLR 2023
Nicola De Cao, Gautier Izacard, Sebastian Riedel, and Fabio Petroni. Autoregressive entity retrieval.
In International Conference on Learning Representations, 2020.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers), pp. 4171–4186, 2019.
Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wizard of
wikipedia: Knowledge-powered conversational agents. In International Conference on Learning
Representations, 2019.
Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim
Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efﬁcient scaling of language
models with mixture-of-experts. In International Conference on Machine Learning, pp. 5547–5569.
PMLR, 2022.
Julian Eisenschlos, Bhuwan Dhingra, Jannis Bulian, Benjamin B¨orschinger, and Jordan Boyd-Graber.
Fool me twice: Entailment from wikipedia gamiﬁcation. In Proceedings of the 2021 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, pp. 352–365, 2021.
Yuwei Fang, Shuohang Wang, Yichong Xu, Ruochen Xu, Siqi Sun, Chenguang Zhu, and Michael
Zeng. Leveraging knowledge in multilingual commonsense reasoning. In Findings of the Associa-
tion for Computational Linguistics: ACL 2022, pp. 3237–3246, 2022.
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza
Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al.
Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022.
Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text
degeneration. In International Conference on Learning Representations, 2020.
Gautier Izacard and Edouard Grave. Distilling knowledge from reader to retriever for question
answering. In International Conference on Learning Representations, 2020.
Gautier Izacard and Edouard Grave. Leveraging passage retrieval with generative models for open
domain question answering. In EACL 2021, pp. 874–880, 2021.
Gautier Izacard, Mathild Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand
Joulin, and Edouard Grave. Unsupervised dense information retrieval with contrastive learning.
Transactions on Machine Learning Research (TMLR), 2022a.
Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane
Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Few-shot learning with
retrieval augmented language models. arXiv preprint arXiv:2208.03299, 2022b.
Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly
supervised challenge dataset for reading comprehension. In ACL 2017, pp. 1601–1611, 2017.
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott
Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models.
arXiv preprint arXiv:2001.08361, 2020.
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi
Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In
Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing
(EMNLP), pp. 6769–6781, 2020.
Omar Khattab, Christopher Potts, and Matei Zaharia. Relevance-guided supervision for openqa with
colbert. Transactions of the Association for Computational Linguistics, 9:929–944, 2021.
Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large
language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022.
11Published as a conference paper at ICLR 2023
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins, Ankur Parikh, Chris
Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: A
benchmark for question answering research. TACL 2019, pp. 452–466, 2019.
Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev. Internet-
augmented language models through few-shot prompting for open-domain question answering.
arXiv preprint arXiv:2203.05115, 2022.
Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised open
domain question answering. In Proceedings of the 57th Annual Meeting of the Association for
Computational Linguistics, pp. 6086–6096, 2019.
Yoav Levine, Itay Dalmedigos, Ori Ram, Yoel Zeldes, Daniel Jannai, Dor Muhlgay, Yoni Osin, Opher
Lieber, Barak Lenz, Shai Shalev-Shwartz, et al. Standing on the shoulders of giant frozen language
models. arXiv preprint arXiv:2204.10019, 2022.
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,
Heinrich K¨uttler, Mike Lewis, Wen-tau Yih, Tim Rockt¨aschel, et al. Retrieval-augmented genera-
tion for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:
9459–9474, 2020.
Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. On the
advance of making language models better reasoners. arXiv preprint arXiv:2206.02336, 2022.
Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin
Raffel. Few-shot parameter-efﬁcient ﬁne-tuning is better and cheaper than in-context learning.
arXiv preprint arXiv:2205.05638, 2022a.
Jiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Peter West, Ronan Le Bras, Yejin Choi, and
Hannaneh Hajishirzi. Generated knowledge prompting for commonsense reasoning. InProceedings
of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers), pp. 3154–3169, 2022b.
Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong Shen, Jianfeng Gao, Jiawei Han, and Weizhu
Chen. Generation-augmented retrieval for open-domain question answering. In Proceedings of the
59th Annual Meeting of the Association for Computational Linguistics and the 11th International
Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 4089–4100, 2021.
OpenAI. Codex (available via openai api for free). https://openai.com/blog/openai-codex/, 2022.
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong
Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow
instructions with human feedback. arXiv preprint arXiv:2203.02155, 2022.
Fabio Petroni, Tim Rockt¨aschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and
Alexander Miller. Language models as knowledge bases? In Proceedings of the 2019 Conference
on Empirical Methods in Natural Language Processing and the 9th International Joint Conference
on Natural Language Processing (EMNLP-IJCNLP), pp. 2463–2473, 2019.
Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James
Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, et al. Kilt: a benchmark for knowledge
intensive language tasks. In Proceedings of the 2021 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language Technologies, pp. 2523–2544,
2021.
Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua
Wu, and Haifeng Wang. Rocketqa: An optimized training approach to dense passage retrieval for
open-domain question answering. In Proceedings of the 2021 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies, pp.
5835–5847, 2021.
Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John
Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models:
Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446, 2021.
12Published as a conference paper at ICLR 2023
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a uniﬁed text-to-text
transformer. J. Mach. Learn. Res., 21(140):1–67, 2020.
Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the
parameters of a language model? In Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP), pp. 5418–5426, 2020.
Stephen Robertson, Hugo Zaragoza, et al. The probabilistic relevance framework: Bm25 and beyond.
Foundations and Trends® in Information Retrieval, 3(4):333–389, 2009.
Devendra Singh Sachan, Mike Lewis, Dani Yogatama, Luke Zettlemoyer, Joelle Pineau, and
Manzil Zaheer. Questions are all you need to train a dense passage retriever. arXiv preprint
arXiv:2206.10658, 2022.
Devendra Singh, Siva Reddy, Will Hamilton, Chris Dyer, and Dani Yogatama. End-to-end training of
multi-document reader and retriever for open-domain question answering. Advances in Neural
Information Processing Systems, 34:25968–25981, 2021.
James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. Fever: a large-
scale dataset for fact extraction and veriﬁcation. In Proceedings of the 2018 Conference of the
North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long Papers), pp. 809–819, 2018.
Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,
Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. InInternational
Conference on Learning Representations, 2021.
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama,
Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models.
arXiv preprint arXiv:2206.07682, 2022a.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny
Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint
arXiv:2201.11903, 2022b.
Donghan Yu, Chenguang Zhu, Yuwei Fang, Wenhao Yu, Shuohang Wang, Yichong Xu, Xiang Ren,
Yiming Yang, and Michael Zeng. Kg-ﬁd: Infusing knowledge graph in fusion-in-decoder for
open-domain question answering. In Proceedings of the 60th Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers), pp. 4961–4974, 2022.
Michael Zhang and Eunsol Choi. Situatedqa: Incorporating extra-linguistic contexts into qa. In
Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp.
7371–7387, 2021.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher
Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language
models. arXiv preprint arXiv:2205.01068, 2022.
Fengbin Zhu, Wenqiang Lei, Chao Wang, Jianming Zheng, Soujanya Poria, and Tat-Seng Chua.
Retrieving and reading: A comprehensive survey on open-domain question answering. arXiv
preprint arXiv:2101.00774, 2021.
13Published as a conference paper at ICLR 2023
A A PPENDIX
Datasets Splits Train Valid Test Test labels
TriviaQA (Joshi et al., 2017) open domain 78,785 8,837 11,313 public
wikipedia split 7,993 public
WebQ (Berant et al., 2013) open domain 3,478 300 2,032 public
NQ (Kwiatkowski et al., 2019) open domain 79,168 8,757 3,610 public
FEVER (Thorne et al., 2018) kilt challenge 104,966 10,444 10,100 hidden
FM2 (Eisenschlos et al., 2021) ofﬁcial split 10,149 1169 1380 public
WoW (Dinan et al., 2019) kilt challenge 63,734 3,054 2,944 hidden
Table 5: Datasets splits and statistics. For FEVER and WoW, labels in the test are hidden, so the
model performance should be evaluated at https://ai.facebook.com/tools/kilt/.
A.1 D ATASETS AND SPLITS
– TRIVIA QA (TQA) (Joshi et al., 2017) contains a set of trivia questions with answers that were
originally scraped from trivia and quiz-league websites.
– WEBQUESTIONS (WebQ) (Berant et al., 2013) consists of questions selected using Google Suggest
API, where the answers are entities in Freebase.
– NATURAL QUESTIONS (NQ) (Kwiatkowski et al., 2019) were mined from real Google search
queries and the answers are spans in Wikipedia articles identiﬁed by human annotators.
We explore the same train / dev / test splits for the open-domain QA setting as used by Izacard &
Grave (2021); Karpukhin et al. (2020). For TriviaQA, GPT-3 / GLaM / PaLM (Brown et al., 2020;
Du et al., 2022; Chowdhery et al., 2022) evaluate on the Wikipedia dev set of 7,993 examples, so we
ran an additional evaluation on that dev set in order to compare with their performance.
– FEVER (Thorne et al., 2018) is one of the largest datasets for fact checking that requires retrieving
evidence from external corpus to support if a statement is supported or refuted.
– FOOL ME TWICE (FM2) (Eisenschlos et al., 2021) is a challenging fact checking dataset collected by
gamiﬁcation. Players write challenging claims either entailed or refuted by evidence from Wikipedia.
They are then tasked to spot the refuted claim among a group.
– WIZARD OF WIKIPEDIA (WoW) (Dinan et al., 2019) is an open-domain dialogue task for training
agents that can converse knowledgeably about open-domain topics. One speaker in the conversation
must ground their utterances in a speciﬁc knowledge sentence from a Wikipedia page.
We use the same train / dev / test splits in KILT challenge (Petroni et al., 2021) for the FEVER
and WoW datasets. Their test labels are hidden, so the performance can only be evaluated through
https://ai.facebook.com/tools/kilt. For FM2, we use its ofﬁcial dataset splits.
A.2 I MPLEMENTATION DETAILS
We use T5-770M (Raffel et al., 2020) and T5-3B as our backbone models to implement FiD (Izacard
& Grave, 2021). We use AdamW as the optimizer, with 2,000 warm-up steps. We set the dropout
probability to 0.1 and weight decay to 0.01. We use one A100 for running T5-770M and set the batch
size of 16. We use 8 A100 for running T5-3B and set the per GPU batch as 2, leading to the total
batch size as 16. We searched different learning rates, ranging from 5e-6 to 4e-5, and we found 3e-5
to 6e-5 performed the best under the T5-3B setting and 5e-5 to 1e-4 performed the best under the
T5-770M setting. We refer to more individual implementation details in Table 6.
We implement other baseline methods by using repositories:
– BM25: https://github.com/castorini/pyserini
– DPR: https://github.com/facebookresearch/DPR
– Contriever: https://github.com/facebookresearch/contriever
14Published as a conference paper at ICLR 2023
Settings / Datasets NQ TriviaQA WebQ FEVER FM2 WoW
Peak learning rate 1e-4 1e-4 1e-4 1e-4 1e-4 5e-5
Total batch size 64 64 64 64 64 16
Total training steps 15,000 10,000 10,000 10,000 10,000 20,000
Best validation steps 6,000 500 8,500 5,000 6,000 20,000
Validation performance 43.27 69.47 60.33 88.97 73.57 18.60
Best validation ⇒test 43.50 70.22 53.33 87.25 74.21 18.49
Peak learning rate 5e-5 6e-5 3e-5 5e-5 5e-5 3e-5
Total batch size 16 16 16 16 16 8
Total training steps 20,000 15,000 15,000 15,000 15,000 20,000
Best validation steps 14,000 8,500 11,500 10,000 6,000 16,500
Validation performance 44.83 70.61 61.00 90.53 76.30 19.12
Best validation ⇒test 45.55 71.55 54.36 89.58 77.78 18.87
Table 6: Hyperparaters settings and validation performance for open-domain QA (numbers reported
in Table 2), fact checking and dialogue system (numbers reported in Table 3). The upper part numbers
are from GENREAD (FiD-l) and the lower part numbers are from GENREAD (FiD-xl).
A.3 R EPRODUCIBILITY VIA OPEN SOURCE LARGE LANGUAGE MODELS
Documents obtained by ↓ TriviaQA WebQ
DPR (Karpukhin et al., 2020) 66.3 50.8
OPT (Zhang et al., 2022) 62.1 51.8
InstructGPT (Ouyang et al., 2022) 71.3 54.5
Codex (OpenAI, 2022) 72.6 55.4
Table 7: Exact match (EM) score with using DPR and
different open-source large language models such as
OPT and Codex to generate contextual documents.
We note that reproducing experiments on
the OpenAI API, though publicly available,
costs money. For this reason, we further add
an evaluation on two open-source large lan-
guage models OPT (Zhang et al., 2022) and
Codex (OpenAI, 2022). As shown in Table
7, OPT performed worse than InstructGPT,
but still achieved comparable performance
with DPR; OpenAI Codex achieved the best
performance on both TriviaQA and WebQ.
A.4 S CALING WITH NUMBER OF LARGE LANGUAGE MODEL PARAMETERS
25
35
45
55
65
75
1e81e91e101e11
EM score
# GPT-3model parameters
(37.4) (40.3)(44.6)
(52.6)DPR-FiD(50.8)
(69.6)
(41.7)(29.8)(27.6) TriviaQAWebQ
DPR-FiD(66.3)
Figure 4: Model performance with different
size of InstructGPT as context generators.
Figure 4 shows the scaling of performance with In-
structGPT generator parameters, including Ada-150M,
Babbage-1.3B, Curie-6.7B and Davinci-175B. We note
that for both FiD and our GENREAD , we use the
FiD-xl with 10 input documents either retrieved from
Wikipedia or generated by InstructGPT. The perfor-
mance of both TriviaQA and WebQ continues to im-
prove as the generator model parameters increase, as
does the slope. Only with the largest size InstructGPT,
GENREAD can outperform the DPR-FiD. This indi-
cates using large language model to generate contextual
documents is an “emergent ability” of scaling, which
is not present in smaller models but is only present in
larger language models (Wei et al., 2022a).
15Published as a conference paper at ICLR 2023
A.5 R EADABILITY ANALYSIS OF RETRIEVED AND GENERATED DOCUMENTS
Documents obtained by ↓ NQ TriviaQA WebQ
DPR (Karpukhin et al., 2020) 63.1 80.2 63.3
GENREAD (nucleus sampling) 58.7 83.7 63.8
GENREAD (clustering prompts) 64.0 86.8 66.7
Table 8: Readability study on retrieved documents and gen-
erated documents. See detailed analysis in §A.5.
After we manually compare some re-
trieved documents from DPR and gen-
erated documents from InstructGPT,
we observe that the readability of dif-
ferent documents, when they contain
the correct answer string, is different.
In other words, documents containing
answers might also contain noisy in-
formation that is irrelevant to the question, which could affect both the model and human reading.
In order to further validate the readability of retrieved documents and generated documents, we
extracted a subset of data examples from NQ, TriviaQA and WebQ datasets, in which both retrieved
and generated documents contain the correct answer. As shown in Table 8, when both retrieved and
generated documents contain the correct answer, the FiD reader can produce more correct answers
when reading the generated documents from large language models (e.g., InstructGPT).
We also provide some case studies in Tables 16-19. For example, in Table 18, the question is
“What city was Zeus the patron god of?”. The ﬁrst document retrieved by DPR is “Like the other
Panhellenic Games, the ancient Olympic Games were a religious festival, held at the sanctuary of
Zeus at Olympia.”. Although it contains the correct answer, it is hard to infer the answer “Olympia”
from it. On the contrary, InstructGPT generates the document “Zeus was the patron god of the city of
Olympia, which was located in the northwestern Peloponnese region of Greece. Olympia was the
site of the Olympic Games, held every four years in honor of Zeus.”, which is much easier to read.
A.6 A DDITIONAL NUMBERS FOR TABLES IN THE MAIN PAPER
– Table 9 contains additional evaluation results for Table 1. It demonstrates zero-shot open-domain
QA performance, compared to recent large language model.
– Figure 5 contains additional retrieval performance evaluation for Figure 3 of experiments on
combining DPR retrieved documents and large language model generated document.
– Table 10 contains additional retrieval performance evaluated by Recall@K of baselines and different
GENREAD variants. Some numbers in the table overlaps with those in Figure 2.
16Published as a conference paper at ICLR 2023
Models # total NQ TriviaQA WebQ
parameters open test open test wiki split open test
GPT-3 (Brown et al., 2020) 175B 14.6 49.2 64.3 14.4
Gopher (Rae et al., 2021) 280B 10.1 43.5 52.8 -
FLAN (Wei et al., 2021) 137B 20.7 56.7 68.1 -
GLaM (Du et al., 2022) 64B 21.5 - 68.0 19.0
Chinchilla (Hoffmann et al., 2022) 70B 16.6 55.4 67.0 -
PaLM (Chowdhery et al., 2022) 540B 21.2 - 76.9 10.9
InstructGPT (Ouyang et al., 2022) 175B 19.5 57.4 68.5 19.9
GENREAD (InstructGPT) 175B 28.2 59.3 70.3 24.8
Table 9: Additional numbers for Table 1. Zero-shot open-domain QA performance, compared
to recent large language models. All models in the table do not leverage any external corpus for
document retrieval. Compared to InstructGPT, our proposed GENREAD can improve the EM score
by +6.9 on average. GENREAD can achieve state-of-the-art performance on open test sets.
78808284868890
204070100
Recall@k
TopKdocuments
DPR only (k documents)DPR (k-10) + LLM (10)
(a) NQ-test (retrieval)
78808284868890
204070100
Recall@k
TopKdocuments
DPR only (k documents)DPR (k-10) + LLM (10) (b) TriviaQA-test (retrieval)
75777981838587
204070100
Recall@k
TopKdocuments
DPR only (k documents)DPR (k-10) + LLM (10) (c) WebQ-test (retrieval)
Figure 5: Additional retrieval performance evaluation for Figure 3 of experiments on combining DPR
retrieved documents and large language model generated documents. Merging documents from two
sources achieved signiﬁcantly better performance than using DPR retrieved documents only.
Models TriviaQA WebQ NQ
R@1 R@10 R@20 R@1 R@10 R@20 R@1 R@10 R@20
BM25 (Robertson et al., 2009) 46.2 71.7 76.4 19.1 51.8 62.6 22.8 55.6 63.9
Contriever (Izacard et al., 2022a) 34.0 67.9 74.3 18.2 55.7 65.7 18.8 54.8 65.1
DPR (Karpukhin et al., 2020) 53.2 75.3 79.0 45.4 70.5 75.2 44.6 74.5 79.5
Google Search engine API 50.0 78.8 - 40.0 65.6 - 35.5 67.5 -
GENREAD (nucleus, p=.95) 65.1 81.6 83.8 49.5 71.4 74.4 40.1 66.2 70.6
GENREAD (10 human prompts) 65.5 81.8 - 50.8 72.7 - 40.5 66.9 -
GENREAD (clustering prompts) 69.6 82.9 85.1 54.5 73.3 75.4 48.0 70.9 74.5
Table 10: Retrieval performance evaluated by Recall@K of baselines and differentGENREAD variants.
Some numbers in the table overlaps with those in Figure 2. The table aims to show the performance
of more methods, and to provide accurate recall numbers for future research comparisons.
TriviaQA WebQ NQ
R@10 EM R@10 EM R@10 EM
Sample 5 documents from entire data 81.5 70.9 72.5 53.3 69.2 44.2
Sample 5 documents from each cluster 82.7 71.8 73.3 54.4 70.6 45.3
Table 11: Ablation study on the strategy of sampling documents as in-context demonstrations.
17Published as a conference paper at ICLR 2023
A.7 D ISCUSSION ON INFERENCE COST OF DPR AND INSTRUCT GPT
We now compare the costs of using DPR and InstructGPT to retrieve or generate contextual documents.
We consider DPR using the BERT-base (Devlin et al., 2019) version with 110M parameters and
InstructGPT using its largest version with 175B parameters. For simplicity, we use the FLOPs-per-
token estimates for Transformer-based language models, which is introduced by Kaplan et al. (2020).
It should be noted that FLOPs are not a direct measure of real-world computing costs, as latency,
power consumption, and other costs can vary widely based on other factors (Liu et al., 2022a).
For the DPR model, all Wikipedia documents (around 21M) only need to be encoded once. Therefore,
as the number of input questions increases, the marginal computational cost gradually decreases.
For fair comparison, we ﬁrst use DPR to encode all 21M Wikipedia documents once. Encoding all
Wikipedia documents requires 110e6 (BERT-base parameters)×21e6 (total number of documents)
×100 (tokens per document) = 2.3e17 FLOPs. When the embedding of all candidate documents are
produced, retrieving documents for a given question requires 110e6 (BERT-base parameters)×20
(tokens per question) +21e6 (total number of documents) ×(768 + 768−1) = 3.2e10 FLOPs.
For InstructGPT, it requires 175e9 (InstructGPT parameters) ×10 (number of documents) ×55
(generated tokens per document) = 9.6e13 FLOPs to generate 10 documents for a given question.
Therefore, the equation for the total cost YDPR-cost to retrieve 10 documents using DPR versus the
number of input questions X is: YDPR-cost = 3.2e10X+ 2.3e17. Besides, the equation for the total
cost YGPT3-cost to generate 10 documents using InstructGPT versus the number of input questions X
is: YGPT3-cost = 9.6e13X. When YDPR-cost = YGPT3-cost, X ≈2473. In conclusion, if the number of
input questions is less than 2473, the total cost of InstructGPT is lower than the DPR; if the number
of input questions is greater than 2473, the total cost of InstructGPT exceeds the DPR.
A.8 E RROR ANALYSIS AND CASE STUDIES ON THE NQ DATASET
As stated in Zhang & Choi (2021), NQ contains a signiﬁcant proportion, roughly 16.5%, of questions
that have time-dependent answers. Similarly, Izacard et al. (2022b) observed using the latest version
of Wikipedia (12 / 2021) could lead to 4.4 drops of the EM score, compared to the Wikipedia version
(12 / 2018) that the NQ questions are created from. We provide case studies in Table 13 in Appendix.
We did case studies of 100 examples from the NQ dataset. The results are shown in Table 12. Among
these 100 examples, we found that 29 examples have data collection and annotation mistakes, mainly
including the temporal question issue (13 / 29) and the incomplete answer issue (16 / 29). A typical
temporal-dependent question is that no speciﬁc time condition is provided. For example, “Who won
the MVP for the National League?” could have different answers in different years. In 2017, the
MVP is Giancarlo Stanton, and in 2018, the MVP is Christian Yelich. Besides, some answer labels
provided in the NQ dataset are not complete. For example, person names in the NQ dataset usually
consist of ﬁrst, middle, and last names, but most names in the generated documents are ﬁrst and last
names. For the question “who played lionel in as time goes by?”, the labeled answer is “Geoffrey
Dyson Palmer”. DPR-FiD produces “Geoffrey Dyson Palmer” butGENREAD produces “Geoffrey
Palmer”, both of which should be considered correct. More examples are provided in Table 14.
Besides, GENREAD produced correct answers for 49 questions. Among the 22 incorrect predictions,
12 of them could be classiﬁed as retrieval errors (i.e., step-I error) and 12 as reading errors (i.e.,
step-II error). In all cases of retrieval errors, none of the generated documents contain the correct
answer. In all cases of reading errors, at least one generated document contains the correct answer
but the reader model failed to infer the correct answer from the documents..
18Published as a conference paper at ICLR 2023
Good
Q&A
(71%)
Correct
prediction
(49%)
- Query: Who got the ﬁrst Nobel Prize in Physics?
- Document: The ﬁrst Nobel Prize in Physics was awarded
in 1901 to Wilhelm Conrad R¨ontgen for his discovery of
the remarkable rays subsequently named after him.
- Predicted answer: Wilhelm Conrad R¨ontgen
- Correct answer: Wilhelm Conrad R¨ontgen
Wrong
retrieval
(12%)
Hallucinations
(8%)
- Query: Who died in the ﬁrst episode of Stranger Things?
- Document: In the ﬁrst episode of Stranger Things, the
character Will Byers dies. He is killed by Demogorgon,
a monster from the Upside Down.
- Predicted answer: Will Byers
- Correct answer: Benny Hammond
No hit
answers
(4%)
- Query: When was coffee ﬁrst made into a drink?
- Document: The history of coffee goes back to the 10th
century, with coffee trees native to Ethiopia. The earliest
substantiated evidence of either coffee drinking or know-
ledge of coffee tree is from sixth century AD in Ethiopia.
- Predicted answer: the 10th century
- Correct answer: the 15th century
Wrong
reading
(10%)
- Query: When is the fourth movie of the Divergent
series coming out?
- Document: The fourth movie in the Divergent series was
originally scheduled to be released in June 2017, but was
delayed indeﬁnitely.
- Predicted answer: June 2017
- Correct answer: never made
Bad
Q&A
(29%)
Temporal
questions
(13%)
- Query: Who won the MVP for the National League?
- Document: In 2017, the National League MVP was won
by Giancarlo Stanton of the Miami Marlins. In 2018, the
National League MVP was won by Christian Yelich of the
Milwaukee Brewers.
- Predicted answer: Christian Yelich
- Correct answer: Giancarlo Stanton
Incomplete
answers
(16%)
- Query: Where do the greasers live in the Outsiders?
- Document: The Outsiders is a novel by S.E. Hinton. It is
about a gang of greasers in Oklahoma in the 1960s. The
National League MVP was won by Christian Yelich of the
Milwaukee Brewers.
- Predicted answer: Oklahoma
- Correct answer: Tulsa, Oklahoma
Table 12: Case study on 100 GENREAD predictions in the NQ dataset. Among 100 examples, there
are 49 correct predictions, i.e., EM = 49%. We further categorized 51 incorrect predictions of our
GENREAD , including errors caused by data collection and annotation, and errors caused by model
prediction. In addition, we provide more case studies in Tables 13-15 (Table 13 for the temporal
question issue; Table 14 for the incomplete answer issue; Table 15 for the hallucination issue).
19Published as a conference paper at ICLR 2023
Original question NQ labels Correct labels
Q:When is the last time the philadelphia won the superbowl?Super Bowl LII; 20172018; February 4, 2018
DPR:2017; Google search:2018; GENREAD : February 4, 2018
Q:Who has the most big ten championships in football? Michigan Ohio State
DPR:Michigan; Google search:Ohio State; GENREAD : Ohio State
Q:Who has the most super bowls in nﬂ history? Pittsburgh Steelers Pittsburgh Steelers;
New England Patriots
DPR:Pittsburgh Steelers; Google search:New England Patriots; GENREAD : New England Patriots
Q:How many casinos are in atlantic city new jersey? 11; eleven 9; nine
DPR:eleven; Google search:nine; GENREAD : nine
Q:When did the us not go to the olympics? 1980 1980; 1984
DPR:1980; Google search:1980 and 1984; GENREAD : 1984
Q:Largest cities in the world by population? Beijing Tokyo
DPR:Beijing; Google search:Tokyo; GENREAD : Tokyo
Q:Who has most followers on instagram in world? Selena Gomez Cristiano Ronaldo
DPR:Instagram; Google search:Cristiano Ronaldo; GENREAD : Cristiano Ronaldo
Q:Who is the no. 1 ranked tennis player in the world? Rafael Nadal Novak Djokovic
DPR:Rafael Nadal; Google search:Novak Djokovic; GENREAD : Novak Djokovic
Table 13: Case studies of temporality issues of the NQ dataset. All these questions are drawn from
Zhang & Choi (2021), which contains a subset of NQ data examples with time-dependent questions.
Original question DPR-FiDpredictions GENREAD predictions
Q:Who played lionel in as time goes by? Geoffrey Dyson Palmer Geoffrey Palmer
Explanation:The labeled answer is “Geoffrey Dyson Palmer”, however, “Geoffrey Palmer” is also correct.
DPR retrieved documents:Geoffrey Dyson Palmer, (born 4 June 1927) is an English actor known for
his roles in British television sitcoms playing Jimmy Anderson in “The Fall and Rise of Reginald Perrin”,
Ben Parkinson in “Butterﬂies” and Lionel Hardcastle in “As Time Goes By”. His ﬁlm appearances include
“A Fish Called Wanda”, “The Madness of King George”, “Mrs. Brown”, and “Tomorrow Never Dies”.
GPT generated documents:As Time Goes By is a British sitcom that aired on BBC One from 1992 to
2005. The show starred Geoffrey Palmerand Judi Dench as Lionel and Jean Pargetter, a middle-aged couple
who reunite after many years apart. Lionel was played by Palmer, who was also a writer on the show.
Q:How many cracker barrels in the united states? 645 over 630
Explanation:The labled answer is “639” or “over 600”, so “over 630” is also a reasonable answer.
Q:Where do the greasers live in the outsiders? Tulsa, Oklahoma Oklahoma
Explanation:The labled answer is “Tulsa, Oklahoma”, but “Oklahoma” is also a correct answer.
DPR retrieved documents:The movie received mostly positive reviews from critics, and performed well
at the box ofﬁce, grossing 33 million on a 10 million budget. In Tulsa, Oklahoma, greasers are a gang of
tough, low-income working-class teens. They include Ponyboy Curtis and his two older brothers, Sodapop
and Darrel, as well as Johnny.
GPT generated documents:The Outsiders is a novel by S.E. Hinton. It is about a gang of greasers in
Oklahomain the 1960s. The greasers are from the poor side of town and constantly in trouble with the law.
Q:Where are unipolar neurons found in spinal cord?the granule region dorsal root ganglia
Explanation:The labled answer is “the distal dorsal root”, but the output “dorsal root ganglia” is the same.
Table 14: Case studies of the incomplete answers issue of the NQ dataset. Since the labels in NQ are
spans identiﬁed from Wikipedia passages, it is easier for DPR-FiD to predict correct answers.
20Published as a conference paper at ICLR 2023
Question: Who wrote the ﬁrst declaration of human rights?Answer: Cyrus Cylinder
Generated document:The ﬁrst declaration of human rights was the Virginia Declaration of Rights,
which was written by George Mason in 1776.
Retrieved document:John Peters Humphrey, OC (April 30, 1905 - 2013 March 14, 1995) was a
Canadian legal scholar, jurist, and human rights advocate. He is most famous as the author of the ﬁrst
draft of the Universal Declaration of Human Rights. ... ... (Wikipedia page: John Peters Humphrey)
Golden Wikipedia evidence:
- George Mason’s writings, including substantial portions of the Fairfax Resolves of 1774, the Virginia
Declaration of Rights of 1776, and his Objections to this Constitution of Government (1787) opposing
ratiﬁcation, have exercised a signiﬁcant inﬂuence on American political thought and events. Mason
prepared the ﬁrst draft of the Virginia Declaration of Rights in 1776, and his words formed much of
the text adopted by the ﬁnal Revolutionary Virginia Convention. (Wikipedia page: George Mason)
- The Cyrus Cylinder was dubbed the “ﬁrst declaration of human rights” by the pre-Revolution Iranian
government, a reading prominently advanced by Shah Mohammed Reza Pahlavi, in a 1967 book, The
White Revolution of Iran. (Wikipedia page: Cyrus Cylinder)
Hallucination: In GPT generated documents,ﬁrst declaration of human rightswas considered as
Virginia Declaration of Rights, which is factually incorrect. This error might occur because Wikipedia
page of George Mason contains similar descriptions, causing the model to confuse their meanings.
Model predictions:DPR-FiDJohn Peters Humphrey; GENREAD George Mason
Question: Who died in the ﬁrst episode of stranger thingsAnswer: Benny or Barb
Generated document:In the ﬁrst episode of Stranger Things, the character Will Byers dies. He is
killed by the Demogorgon, a monster from the Upside Down.
Retrieved document:... ... was the character of Barb, the nerdy friend and classmate of Nancy who
is taken and killed by the monster early in the season. According to actress Shannon Purser, Barb
¨wasn’t supposed to be a big deal¨, and the Duffer Brothers had not gone into great detail about the
character since the focus was on ﬁnding Will. ... ... (Wikipedia page: Stranger Things)
Golden Wikipedia evidence:
One area of focus from these fans was the character of Barb, the nerdy friend and classmate of Nancy
who is taken and killed by the monster early in the season. According to actress Shannon Purser,
Barb ¨wasn’t supposed to be a big deal¨, and the Duffer Brothers had not gone into great detail about
the character since the focus was on ﬁnding Will. ... ... (Wikipedia page: Stranger Things)
The owner, Benny, learns from a tattoo of ”011” on her arm that her name is Eleven. Brenner,
monitoring the phone lines, sends agents to the diner after Benny calls social services. The agents kill
Benny, but Eleven manages to escape using telekinetic abilities. Joyce’s phone short circuits after
receiving a mysterious phone call that she believes is from Will. While searching for Will in the
woods, Mike, Dustin and Lucas come across Eleven. (Wikipedia page: Stranger Things)
Hallucination: In GPT generated documents,Will Byersis said to be dead, which is actually incorrect.
Will Byers is actually a fake deaths in season 1 and he is still alive at the end of the season.
Model predictions:DPR-FiDBarb; GENREAD Will
Question: Who played the judge in good will hunting?Answer: James P. Flynn
Generated document:The character of the Judge in Good Will Hunting was played by actor Robert
Sean Leonard. Leonard is best known for his role as Dr. James Wilson in the television series House.
Retrieved document:Janitor could show such facility with matrices applied this way. Good Will
Hunting Good Will Hunting is a 1997 American drama ﬁlm, directed by Gus Van Sant, and starring
Robin Williams, Matt Damon, Ben Afﬂeck, ... ... (Wikipedia page: Good Will Hunting)
Golden Wikipedia evidence:
- James P. Flynn appeared in many ﬁlms shot in the New England area. Flynn was cast as a judge in
the Boston-based ﬁlm Good Will Hunting in 1997. Later, he acted in the 1999 ﬁlm The Cider House
Rules and What’s the Worst That Could Happen? in 2001. (Wikipedia page: James P. Flynn)
- Cast: James P. Flynn as Judge George H. Malone (Wikipedia page: Good Will Hunting)
Hallucination: In GPT generated documents,Robert Sean Leonardis considered as ajudge in Good
Will Hunting, which is factually incorrect. This error leads to wrong answer prediction.
Model predictions:DPR-FiDStellan Skarsgrd; GENREAD Alan Arkin
Table 15: Case studies of hallucination errors in InstructGPT generated documents. The documents
contain contents that contradict to the facts and world knowledge, resulting in wrong predictions.
21Published as a conference paper at ICLR 2023
Query:What type of music did Mozart composed?
Answers:opera, classical music, art song, chamber music, ballet
DPR #1:The following is a list of surviving works that Mozart composed for performance at gatherings
of Masons. The story and music of his opera “The Magic Flute” is also considered to have strong
Masonic inﬂuences. The following is a partial list of family members, patrons, and colleagues who were
Masons. Mozart’s grandfather Johann Georg, a bookbinder, was raised among the extended Mozart family
in Augsburg, in the house of Johann’s own grandfather David Mozart. David and his children were
distinguished architects and master ... (Wikipedia id: 10988168 – “Mozart and Freemasonry”)
DPR #2:Progressively, and in large part at the hands of Mozart himself, the contrapuntal complexities
of the late Baroque emerged once more, moderated and disciplined by new forms, and adapted to a new
aesthetic and social milieu. Mozart was a versatile composer, and wrote in every major genre, including
symphony, opera, the solo concerto, chamber music including string quartet and string quintet, and the
piano sonata. These forms were not new, but Mozart advanced their technical sophistication and emotional
reach. He almost single-handedly developed and popularized the Classical piano concerto. He wrote a
great deal ... (Wikipedia id: 451968 – “Wolfgang Amadeus Mozart”)
Among top-10 retrieved documents:opera: 2 times chamber music: 1 time
GPT nucleus #1:Mozart was a proliﬁc composer and wrote in many different genres. He composed
symphonies, operas, concertos, chamber music, and more. Many of his works are considered some of the
greatest ever written in those genres.
GPT nucleus #2:Mozart composed a variety of music, including operas, symphonies, chamber music,
concertos, and sonatas. His early works were inﬂuenced by the music of Haydn and Bach, and his later
works show the inﬂuence of the French and Italian composers of his time. Mozart’s music is characterized
by its beauty, elegance, and precision, and he is considered one of the greatest composers of all time.
GPT nucleus #3:Mozart composed a wide variety of music, including operas, symphonies, concertos,
solo pieces, and chamber music. He also wrote a number of religious works, including masses and motets.
Among top-10 generated documents:chamber music: 9 times
GPT cluster #1:Mozart found work in Vienna as a piano teacher and composer for opera and other works.
There he met a fellow Mason, Joseph Haydn, who was to become an important mentor. Mozart married
Constanze Weber in 1782; their marriage is thought to have been happy, in contrast to his earlier unhappy
arranged marriage to his cousin, Maria Anna¨Nannerl¨Mozart. Mozart composed more than 600 works,
including works widely.
GPT cluster #2:Mozart was a proliﬁc composer, and wrote in a wide variety of genres. He composed
over 600 works, including operas, symphonies, concertos, solo works, and chamber music. His operas
include The Marriage of Figaro, Don Giovanni, and Cosi fan tutte. His symphonies include the well-known
Symphony No. 40 in G minor, and his concertos include the Piano Concerto No. 21 in C major, known as
the ¨Elvira Madigan¨concerto.
GPT cluster #3:Mozart was a proliﬁc composer who wrote music in a wide variety of genres. He is best
known for his work in the classical genre, which includes such works as his Symphony No. 40 in G minor
and his piano concertos. He also wrote operas, including The Marriage of Figaro and The Magic Flute. In
addition to his work in classical music, Mozart also composed works in the popular genre, such as his Eine
kleine Nachtmusik.
Among top-10 generated documents:opera: 2 times chamber music: 7 times classical music: 1 time
Table 16: Case study of retrieved documents and generated documents from language models (1/4).
22Published as a conference paper at ICLR 2023
Query:What does Monsanto own?
Answers:agricultural chemicals, seed, agriculture, chemical industry
DPR #1:In September 2016, Monsanto agreed to be acquired by Bayer for US$66 billion. In an effort to
receive regulatory clearance for the deal, Bayer announced the sale of signiﬁcant portions of its current
agriculture businesses, including its seed and herbicide businesses, to BASF. The deal was approved by
the European Union on March 21, 2018, and approved in the United States on May 29, 2018. The sale
closed on June 7, 2018; Bayer announced its intent to discontinue the Monsanto name, with the combined
company operating solely under the Bayer brand. ... (Wikipedia id: 935126 – “Monsanto”)
DPR #2:Behind it all is of course an evil corporation: Monsanto. GMO conspiracy theories GMO
conspiracy theories are conspiracy theories related to the production and sale of genetically modiﬁed crops
and genetically modiﬁed food (also referred to as genetically modiﬁed organisms or “GMOs”). These
conspiracy theories include claims that agribusinesses, especially Monsanto, have suppressed data showing
that GMOs cause harm, deliberately cause food shortages to promote the use of GM food, or have co-opted
government agencies ... (Wikipedia id: 19304897 – “GMO conspiracy theories”)
Among top-10 retrieved documents:agriculture: 1 time seed: 2 times
GPT nucleus #1:Monsanto is an American agricultural biotechnology corporation founded in 1901. It is
headquartered in Creve Coeur, Missouri. Monsanto is a leading producer of genetically engineered seed
and Roundup, a glyphosate-based herbicide.
GPT nucleus #2:Monsanto is a multinational agrochemical and agricultural biotechnology corporation
headquartered in Creve Coeur, Missouri. It is one of the world’s leading producers of Roundup, a glyphosate-
based herbicide. The company also manufactures genetically engineered seed and Roundup Ready crops,
including corn, soybeans, and cotton. As of 2018, Monsanto is the world’s largest producer of glyphosate.
In 2018, Bayer acquired Monsanto for $63 billion.
Among top-10 generated documents:seed: 6 times
GPT cluster #1:Monsanto Company is an American multinational agrochemical and agricultural biotech-
nology corporation headquartered in Creve Coeur, St. Louis, Missouri. It is a leading producer of
genetically engineered (GE) seed and of the herbicide glyphosate, which it markets under the Roundup
brand. Founded in 1901 by John Francis Queeny, Monsanto spent most of the 20th century as a chemical
company, becoming one of the world’s leading manufacturers of plastics, resins, and other chemicals.
GPT cluster #2:Monsanto is a biotechnology and agriculture company that specializes in genetically
modiﬁed organisms (GMOs). The company owns a number of patents on GMOs, and it produces a range
of products that contain GMOs, including herbicides, pesticides, and seeds.
GPT cluster #3:Monsanto is a multinational agricultural biotechnology corporation. It is one of the
world’s leading producers of genetically engineered seed and Roundup, a glyphosate-based herbicide. The
company also manufactures other agricultural chemicals, such as insecticides and fungicides.
Among top-10 generated documents:agriculture: 2 times seed: 5 times agricultural chemicals: 1 time
Table 17: Case study of retrieved documents and generated documents from language models (2/4).
23Published as a conference paper at ICLR 2023
Query:What city was Zeus the patron god of?
Answers:Olympia, Elis
DPR #1:Many were speciﬁc only to a particular deity or city-state. For example, the festival of Lykaia was
celebrated in Arcadia in Greece, which was dedicated to the pastoral god Pan. Like the other Panhellenic
Games, the ancient Olympic Games were a religious festival, held at the sanctuary of Zeus at Olympia.
Other festivals centred on Greek theatre, of which the Dionysia in Athens was the most important. More
typical festivals featured a procession, large sacriﬁces and a feast to eat the offerings, and many included
entertainments and customs such as visiting friends, wearing fancy dress and unusual behaviour in the ...
(Wikipedia id: 1945872 – “Ancient Greek religion”)
DPR #2:Theatre of Dionysus The Theatre of Dionysus Eleuthereus is a major theatre in Athens, considered
to be the world’s ﬁrst theatre, built at the foot of the Athenian Acropolis. Dedicated to Dionysus, the god
of plays and wine (among other things), the theatre could seat as many as 17,000 people with excellent
acoustics, making it an ideal location for ancient Athens’ biggest theatrical celebration, the Dionysia. It was
the ﬁrst theatre ever built, cut into the southern cliff face of the Acropolis, and supposedly the birthplace
of Greek tragedy. The remains of a restored and redesigned Roman version can still ... (Wikipedia id:
3732515 – “Theatre of Dionysus”)
Among top-10 retrieved documents:Olympia: 1 time
GPT nucleus #1:Zeus was the patron god of the city of Olympia in Greece.
GPT nucleus #2:The city of Olympia in Greece was the site of the ancient Olympics, and it was also the
home of Zeus, the patron god of the Olympics.
Among top-10 generated documents:Olympia: 6 times
GPT cluster #1:Zeus was the patron god of the city of Olympia, which was located in the northwestern
Peloponnese region of Greece. Olympia was the site of the Olympic Games, which were held every four
years in honor of Zeus.
GPT cluster #2:Zeus was the patron god of the city of Olympia. Olympia was a sanctuary of ancient
Greece in Elis on the Peloponnesos peninsula, in south-western Greece. The sanctuary was known as the
Panhellenic Sanctuary of Zeus, and the altar of Zeus was located in the Temple of Zeus. The Olympic
Games were held in Olympia every four years from 776 BC to 393 AD.
GPT cluster #3:Zeus was the patron god of the city of Olympia.
Among top-10 generated documents:Olympia: 9 times Elis: 1 time
Table 18: Case study of retrieved documents and generated documents from language models (3/4).
24Published as a conference paper at ICLR 2023
Query:What is the collection of the districts to the east of the Jordan River?
Answers:Jordan, Golan Heights
DPR #1:Water politics in the Jordan River basin Water politics in the Jordan River basin refers to political
issues of water within the Jordan River drainage basin, including competing claims and water usage, and
issues of riparian rights of surface water along transnational rivers, as well as the availability and usage of
ground water. Water resources in the region are scarce, and these issues directly affect the ﬁve political
subdivisions (Israel, the West Bank, Lebanon, Syria and Jordan) located within and bordering the basin,
which were created since the collapse, , during World War I, of the former single controlling entity, the ...
(Wikipedia id: 12822413 – “Water politics in the Jordan River basin”)
DPR #2:Jordan’s western border follows the bottom of the rift. Although an earthquake-prone region, no
severe shocks had been recorded for several centuries. By far the greatest part of the East Bank is desert,
displaying the land forms and other features associated with great aridity. Most of this land is part of the
Syrian Desert and northern Arabian Desert. There are broad expanses of sand and dunes, particularly in
the south and southeast, together with salt ﬂats. Occasional jumbles of sandstone hills or low mountains
support only meager and stunted vegetation that ... (Wikipedia id: 210052 – “Geography of Jordan”)
Among top-10 retrieved documents:Jordan: 8 times
GPT nucleus #1:The collection of the districts to the east of the Jordan River is known as the West Bank.
The West Bank is a landlocked territory situated in Western Asia. It is bordered by Jordan to the east, Egypt
to the southwest, and Israel to the west and north.
GPT nucleus #2:The collection of the districts to the east of the Jordan River is the West Bank. The West
Bank is a landlocked territory near the Mediterranean coast of Western Asia, bordered by Lebanon to the
north, Syria to the northeast, Jordan to the east, and Egypt to the southwest. It is the smaller of two political
entities that comprise the Palestinian territories, the other being the Gaza Strip.
Among top-10 generated documents:Jordan: 10 times
GPT cluster #1:The collection of the districts to the east of the Jordan River is known as the West Bank.
The West Bank is a landlocked territory situated in Western Asia. It is bordered by Jordan to the east, Egypt
to the southwest, and Israel to the west and north.
GPT cluster #2:The Jordan River is a major river in the Middle East, the source of which is in the northern
part of Israel. The river ﬂows southward through Israel and then Jordan, emptying into the Dead Sea. East
of the river is the collection of districts known as the East Bank.
GPT cluster #3:There is no single answer to this question as the east bank of the Jordan River is home to
a number of different districts and regions, each with its own unique history, culture, and customs. However,
some of the more well-known districts on the east bank include the West Bank, the Gaza Strip, and the
Golan Heights.
Among top-10 generated documents:Jordan: 10 times Golan Heights: 2 times
Table 19: Case study of retrieved documents and generated documents from language models (4/4).
25Published as a conference paper at ICLR 2023
B P ROMPTS CHOICES
B.1 Z ERO -SHOT LEARNING PROMPTS (FOR TABLE 1)
B.1.1 P ROMPTS FOR ”INSTRUCT GPT ( NO DOCS .)”
We observed the prompts (i.e., “Q: {query}\n\nA:”) used in GPT-3 paper (Brown et al., 2020)
perform poorly on its text-davinci-002 version. Therefore, we experimented with multiple prompts
and found the following two prompts work best on open-domain QA datasets.
– (1) “{query}\n\nThe answer is” (no space between {query}and \n)
– (2) “{query}\n\n The answer is” (performance reported in Table 1)
For fact checking and dialogue system, we used the following prompts.
– Fact Checking “{claim}\n\n Is the claim true or false?”
– Open-domain Dialogue System “{query}\n\n”
B.1.2 P ROMPTS FOR BACKGROUND GENERATION (STEP -1)
– Open-domain Question Answering “Generate a background document from Wikipedia to answer
the given question. \n\n {query}\n\n”
– Fact checking “Generate a background document from Wikipedia to support or refute the statement.
\n\n Statement: {claim}\n\n”
– Open-domain Dialogue System “Generate a background document from Wikipedia to answer the
given question. \n\n {utterance}\n\n”
B.1.3 P ROMPTS FOR READING COMPREHENSION (STEP -2)
We collected the prompt from P3 (Bach et al., 2022), which includes over 2,000 open-source
prompts for roughly 170 datasets. For zero-shot QA, we experimented with three different reading
comprehension prompts. We reported the performance for each prompt in Table 20.
– (1) “Refer to the passage below and answer the following question with just a few words. Passage:
{background}\n\n Question: {query}\n\n The answer is”
– (2) “Passage: {background}\n\n Question: {query}\n\n Referring to the passage above, the
correct answer (just one entity) to the given question is”
– (3) “Refer to the passage below and answer the following question with just one entity.\n\n Passage:
background \n\n Question: query \n\n The answer is”
For fact checking and dialogue system, we chose the simplest prompt from P3.
– Fact Checking “{background}\n\n claim: {claim}\n\n Is the claim true or false?”
– Open-domain Dialogue System “{background}\n\n utterance \n\n”
B.2 H UMAN PROMPT ANNOTATIONS (FOR SECTION 3.2.1)
In order to get a better prompt for large language models to generate better contextual documents, we
asked 30 students in the computer science department to write different prompts. We ﬁrst constructed
a small validation set with 200 examples by combining 50 random question-answer pairs from NQ,
100 random pairs from TriviaQA and 50 random pairs from WebQ. When an annotator wrote down a
prompt, our system can immediately evaluate the prompt by using the validation set and return the
performance to the annotator. Then, the annotator can modify the previous prompt until the recall
performance reaches a threshold, which is set as 50 in our experiments. Finally, we got 29 prompts
from human annotators due to two of them are the same. We used the top-10 prompts (shown in
Table 21 and Table 22) in the human prompt setting, as described in§3.2.1.
26Published as a conference paper at ICLR 2023
Models NQ TriviaQA WebQ
(1) (2) (3) avg. (1) (2) (3) avg. (1) (2) (3) avg.
*with retriever, AND directly trained on these datasets
DPR + InstructGPT* 28.8 28.5 29.9 29.1 56.0 50.1 55.3 53.8 18.9 21.7 20.1 20.2
*with retriever, BUT NOT trained on these datasets
BM25 + InstructGPT 20.1 18.4 20.5 19.7 54.2 49.0 53.4 52.2 14.9 16.6 16.0 15.8
Contriever + InstructGPT 18.3 16.5 19.1 18.0 53.1 48.5 52.4 51.3 14.9 18.2 16.8 16.6
Google + InstructGPT 29.1 29.3 27.8 28.8 60.3 57.5 58.7 58.8 19.5 21.8 19.9 20.4
*without retriever, and not using external documents
GENREAD (InstructGPT) 27.0 28.7 28.2 28.0 58.5 59.3 59.3 59.0 22.7 26.4 24.8 24.6
Table 20: Zero-shot QA performance under different prompts. The prompts are listed in §B.1.
No. Prompts Validation
#1 Generate a background document from Wikipedia to answer the given question. 66.0
#2 Provide a background document from Wikipedia to answer the given question. 65.0
#3 Generate a background document from web to answer the given question. 64.0
#4 Generate a Wikipedia document to support the given question. 63.5
#5 Provide a background document for the given question. 63.0
#6 Prepare a background document to support the given question. 63.0
#7 To support the given question, prepare a background document. 62.5
#8 Create a background document that supports the given question. 61.5
#9 Retrieve a document from Wikipedia to answer the given question. 60.5
#10 Retrieve a Wikipedia article to address the posed question. 59.5
Table 21: Top-10 human prompts, evaluated on merged validation set of NQ, TriviaQA and WebQ.
Prompt No. Validation NQ WebQ TriviaQA Avg.
#1 (Generate ...) 66.0 45.9 51.9 68.7 55.5
#2 (Provide ...) 65.0 43.9 51.0 68.3 54.4
#3 (Generate ...) 64.0 44.0 50.6 67.7 54.2
#4 (Generate ...) 63.5 43.2 51.2 67.5 54.0
#5 (Provide ...) 63.0 43.6 50.3 67.9 54.0
#6 (Prepare ...) 63.0 43.5 50.5 67.7 54.0
#7 (To support ...) 62.5 43.5 50.3 67.5 53.8
#8 (Create ...) 61.5 42.7 50.2 66.8 53.3
#9 (Retrieve ...) 60.5 41.6 49.0 68.2 53.0
#10 (Retrieve ...) 59.5 40.7 49.5 67.7 52.7
Table 22: Performance on NQ, TriviaQA and WebQ test sets of top-10 human prompts.
27

1
Retrieval-Augmented Generation for Large
Language Models: A Survey
Yunfan Gaoa, Yun Xiongb, Xinyu Gao b, Kangxiang Jia b, Jinliu Pan b, Yuxi Bic, Yi Dai a, Jiawei Sun a, Meng
Wangc, and Haofen Wang a,c
aShanghai Research Institute for Intelligent Autonomous Systems, Tongji University
bShanghai Key Laboratory of Data Science, School of Computer Science, Fudan University
cCollege of Design and Innovation, Tongji University
Abstract—Large Language Models (LLMs) showcase impres-
sive capabilities but encounter challenges like hallucination,
outdated knowledge, and non-transparent, untraceable reasoning
processes. Retrieval-Augmented Generation (RAG) has emerged
as a promising solution by incorporating knowledge from external
databases. This enhances the accuracy and credibility of the
generation, particularly for knowledge-intensive tasks, and allows
for continuous knowledge updates and integration of domain-
specific information. RAG synergistically merges LLMs’ intrin-
sic knowledge with the vast, dynamic repositories of external
databases. This comprehensive review paper offers a detailed
examination of the progression of RAG paradigms, encompassing
the Naive RAG, the Advanced RAG, and the Modular RAG.
It meticulously scrutinizes the tripartite foundation of RAG
frameworks, which includes the retrieval, the generation and the
augmentation techniques. The paper highlights the state-of-the-
art technologies embedded in each of these critical components,
providing a profound understanding of the advancements in RAG
systems. Furthermore, this paper introduces up-to-date evalua-
tion framework and benchmark. At the end, this article delineates
the challenges currently faced and points out prospective avenues
for research and development 1.
Index Terms—Large language model, retrieval-augmented gen-
eration, natural language processing, information retrieval
I. I NTRODUCTION
L
ARGE language models (LLMs) have achieved remark-
able success, though they still face significant limitations,
especially in domain-specific or knowledge-intensive tasks [1],
notably producing “hallucinations” [2] when handling queries
beyond their training data or requiring current information. To
overcome challenges, Retrieval-Augmented Generation (RAG)
enhances LLMs by retrieving relevant document chunks from
external knowledge base through semantic similarity calcu-
lation. By referencing external knowledge, RAG effectively
reduces the problem of generating factually incorrect content.
Its integration into LLMs has resulted in widespread adoption,
establishing RAG as a key technology in advancing chatbots
and enhancing the suitability of LLMs for real-world applica-
tions.
RAG technology has rapidly developed in recent years, and
the technology tree summarizing related research is shown
Corresponding Author.Email:haofen.wang@tongji.edu.cn
1Resources are available at https://github.com/Tongji-KGLLM/
RAG-Survey
in Figure 1. The development trajectory of RAG in the era
of large models exhibits several distinct stage characteristics.
Initially, RAG’s inception coincided with the rise of the
Transformer architecture, focusing on enhancing language
models by incorporating additional knowledge through Pre-
Training Models (PTM). This early stage was characterized
by foundational work aimed at refining pre-training techniques
[3]–[5].The subsequent arrival of ChatGPT [6] marked a
pivotal moment, with LLM demonstrating powerful in context
learning (ICL) capabilities. RAG research shifted towards
providing better information for LLMs to answer more com-
plex and knowledge-intensive tasks during the inference stage,
leading to rapid development in RAG studies. As research
progressed, the enhancement of RAG was no longer limited
to the inference stage but began to incorporate more with LLM
fine-tuning techniques.
The burgeoning field of RAG has experienced swift growth,
yet it has not been accompanied by a systematic synthesis that
could clarify its broader trajectory. This survey endeavors to
fill this gap by mapping out the RAG process and charting
its evolution and anticipated future paths, with a focus on the
integration of RAG within LLMs. This paper considers both
technical paradigms and research methods, summarizing three
main research paradigms from over 100 RAG studies, and
analyzing key technologies in the core stages of “Retrieval,”
“Generation,” and “Augmentation.” On the other hand, current
research tends to focus more on methods, lacking analysis and
summarization of how to evaluate RAG. This paper compre-
hensively reviews the downstream tasks, datasets, benchmarks,
and evaluation methods applicable to RAG. Overall, this
paper sets out to meticulously compile and categorize the
foundational technical concepts, historical progression, and
the spectrum of RAG methodologies and applications that
have emerged post-LLMs. It is designed to equip readers and
professionals with a detailed and structured understanding of
both large models and RAG. It aims to illuminate the evolution
of retrieval augmentation techniques, assess the strengths and
weaknesses of various approaches in their respective contexts,
and speculate on upcoming trends and innovations.
Our contributions are as follows:
• In this survey, we present a thorough and systematic
review of the state-of-the-art RAG methods, delineating
its evolution through paradigms including naive RAG,
arXiv:2312.10997v5  [cs.CL]  27 Mar 20242
Fig. 1. Technology tree of RAG research. The stages of involving RAG mainly include pre-training, fine-tuning, and inference. With the emergence of LLMs,
research on RAG initially focused on leveraging the powerful in context learning abilities of LLMs, primarily concentrating on the inference stage. Subsequent
research has delved deeper, gradually integrating more with the fine-tuning of LLMs. Researchers have also been exploring ways to enhance language models
in the pre-training stage through retrieval-augmented techniques.
advanced RAG, and modular RAG. This review contex-
tualizes the broader scope of RAG research within the
landscape of LLMs.
• We identify and discuss the central technologies integral
to the RAG process, specifically focusing on the aspects
of “Retrieval”, “Generation” and “Augmentation”, and
delve into their synergies, elucidating how these com-
ponents intricately collaborate to form a cohesive and
effective RAG framework.
• We have summarized the current assessment methods of
RAG, covering 26 tasks, nearly 50 datasets, outlining
the evaluation objectives and metrics, as well as the
current evaluation benchmarks and tools. Additionally,
we anticipate future directions for RAG, emphasizing
potential enhancements to tackle current challenges.
The paper unfolds as follows: Section II introduces the
main concept and current paradigms of RAG. The following
three sections explore core components—“Retrieval”, “Gen-
eration” and “Augmentation”, respectively. Section III focuses
on optimization methods in retrieval,including indexing, query
and embedding optimization. Section IV concentrates on post-
retrieval process and LLM fine-tuning in generation. Section V
analyzes the three augmentation processes. Section VI focuses
on RAG’s downstream tasks and evaluation system. Sec-
tion VII mainly discusses the challenges that RAG currently
faces and its future development directions. At last, the paper
concludes in Section VIII.
II. O VERVIEW OF RAG
A typical application of RAG is illustrated in Figure 2.
Here, a user poses a question to ChatGPT about a recent,
widely discussed news. Given ChatGPT’s reliance on pre-
training data, it initially lacks the capacity to provide up-
dates on recent developments. RAG bridges this information
gap by sourcing and incorporating knowledge from external
databases. In this case, it gathers relevant news articles related
to the user’s query. These articles, combined with the original
question, form a comprehensive prompt that empowers LLMs
to generate a well-informed answer.
The RAG research paradigm is continuously evolving, and
we categorize it into three stages: Naive RAG, Advanced
RAG, and Modular RAG, as showed in Figure 3. Despite
RAG method are cost-effective and surpass the performance
of the native LLM, they also exhibit several limitations.
The development of Advanced RAG and Modular RAG is
a response to these specific shortcomings in Naive RAG.
A. Naive RAG
The Naive RAG research paradigm represents the earli-
est methodology, which gained prominence shortly after the3
Fig. 2. A representative instance of the RAG process applied to question answering. It mainly consists of 3 steps. 1) Indexing. Documents are split into chunks,
encoded into vectors, and stored in a vector database. 2) Retrieval. Retrieve the Top k chunks most relevant to the question based on semantic similarity. 3)
Generation. Input the original question and the retrieved chunks together into LLM to generate the final answer.
widespread adoption of ChatGPT. The Naive RAG follows
a traditional process that includes indexing, retrieval, and
generation, which is also characterized as a “Retrieve-Read”
framework [7].
Indexing starts with the cleaning and extraction of raw data
in diverse formats like PDF, HTML, Word, and Markdown,
which is then converted into a uniform plain text format. To
accommodate the context limitations of language models, text
is segmented into smaller, digestible chunks. Chunks are then
encoded into vector representations using an embedding model
and stored in vector database. This step is crucial for enabling
efficient similarity searches in the subsequent retrieval phase.
Retrieval. Upon receipt of a user query, the RAG system
employs the same encoding model utilized during the indexing
phase to transform the query into a vector representation.
It then computes the similarity scores between the query
vector and the vector of chunks within the indexed corpus.
The system prioritizes and retrieves the top K chunks that
demonstrate the greatest similarity to the query. These chunks
are subsequently used as the expanded context in prompt.
Generation. The posed query and selected documents are
synthesized into a coherent prompt to which a large language
model is tasked with formulating a response. The model’s
approach to answering may vary depending on task-specific
criteria, allowing it to either draw upon its inherent parametric
knowledge or restrict its responses to the information con-
tained within the provided documents. In cases of ongoing
dialogues, any existing conversational history can be integrated
into the prompt, enabling the model to engage in multi-turn
dialogue interactions effectively.
However, Naive RAG encounters notable drawbacks:
Retrieval Challenges . The retrieval phase often struggles
with precision and recall, leading to the selection of misaligned
or irrelevant chunks, and the missing of crucial information.
Generation Difficulties. In generating responses, the model
may face the issue of hallucination, where it produces con-
tent not supported by the retrieved context. This phase can
also suffer from irrelevance, toxicity, or bias in the outputs,
detracting from the quality and reliability of the responses.
Augmentation Hurdles . Integrating retrieved information
with the different task can be challenging, sometimes resulting
in disjointed or incoherent outputs. The process may also
encounter redundancy when similar information is retrieved
from multiple sources, leading to repetitive responses. Deter-
mining the significance and relevance of various passages and
ensuring stylistic and tonal consistency add further complexity.
Facing complex issues, a single retrieval based on the original
query may not suffice to acquire adequate context information.
Moreover, there’s a concern that generation models might
overly rely on augmented information, leading to outputs that
simply echo retrieved content without adding insightful or
synthesized information.
B. Advanced RAG
Advanced RAG introduces specific improvements to over-
come the limitations of Naive RAG. Focusing on enhancing re-
trieval quality, it employs pre-retrieval and post-retrieval strate-
gies. To tackle the indexing issues, Advanced RAG refines
its indexing techniques through the use of a sliding window
approach, fine-grained segmentation, and the incorporation of
metadata. Additionally, it incorporates several optimization
methods to streamline the retrieval process [8].4
Fig. 3. Comparison between the three paradigms of RAG. (Left) Naive RAG mainly consists of three parts: indexing, retrieval and generation. (Middle)
Advanced RAG proposes multiple optimization strategies around pre-retrieval and post-retrieval, with a process similar to the Naive RAG, still following a
chain-like structure. (Right) Modular RAG inherits and develops from the previous paradigm, showcasing greater flexibility overall. This is evident in the
introduction of multiple specific functional modules and the replacement of existing modules. The overall process is not limited to sequential retrieval and
generation; it includes methods such as iterative and adaptive retrieval.
Pre-retrieval process. In this stage, the primary focus is
on optimizing the indexing structure and the original query.
The goal of optimizing indexing is to enhance the quality of
the content being indexed. This involves strategies: enhancing
data granularity, optimizing index structures, adding metadata,
alignment optimization, and mixed retrieval. While the goal
of query optimization is to make the user’s original question
clearer and more suitable for the retrieval task. Common
methods include query rewriting query transformation, query
expansion and other techniques [7], [9]–[11].
Post-Retrieval Process. Once relevant context is retrieved,
it’s crucial to integrate it effectively with the query. The main
methods in post-retrieval process include rerank chunks and
context compressing. Re-ranking the retrieved information to
relocate the most relevant content to the edges of the prompt is
a key strategy. This concept has been implemented in frame-
works such as LlamaIndex 2, LangChain3, and HayStack [12].
Feeding all relevant documents directly into LLMs can lead
to information overload, diluting the focus on key details with
irrelevant content.To mitigate this, post-retrieval efforts con-
centrate on selecting the essential information, emphasizing
critical sections, and shortening the context to be processed.
2https://www.llamaindex.ai
3https://www.langchain.com/
C. Modular RAG
The modular RAG architecture advances beyond the for-
mer two RAG paradigms, offering enhanced adaptability and
versatility. It incorporates diverse strategies for improving its
components, such as adding a search module for similarity
searches and refining the retriever through fine-tuning. Inno-
vations like restructured RAG modules [13] and rearranged
RAG pipelines [14] have been introduced to tackle specific
challenges. The shift towards a modular RAG approach is
becoming prevalent, supporting both sequential processing and
integrated end-to-end training across its components. Despite
its distinctiveness, Modular RAG builds upon the foundational
principles of Advanced and Naive RAG, illustrating a progres-
sion and refinement within the RAG family.
1) New Modules: The Modular RAG framework introduces
additional specialized components to enhance retrieval and
processing capabilities. The Search module adapts to spe-
cific scenarios, enabling direct searches across various data
sources like search engines, databases, and knowledge graphs,
using LLM-generated code and query languages [15]. RAG-
Fusion addresses traditional search limitations by employing
a multi-query strategy that expands user queries into diverse
perspectives, utilizing parallel vector searches and intelligent
re-ranking to uncover both explicit and transformative knowl-
edge [16]. The Memory module leverages the LLM’s memory
to guide retrieval, creating an unbounded memory pool that5
aligns the text more closely with data distribution through iter-
ative self-enhancement [17], [18]. Routing in the RAG system
navigates through diverse data sources, selecting the optimal
pathway for a query, whether it involves summarization,
specific database searches, or merging different information
streams [19]. The Predict module aims to reduce redundancy
and noise by generating context directly through the LLM,
ensuring relevance and accuracy [13]. Lastly, the Task Adapter
module tailors RAG to various downstream tasks, automating
prompt retrieval for zero-shot inputs and creating task-specific
retrievers through few-shot query generation [20], [21] .This
comprehensive approach not only streamlines the retrieval pro-
cess but also significantly improves the quality and relevance
of the information retrieved, catering to a wide array of tasks
and queries with enhanced precision and flexibility.
2) New Patterns: Modular RAG offers remarkable adapt-
ability by allowing module substitution or reconfiguration
to address specific challenges. This goes beyond the fixed
structures of Naive and Advanced RAG, characterized by a
simple “Retrieve” and “Read” mechanism. Moreover, Modular
RAG expands this flexibility by integrating new modules or
adjusting interaction flow among existing ones, enhancing its
applicability across different tasks.
Innovations such as the Rewrite-Retrieve-Read [7]model
leverage the LLM’s capabilities to refine retrieval queries
through a rewriting module and a LM-feedback mechanism
to update rewriting model., improving task performance.
Similarly, approaches like Generate-Read [13] replace tradi-
tional retrieval with LLM-generated content, while Recite-
Read [22] emphasizes retrieval from model weights, enhanc-
ing the model’s ability to handle knowledge-intensive tasks.
Hybrid retrieval strategies integrate keyword, semantic, and
vector searches to cater to diverse queries. Additionally, em-
ploying sub-queries and hypothetical document embeddings
(HyDE) [11] seeks to improve retrieval relevance by focusing
on embedding similarities between generated answers and real
documents.
Adjustments in module arrangement and interaction, such
as the Demonstrate-Search-Predict (DSP) [23] framework
and the iterative Retrieve-Read-Retrieve-Read flow of ITER-
RETGEN [14], showcase the dynamic use of module out-
puts to bolster another module’s functionality, illustrating a
sophisticated understanding of enhancing module synergy.
The flexible orchestration of Modular RAG Flow showcases
the benefits of adaptive retrieval through techniques such as
FLARE [24] and Self-RAG [25]. This approach transcends
the fixed RAG retrieval process by evaluating the necessity
of retrieval based on different scenarios. Another benefit of
a flexible architecture is that the RAG system can more
easily integrate with other technologies (such as fine-tuning
or reinforcement learning) [26]. For example, this can involve
fine-tuning the retriever for better retrieval results, fine-tuning
the generator for more personalized outputs, or engaging in
collaborative fine-tuning [27].
D. RAG vs Fine-tuning
The augmentation of LLMs has attracted considerable atten-
tion due to their growing prevalence. Among the optimization
methods for LLMs, RAG is often compared with Fine-tuning
(FT) and prompt engineering. Each method has distinct charac-
teristics as illustrated in Figure 4. We used a quadrant chart to
illustrate the differences among three methods in two dimen-
sions: external knowledge requirements and model adaption
requirements. Prompt engineering leverages a model’s inherent
capabilities with minimum necessity for external knowledge
and model adaption. RAG can be likened to providing a model
with a tailored textbook for information retrieval, ideal for pre-
cise information retrieval tasks. In contrast, FT is comparable
to a student internalizing knowledge over time, suitable for
scenarios requiring replication of specific structures, styles, or
formats.
RAG excels in dynamic environments by offering real-
time knowledge updates and effective utilization of external
knowledge sources with high interpretability. However, it
comes with higher latency and ethical considerations regarding
data retrieval. On the other hand, FT is more static, requiring
retraining for updates but enabling deep customization of the
model’s behavior and style. It demands significant compu-
tational resources for dataset preparation and training, and
while it can reduce hallucinations, it may face challenges with
unfamiliar data.
In multiple evaluations of their performance on various
knowledge-intensive tasks across different topics, [28] re-
vealed that while unsupervised fine-tuning shows some im-
provement, RAG consistently outperforms it, for both exist-
ing knowledge encountered during training and entirely new
knowledge. Additionally, it was found that LLMs struggle
to learn new factual information through unsupervised fine-
tuning. The choice between RAG and FT depends on the
specific needs for data dynamics, customization, and com-
putational capabilities in the application context. RAG and
FT are not mutually exclusive and can complement each
other, enhancing a model’s capabilities at different levels.
In some instances, their combined use may lead to optimal
performance. The optimization process involving RAG and FT
may require multiple iterations to achieve satisfactory results.
III. R ETRIEVAL
In the context of RAG, it is crucial to efficiently retrieve
relevant documents from the data source. There are several
key issues involved, such as the retrieval source, retrieval
granularity, pre-processing of the retrieval, and selection of
the corresponding embedding model.
A. Retrieval Source
RAG relies on external knowledge to enhance LLMs, while
the type of retrieval source and the granularity of retrieval
units both affect the final generation results.
1) Data Structure: Initially, text is s the mainstream source
of retrieval. Subsequently, the retrieval source expanded to in-
clude semi-structured data (PDF) and structured data (Knowl-
edge Graph, KG) for enhancement. In addition to retrieving
from original external sources, there is also a growing trend in
recent researches towards utilizing content generated by LLMs
themselves for retrieval and enhancement purposes.6
TABLE I
SUMMARY OF RAG METHODS
Method Retrieval Source Retrieval
Data Type
Retrieval
Granularity
Augmentation
Stage
Retrieval
process
CoG [29] Wikipedia Text Phrase Pre-training Iterative
DenseX [30] FactoidWiki Text Proposition Inference Once
EAR [31] Dataset-base Text Sentence Tuning Once
UPRISE [20] Dataset-base Text Sentence Tuning Once
RAST [32] Dataset-base Text Sentence Tuning Once
Self-Mem [17] Dataset-base Text Sentence Tuning Iterative
FLARE [24] Search Engine,Wikipedia Text Sentence Tuning Adaptive
PGRA [33] Wikipedia Text Sentence Inference Once
FILCO [34] Wikipedia Text Sentence Inference Once
RADA [35] Dataset-base Text Sentence Inference Once
Filter-rerank [36] Synthesized dataset Text Sentence Inference Once
R-GQA [37] Dataset-base Text Sentence Pair Tuning Once
LLM-R [38] Dataset-base Text Sentence Pair Inference Iterative
TIGER [39] Dataset-base Text Item-base Pre-training Once
LM-Indexer [40] Dataset-base Text Item-base Tuning Once
BEQUE [9] Dataset-base Text Item-base Tuning Once
CT-RAG [41] Synthesized dataset Text Item-base Tuning Once
Atlas [42] Wikipedia, Common Crawl Text Chunk Pre-training Iterative
RA VEN [43] Wikipedia Text Chunk Pre-training Once
RETRO++ [44] Pre-training Corpus Text Chunk Pre-training Iterative
INSTRUCTRETRO [45] Pre-training corpus Text Chunk Pre-training Iterative
RRR [7] Search Engine Text Chunk Tuning Once
RA-e2e [46] Dataset-base Text Chunk Tuning Once
PROMPTAGATOR [21] BEIR Text Chunk Tuning Once
AAR [47] MSMARCO,Wikipedia Text Chunk Tuning Once
RA-DIT [27] Common Crawl,Wikipedia Text Chunk Tuning Once
RAG-Robust [48] Wikipedia Text Chunk Tuning Once
RA-Long-Form [49] Dataset-base Text Chunk Tuning Once
CoN [50] Wikipedia Text Chunk Tuning Once
Self-RAG [25] Wikipedia Text Chunk Tuning Adaptive
BGM [26] Wikipedia Text Chunk Inference Once
CoQ [51] Wikipedia Text Chunk Inference Iterative
Token-Elimination [52] Wikipedia Text Chunk Inference Once
PaperQA [53] Arxiv,Online Database,PubMed Text Chunk Inference Iterative
NoiseRAG [54] FactoidWiki Text Chunk Inference Once
IAG [55] Search Engine,Wikipedia Text Chunk Inference Once
NoMIRACL [56] Wikipedia Text Chunk Inference Once
ToC [57] Search Engine,Wikipedia Text Chunk Inference Recursive
SKR [58] Dataset-base,Wikipedia Text Chunk Inference Adaptive
ITRG [59] Wikipedia Text Chunk Inference Iterative
RAG-LongContext [60] Dataset-base Text Chunk Inference Once
ITER-RETGEN [14] Wikipedia Text Chunk Inference Iterative
IRCoT [61] Wikipedia Text Chunk Inference Recursive
LLM-Knowledge-Boundary [62] Wikipedia Text Chunk Inference Once
RAPTOR [63] Dataset-base Text Chunk Inference Recursive
RECITE [22] LLMs Text Chunk Inference Once
ICRALM [64] Pile,Wikipedia Text Chunk Inference Iterative
Retrieve-and-Sample [65] Dataset-base Text Doc Tuning Once
Zemi [66] C4 Text Doc Tuning Once
CRAG [67] Arxiv Text Doc Inference Once
1-PAGER [68] Wikipedia Text Doc Inference Iterative
PRCA [69] Dataset-base Text Doc Inference Once
QLM-Doc-ranking [70] Dataset-base Text Doc Inference Once
Recomp [71] Wikipedia Text Doc Inference Once
DSP [23] Wikipedia Text Doc Inference Iterative
RePLUG [72] Pile Text Doc Inference Once
ARM-RAG [73] Dataset-base Text Doc Inference Iterative
GenRead [13] LLMs Text Doc Inference Iterative
UniMS-RAG [74] Dataset-base Text Multi Tuning Once
CREA-ICL [19] Dataset-base Crosslingual,Text Sentence Inference Once
PKG [75] LLM Tabular,Text Chunk Inference Once
SANTA [76] Dataset-base Code,Text Item Pre-training Once
SURGE [77] Freebase KG Sub-Graph Tuning Once
MK-ToD [78] Dataset-base KG Entity Tuning Once
Dual-Feedback-ToD [79] Dataset-base KG Entity Sequence Tuning Once
KnowledGPT [15] Dataset-base KG Triplet Inference Muti-time
FABULA [80] Dataset-base,Graph KG Entity Inference Once
HyKGE [81] CMeKG KG Entity Inference Once
KALMV [82] Wikipedia KG Triplet Inference Iterative
RoG [83] Freebase KG Triplet Inference Iterative
G-Retriever [84] Dataset-base TextGraph Sub-Graph Inference Once7
Fig. 4. RAG compared with other model optimization methods in the aspects of “External Knowledge Required” and “Model Adaption Required”. Prompt
Engineering requires low modifications to the model and external knowledge, focusing on harnessing the capabilities of LLMs themselves. Fine-tuning, on
the other hand, involves further training the model. In the early stages of RAG (Naive RAG), there is a low demand for model modifications. As research
progresses, Modular RAG has become more integrated with fine-tuning techniques.
Unstructured Data , such as text, is the most widely used
retrieval source, which are mainly gathered from corpus. For
open-domain question-answering (ODQA) tasks, the primary
retrieval sources are Wikipedia Dump with the current major
versions including HotpotQA 4 (1st October , 2017), DPR5 (20
December, 2018). In addition to encyclopedic data, common
unstructured data includes cross-lingual text [19] and domain-
specific data (such as medical [67]and legal domains [29]).
Semi-structured data. typically refers to data that contains a
combination of text and table information, such as PDF. Han-
dling semi-structured data poses challenges for conventional
RAG systems due to two main reasons. Firstly, text splitting
processes may inadvertently separate tables, leading to data
corruption during retrieval. Secondly, incorporating tables into
the data can complicate semantic similarity searches. When
dealing with semi-structured data, one approach involves lever-
aging the code capabilities of LLMs to execute Text-2-SQL
queries on tables within databases, such as TableGPT [85].
Alternatively, tables can be transformed into text format for
further analysis using text-based methods [75]. However, both
of these methods are not optimal solutions, indicating substan-
tial research opportunities in this area.
Structured data , such as knowledge graphs (KGs) [86] ,
which are typically verified and can provide more precise in-
formation. KnowledGPT [15] generates KB search queries and
stores knowledge in a personalized base, enhancing the RAG
model’s knowledge richness. In response to the limitations of
LLMs in understanding and answering questions about textual
graphs, G-Retriever [84] integrates Graph Neural Networks
4https://hotpotqa.github.io/wiki-readme.html
5https://github.com/facebookresearch/DPR
(GNNs), LLMs and RAG, enhancing graph comprehension
and question-answering capabilities through soft prompting
of the LLM, and employs the Prize-Collecting Steiner Tree
(PCST) optimization problem for targeted graph retrieval. On
the contrary, it requires additional effort to build, validate,
and maintain structured databases. On the contrary, it requires
additional effort to build, validate, and maintain structured
databases.
LLMs-Generated Content. Addressing the limitations of
external auxiliary information in RAG, some research has
focused on exploiting LLMs’ internal knowledge. SKR [58]
classifies questions as known or unknown, applying retrieval
enhancement selectively. GenRead [13] replaces the retriever
with an LLM generator, finding that LLM-generated contexts
often contain more accurate answers due to better alignment
with the pre-training objectives of causal language modeling.
Selfmem [17] iteratively creates an unbounded memory pool
with a retrieval-enhanced generator, using a memory selec-
tor to choose outputs that serve as dual problems to the
original question, thus self-enhancing the generative model.
These methodologies underscore the breadth of innovative
data source utilization in RAG, striving to improve model
performance and task effectiveness.
2) Retrieval Granularity: Another important factor besides
the data format of the retrieval source is the granularity of
the retrieved data. Coarse-grained retrieval units theoretically
can provide more relevant information for the problem, but
they may also contain redundant content, which could distract
the retriever and language models in downstream tasks [50],
[87]. On the other hand, fine-grained retrieval unit granularity
increases the burden of retrieval and does not guarantee seman-
tic integrity and meeting the required knowledge. Choosing8
the appropriate retrieval granularity during inference can be
a simple and effective strategy to improve the retrieval and
downstream task performance of dense retrievers.
In text, retrieval granularity ranges from fine to coarse,
including Token, Phrase, Sentence, Proposition, Chunks, Doc-
ument. Among them, DenseX [30]proposed the concept of
using propositions as retrieval units. Propositions are defined
as atomic expressions in the text, each encapsulating a unique
factual segment and presented in a concise, self-contained nat-
ural language format. This approach aims to enhance retrieval
precision and relevance. On the Knowledge Graph (KG),
retrieval granularity includes Entity, Triplet, and sub-Graph.
The granularity of retrieval can also be adapted to downstream
tasks, such as retrieving Item IDs [40]in recommendation tasks
and Sentence pairs [38]. Detailed information is illustrated in
Table I.
B. Indexing Optimization
In the Indexing phase, documents will be processed, seg-
mented, and transformed into Embeddings to be stored in a
vector database. The quality of index construction determines
whether the correct context can be obtained in the retrieval
phase.
1) Chunking Strategy: The most common method is to split
the document into chunks on a fixed number of tokens (e.g.,
100, 256, 512) [88]. Larger chunks can capture more context,
but they also generate more noise, requiring longer processing
time and higher costs. While smaller chunks may not fully
convey the necessary context, they do have less noise. How-
ever, chunks leads to truncation within sentences, prompting
the optimization of a recursive splits and sliding window meth-
ods, enabling layered retrieval by merging globally related
information across multiple retrieval processes [89]. Never-
theless, these approaches still cannot strike a balance between
semantic completeness and context length. Therefore, methods
like Small2Big have been proposed, where sentences (small)
are used as the retrieval unit, and the preceding and following
sentences are provided as (big) context to LLMs [90].
2) Metadata Attachments: Chunks can be enriched with
metadata information such as page number, file name, au-
thor,category timestamp. Subsequently, retrieval can be filtered
based on this metadata, limiting the scope of the retrieval.
Assigning different weights to document timestamps during
retrieval can achieve time-aware RAG, ensuring the freshness
of knowledge and avoiding outdated information.
In addition to extracting metadata from the original doc-
uments, metadata can also be artificially constructed. For
example, adding summaries of paragraph, as well as intro-
ducing hypothetical questions. This method is also known as
Reverse HyDE. Specifically, using LLM to generate questions
that can be answered by the document, then calculating the
similarity between the original question and the hypothetical
question during retrieval to reduce the semantic gap between
the question and the answer.
3) Structural Index: One effective method for enhancing
information retrieval is to establish a hierarchical structure for
the documents. By constructing In structure, RAG system can
expedite the retrieval and processing of pertinent data.
Hierarchical index structure . File are arranged in parent-
child relationships, with chunks linked to them. Data sum-
maries are stored at each node, aiding in the swift traversal
of data and assisting the RAG system in determining which
chunks to extract. This approach can also mitigate the illusion
caused by block extraction issues.
Knowledge Graph index . Utilize KG in constructing the
hierarchical structure of documents contributes to maintaining
consistency. It delineates the connections between different
concepts and entities, markedly reducing the potential for
illusions. Another advantage is the transformation of the
information retrieval process into instructions that LLM can
comprehend, thereby enhancing the accuracy of knowledge
retrieval and enabling LLM to generate contextually coherent
responses, thus improving the overall efficiency of the RAG
system. To capture the logical relationship between document
content and structure, KGP [91] proposed a method of building
an index between multiple documents using KG. This KG
consists of nodes (representing paragraphs or structures in the
documents, such as pages and tables) and edges (indicating
semantic/lexical similarity between paragraphs or relationships
within the document structure), effectively addressing knowl-
edge retrieval and reasoning problems in a multi-document
environment.
C. Query Optimization
One of the primary challenges with Naive RAG is its
direct reliance on the user’s original query as the basis for
retrieval. Formulating a precise and clear question is difficult,
and imprudent queries result in subpar retrieval effectiveness.
Sometimes, the question itself is complex, and the language
is not well-organized. Another difficulty lies in language
complexity ambiguity. Language models often struggle when
dealing with specialized vocabulary or ambiguous abbrevi-
ations with multiple meanings. For instance, they may not
discern whether “LLM” refers to large language model or a
Master of Laws in a legal context.
1) Query Expansion: Expanding a single query into mul-
tiple queries enriches the content of the query, providing
further context to address any lack of specific nuances, thereby
ensuring the optimal relevance of the generated answers.
Multi-Query. By employing prompt engineering to expand
queries via LLMs, these queries can then be executed in
parallel. The expansion of queries is not random, but rather
meticulously designed.
Sub-Query. The process of sub-question planning represents
the generation of the necessary sub-questions to contextualize
and fully answer the original question when combined. This
process of adding relevant context is, in principle, similar
to query expansion. Specifically, a complex question can be
decomposed into a series of simpler sub-questions using the
least-to-most prompting method [92].
Chain-of-Verification(CoVe). The expanded queries undergo
validation by LLM to achieve the effect of reducing halluci-
nations. Validated expanded queries typically exhibit higher
reliability [93].9
2) Query Transformation: The core concept is to retrieve
chunks based on a transformed query instead of the user’s
original query.
Query Rewrite.The original queries are not always optimal
for LLM retrieval, especially in real-world scenarios. There-
fore, we can prompt LLM to rewrite the queries. In addition to
using LLM for query rewriting, specialized smaller language
models, such as RRR (Rewrite-retrieve-read) [7]. The imple-
mentation of the query rewrite method in the Taobao, known
as BEQUE [9] has notably enhanced recall effectiveness for
long-tail queries, resulting in a rise in GMV .
Another query transformation method is to use prompt
engineering to let LLM generate a query based on the original
query for subsequent retrieval. HyDE [11] construct hypothet-
ical documents (assumed answers to the original query). It
focuses on embedding similarity from answer to answer rather
than seeking embedding similarity for the problem or query.
Using the Step-back Prompting method [10], the original
query is abstracted to generate a high-level concept question
(step-back question). In the RAG system, both the step-back
question and the original query are used for retrieval, and both
the results are utilized as the basis for language model answer
generation.
3) Query Routing: Based on varying queries, routing to
distinct RAG pipeline,which is suitable for a versatile RAG
system designed to accommodate diverse scenarios.
Metadata Router/ Filter . The first step involves extracting
keywords (entity) from the query, followed by filtering based
on the keywords and metadata within the chunks to narrow
down the search scope.
Semantic Router is another method of routing involves
leveraging the semantic information of the query. Specific
apprach see Semantic Router 6. Certainly, a hybrid routing
approach can also be employed, combining both semantic and
metadata-based methods for enhanced query routing.
D. Embedding
In RAG, retrieval is achieved by calculating the similarity
(e.g. cosine similarity) between the embeddings of the ques-
tion and document chunks, where the semantic representation
capability of embedding models plays a key role. This mainly
includes a sparse encoder (BM25) and a dense retriever (BERT
architecture Pre-training language models). Recent research
has introduced prominent embedding models such as AngIE,
V oyage, BGE,etc [94]–[96], which are benefit from multi-task
instruct tuning. Hugging Face’s MTEB leaderboard 7 evaluates
embedding models across 8 tasks, covering 58 datasests. Ad-
ditionally, C-MTEB focuses on Chinese capability, covering
6 tasks and 35 datasets. There is no one-size-fits-all answer
to “which embedding model to use.” However, some specific
models are better suited for particular use cases.
1) Mix/hybrid Retrieval : Sparse and dense embedding
approaches capture different relevance features and can ben-
efit from each other by leveraging complementary relevance
information. For instance, sparse retrieval models can be used
6https://github.com/aurelio-labs/semantic-router
7https://huggingface.co/spaces/mteb/leaderboard
to provide initial search results for training dense retrieval
models. Additionally, pre-training language models (PLMs)
can be utilized to learn term weights to enhance sparse
retrieval. Specifically, it also demonstrates that sparse retrieval
models can enhance the zero-shot retrieval capability of dense
retrieval models and assist dense retrievers in handling queries
containing rare entities, thereby improving robustness.
2) Fine-tuning Embedding Model: In instances where the
context significantly deviates from pre-training corpus, partic-
ularly within highly specialized disciplines such as healthcare,
legal practice, and other sectors replete with proprietary jargon,
fine-tuning the embedding model on your own domain dataset
becomes essential to mitigate such discrepancies.
In addition to supplementing domain knowledge, another
purpose of fine-tuning is to align the retriever and generator,
for example, using the results of LLM as the supervision signal
for fine-tuning, known as LSR (LM-supervised Retriever).
PROMPTAGATOR [21] utilizes the LLM as a few-shot query
generator to create task-specific retrievers, addressing chal-
lenges in supervised fine-tuning, particularly in data-scarce
domains. Another approach, LLM-Embedder [97], exploits
LLMs to generate reward signals across multiple downstream
tasks. The retriever is fine-tuned with two types of supervised
signals: hard labels for the dataset and soft rewards from
the LLMs. This dual-signal approach fosters a more effective
fine-tuning process, tailoring the embedding model to diverse
downstream applications. REPLUG [72] utilizes a retriever
and an LLM to calculate the probability distributions of the
retrieved documents and then performs supervised training
by computing the KL divergence. This straightforward and
effective training method enhances the performance of the
retrieval model by using an LM as the supervisory signal,
eliminating the need for specific cross-attention mechanisms.
Moreover, inspired by RLHF (Reinforcement Learning from
Human Feedback), utilizing LM-based feedback to reinforce
the retriever through reinforcement learning.
E. Adapter
Fine-tuning models may present challenges, such as in-
tegrating functionality through an API or addressing con-
straints arising from limited local computational resources.
Consequently, some approaches opt to incorporate an external
adapter to aid in alignment.
To optimize the multi-task capabilities of LLM, UP-
RISE [20] trained a lightweight prompt retriever that can
automatically retrieve prompts from a pre-built prompt pool
that are suitable for a given zero-shot task input. AAR
(Augmentation-Adapted Retriver) [47] introduces a universal
adapter designed to accommodate multiple downstream tasks.
While PRCA [69] add a pluggable reward-driven contextual
adapter to enhance performance on specific tasks. BGM [26]
keeps the retriever and LLM fixed,and trains a bridge Seq2Seq
model in between. The bridge model aims to transform the
retrieved information into a format that LLMs can work with
effectively, allowing it to not only rerank but also dynami-
cally select passages for each query, and potentially employ
more advanced strategies like repetition. Furthermore, PKG10
introduces an innovative method for integrating knowledge
into white-box models via directive fine-tuning [75]. In this
approach, the retriever module is directly substituted to gen-
erate relevant documents according to a query. This method
assists in addressing the difficulties encountered during the
fine-tuning process and enhances model performance.
IV. G ENERATION
After retrieval, it is not a good practice to directly input all
the retrieved information to the LLM for answering questions.
Following will introduce adjustments from two perspectives:
adjusting the retrieved content and adjusting the LLM.
A. Context Curation
Redundant information can interfere with the final gener-
ation of LLM, and overly long contexts can also lead LLM
to the “Lost in the middle” problem [98]. Like humans, LLM
tends to only focus on the beginning and end of long texts,
while forgetting the middle portion. Therefore, in the RAG
system, we typically need to further process the retrieved
content.
1) Reranking: Reranking fundamentally reorders document
chunks to highlight the most pertinent results first, effectively
reducing the overall document pool, severing a dual purpose
in information retrieval, acting as both an enhancer and a
filter, delivering refined inputs for more precise language
model processing [70]. Reranking can be performed using
rule-based methods that depend on predefined metrics like
Diversity, Relevance, and MRR, or model-based approaches
like Encoder-Decoder models from the BERT series (e.g.,
SpanBERT), specialized reranking models such as Cohere
rerank or bge-raranker-large, and general large language mod-
els like GPT [12], [99].
2) Context Selection/Compression: A common misconcep-
tion in the RAG process is the belief that retrieving as many
relevant documents as possible and concatenating them to form
a lengthy retrieval prompt is beneficial. However, excessive
context can introduce more noise, diminishing the LLM’s
perception of key information .
(Long) LLMLingua [100], [101] utilize small language
models (SLMs) such as GPT-2 Small or LLaMA-7B, to
detect and remove unimportant tokens, transforming it into
a form that is challenging for humans to comprehend but
well understood by LLMs. This approach presents a direct
and practical method for prompt compression, eliminating the
need for additional training of LLMs while balancing language
integrity and compression ratio. PRCA tackled this issue by
training an information extractor [69]. Similarly, RECOMP
adopts a comparable approach by training an information
condenser using contrastive learning [71]. Each training data
point consists of one positive sample and five negative sam-
ples, and the encoder undergoes training using contrastive loss
throughout this process [102] .
In addition to compressing the context, reducing the num-
ber of documents aslo helps improve the accuracy of the
model’s answers. Ma et al. [103] propose the “Filter-Reranker”
paradigm, which combines the strengths of LLMs and SLMs.
In this paradigm, SLMs serve as filters, while LLMs function
as reordering agents. The research shows that instructing
LLMs to rearrange challenging samples identified by SLMs
leads to significant improvements in various Information
Extraction (IE) tasks. Another straightforward and effective
approach involves having the LLM evaluate the retrieved
content before generating the final answer. This allows the
LLM to filter out documents with poor relevance through LLM
critique. For instance, in Chatlaw [104], the LLM is prompted
to self-suggestion on the referenced legal provisions to assess
their relevance.
B. LLM Fine-tuning
Targeted fine-tuning based on the scenario and data char-
acteristics on LLMs can yield better results. This is also one
of the greatest advantages of using on-premise LLMs. When
LLMs lack data in a specific domain, additional knowledge can
be provided to the LLM through fine-tuning. Huggingface’s
fine-tuning data can also be used as an initial step.
Another benefit of fine-tuning is the ability to adjust the
model’s input and output. For example, it can enable LLM to
adapt to specific data formats and generate responses in a par-
ticular style as instructed [37]. For retrieval tasks that engage
with structured data, the SANTA framework [76] implements
a tripartite training regimen to effectively encapsulate both
structural and semantic nuances. The initial phase focuses on
the retriever, where contrastive learning is harnessed to refine
the query and document embeddings.
Aligning LLM outputs with human or retriever preferences
through reinforcement learning is a potential approach. For
instance, manually annotating the final generated answers
and then providing feedback through reinforcement learning.
In addition to aligning with human preferences, it is also
possible to align with the preferences of fine-tuned models
and retrievers [79]. When circumstances prevent access to
powerful proprietary models or larger parameter open-source
models, a simple and effective method is to distill the more
powerful models(e.g. GPT-4). Fine-tuning of LLM can also
be coordinated with fine-tuning of the retriever to align pref-
erences. A typical approach, such as RA-DIT [27], aligns the
scoring functions between Retriever and Generator using KL
divergence.
V. A UGMENTATION PROCESS IN RAG
In the domain of RAG, the standard practice often involves
a singular (once) retrieval step followed by generation, which
can lead to inefficiencies and sometimes is typically insuffi-
cient for complex problems demanding multi-step reasoning,
as it provides a limited scope of information [105]. Many
studies have optimized the retrieval process in response to this
issue, and we have summarised them in Figure 5.
A. Iterative Retrieval
Iterative retrieval is a process where the knowledge base
is repeatedly searched based on the initial query and the text
generated so far, providing a more comprehensive knowledge11
Fig. 5. In addition to the most common once retrieval, RAG also includes three types of retrieval augmentation processes. (left) Iterative retrieval involves
alternating between retrieval and generation, allowing for richer and more targeted context from the knowledge base at each step. (Middle) Recursive retrieval
involves gradually refining the user query and breaking down the problem into sub-problems, then continuously solving complex problems through retrieval
and generation. (Right) Adaptive retrieval focuses on enabling the RAG system to autonomously determine whether external knowledge retrieval is necessary
and when to stop retrieval and generation, often utilizing LLM-generated special tokens for control.
base for LLMs. This approach has been shown to enhance
the robustness of subsequent answer generation by offering
additional contextual references through multiple retrieval
iterations. However, it may be affected by semantic discon-
tinuity and the accumulation of irrelevant information. ITER-
RETGEN [14] employs a synergistic approach that lever-
ages “retrieval-enhanced generation” alongside “generation-
enhanced retrieval” for tasks that necessitate the reproduction
of specific information. The model harnesses the content
required to address the input task as a contextual basis for
retrieving pertinent knowledge, which in turn facilitates the
generation of improved responses in subsequent iterations.
B. Recursive Retrieval
Recursive retrieval is often used in information retrieval and
NLP to improve the depth and relevance of search results.
The process involves iteratively refining search queries based
on the results obtained from previous searches. Recursive
Retrieval aims to enhance the search experience by gradu-
ally converging on the most pertinent information through a
feedback loop. IRCoT [61] uses chain-of-thought to guide
the retrieval process and refines the CoT with the obtained
retrieval results. ToC [57] creates a clarification tree that
systematically optimizes the ambiguous parts in the Query. It
can be particularly useful in complex search scenarios where
the user’s needs are not entirely clear from the outset or where
the information sought is highly specialized or nuanced. The
recursive nature of the process allows for continuous learning
and adaptation to the user’s requirements, often resulting in
improved satisfaction with the search outcomes.
To address specific data scenarios, recursive retrieval and
multi-hop retrieval techniques are utilized together. Recursive
retrieval involves a structured index to process and retrieve
data in a hierarchical manner, which may include summarizing
sections of a document or lengthy PDF before performing a
retrieval based on this summary. Subsequently, a secondary
retrieval within the document refines the search, embodying
the recursive nature of the process. In contrast, multi-hop
retrieval is designed to delve deeper into graph-structured data
sources, extracting interconnected information [106].
C. Adaptive Retrieval
Adaptive retrieval methods, exemplified by Flare [24] and
Self-RAG [25], refine the RAG framework by enabling LLMs
to actively determine the optimal moments and content for
retrieval, thus enhancing the efficiency and relevance of the
information sourced.
These methods are part of a broader trend wherein
LLMs employ active judgment in their operations, as seen
in model agents like AutoGPT, Toolformer, and Graph-
Toolformer [107]–[109]. Graph-Toolformer, for instance, di-
vides its retrieval process into distinct steps where LLMs
proactively use retrievers, apply Self-Ask techniques, and em-
ploy few-shot prompts to initiate search queries. This proactive
stance allows LLMs to decide when to search for necessary
information, akin to how an agent utilizes tools.
WebGPT [110] integrates a reinforcement learning frame-
work to train the GPT-3 model in autonomously using a
search engine during text generation. It navigates this process
using special tokens that facilitate actions such as search
engine queries, browsing results, and citing references, thereby
expanding GPT-3’s capabilities through the use of external
search engines. Flare automates timing retrieval by monitoring
the confidence of the generation process, as indicated by the12
probability of generated terms [24]. When the probability falls
below a certain threshold would activates the retrieval system
to collect relevant information, thus optimizing the retrieval
cycle. Self-RAG [25] introduces “reflection tokens” that allow
the model to introspect its outputs. These tokens come in
two varieties: “retrieve” and “critic”. The model autonomously
decides when to activate retrieval, or alternatively, a predefined
threshold may trigger the process. During retrieval, the gen-
erator conducts a fragment-level beam search across multiple
paragraphs to derive the most coherent sequence. Critic scores
are used to update the subdivision scores, with the flexibility
to adjust these weights during inference, tailoring the model’s
behavior. Self-RAG’s design obviates the need for additional
classifiers or reliance on Natural Language Inference (NLI)
models, thus streamlining the decision-making process for
when to engage retrieval mechanisms and improving the
model’s autonomous judgment capabilities in generating ac-
curate responses.
VI. T ASK AND EVALUATION
The rapid advancement and growing adoption of RAG
in the field of NLP have propelled the evaluation of RAG
models to the forefront of research in the LLMs community.
The primary objective of this evaluation is to comprehend
and optimize the performance of RAG models across diverse
application scenarios.This chapter will mainly introduce the
main downstream tasks of RAG, datasets, and how to evaluate
RAG systems.
A. Downstream Task
The core task of RAG remains Question Answering (QA),
including traditional single-hop/multi-hop QA, multiple-
choice, domain-specific QA as well as long-form scenarios
suitable for RAG. In addition to QA, RAG is continuously
being expanded into multiple downstream tasks, such as Infor-
mation Extraction (IE), dialogue generation, code search, etc.
The main downstream tasks of RAG and their corresponding
datasets are summarized in Table II.
B. Evaluation Target
Historically, RAG models assessments have centered on
their execution in specific downstream tasks. These evaluations
employ established metrics suitable to the tasks at hand. For
instance, question answering evaluations might rely on EM
and F1 scores [7], [45], [59], [72], whereas fact-checking
tasks often hinge on Accuracy as the primary metric [4],
[14], [42]. BLEU and ROUGE metrics are also commonly
used to evaluate answer quality [26], [32], [52], [78]. Tools
like RALLE, designed for the automatic evaluation of RAG
applications, similarly base their assessments on these task-
specific metrics [160]. Despite this, there is a notable paucity
of research dedicated to evaluating the distinct characteristics
of RAG models.The main evaluation objectives include:
Retrieval Quality. Evaluating the retrieval quality is crucial
for determining the effectiveness of the context sourced by
the retriever component. Standard metrics from the domains
of search engines, recommendation systems, and information
retrieval systems are employed to measure the performance of
the RAG retrieval module. Metrics such as Hit Rate, MRR, and
NDCG are commonly utilized for this purpose [161], [162].
Generation Quality . The assessment of generation quality
centers on the generator’s capacity to synthesize coherent and
relevant answers from the retrieved context. This evaluation
can be categorized based on the content’s objectives: unlabeled
and labeled content. For unlabeled content, the evaluation
encompasses the faithfulness, relevance, and non-harmfulness
of the generated answers. In contrast, for labeled content,
the focus is on the accuracy of the information produced by
the model [161]. Additionally, both retrieval and generation
quality assessments can be conducted through manual or
automatic evaluation methods [29], [161], [163].
C. Evaluation Aspects
Contemporary evaluation practices of RAG models empha-
size three primary quality scores and four essential abilities,
which collectively inform the evaluation of the two principal
targets of the RAG model: retrieval and generation.
1) Quality Scores: Quality scores include context rele-
vance, answer faithfulness, and answer relevance. These qual-
ity scores evaluate the efficiency of the RAG model from
different perspectives in the process of information retrieval
and generation [164]–[166].
Context Relevance evaluates the precision and specificity
of the retrieved context, ensuring relevance and minimizing
processing costs associated with extraneous content.
Answer Faithfulness ensures that the generated answers
remain true to the retrieved context, maintaining consistency
and avoiding contradictions.
Answer Relevance requires that the generated answers are
directly pertinent to the posed questions, effectively addressing
the core inquiry.
2) Required Abilities: RAG evaluation also encompasses
four abilities indicative of its adaptability and efficiency:
noise robustness, negative rejection, information integration,
and counterfactual robustness [167], [168]. These abilities are
critical for the model’s performance under various challenges
and complex scenarios, impacting the quality scores.
Noise Robustness appraises the model’s capability to man-
age noise documents that are question-related but lack sub-
stantive information.
Negative Rejection assesses the model’s discernment in
refraining from responding when the retrieved documents do
not contain the necessary knowledge to answer a question.
Information Integration evaluates the model’s proficiency in
synthesizing information from multiple documents to address
complex questions.
Counterfactual Robustness tests the model’s ability to rec-
ognize and disregard known inaccuracies within documents,
even when instructed about potential misinformation.
Context relevance and noise robustness are important for
evaluating the quality of retrieval, while answer faithfulness,
answer relevance, negative rejection, information integration,
and counterfactual robustness are important for evaluating the
quality of generation.13
TABLE II
DOWNSTREAM TASKS AND DATASETS OF RAG
Task Sub Task Dataset Method
QA Single-hop Natural Qustion(NQ) [111]
[26], [30], [34], [42], [45], [50], [52], [59], [64], [82]
[3], [4], [22], [27], [40], [43], [54], [62], [71], [112]
[20], [44], [72]
TriviaQA(TQA) [113]
[13], [30], [34], [45], [50], [64]
[4], [27], [59], [62], [112]
[22], [25], [43], [44], [71], [72]
SQuAD [114] [20], [23], [30], [32], [45], [69], [112]
Web Questions(WebQ) [115] [3], [4], [13], [30], [50], [68]
PopQA [116] [7], [25], [67]
MS MARCO [117] [4], [40], [52]
Multi-hop HotpotQA [118] [23], [26], [31], [34], [47], [51], [61], [82]
[7], [14], [22], [27], [59], [62], [69], [71], [91]
2WikiMultiHopQA [119] [14], [24], [48], [59], [61], [91]
MuSiQue [120] [14], [51], [61], [91]
Long-form QA ELI5 [121] [27], [34], [43], [49], [51]
NarrativeQA(NQA) [122] [45], [60], [63], [123]
ASQA [124] [24], [57]
QMSum(QM) [125] [60], [123]
Domain QA Qasper [126] [60], [63]
COVID-QA [127] [35], [46]
CMB [128],MMCU Medical [129] [81]
Multi-Choice QA QuALITY [130] [60], [63]
ARC [131] [25], [67]
CommonsenseQA [132] [58], [66]
Graph QA GraphQA [84] [84]
Dialog Dialog Generation Wizard of Wikipedia (WoW) [133] [13], [27], [34], [42]
Personal Dialog KBP [134] [74], [135]
DuleMon [136] [74]
Task-oriented Dialog CamRest [137] [78], [79]
Recommendation Amazon(Toys,Sport,Beauty) [138] [39], [40]
IE Event Argument Extraction WikiEvent [139] [13], [27], [37], [42]
RAMS [140] [36], [37]
Relation Extraction T-REx [141],ZsRE [142] [27], [51]
Reasoning Commonsense Reasoning HellaSwag [143] [20], [66]
CoT Reasoning CoT Reasoning [144] [27]
Complex Reasoning CSQA [145] [55]
Others Language Understanding MMLU [146] [7], [27], [28], [42], [43], [47], [72]
Language Modeling WikiText-103 [147] [5], [29], [64], [71]
StrategyQA [148] [14], [24], [48], [51], [55], [58]
Fact Checking/Verification FEVER [149] [4], [13], [27], [34], [42], [50]
PubHealth [150] [25], [67]
Text Generation Biography [151] [67]
Text Summarization WikiASP [152] [24]
XSum [153] [17]
Text Classification VioLens [154] [19]
TREC [155] [33]
Sentiment SST-2 [156] [20], [33], [38]
Code Search CodeSearchNet [157] [76]
Robustness Evaluation NoMIRACL [56] [56]
Math GSM8K [158] [73]
Machine Translation JRC-Acquis [159] [17]14
TABLE III
SUMMARY OF METRICS APPLICABLE FOR EVALUATION ASPECTS OF RAG
Context
Relevance Faithfulness Answer
Relevance
Noise
Robustness
Negative
Rejection
Information
Integration
Counterfactual
Robustness
Accuracy ✓ ✓ ✓ ✓ ✓ ✓ ✓
EM ✓
Recall ✓
Precision ✓ ✓
R-Rate ✓
Cosine Similarity ✓
Hit Rate ✓
MRR ✓
NDCG ✓
BLEU ✓ ✓ ✓
ROUGE/ROUGE-L ✓ ✓ ✓
The specific metrics for each evaluation aspect are sum-
marized in Table III. It is essential to recognize that these
metrics, derived from related work, are traditional measures
and do not yet represent a mature or standardized approach for
quantifying RAG evaluation aspects. Custom metrics tailored
to the nuances of RAG models, though not included here, have
also been developed in some evaluation studies.
D. Evaluation Benchmarks and Tools
A series of benchmark tests and tools have been proposed
to facilitate the evaluation of RAG.These instruments furnish
quantitative metrics that not only gauge RAG model perfor-
mance but also enhance comprehension of the model’s capabil-
ities across various evaluation aspects. Prominent benchmarks
such as RGB, RECALL and CRUD [167]–[169] focus on
appraising the essential abilities of RAG models. Concur-
rently, state-of-the-art automated tools like RAGAS [164],
ARES [165], and TruLens 8 employ LLMs to adjudicate the
quality scores. These tools and benchmarks collectively form
a robust framework for the systematic evaluation of RAG
models, as summarized in Table IV.
VII. D ISCUSSION AND FUTURE PROSPECTS
Despite the considerable progress in RAG technology, sev-
eral challenges persist that warrant in-depth research.This
chapter will mainly introduce the current challenges and future
research directions faced by RAG.
A. RAG vs Long Context
With the deepening of related research, the context of LLMs
is continuously expanding [170]–[172]. Presently, LLMs can
effortlessly manage contexts exceeding 200,000 tokens 9. This
capability signifies that long-document question answering,
previously reliant on RAG, can now incorporate the entire
document directly into the prompt. This has also sparked
discussions on whether RAG is still necessary when LLMs
8https://www.trulens.org/trulens eval/core concepts rag triad/
9https://kimi.moonshot.cn
are not constrained by context. In fact, RAG still plays an
irreplaceable role. On one hand, providing LLMs with a
large amount of context at once will significantly impact its
inference speed, while chunked retrieval and on-demand input
can significantly improve operational efficiency. On the other
hand, RAG-based generation can quickly locate the original
references for LLMs to help users verify the generated an-
swers. The entire retrieval and reasoning process is observable,
while generation solely relying on long context remains a
black box. Conversely, the expansion of context provides new
opportunities for the development of RAG, enabling it to
address more complex problems and integrative or summary
questions that require reading a large amount of material to
answer [49]. Developing new RAG methods in the context of
super-long contexts is one of the future research trends.
B. RAG Robustness
The presence of noise or contradictory information during
retrieval can detrimentally affect RAG’s output quality. This
situation is figuratively referred to as “Misinformation can
be worse than no information at all”. Improving RAG’s
resistance to such adversarial or counterfactual inputs is gain-
ing research momentum and has become a key performance
metric [48], [50], [82]. Cuconasu et al. [54] analyze which
type of documents should be retrieved, evaluate the relevance
of the documents to the prompt, their position, and the
number included in the context. The research findings reveal
that including irrelevant documents can unexpectedly increase
accuracy by over 30%, contradicting the initial assumption
of reduced quality. These results underscore the importance
of developing specialized strategies to integrate retrieval with
language generation models, highlighting the need for further
research and exploration into the robustness of RAG.
C. Hybrid Approaches
Combining RAG with fine-tuning is emerging as a leading
strategy. Determining the optimal integration of RAG and
fine-tuning whether sequential, alternating, or through end-to-
end joint training—and how to harness both parameterized15
TABLE IV
SUMMARY OF EVALUATION FRAMEWORKS
Evaluation Framework Evaluation Targets Evaluation Aspects Quantitative Metrics
RGB† Retrieval Quality
Generation Quality
Noise Robustness
Negative Rejection
Information Integration
Counterfactual Robustness
Accuracy
EM
Accuracy
Accuracy
RECALL† Generation Quality Counterfactual Robustness R-Rate (Reappearance Rate)
RAGAS‡ Retrieval Quality
Generation Quality
Context Relevance
Faithfulness
Answer Relevance
*
*
Cosine Similarity
ARES‡ Retrieval Quality
Generation Quality
Context Relevance
Faithfulness
Answer Relevance
Accuracy
Accuracy
Accuracy
TruLens‡ Retrieval Quality
Generation Quality
Context Relevance
Faithfulness
Answer Relevance
*
*
*
CRUD† Retrieval Quality
Generation Quality
Creative Generation
Knowledge-intensive QA
Error Correction
Summarization
BLEU
ROUGE-L
BertScore
RAGQuestEval
† represents a benchmark, and ‡ represents a tool. * denotes customized quantitative metrics, which deviate from traditional
metrics. Readers are encouraged to consult pertinent literature for the specific quantification formulas associated with these
metrics, as required.
and non-parameterized advantages are areas ripe for explo-
ration [27]. Another trend is to introduce SLMs with specific
functionalities into RAG and fine-tuned by the results of RAG
system. For example, CRAG [67] trains a lightweight retrieval
evaluator to assess the overall quality of the retrieved docu-
ments for a query and triggers different knowledge retrieval
actions based on confidence levels.
D. Scaling laws of RAG
End-to-end RAG models and pre-trained models based
on RAG are still one of the focuses of current re-
searchers [173].The parameters of these models are one of
the key factors.While scaling laws [174] are established for
LLMs, their applicability to RAG remains uncertain. Initial
studies like RETRO++ [44] have begun to address this, yet the
parameter count in RAG models still lags behind that of LLMs.
The possibility of an Inverse Scaling Law 10, where smaller
models outperform larger ones, is particularly intriguing and
merits further investigation.
E. Production-Ready RAG
RAG’s practicality and alignment with engineering require-
ments have facilitated its adoption. However, enhancing re-
trieval efficiency, improving document recall in large knowl-
edge bases, and ensuring data security—such as preventing
10https://github.com/inverse-scaling/prize
inadvertent disclosure of document sources or metadata by
LLMs—are critical engineering challenges that remain to be
addressed [175].
The development of the RAG ecosystem is greatly impacted
by the progression of its technical stack. Key tools like
LangChain and LLamaIndex have quickly gained popularity
with the emergence of ChatGPT, providing extensive RAG-
related APIs and becoming essential in the realm of LLMs.The
emerging technology stack, while not as rich in features as
LangChain and LLamaIndex, stands out through its specialized
products. For example, Flowise AI prioritizes a low-code
approach, allowing users to deploy AI applications, including
RAG, through a user-friendly drag-and-drop interface. Other
technologies like HayStack, Meltano, and Cohere Coral are
also gaining attention for their unique contributions to the field.
In addition to AI-focused vendors, traditional software and
cloud service providers are expanding their offerings to include
RAG-centric services. Weaviate’s Verba 11 is designed for
personal assistant applications, while Amazon’s Kendra 12
offers intelligent enterprise search services, enabling users to
browse various content repositories using built-in connectors.
In the development of RAG technology, there is a clear
trend towards different specialization directions, such as: 1)
Customization - tailoring RAG to meet specific requirements.
2) Simplification - making RAG easier to use to reduce the
11https://github.com/weaviate/Verba
12https://aws.amazon.com/cn/kendra/16
Fig. 6. Summary of RAG ecosystem
initial learning curve. 3) Specialization - optimizing RAG to
better serve production environments.
The mutual growth of RAG models and their technology
stacks is evident; technological advancements continuously
establish new standards for existing infrastructure. In turn,
enhancements to the technology stack drive the development
of RAG capabilities. RAG toolkits are converging into a
foundational technology stack, laying the groundwork for
advanced enterprise applications. However, a fully integrated,
comprehensive platform concept is still in the future, requiring
further innovation and development.
F . Multi-modal RAG
RAG has transcended its initial text-based question-
answering confines, embracing a diverse array of modal data.
This expansion has spawned innovative multimodal models
that integrate RAG concepts across various domains:
Image. RA-CM3 [176] stands as a pioneering multimodal
model of both retrieving and generating text and images.
BLIP-2 [177] leverages frozen image encoders alongside
LLMs for efficient visual language pre-training, enabling zero-
shot image-to-text conversions. The “Visualize Before You
Write” method [178] employs image generation to steer the
LM’s text generation, showing promise in open-ended text
generation tasks.
Audio and Video . The GSS method retrieves and stitches
together audio clips to convert machine-translated data into
speech-translated data [179]. UEOP marks a significant ad-
vancement in end-to-end automatic speech recognition by
incorporating external, offline strategies for voice-to-text con-
version [180]. Additionally, KNN-based attention fusion lever-
ages audio embeddings and semantically related text embed-
dings to refine ASR, thereby accelerating domain adaptation.
Vid2Seq augments language models with specialized temporal
markers, facilitating the prediction of event boundaries and
textual descriptions within a unified output sequence [181].
Code. RBPS [182] excels in small-scale learning tasks by
retrieving code examples that align with developers’ objectives
through encoding and frequency analysis. This approach has
demonstrated efficacy in tasks such as test assertion genera-
tion and program repair. For structured knowledge, the CoK
method [106] first extracts facts pertinent to the input query
from a knowledge graph, then integrates these facts as hints
within the input, enhancing performance in knowledge graph
question-answering tasks.
VIII. C ONCLUSION
The summary of this paper, as depicted in Figure 6, empha-
sizes RAG’s significant advancement in enhancing the capa-
bilities of LLMs by integrating parameterized knowledge from
language models with extensive non-parameterized data from
external knowledge bases. The survey showcases the evolution
of RAG technologies and their application on many different
tasks. The analysis outlines three developmental paradigms
within the RAG framework: Naive, Advanced, and Modu-
lar RAG, each representing a progressive enhancement over
its predecessors. RAG’s technical integration with other AI
methodologies, such as fine-tuning and reinforcement learning,
has further expanded its capabilities. Despite the progress in
RAG technology, there are research opportunities to improve
its robustness and its ability to handle extended contexts.
RAG’s application scope is expanding into multimodal do-
mains, adapting its principles to interpret and process diverse
data forms like images, videos, and code. This expansion high-
lights RAG’s significant practical implications for AI deploy-
ment, attracting interest from academic and industrial sectors.17
The growing ecosystem of RAG is evidenced by the rise in
RAG-centric AI applications and the continuous development
of supportive tools. As RAG’s application landscape broadens,
there is a need to refine evaluation methodologies to keep
pace with its evolution. Ensuring accurate and representative
performance assessments is crucial for fully capturing RAG’s
contributions to the AI research and development community.
REFERENCES
[1] N. Kandpal, H. Deng, A. Roberts, E. Wallace, and C. Raffel, “Large
language models struggle to learn long-tail knowledge,” in Interna-
tional Conference on Machine Learning . PMLR, 2023, pp. 15 696–
15 707.
[2] Y . Zhang, Y . Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang, E. Zhao,
Y . Zhang, Y . Chenet al., “Siren’s song in the ai ocean: A survey on hal-
lucination in large language models,” arXiv preprint arXiv:2309.01219,
2023.
[3] D. Arora, A. Kini, S. R. Chowdhury, N. Natarajan, G. Sinha, and
A. Sharma, “Gar-meets-rag paradigm for zero-shot information re-
trieval,” arXiv preprint arXiv:2310.20158 , 2023.
[4] P. Lewis, E. Perez, A. Piktus, F. Petroni, V . Karpukhin, N. Goyal,
H. K ¨uttler, M. Lewis, W.-t. Yih, T. Rockt ¨aschel et al. , “Retrieval-
augmented generation for knowledge-intensive nlp tasks,” Advances in
Neural Information Processing Systems, vol. 33, pp. 9459–9474, 2020.
[5] S. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Milli-
can, G. B. Van Den Driessche, J.-B. Lespiau, B. Damoc, A. Clarket al.,
“Improving language models by retrieving from trillions of tokens,”
in International conference on machine learning . PMLR, 2022, pp.
2206–2240.
[6] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,
C. Zhang, S. Agarwal, K. Slama, A. Ray et al. , “Training language
models to follow instructions with human feedback,” Advances in
neural information processing systems , vol. 35, pp. 27 730–27 744,
2022.
[7] X. Ma, Y . Gong, P. He, H. Zhao, and N. Duan, “Query rewrit-
ing for retrieval-augmented large language models,” arXiv preprint
arXiv:2305.14283, 2023.
[8] I. ILIN, “Advanced rag techniques: an il-
lustrated overview,” https://pub.towardsai.net/
advanced-rag-techniques-an-illustrated-overview-04d193d8fec6,
2023.
[9] W. Peng, G. Li, Y . Jiang, Z. Wang, D. Ou, X. Zeng, E. Chen et al. ,
“Large language model based long-tail query rewriting in taobao
search,” arXiv preprint arXiv:2311.03758 , 2023.
[10] H. S. Zheng, S. Mishra, X. Chen, H.-T. Cheng, E. H. Chi, Q. V . Le,
and D. Zhou, “Take a step back: Evoking reasoning via abstraction in
large language models,” arXiv preprint arXiv:2310.06117 , 2023.
[11] L. Gao, X. Ma, J. Lin, and J. Callan, “Precise zero-shot dense retrieval
without relevance labels,” arXiv preprint arXiv:2212.10496 , 2022.
[12] V . Blagojevi, “Enhancing rag pipelines in haystack: Introducing diver-
sityranker and lostinthemiddleranker,” https://towardsdatascience.com/
enhancing-rag-pipelines-in-haystack-45f14e2bc9f5, 2023.
[13] W. Yu, D. Iter, S. Wang, Y . Xu, M. Ju, S. Sanyal, C. Zhu, M. Zeng,
and M. Jiang, “Generate rather than retrieve: Large language models
are strong context generators,” arXiv preprint arXiv:2209.10063, 2022.
[14] Z. Shao, Y . Gong, Y . Shen, M. Huang, N. Duan, and W. Chen,
“Enhancing retrieval-augmented large language models with iterative
retrieval-generation synergy,” arXiv preprint arXiv:2305.15294 , 2023.
[15] X. Wang, Q. Yang, Y . Qiu, J. Liang, Q. He, Z. Gu, Y . Xiao,
and W. Wang, “Knowledgpt: Enhancing large language models with
retrieval and storage access on knowledge bases,” arXiv preprint
arXiv:2308.11761, 2023.
[16] A. H. Raudaschl, “Forget rag, the future
is rag-fusion,” https://towardsdatascience.com/
forget-rag-the-future-is-rag-fusion-1147298d8ad1, 2023.
[17] X. Cheng, D. Luo, X. Chen, L. Liu, D. Zhao, and R. Yan, “Lift
yourself up: Retrieval-augmented text generation with self memory,”
arXiv preprint arXiv:2305.02437 , 2023.
[18] S. Wang, Y . Xu, Y . Fang, Y . Liu, S. Sun, R. Xu, C. Zhu, and
M. Zeng, “Training data is more valuable than you think: A simple
and effective method by retrieving from training data,” arXiv preprint
arXiv:2203.08773, 2022.
[19] X. Li, E. Nie, and S. Liang, “From classification to generation:
Insights into crosslingual retrieval augmented icl,” arXiv preprint
arXiv:2311.06595, 2023.
[20] D. Cheng, S. Huang, J. Bi, Y . Zhan, J. Liu, Y . Wang, H. Sun,
F. Wei, D. Deng, and Q. Zhang, “Uprise: Universal prompt retrieval
for improving zero-shot evaluation,” arXiv preprint arXiv:2303.08518,
2023.
[21] Z. Dai, V . Y . Zhao, J. Ma, Y . Luan, J. Ni, J. Lu, A. Bakalov, K. Guu,
K. B. Hall, and M.-W. Chang, “Promptagator: Few-shot dense retrieval
from 8 examples,” arXiv preprint arXiv:2209.11755 , 2022.
[22] Z. Sun, X. Wang, Y . Tay, Y . Yang, and D. Zhou, “Recitation-augmented
language models,” arXiv preprint arXiv:2210.01296 , 2022.
[23] O. Khattab, K. Santhanam, X. L. Li, D. Hall, P. Liang, C. Potts,
and M. Zaharia, “Demonstrate-search-predict: Composing retrieval
and language models for knowledge-intensive nlp,” arXiv preprint
arXiv:2212.14024, 2022.
[24] Z. Jiang, F. F. Xu, L. Gao, Z. Sun, Q. Liu, J. Dwivedi-Yu, Y . Yang,
J. Callan, and G. Neubig, “Active retrieval augmented generation,”
arXiv preprint arXiv:2305.06983 , 2023.
[25] A. Asai, Z. Wu, Y . Wang, A. Sil, and H. Hajishirzi, “Self-rag:
Learning to retrieve, generate, and critique through self-reflection,”
arXiv preprint arXiv:2310.11511 , 2023.
[26] Z. Ke, W. Kong, C. Li, M. Zhang, Q. Mei, and M. Bendersky,
“Bridging the preference gap between retrievers and llms,” arXiv
preprint arXiv:2401.06954, 2024.
[27] X. V . Lin, X. Chen, M. Chen, W. Shi, M. Lomeli, R. James, P. Ro-
driguez, J. Kahn, G. Szilvasy, M. Lewis et al. , “Ra-dit: Retrieval-
augmented dual instruction tuning,” arXiv preprint arXiv:2310.01352 ,
2023.
[28] O. Ovadia, M. Brief, M. Mishaeli, and O. Elisha, “Fine-tuning or
retrieval? comparing knowledge injection in llms,” arXiv preprint
arXiv:2312.05934, 2023.
[29] T. Lan, D. Cai, Y . Wang, H. Huang, and X.-L. Mao, “Copy is all
you need,” in The Eleventh International Conference on Learning
Representations, 2022.
[30] T. Chen, H. Wang, S. Chen, W. Yu, K. Ma, X. Zhao, D. Yu, and
H. Zhang, “Dense x retrieval: What retrieval granularity should we
use?” arXiv preprint arXiv:2312.06648 , 2023.
[31] F. Luo and M. Surdeanu, “Divide & conquer for entailment-aware
multi-hop evidence retrieval,” arXiv preprint arXiv:2311.02616 , 2023.
[32] Q. Gou, Z. Xia, B. Yu, H. Yu, F. Huang, Y . Li, and N. Cam-Tu,
“Diversify question generation with retrieval-augmented style transfer,”
arXiv preprint arXiv:2310.14503 , 2023.
[33] Z. Guo, S. Cheng, Y . Wang, P. Li, and Y . Liu, “Prompt-guided re-
trieval augmentation for non-knowledge-intensive tasks,”arXiv preprint
arXiv:2305.17653, 2023.
[34] Z. Wang, J. Araki, Z. Jiang, M. R. Parvez, and G. Neubig, “Learning
to filter context for retrieval-augmented generation,” arXiv preprint
arXiv:2311.08377, 2023.
[35] M. Seo, J. Baek, J. Thorne, and S. J. Hwang, “Retrieval-augmented
data augmentation for low-resource domain tasks,” arXiv preprint
arXiv:2402.13482, 2024.
[36] Y . Ma, Y . Cao, Y . Hong, and A. Sun, “Large language model is not
a good few-shot information extractor, but a good reranker for hard
samples!” arXiv preprint arXiv:2303.08559 , 2023.
[37] X. Du and H. Ji, “Retrieval-augmented generative question answering
for event argument extraction,” arXiv preprint arXiv:2211.07067, 2022.
[38] L. Wang, N. Yang, and F. Wei, “Learning to retrieve in-context
examples for large language models,”arXiv preprint arXiv:2307.07164,
2023.
[39] S. Rajput, N. Mehta, A. Singh, R. H. Keshavan, T. Vu, L. Heldt,
L. Hong, Y . Tay, V . Q. Tran, J. Samostet al., “Recommender systems
with generative retrieval,” arXiv preprint arXiv:2305.05065 , 2023.
[40] B. Jin, H. Zeng, G. Wang, X. Chen, T. Wei, R. Li, Z. Wang, Z. Li,
Y . Li, H. Lu et al. , “Language models as semantic indexers,” arXiv
preprint arXiv:2310.07815, 2023.
[41] R. Anantha, T. Bethi, D. V odianik, and S. Chappidi, “Context tuning
for retrieval augmented generation,” arXiv preprint arXiv:2312.05708 ,
2023.
[42] G. Izacard, P. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick,
J. Dwivedi-Yu, A. Joulin, S. Riedel, and E. Grave, “Few-shot
learning with retrieval augmented language models,” arXiv preprint
arXiv:2208.03299, 2022.
[43] J. Huang, W. Ping, P. Xu, M. Shoeybi, K. C.-C. Chang, and B. Catan-
zaro, “Raven: In-context learning with retrieval augmented encoder-
decoder language models,” arXiv preprint arXiv:2308.07922 , 2023.18
[44] B. Wang, W. Ping, P. Xu, L. McAfee, Z. Liu, M. Shoeybi, Y . Dong,
O. Kuchaiev, B. Li, C. Xiao et al. , “Shall we pretrain autoregressive
language models with retrieval? a comprehensive study,”arXiv preprint
arXiv:2304.06762, 2023.
[45] B. Wang, W. Ping, L. McAfee, P. Xu, B. Li, M. Shoeybi, and B. Catan-
zaro, “Instructretro: Instruction tuning post retrieval-augmented pre-
training,” arXiv preprint arXiv:2310.07713 , 2023.
[46] S. Siriwardhana, R. Weerasekera, E. Wen, T. Kaluarachchi, R. Rana,
and S. Nanayakkara, “Improving the domain adaptation of retrieval
augmented generation (rag) models for open domain question answer-
ing,” Transactions of the Association for Computational Linguistics ,
vol. 11, pp. 1–17, 2023.
[47] Z. Yu, C. Xiong, S. Yu, and Z. Liu, “Augmentation-adapted retriever
improves generalization of language models as generic plug-in,” arXiv
preprint arXiv:2305.17331, 2023.
[48] O. Yoran, T. Wolfson, O. Ram, and J. Berant, “Making retrieval-
augmented language models robust to irrelevant context,” arXiv
preprint arXiv:2310.01558, 2023.
[49] H.-T. Chen, F. Xu, S. A. Arora, and E. Choi, “Understanding re-
trieval augmentation for long-form question answering,” arXiv preprint
arXiv:2310.12150, 2023.
[50] W. Yu, H. Zhang, X. Pan, K. Ma, H. Wang, and D. Yu, “Chain-of-note:
Enhancing robustness in retrieval-augmented language models,” arXiv
preprint arXiv:2311.09210, 2023.
[51] S. Xu, L. Pang, H. Shen, X. Cheng, and T.-S. Chua, “Search-in-the-
chain: Towards accurate, credible and traceable large language models
for knowledgeintensive tasks,” CoRR, vol. abs/2304.14732 , 2023.
[52] M. Berchansky, P. Izsak, A. Caciularu, I. Dagan, and M. Wasserblat,
“Optimizing retrieval-augmented reader models via token elimination,”
arXiv preprint arXiv:2310.13682 , 2023.
[53] J. L ´ala, O. O’Donoghue, A. Shtedritski, S. Cox, S. G. Rodriques,
and A. D. White, “Paperqa: Retrieval-augmented generative agent for
scientific research,” arXiv preprint arXiv:2312.07559 , 2023.
[54] F. Cuconasu, G. Trappolini, F. Siciliano, S. Filice, C. Campagnano,
Y . Maarek, N. Tonellotto, and F. Silvestri, “The power of noise:
Redefining retrieval for rag systems,” arXiv preprint arXiv:2401.14887,
2024.
[55] Z. Zhang, X. Zhang, Y . Ren, S. Shi, M. Han, Y . Wu, R. Lai, and
Z. Cao, “Iag: Induction-augmented generation framework for answer-
ing reasoning questions,” in Proceedings of the 2023 Conference on
Empirical Methods in Natural Language Processing , 2023, pp. 1–14.
[56] N. Thakur, L. Bonifacio, X. Zhang, O. Ogundepo, E. Kamalloo,
D. Alfonso-Hermelo, X. Li, Q. Liu, B. Chen, M. Rezagholizadeh et al.,
“Nomiracl: Knowing when you don’t know for robust multilingual
retrieval-augmented generation,” arXiv preprint arXiv:2312.11361 ,
2023.
[57] G. Kim, S. Kim, B. Jeon, J. Park, and J. Kang, “Tree of clarifica-
tions: Answering ambiguous questions with retrieval-augmented large
language models,” arXiv preprint arXiv:2310.14696 , 2023.
[58] Y . Wang, P. Li, M. Sun, and Y . Liu, “Self-knowledge guided
retrieval augmentation for large language models,” arXiv preprint
arXiv:2310.05002, 2023.
[59] Z. Feng, X. Feng, D. Zhao, M. Yang, and B. Qin, “Retrieval-
generation synergy augmented large language models,” arXiv preprint
arXiv:2310.05149, 2023.
[60] P. Xu, W. Ping, X. Wu, L. McAfee, C. Zhu, Z. Liu, S. Subramanian,
E. Bakhturina, M. Shoeybi, and B. Catanzaro, “Retrieval meets long
context large language models,” arXiv preprint arXiv:2310.03025 ,
2023.
[61] H. Trivedi, N. Balasubramanian, T. Khot, and A. Sabharwal, “Interleav-
ing retrieval with chain-of-thought reasoning for knowledge-intensive
multi-step questions,” arXiv preprint arXiv:2212.10509 , 2022.
[62] R. Ren, Y . Wang, Y . Qu, W. X. Zhao, J. Liu, H. Tian, H. Wu, J.-
R. Wen, and H. Wang, “Investigating the factual knowledge boundary
of large language models with retrieval augmentation,” arXiv preprint
arXiv:2307.11019, 2023.
[63] P. Sarthi, S. Abdullah, A. Tuli, S. Khanna, A. Goldie, and C. D.
Manning, “Raptor: Recursive abstractive processing for tree-organized
retrieval,” arXiv preprint arXiv:2401.18059 , 2024.
[64] O. Ram, Y . Levine, I. Dalmedigos, D. Muhlgay, A. Shashua, K. Leyton-
Brown, and Y . Shoham, “In-context retrieval-augmented language
models,” arXiv preprint arXiv:2302.00083 , 2023.
[65] Y . Ren, Y . Cao, P. Guo, F. Fang, W. Ma, and Z. Lin, “Retrieve-and-
sample: Document-level event argument extraction via hybrid retrieval
augmentation,” in Proceedings of the 61st Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers) ,
2023, pp. 293–306.
[66] Z. Wang, X. Pan, D. Yu, D. Yu, J. Chen, and H. Ji, “Zemi: Learning
zero-shot semi-parametric language models from multiple tasks,” arXiv
preprint arXiv:2210.00185, 2022.
[67] S.-Q. Yan, J.-C. Gu, Y . Zhu, and Z.-H. Ling, “Corrective retrieval
augmented generation,” arXiv preprint arXiv:2401.15884 , 2024.
[68] P. Jain, L. B. Soares, and T. Kwiatkowski, “1-pager: One pass answer
generation and evidence retrieval,” arXiv preprint arXiv:2310.16568 ,
2023.
[69] H. Yang, Z. Li, Y . Zhang, J. Wang, N. Cheng, M. Li, and J. Xiao, “Prca:
Fitting black-box large language models for retrieval question answer-
ing via pluggable reward-driven contextual adapter,” arXiv preprint
arXiv:2310.18347, 2023.
[70] S. Zhuang, B. Liu, B. Koopman, and G. Zuccon, “Open-source large
language models are strong zero-shot query likelihood models for
document ranking,” arXiv preprint arXiv:2310.13243 , 2023.
[71] F. Xu, W. Shi, and E. Choi, “Recomp: Improving retrieval-augmented
lms with compression and selective augmentation,” arXiv preprint
arXiv:2310.04408, 2023.
[72] W. Shi, S. Min, M. Yasunaga, M. Seo, R. James, M. Lewis, L. Zettle-
moyer, and W.-t. Yih, “Replug: Retrieval-augmented black-box lan-
guage models,” arXiv preprint arXiv:2301.12652 , 2023.
[73] E. Melz, “Enhancing llm intelligence with arm-rag: Auxiliary ra-
tionale memory for retrieval augmented generation,” arXiv preprint
arXiv:2311.04177, 2023.
[74] H. Wang, W. Huang, Y . Deng, R. Wang, Z. Wang, Y . Wang, F. Mi,
J. Z. Pan, and K.-F. Wong, “Unims-rag: A unified multi-source
retrieval-augmented generation for personalized dialogue systems,”
arXiv preprint arXiv:2401.13256 , 2024.
[75] Z. Luo, C. Xu, P. Zhao, X. Geng, C. Tao, J. Ma, Q. Lin, and D. Jiang,
“Augmented large language models with parametric knowledge guid-
ing,” arXiv preprint arXiv:2305.04757 , 2023.
[76] X. Li, Z. Liu, C. Xiong, S. Yu, Y . Gu, Z. Liu, and G. Yu, “Structure-
aware language model pretraining improves dense retrieval on struc-
tured data,” arXiv preprint arXiv:2305.19912 , 2023.
[77] M. Kang, J. M. Kwak, J. Baek, and S. J. Hwang, “Knowledge
graph-augmented language models for knowledge-grounded dialogue
generation,” arXiv preprint arXiv:2305.18846 , 2023.
[78] W. Shen, Y . Gao, C. Huang, F. Wan, X. Quan, and W. Bi, “Retrieval-
generation alignment for end-to-end task-oriented dialogue system,”
arXiv preprint arXiv:2310.08877 , 2023.
[79] T. Shi, L. Li, Z. Lin, T. Yang, X. Quan, and Q. Wang, “Dual-feedback
knowledge retrieval for task-oriented dialogue systems,” arXiv preprint
arXiv:2310.14528, 2023.
[80] P. Ranade and A. Joshi, “Fabula: Intelligence report generation
using retrieval-augmented narrative construction,” arXiv preprint
arXiv:2310.13848, 2023.
[81] X. Jiang, R. Zhang, Y . Xu, R. Qiu, Y . Fang, Z. Wang, J. Tang,
H. Ding, X. Chu, J. Zhao et al. , “Think and retrieval: A hypothesis
knowledge graph enhanced medical large language models,” arXiv
preprint arXiv:2312.15883, 2023.
[82] J. Baek, S. Jeong, M. Kang, J. C. Park, and S. J. Hwang,
“Knowledge-augmented language model verification,” arXiv preprint
arXiv:2310.12836, 2023.
[83] L. Luo, Y .-F. Li, G. Haffari, and S. Pan, “Reasoning on graphs: Faithful
and interpretable large language model reasoning,” arXiv preprint
arXiv:2310.01061, 2023.
[84] X. He, Y . Tian, Y . Sun, N. V . Chawla, T. Laurent, Y . LeCun,
X. Bresson, and B. Hooi, “G-retriever: Retrieval-augmented generation
for textual graph understanding and question answering,”arXiv preprint
arXiv:2402.07630, 2024.
[85] L. Zha, J. Zhou, L. Li, R. Wang, Q. Huang, S. Yang, J. Yuan, C. Su,
X. Li, A. Su et al., “Tablegpt: Towards unifying tables, nature language
and commands into one gpt,” arXiv preprint arXiv:2307.08674 , 2023.
[86] M. Gaur, K. Gunaratna, V . Srinivasan, and H. Jin, “Iseeq: Information
seeking question generation using dynamic meta-information retrieval
and knowledge graphs,” in Proceedings of the AAAI Conference on
Artificial Intelligence, vol. 36, no. 10, 2022, pp. 10 672–10 680.
[87] F. Shi, X. Chen, K. Misra, N. Scales, D. Dohan, E. H. Chi, N. Sch ¨arli,
and D. Zhou, “Large language models can be easily distracted by
irrelevant context,” in International Conference on Machine Learning .
PMLR, 2023, pp. 31 210–31 227.
[88] R. Teja, “Evaluating the ideal chunk size for a rag
system using llamaindex,” https://www.llamaindex.ai/blog/
evaluating-the-ideal-chunk-size-for-a-rag-system-using-llamaindex-6207e5d3fec5,
2023.19
[89] Langchain, “Recursively split by character,” https://python.langchain.
com/docs/modules/data connection/document transformers/recursive
text splitter, 2023.
[90] S. Yang, “Advanced rag 01: Small-to-
big retrieval,” https://towardsdatascience.com/
advanced-rag-01-small-to-big-retrieval-172181b396d4, 2023.
[91] Y . Wang, N. Lipka, R. A. Rossi, A. Siu, R. Zhang, and T. Derr,
“Knowledge graph prompting for multi-document question answering,”
arXiv preprint arXiv:2308.11730 , 2023.
[92] D. Zhou, N. Sch ¨arli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schu-
urmans, C. Cui, O. Bousquet, Q. Le et al., “Least-to-most prompting
enables complex reasoning in large language models,” arXiv preprint
arXiv:2205.10625, 2022.
[93] S. Dhuliawala, M. Komeili, J. Xu, R. Raileanu, X. Li, A. Celikyilmaz,
and J. Weston, “Chain-of-verification reduces hallucination in large
language models,” arXiv preprint arXiv:2309.11495 , 2023.
[94] X. Li and J. Li, “Angle-optimized text embeddings,” arXiv preprint
arXiv:2309.12871, 2023.
[95] V oyageAI, “V oyage’s embedding models,” https://docs.voyageai.com/
embeddings/, 2023.
[96] BAAI, “Flagembedding,” https://github.com/FlagOpen/
FlagEmbedding, 2023.
[97] P. Zhang, S. Xiao, Z. Liu, Z. Dou, and J.-Y . Nie, “Retrieve anything
to augment large language models,” arXiv preprint arXiv:2310.07554 ,
2023.
[98] N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni,
and P. Liang, “Lost in the middle: How language models use long
contexts,” arXiv preprint arXiv:2307.03172 , 2023.
[99] Y . Gao, T. Sheng, Y . Xiang, Y . Xiong, H. Wang, and J. Zhang, “Chat-
rec: Towards interactive and explainable llms-augmented recommender
system,” arXiv preprint arXiv:2303.14524 , 2023.
[100] N. Anderson, C. Wilson, and S. D. Richardson, “Lingua: Addressing
scenarios for live interpretation and automatic dubbing,” inProceedings
of the 15th Biennial Conference of the Association for Machine
Translation in the Americas (Volume 2: Users and Providers Track
and Government Track) , J. Campbell, S. Larocca, J. Marciano,
K. Savenkov, and A. Yanishevsky, Eds. Orlando, USA: Association
for Machine Translation in the Americas, Sep. 2022, pp. 202–209.
[Online]. Available: https://aclanthology.org/2022.amta-upg.14
[101] H. Jiang, Q. Wu, X. Luo, D. Li, C.-Y . Lin, Y . Yang, and L. Qiu,
“Longllmlingua: Accelerating and enhancing llms in long context
scenarios via prompt compression,” arXiv preprint arXiv:2310.06839 ,
2023.
[102] V . Karpukhin, B. O ˘guz, S. Min, P. Lewis, L. Wu, S. Edunov, D. Chen,
and W.-t. Yih, “Dense passage retrieval for open-domain question
answering,” arXiv preprint arXiv:2004.04906 , 2020.
[103] Y . Ma, Y . Cao, Y . Hong, and A. Sun, “Large language model is
not a good few-shot information extractor, but a good reranker for
hard samples!” ArXiv, vol. abs/2303.08559, 2023. [Online]. Available:
https://api.semanticscholar.org/CorpusID:257532405
[104] J. Cui, Z. Li, Y . Yan, B. Chen, and L. Yuan, “Chatlaw: Open-source
legal large language model with integrated external knowledge bases,”
arXiv preprint arXiv:2306.16092 , 2023.
[105] O. Yoran, T. Wolfson, O. Ram, and J. Berant, “Making retrieval-
augmented language models robust to irrelevant context,” arXiv
preprint arXiv:2310.01558, 2023.
[106] X. Li, R. Zhao, Y . K. Chia, B. Ding, L. Bing, S. Joty, and S. Poria,
“Chain of knowledge: A framework for grounding large language mod-
els with structured knowledge bases,”arXiv preprint arXiv:2305.13269,
2023.
[107] H. Yang, S. Yue, and Y . He, “Auto-gpt for online decision
making: Benchmarks and additional opinions,” arXiv preprint
arXiv:2306.02224, 2023.
[108] T. Schick, J. Dwivedi-Yu, R. Dess `ı, R. Raileanu, M. Lomeli, L. Zettle-
moyer, N. Cancedda, and T. Scialom, “Toolformer: Language models
can teach themselves to use tools,” arXiv preprint arXiv:2302.04761 ,
2023.
[109] J. Zhang, “Graph-toolformer: To empower llms with graph rea-
soning ability via prompt augmented by chatgpt,” arXiv preprint
arXiv:2304.11116, 2023.
[110] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim,
C. Hesse, S. Jain, V . Kosaraju, W. Saunders et al., “Webgpt: Browser-
assisted question-answering with human feedback,” arXiv preprint
arXiv:2112.09332, 2021.
[111] T. Kwiatkowski, J. Palomaki, O. Redfield, M. Collins, A. Parikh,
C. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee et al., “Natural
questions: a benchmark for question answering research,” Transactions
of the Association for Computational Linguistics , vol. 7, pp. 453–466,
2019.
[112] Y . Liu, S. Yavuz, R. Meng, M. Moorthy, S. Joty, C. Xiong, and Y . Zhou,
“Exploring the integration strategies of retriever and large language
models,” arXiv preprint arXiv:2308.12574 , 2023.
[113] M. Joshi, E. Choi, D. S. Weld, and L. Zettlemoyer, “Triviaqa: A large
scale distantly supervised challenge dataset for reading comprehen-
sion,” arXiv preprint arXiv:1705.03551 , 2017.
[114] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, “Squad: 100,000+
questions for machine comprehension of text,” arXiv preprint
arXiv:1606.05250, 2016.
[115] J. Berant, A. Chou, R. Frostig, and P. Liang, “Semantic parsing on
freebase from question-answer pairs,” in Proceedings of the 2013
conference on empirical methods in natural language processing, 2013,
pp. 1533–1544.
[116] A. Mallen, A. Asai, V . Zhong, R. Das, H. Hajishirzi, and D. Khashabi,
“When not to trust language models: Investigating effectiveness and
limitations of parametric and non-parametric memories,” arXiv preprint
arXiv:2212.10511, 2022.
[117] T. Nguyen, M. Rosenberg, X. Song, J. Gao, S. Tiwary, R. Majumder,
and L. Deng, “Ms marco: A human-generated machine reading com-
prehension dataset,” 2016.
[118] Z. Yang, P. Qi, S. Zhang, Y . Bengio, W. W. Cohen, R. Salakhutdi-
nov, and C. D. Manning, “Hotpotqa: A dataset for diverse, explain-
able multi-hop question answering,” arXiv preprint arXiv:1809.09600,
2018.
[119] X. Ho, A.-K. D. Nguyen, S. Sugawara, and A. Aizawa, “Constructing a
multi-hop qa dataset for comprehensive evaluation of reasoning steps,”
arXiv preprint arXiv:2011.01060 , 2020.
[120] H. Trivedi, N. Balasubramanian, T. Khot, and A. Sabharwal, “Musique:
Multihop questions via single-hop question composition,” Transactions
of the Association for Computational Linguistics , vol. 10, pp. 539–554,
2022.
[121] A. Fan, Y . Jernite, E. Perez, D. Grangier, J. Weston, and M. Auli, “Eli5:
Long form question answering,” arXiv preprint arXiv:1907.09190 ,
2019.
[122] T. Ko ˇcisk`y, J. Schwarz, P. Blunsom, C. Dyer, K. M. Hermann, G. Melis,
and E. Grefenstette, “The narrativeqa reading comprehension chal-
lenge,” Transactions of the Association for Computational Linguistics ,
vol. 6, pp. 317–328, 2018.
[123] K.-H. Lee, X. Chen, H. Furuta, J. Canny, and I. Fischer, “A human-
inspired reading agent with gist memory of very long contexts,” arXiv
preprint arXiv:2402.09727, 2024.
[124] I. Stelmakh, Y . Luan, B. Dhingra, and M.-W. Chang, “Asqa: Factoid
questions meet long-form answers,” arXiv preprint arXiv:2204.06092 ,
2022.
[125] M. Zhong, D. Yin, T. Yu, A. Zaidi, M. Mutuma, R. Jha, A. H.
Awadallah, A. Celikyilmaz, Y . Liu, X. Qiu et al. , “Qmsum: A new
benchmark for query-based multi-domain meeting summarization,”
arXiv preprint arXiv:2104.05938 , 2021.
[126] P. Dasigi, K. Lo, I. Beltagy, A. Cohan, N. A. Smith, and M. Gardner,
“A dataset of information-seeking questions and answers anchored in
research papers,” arXiv preprint arXiv:2105.03011 , 2021.
[127] T. M ¨oller, A. Reina, R. Jayakumar, and M. Pietsch, “Covid-qa: A
question answering dataset for covid-19,” in ACL 2020 Workshop on
Natural Language Processing for COVID-19 (NLP-COVID) , 2020.
[128] X. Wang, G. H. Chen, D. Song, Z. Zhang, Z. Chen, Q. Xiao, F. Jiang,
J. Li, X. Wan, B. Wang et al. , “Cmb: A comprehensive medical
benchmark in chinese,” arXiv preprint arXiv:2308.08833 , 2023.
[129] H. Zeng, “Measuring massive multitask chinese understanding,” arXiv
preprint arXiv:2304.12986, 2023.
[130] R. Y . Pang, A. Parrish, N. Joshi, N. Nangia, J. Phang, A. Chen, V . Pad-
makumar, J. Ma, J. Thompson, H. He et al. , “Quality: Question an-
swering with long input texts, yes!” arXiv preprint arXiv:2112.08608 ,
2021.
[131] P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick,
and O. Tafjord, “Think you have solved question answering? try arc,
the ai2 reasoning challenge,” arXiv preprint arXiv:1803.05457 , 2018.
[132] A. Talmor, J. Herzig, N. Lourie, and J. Berant, “Commonsenseqa:
A question answering challenge targeting commonsense knowledge,”
arXiv preprint arXiv:1811.00937 , 2018.
[133] E. Dinan, S. Roller, K. Shuster, A. Fan, M. Auli, and J. Weston,
“Wizard of wikipedia: Knowledge-powered conversational agents,”
arXiv preprint arXiv:1811.01241 , 2018.
[134] H. Wang, M. Hu, Y . Deng, R. Wang, F. Mi, W. Wang, Y . Wang, W.-
C. Kwan, I. King, and K.-F. Wong, “Large language models as source20
planner for personalized knowledge-grounded dialogue,” arXiv preprint
arXiv:2310.08840, 2023.
[135] ——, “Large language models as source planner for personal-
ized knowledge-grounded dialogue,” arXiv preprint arXiv:2310.08840,
2023.
[136] X. Xu, Z. Gou, W. Wu, Z.-Y . Niu, H. Wu, H. Wang, and S. Wang,
“Long time no see! open-domain conversation with long-term persona
memory,” arXiv preprint arXiv:2203.05797 , 2022.
[137] T.-H. Wen, M. Gasic, N. Mrksic, L. M. Rojas-Barahona, P.-H.
Su, S. Ultes, D. Vandyke, and S. Young, “Conditional generation
and snapshot learning in neural dialogue systems,” arXiv preprint
arXiv:1606.03352, 2016.
[138] R. He and J. McAuley, “Ups and downs: Modeling the visual evolution
of fashion trends with one-class collaborative filtering,” in proceedings
of the 25th international conference on world wide web , 2016, pp.
507–517.
[139] S. Li, H. Ji, and J. Han, “Document-level event argument extraction
by conditional generation,” arXiv preprint arXiv:2104.05919 , 2021.
[140] S. Ebner, P. Xia, R. Culkin, K. Rawlins, and B. Van Durme, “Multi-
sentence argument linking,” arXiv preprint arXiv:1911.03766 , 2019.
[141] H. Elsahar, P. V ougiouklis, A. Remaci, C. Gravier, J. Hare, F. Laforest,
and E. Simperl, “T-rex: A large scale alignment of natural language
with knowledge base triples,” in Proceedings of the Eleventh Inter-
national Conference on Language Resources and Evaluation (LREC
2018), 2018.
[142] O. Levy, M. Seo, E. Choi, and L. Zettlemoyer, “Zero-shot relation ex-
traction via reading comprehension,” arXiv preprint arXiv:1706.04115,
2017.
[143] R. Zellers, A. Holtzman, Y . Bisk, A. Farhadi, and Y . Choi, “Hel-
laswag: Can a machine really finish your sentence?” arXiv preprint
arXiv:1905.07830, 2019.
[144] S. Kim, S. J. Joo, D. Kim, J. Jang, S. Ye, J. Shin, and M. Seo,
“The cot collection: Improving zero-shot and few-shot learning of
language models via chain-of-thought fine-tuning,” arXiv preprint
arXiv:2305.14045, 2023.
[145] A. Saha, V . Pahuja, M. Khapra, K. Sankaranarayanan, and S. Chandar,
“Complex sequential question answering: Towards learning to converse
over linked question answer pairs with a knowledge graph,” inProceed-
ings of the AAAI conference on artificial intelligence , vol. 32, no. 1,
2018.
[146] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and
J. Steinhardt, “Measuring massive multitask language understanding,”
arXiv preprint arXiv:2009.03300 , 2020.
[147] S. Merity, C. Xiong, J. Bradbury, and R. Socher, “Pointer sentinel
mixture models,” arXiv preprint arXiv:1609.07843 , 2016.
[148] M. Geva, D. Khashabi, E. Segal, T. Khot, D. Roth, and J. Berant,
“Did aristotle use a laptop? a question answering benchmark with
implicit reasoning strategies,” Transactions of the Association for
Computational Linguistics, vol. 9, pp. 346–361, 2021.
[149] J. Thorne, A. Vlachos, C. Christodoulopoulos, and A. Mittal, “Fever: a
large-scale dataset for fact extraction and verification,” arXiv preprint
arXiv:1803.05355, 2018.
[150] N. Kotonya and F. Toni, “Explainable automated fact-checking for
public health claims,” arXiv preprint arXiv:2010.09926 , 2020.
[151] R. Lebret, D. Grangier, and M. Auli, “Neural text generation from
structured data with application to the biography domain,” arXiv
preprint arXiv:1603.07771, 2016.
[152] H. Hayashi, P. Budania, P. Wang, C. Ackerson, R. Neervannan,
and G. Neubig, “Wikiasp: A dataset for multi-domain aspect-based
summarization,” Transactions of the Association for Computational
Linguistics, vol. 9, pp. 211–225, 2021.
[153] S. Narayan, S. B. Cohen, and M. Lapata, “Don’t give me the details,
just the summary! topic-aware convolutional neural networks for ex-
treme summarization,” arXiv preprint arXiv:1808.08745 , 2018.
[154] S. Saha, J. A. Junaed, M. Saleki, A. S. Sharma, M. R. Rifat, M. Rahouti,
S. I. Ahmed, N. Mohammed, and M. R. Amin, “Vio-lens: A novel
dataset of annotated social network posts leading to different forms
of communal violence and its evaluation,” in Proceedings of the First
Workshop on Bangla Language Processing (BLP-2023), 2023, pp. 72–
84.
[155] X. Li and D. Roth, “Learning question classifiers,” in COLING 2002:
The 19th International Conference on Computational Linguistics, 2002.
[156] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Y . Ng,
and C. Potts, “Recursive deep models for semantic compositionality
over a sentiment treebank,” in Proceedings of the 2013 conference on
empirical methods in natural language processing , 2013, pp. 1631–
1642.
[157] H. Husain, H.-H. Wu, T. Gazit, M. Allamanis, and M. Brockschmidt,
“Codesearchnet challenge: Evaluating the state of semantic code
search,” arXiv preprint arXiv:1909.09436 , 2019.
[158] K. Cobbe, V . Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser,
M. Plappert, J. Tworek, J. Hilton, R. Nakano et al., “Training verifiers
to solve math word problems,” arXiv preprint arXiv:2110.14168, 2021.
[159] R. Steinberger, B. Pouliquen, A. Widiger, C. Ignat, T. Erjavec, D. Tufis,
and D. Varga, “The jrc-acquis: A multilingual aligned parallel corpus
with 20+ languages,” arXiv preprint cs/0609058 , 2006.
[160] Y . Hoshi, D. Miyashita, Y . Ng, K. Tatsuno, Y . Morioka, O. Torii,
and J. Deguchi, “Ralle: A framework for developing and eval-
uating retrieval-augmented large language models,” arXiv preprint
arXiv:2308.10633, 2023.
[161] J. Liu, “Building production-ready rag applications,” https://www.ai.
engineer/summit/schedule/building-production-ready-rag-applications,
2023.
[162] I. Nguyen, “Evaluating rag part i: How to evaluate document retrieval,”
https://www.deepset.ai/blog/rag-evaluation-retrieval, 2023.
[163] Q. Leng, K. Uhlenhuth, and A. Polyzotis, “Best practices for
llm evaluation of rag applications,” https://www.databricks.com/blog/
LLM-auto-eval-best-practices-RAG, 2023.
[164] S. Es, J. James, L. Espinosa-Anke, and S. Schockaert, “Ragas: Au-
tomated evaluation of retrieval augmented generation,” arXiv preprint
arXiv:2309.15217, 2023.
[165] J. Saad-Falcon, O. Khattab, C. Potts, and M. Zaharia, “Ares: An
automated evaluation framework for retrieval-augmented generation
systems,” arXiv preprint arXiv:2311.09476 , 2023.
[166] C. Jarvis and J. Allard, “A survey of techniques for
maximizing llm performance,” https://community.openai.
com/t/openai-dev-day-2023-breakout-sessions/505213#
a-survey-of-techniques-for-maximizing-llm-performance-2, 2023.
[167] J. Chen, H. Lin, X. Han, and L. Sun, “Benchmarking large lan-
guage models in retrieval-augmented generation,” arXiv preprint
arXiv:2309.01431, 2023.
[168] Y . Liu, L. Huang, S. Li, S. Chen, H. Zhou, F. Meng, J. Zhou, and
X. Sun, “Recall: A benchmark for llms robustness against external
counterfactual knowledge,” arXiv preprint arXiv:2311.08147 , 2023.
[169] Y . Lyu, Z. Li, S. Niu, F. Xiong, B. Tang, W. Wang, H. Wu, H. Liu,
T. Xu, and E. Chen, “Crud-rag: A comprehensive chinese benchmark
for retrieval-augmented generation of large language models,” arXiv
preprint arXiv:2401.17043, 2024.
[170] P. Xu, W. Ping, X. Wu, L. McAfee, C. Zhu, Z. Liu, S. Subramanian,
E. Bakhturina, M. Shoeybi, and B. Catanzaro, “Retrieval meets long
context large language models,” arXiv preprint arXiv:2310.03025 ,
2023.
[171] C. Packer, V . Fang, S. G. Patil, K. Lin, S. Wooders, and J. E. Gon-
zalez, “Memgpt: Towards llms as operating systems,” arXiv preprint
arXiv:2310.08560, 2023.
[172] G. Xiao, Y . Tian, B. Chen, S. Han, and M. Lewis, “Efficient
streaming language models with attention sinks,” arXiv preprint
arXiv:2309.17453, 2023.
[173] T. Zhang, S. G. Patil, N. Jain, S. Shen, M. Zaharia, I. Stoica, and J. E.
Gonzalez, “Raft: Adapting language model to domain specific rag,”
arXiv preprint arXiv:2403.10131 , 2024.
[174] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess,
R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei, “Scaling laws
for neural language models,” arXiv preprint arXiv:2001.08361 , 2020.
[175] U. Alon, F. Xu, J. He, S. Sengupta, D. Roth, and G. Neubig, “Neuro-
symbolic language modeling with automaton-augmented retrieval,” in
International Conference on Machine Learning . PMLR, 2022, pp.
468–485.
[176] M. Yasunaga, A. Aghajanyan, W. Shi, R. James, J. Leskovec, P. Liang,
M. Lewis, L. Zettlemoyer, and W.-t. Yih, “Retrieval-augmented multi-
modal language modeling,” arXiv preprint arXiv:2211.12561 , 2022.
[177] J. Li, D. Li, S. Savarese, and S. Hoi, “Blip-2: Bootstrapping language-
image pre-training with frozen image encoders and large language
models,” arXiv preprint arXiv:2301.12597 , 2023.
[178] W. Zhu, A. Yan, Y . Lu, W. Xu, X. E. Wang, M. Eckstein, and W. Y .
Wang, “Visualize before you write: Imagination-guided open-ended
text generation,” arXiv preprint arXiv:2210.03765 , 2022.
[179] J. Zhao, G. Haffar, and E. Shareghi, “Generating synthetic speech from
spokenvocab for speech translation,” arXiv preprint arXiv:2210.08174,
2022.
[180] D. M. Chan, S. Ghosh, A. Rastrow, and B. Hoffmeister, “Using external
off-policy speech-to-text mappings in contextual end-to-end automated
speech recognition,” arXiv preprint arXiv:2301.02736 , 2023.21
[181] A. Yang, A. Nagrani, P. H. Seo, A. Miech, J. Pont-Tuset, I. Laptev,
J. Sivic, and C. Schmid, “Vid2seq: Large-scale pretraining of a visual
language model for dense video captioning,” in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition ,
2023, pp. 10 714–10 726.
[182] N. Nashid, M. Sintaha, and A. Mesbah, “Retrieval-based prompt
selection for code-related few-shot learning,” in 2023 IEEE/ACM 45th
International Conference on Software Engineering (ICSE) , 2023, pp.
2450–2462.

Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, pages 2655 - 2671
July 10-15, 2022 ©2022 Association for Computational Linguistics
Learning To Retrieve Prompts for In-Context Learning
Ohad Rubin Jonathan Herzig Jonathan Berant
The Blavatnik School of Computer Science, Tel Aviv University
{ohad.rubin,jonathan.herzig,joberant}@cs.tau.ac.il
Abstract
In-context learning is a recent paradigm in nat-
ural language understanding, where a large pre-
trained language model (LM) observes a test in-
stance and a few training examples as its input,
and directly decodes the output without any up-
date to its parameters. However, performance
has been shown to strongly depend on the se-
lected training examples (termed prompts). In
this work, we propose an efficient method for
retrieving prompts for in-context learning us-
ing annotated data and an LM. Given an input-
output pair, we estimate the probability of the
output given the input and a candidate train-
ing example as the prompt, and label training
examples as positive or negative based on this
probability. We then train an efficient dense
retriever from this data, which is used to re-
trieve training examples as prompts at test time.
We evaluate our approach on three sequence-to-
sequence tasks where language utterances are
mapped to meaning representations, and find
that it substantially outperforms prior work and
multiple baselines across the board.
1 Introduction
The striking language skills and world knowledge
embedded in large pre-trained language models
(LMs) (Devlin et al., 2019; Petroni et al., 2019; Raf-
fel et al., 2020; Brown et al., 2020) have recently
led to in-context learning, a new paradigm in natu-
ral language understanding. Under this paradigm,
a language model is given a prompt, which typi-
cally contains a few training examples, as well as a
test instance as input, and generates the output for
the test instance directly, without any update to its
parameters. This approach was first introduced in
GPT-3 (Brown et al., 2020), but has quickly spread
to other LMs (Lieber et al., 2021; Du et al., 2021;
Rae et al., 2021).
An attractive property of in-context learning is
that it provides a single model for multiple lan-
guage understanding tasks. However, Liu et al.
Retriever
Retriever Index
What is the longest river in 
the smallest state in the usa?
1) states
2) size of #1
3) #1 where #2 is the lowest
4) rivers of #3
5) how long are #4
6) #4 where #5 is the highest
Which states border the 
shortest river in the usa?
1) the usa
2) rivers of #1
3) how long are #2
4) #2 where #3 is the lowest
5) border states of #4
Which states border the
 longest river in the usa?
1) the usa
2) rivers of #1
3) how long are #2
4) #2 where #3 is the highest
5) border states of #4
1) rivers
2) #1 in the usa 
3) lengths of #2
4) #2 where #3 is longest
5) length of #4
Inference LM
What is the length of the 
longest river in the usa?
Similar examples
Question
Figure 1: An overview of prompt retrieval: Given a
question from BREAK , one retrieves similar training
examples from an index of the training set. The question
and training examples (the prompt) are passed to an
inference LM that decodes the output.
(2021a) showed that downstream performance can
vary widely depending on the choice of in-context
examples. This has sparked interest in prompt re-
trieval (see Fig. 1), where given a test instance,
training examples are chosen for the prompt based
on some similarity metric. Recent work has either
used off-the-shelf unsupervised similarity metrics,
or trained a prompt retriever to select examples
based on surface similarity (Das et al., 2021).
In this work, we suggest to use language mod-
els themselves to label examples that can serve as
good prompts, and train a prompt retriever from
this signal. To train the retriever (see Fig. 2), we
assume access to a training set of input-output pairs
and to a scoring LM, i.e., a language model that
will be used to score prompts. For each training
example (x, y), we go over other candidate train-
ing examples, and estimate the probability, accord-
ing to the scoring LM, of y conditioned on x and
the candidate prompt. We label training examples
that lead to high probability as positive and low
probability as negative and train a prompt retriever
2655D
Figure 2: An overview of our approach for training EPR. Given a training example, we use an unsupervised retriever
Ru to obtain a set of candidates. We then pass the candidates to a scoring LM and label the top-k and the bottom-k
as positive and negative examples, respectively. Last, we use this training data to train a dense retriever.
from this data using contrastive learning. We ar-
gue that using an LM for labeling examples is a
better proxy for training a retriever compared to
previously-proposed surface similarity heuristics.
Importantly, when creating the training data, we
have access to the gold label y, which can be used
to obtain a high-quality set of candidate prompts.
This leads to good positive examples and hard neg-
ative examples, which are beneficial for training
with a contrastive objective.
Using a scoring LM to train an efficient retriever
for a potentially different test time inference LM is
beneficial in two scenarios. First, when the scoring
LM is smaller than the inference LM and serves as
a proxy for it. This results in cheap and efficient
data generation for the retriever, accessible to a
wide range of researchers. Second, our approach
can be used even when the scoring and inference
LMs are identical (e.g., both are GPT-3). This is
beneficial when we do not have access to model
parameters and can only use it as a service, an
increasingly popular paradigm. In this case, we use
the LM to train a light-weight retriever that is only
tasked with learning a similarity function. More
generally, given that the scale of LMs is likely to
keep increasing in the foreseeable future, one can
view our approach for Efficient Prompt Retrieval,
or EPR, as a method for interfacing and learning to
interact with large LMs.
We empirically test EPR on three structured
sequence-to-sequence tasks, where input natural
language utterances are mapped to a meaning rep-
resentation: MTOP (Li et al., 2021) and SM-
CALFLOW(Andreas et al., 2020), which focus on
task-oriented dialogue, and BREAK (Wolfson et al.,
2020), a benchmark for mapping questions to a
language-based meaning representation. We ob-
serve that EPR substantially improves performance
compared to prior work on prompt retrieval. When
the scoring LM and inference LM are identical
(using GPT-N EO (Black et al., 2021)), perfor-
mance compared to the best baseline improves
from 26% to 31.9% on BREAK , from 57% to
64.2% on MTOP, and from 51.4% to 54.3% on
SMC ALFLOW. When using GPT-N EO as a proxy
for larger LMs (GPT-J, GPT-3, and CODEX ), we
observe similar gains, where performance improves
substantially in all cases.
To conclude, we propose an approach for retriev-
ing training examples for in-context learning in
large language models, and show it substantially
outperforms prior methods. Given recent develop-
ments in scaling LMs, designing efficient methods
for interacting with LMs is an important direction
for future research.
2 Background: Prompt Retrieval
Problem setup Given a training set D =
{(xi, yi)}n
i=1 of input-output sequences, and a
test example xtest, our goal is to train a retriever,
R(xtest, D), that will retrieve a subset of training
examples P= {(xj, yj)}m
j=1 ⊂D, where m ≪n.
We succinctly refer to Pas the prompt.1
Given an inference LM,g, a good prompt should
lead to the target output sequence when the test
example xtest is concatenated to the prompt Pand
passed as a prefix to g. Specifically, decoding from
1Prompt often refers to a natural language template filled
by an input example (Liu et al., 2021b), but here it denotes the
sequence of training examples provided as input to the LM.
2656the LM g([P; xtest]) should yield ytest. In this work,
we focus on structured tasks, such as semantic pars-
ing, where x is a natural language utterance and y
is a meaning representation for that utterance.
Prior work Liu et al. (2021a) investigated the
effect of different prompts on the performance
of GPT-3 and demonstrated that the choice of in-
context examples strongly affects downstream per-
formance. They used an unsupervised sentence
encoder to encode training examples, and retrieved
for every test instance its nearest neighbors.
Das et al. (2021) trained a supervised prompt
retriever for knowledge-base question answering.
The retriever was trained with supervision that is
tailored for knowledge-base queries, and relies on
surface similarity between formal queries. Con-
versely, our approach takes advantage of the gener-
ative LM itself and is thus more general.
Shin et al. (2021) used GPT-3 to select examples
for the prompt for few-shot semantic parsing. How-
ever, rather than training a retriever, they randomly
sample a large set of utterance-program pairs from
the training set, and choose those that are similar
to the target instance question according to GPT-3.
This results in an expensive inference procedure,
where GPT-3 is run hundreds of times for each test
instance, unlike our approach, which is based on a
light-weight sub-linear retriever.
3 Efficient Prompt Retriever
We now describe our method for training EPR,
an efficient prompt retriever for in-context learn-
ing. We first describe how to generate labeled data
(§3.1), and then how to use the training data for
training and inference (§3.2). Fig. 2 provides an
overview of the training procedure.
3.1 Generating the Training Data
Our approach relies on finding which training ex-
amples can serve as good prompts for other training
examples. Scoring all pairs of training examples is
quadratic in |D|, and thus prohibitive. Hence, we
present a method for choosing a set of candidate ex-
amples ¯E⊂ D, from which we will choose positive
and negative examples for training. Importantly,
since we are not at test time and are only generating
data for training, we can use the target sequence
y to retrieve a good set of candidates. This can be
approached using a simple retrieval method, given
that our goal is to retrieve examples that are similar
to the input in terms of their output sequence, y.
To obtain a high-quality candidate set of train-
ing examples, we take advantage of an unsuper-
vised retriever, ¯E= Ru((x, y), D). For the choice
of the unsupervised retriever, we experiment with
BM25 (Robertson and Zaragoza, 2009), a sparse
retriever that relies on surface text similarity, and
SBERT (Reimers and Gurevych, 2019), which is
based on dense sentence encoding. For both, we
experimented with passing the retriever the training
pair (x, y) or the target sequence y only, and found
that using y leads to slightly higher performance.
Scoring the candidate set Once we retrieve the
set of candidates ¯E= {¯e1, ··· , ¯eL}for a training
example (x, y),2 we score each candidate ¯el ∈ ¯E
independently with a scoring LM, ˆg, which serves
as a proxy for the inference LM, g. Specifically,
the score for a candidate prompt is
s(¯el) = Probˆg(y |¯el, x),
which is the probability under the LM,ˆg, of the out-
put sequence conditioned on the candidate prompt
and input sequence. This indicates how helpful this
candidate is for decoding the target (independent
of all other candidates). We argue this score is a
better proxy for the utility of a training example at
inference time compared to prior approaches.
We apply this scoring function to all training ex-
amples, and define for each training example a set
of positive examples Epos, which includes the top-k
candidates in ¯Eaccording to s(¯el), and a set of neg-
ative examples Eneg, which includes the bottom-k
candidates in ¯Eaccording to s(¯el). This should lead
to relevant positive examples, assuming that the set
of candidates, ¯Eincludes good prompt candidates
and hard negatives, since all candidates have high
similarity with (x, y) according to Ru(y, D). With
positive and negative examples at our disposal, we
can now apply contrastive learning, which we de-
scribe next.
3.2 Training and Inference
Training Our training procedure proceeds ex-
actly like the contrastive learning procedure from
DPR (Karpukhin et al., 2020). This procedure re-
sults in an input encoder EX(·), which receives the
sequence of input tokens, x, and a prompt encoder
EP (·), which receives a candidate prompt, namely,
a concatenation of the tokens in an input-output
pair. Both encoders are initialized with BERT-base
2We omit the dependence of ¯E on (x, y) for simplicity.
2657(Devlin et al., 2019), and the output vector repre-
sentation is given by the CLS token, as usual. The
goal of training is to learn a similarity metric such
that given a test example xtest, it will be similar to
training examples that lead to decoding of ytest.
Our training instances are of the form
⟨xi, e+
i , e−
i,1, . . . e−
i,2B−1⟩. Where the positive ex-
ample e+
i is sampled from the set E(i)
pos, and our
negative examples consist of one hard negative ex-
ample sampled from E(i)
neg, B −1 positive examples
from the other instances in the same mini-batch,
and the B −1 hard negatives from those instances.
We define the similarity score between an input
and an input-output pair to be the inner product
sim(x, e) = EX(x)⊤EP (e). We can now define
the typical contrastive learning objective and mini-
mize for each example the negative log likelihood
of the positive example:
L(xi, e+
i , e−
i,1, . . . e−
i,2B−1) (1)
= −log esim(xi,e+
i )
esim(xi,e+
i ) + ∑2B−1
j=1 esim(xi,e−
i,j) .
An advantage of this approach is that for batch size
B the effective batch size is of order B2, with the
in-batch negatives trick (Henderson et al., 2017).
Inference After training the input encoder and
prompt encoder, we encode the entire set of train-
ing examples with EP (·) in a pre-processing step
using FAISS (Johnson et al., 2017). At test time,
given an input sequence, xtest, we compute its en-
coding EX(xtest), and then use maximum inner-
product search over the training data to find the L
most similar training examples, sorted by their in-
ner product (from high to low): P= (e1, . . . , eL).
The final prompt P′is determined by C, the max-
imal context size supported by the inference LM,
g. Specifically, L′ ≤ L is the largest L′ such∑L′
i=1 |ei|+ |xtest|+ |y′|≤ C, where |y′|is the
desired maximal length of the generated output. Fi-
nally, we return the output of greedy decoding on
g([eL′; eL′−1; . . .; e1; xtest]).
We note that while at training time we score each
training example independently, at test time the
language model observes a prompt, i.e.,a sequence
of examples. We leave modeling the dependence
between different training examples to future work.
4 Experimental Results
We now compare EPR to a wide range of unsu-
pervised and supervised baselines, both when the
scoring LM, ˆg, is smaller than the inference LM, g,
and when they are identical.
4.1 Datasets
We focus on tasks that map utterances to meaning
representations, where in-context examples can be
used to learn the mapping from inputs to outputs.
Examples from each dataset and the number of
examples are in Table 1.
• BREAK (Wolfson et al., 2020): A dataset map-
ping complex natural language questions into a
language-based meaning representation, where
a question is decomposed into an ordered list
of atomic steps. We use the low-level BREAK
subset, containing 44K/7K/8K examples in its
training/development/test sets.
• MTOP (Li et al., 2021): A semantic parsing
dataset, focused on task-oriented dialogue, where
commands are mapped to complex nested queries
across 11 domains. Similar to past work (Pasu-
pat et al., 2021), we use the English subset of
MTOP, containing 16K/2K/4K examples in its
training/development/test sets.
• SMC ALFLOW (Andreas et al., 2020): A large
English-language task-oriented dataset that cov-
ers tasks such as calendar, weather, places, and
people. The meaning representation is a dataflow
program, which includes API calls, function com-
position and complex constraints. SMC ALFLOW
includes 15K development set examples and
134K training examples, from which we sample
a random set of 44K examples for training.
4.2 Baselines and Oracles
We consider the following unsupervised baselines,
which are applied at test time only.
• RANDOM : we randomly sample examples from
the training set D.
• SBERT : We use SentenceTransformers,
a library providing BERT-based sen-
tence embeddings. 3 Specifically, we use
paraphrase-mpnet-base-v2, a 110M
parameter model to encode the test utterance
xtest and retrieve the examples with the most
similar utterances as in-context examples.
• BM25 : We use the classical sparse retrieval
method BM25 (Robertson and Zaragoza, 2009),
which is an extension of TF-IDF, to retrieve for
3https://www.sbert.net/index.html.
2658Dataset Size Utterance Meaning Representation
BREAK 60K There are more birds in the image on
the right than in the image on the left.
1) return right image;
2) return birds in #1;
3) return number of #2;
4) return left image;
5) return birds in #4
6) return number of #5;
7) return if #3 is higher than #6;
MTOP 22K call Zoey’s wife. [IN:CREATE_CALL =
[SL:CONTACT = [IN:GET_CONTACT =
[SL:CONTACT_RELATED = Zoey]
[SL:TYPE_RELATION = wife]]]]
SMCALFLOW 148K Can you create me a new meeting
on thursday morning?
(Yield (CreateCommitEventWrapper
(CreatePreflightEventWrapper
(Event.start_?
(DateTimeConstraint (Morning)
(NextDOW (Thursday)))))))
Table 1: Examples from each of the datasets we evaluate on.
each test utterance xtest the training examples
with the most similar utterance.
• BRUTE FORCE : We apply the prompt selection
method for few-shot semantic parsing from Shin
et al. (2021). Given a test example xtest, we sam-
ple 200 training examples. For each training
example (xi, yi), compute Probg(xtest |xi), and
use the highest scoring examples for the prompt.
Similar to us, this approach uses the inference
LM to choose prompts. However, it does so at
test time, which results in slow inference.
Next, we describe baselines that use the train-
ing set, D, to train a prompt retriever. All super-
vised methods share the following template. First,
a candidate set ¯Eof L = 50 examples is retrieved
with the unsupervised retriever Ru(y, D). We use
BM25 as an unsupervised retriever, since it outper-
formed SBERT (see §4.4). We then score each can-
didate prompt ¯el ∈¯Ewith some scoring function,
and label the top-k prompts as positive examples
and the bottom- k as negative examples ( k = 5 ).
Different supervised methods only differ in the
scoring function itself.4
• DR-BM25 : Here, we use the original BM25
scores for labeling positive and negative exam-
ples and training a dense retriever.
• CASE -BASED REASONING (CBR) : We adapt
the scoring function from Das et al. (2021),
which focused on knowledge-base question an-
swering. They define the weight for a pair of log-
ical forms to be the F1 score between the two sets
of relations appearing in those logical forms, and
use this weight to softly label their data. Since
in our setting we do not assume logical forms,
4Results for k ∈ {1, 5, 10} and L ∈ {50, 100} are in
Appendix A.
we define the score between two output sequence
yi and yj to be the F 1 between the two sets of
tokens in yi and yj, omitting stop words.
• EFFICIENT PROMPT RETRIEVAL (EPR) : Our
full approach from §3.
Last, we also consider two oracle models.
• BM25-O RACLE : We score test examples
with BM25 using the gold output sequence
RBM25(ytest, D). This provides an upper-bound
on what can be learned by DR-BM25. EPR can
potentially outperform this oracle, since its train-
ing signal goes beyond surface text similarity.
• LM-O RACLE : We use the procedure for labeling
training data at test time. Given a test example
(xtest, ytest), we first retrieve L candidate training
examples with RBM25(ytest, D), we then sort the
candidates with the scoring LM ˆg, estimating the
probability of ytest given xtest and the candidate
prompt. This provides an upper bound for EPR,
since EPR is trained to emulate this behaviour.
4.3 Experimental Details
Language models In this work, we only train
a dense retriever, but use scoring and inference
LMs. For our scoring LM, ˆg, we use GPT-N EO
(Black et al., 2021), a 2.7B-parameter LM trained
on The Pile (Gao et al., 2021), an 825 GB English
text corpus, constructed from a wide range of high-
quality resources.
In addition, we consider the following infer-
ence LMs: (a) GPT-J (Wang and Komatsuzaki,
2021): a 6B-parameter LM, also trained on The
Pile. The advantage in this setup, is that GPT-J
was trained on the same corpus as GPT-N EO. (b)
GPT-3 (Brown et al., 2020): A 175B-parameter
2659Model B REAK MTOP SMCALFLOW
Unsuper.
RANDOM 1.7 7.3 8.9
SBERT 21.6 48.7 43.6
BM25 26.0 52.9 46.1
BRUTEFORCE 7.7 18.1 11.1
Super.
DR-BM25 23.6 50.2 43.1
CBR 25.7 57.0 51.4
EPR (ours) 31.9 64.2 54.3
Oracle BM25-ORACLE 32.3 58.9 47.3
LM-ORACLE 43.1 71.6 73.7
Table 2: Development results when GPT-N EO is the
scoring and inference LM. Numbers for BREAK are
LF-EM, and for MTOP and SMC ALFLOW are EM.
Model B REAK MTOP
Unsuper. BM25 17.6 49.0
Super. CBR 18.4 57.5
EPR (ours) 23.9 64.4
Table 3: Test results where GPT-N EO is the scoring
and inference LM. Numbers for BREAK are NEM, the
official metric, and for MTOP are EM.
model, trained mostly on a filtered subset of com-
mon crawl. (c) CODEX (Chen et al., 2021): A
GPT-3 175B-parameter model finedtuned on code
from GitHub. Since our tasks involve mapping
from utterances to programs or meaning represen-
tations, CODEX might potentially perform well at
in-context learning.
For all LMs, we use a maximum context size of
C =2,048 tokens.
Evaluation On BREAK , we evaluate perfor-
mance on the development set with LF-EM (Has-
son and Berant, 2021), which is a better metric
compared to Normalized Exact Match (NEM), the
official metric, as it measures whether two mean-
ing representations are semantically equivalent. On
the test set, we use NEM. On MTOP and SM-
CALFLOW, we evaluate with Exact Match (EM),
i.e., whether the string output by the inference LM
is identical to the reference string.
We evaluate EPR in two settings: (a) LM-as-a-
service, and (b) LM-as-a-proxy. In the first set-
ting, we use GPT-N EO as both the scoring LM
and inference LM. In this setting, we evaluate on
the full development sets of BREAK , MTOP, and
SMC ALFLOW. In the latter setting, as we access
GPT-3 and CODEX through a paid API, we sample
a random subset of 1,000 development examples
from each dataset and evaluate each model once on
this subset.
Model One-shot Full-context
Unsuper. RANDOM 1.1 1.7
BM25 15.2 26.0
Super.
DR-BM25 14.1 23.6
CBR 14.5 25.7
EPR 23.0 31.9
Oracle
BM25-ORACLE 18.0 32.3
LM-ORACLE 33.3 43.1
ANYCORRECT-ORACLE 53.6 -
Table 4: Development results on BREAK with GPT-
NEO in the one-shot setting. Numbers are LF-EM . Full-
context is the corresponding numbers from Table 2.
4.4 Results
LM-as-a-service Table 2 reports results where
the scoring and inference LMs are identical.
EPR substantially outperforms all other baselines.
Specifically, when comparing to the best baseline,
it improves performance from 26.0 to 31.9 on
BREAK , from 57.0 to 64.2 on MTOP, and from
51.4 to 54.3 on SMC ALFLOW. This shows that
using the LM itself to label examples is an effective
approach for obtaining a strong prompt retriever.
Table 3 shows test results on BREAK and MTOP
corroborating that EPR substantially improves per-
formance compared to BM25 and CBR.
For the unsupervised methods, the RANDOM
baseline shows that random sampling of training
examples leads to poor performance. BM25 out-
performs SBERT for prompt retrieval, and con-
sequently we use BM25 in all of our supervised
approaches to retrieve the set of candidates,¯E. Last,
BRUTE FORCE performs worse than BM25. We as-
sume this is since the training sets are large (∼14-
120K examples), and sampling 200 examples does
not cover examples that are useful for GPT-N EO.
Interestingly, EPR outperforms BM25-O RACLE
on MTOP and SMC ALFLOW and is comparable on
BREAK . This is surprising since BM25-O RACLE
has access to the output sequence ytest at test time,
illustrating that the signal provided by the scoring
LM for training goes beyond surface text similarity.
The performance of LM-O RACLE is substantially
higher than EPR, showing that the supervision pro-
vided by the scoring LM is strong, and training a
better retriever from this signal can substantially
enhance performance.
We further evaluate our models in the one-shot
setup, i.e., when the prompt given to the inference
LM includes the highest scoring example only. In
this setup, the inference LM is applied in the same
setting as when we generate labeled data, where
we go over each prompt candidate independently.
2660BREAK MTOP SMCALFLOW
Method RANDOM BM25 CBR EPR RANDOM BM25 CBR EPR RANDOM BM25 CBR EPR
GPT-3 4.2 20.1 21.3 25.3 7.6 52.5 54.8 62.6 5.8 35.3 41.6 46.5
CODEX 8.9 24.5 24.2 29.5 10.8 60.6 59.4 66.1 7.2 45.1 48.7 50.3
GPT-J 3.3 26.7 26.7 31.5 8.8 56.6 58.0 65.4 10.6 50.4 50.9 57.4
GPT-NEO 1.0 22.8 25.8 29.9 7.6 52.8 55.4 63.6 8.0 46.1 50.1 53.5
Table 5: Results on a random sample of 1,000 examples from the development set when using GPT-Neo as a scoring
LM across different inference LMs and datasets.
EPR CBR
Test
Example
Utterance Give the code of the airport with the
least flights.
Meaning
Representation 1) airports
2) flights of #1
3) number of #2 for each #1
4) #1 where #3 is lowest
5) code of #4
Top-1 Utterance What is the code of the city with the
most students?
What destination has the fewest number
of flights?
Meaning
Representation 1) cities
2) students in #1
3) number of #2 for each #1
4) #1 where #3 is highest
5) code of #4
1) destinations
2) flights of #1
3) number of #2 for each #1
4) #1 where #3 is lowest
Top-2 Utterance Return the code of the city that has the
most students.
Which destination has least number of
flights?
Meaning
Representation 1) cities
2) students in #1
3) number of #2 for each #1
4) #1 where #3 is highest
5) code of #4
1) destinations
2) flights to #1
3) number of #2 for each #1
4) #1 where #3 is lowest
Top-3 Utterance Find the count and code of the job has
most employees.
What is the number of airports per
country, ordered from most to least?
Meaning
Representation 1) jobs
2) employees of #1
3) number of #2 for each #1
4) #1 where #3 is highest
5) employees of #4
6) number of #5
7) code of #4
8) #6 , #7
1) countries
2) airports in #1
3) number of #2 for each #1
4) #3 sorted by most to least
Table 6: An example from BREAK development set where EPR is correct and CBR is incorrect along with the top-3
training examples retrieved from each retriever.
Since train and test time are now closer, we can ex-
pect the advantage of EPR to be more pronounced.
Table 4 shows the results. Indeed, EPR out-
performs the best baseline by 8.5%, and BM25-
ORACLE by 5%. In addition, we examine
ANYCORRECT -ORACLE , which tests whether any
of the candidates returned by BM25 leads to the
correct output. ANYCORRECT -ORACLE reaches
53.6%, 20 points above LM-O RACLE . This shows
the high quality of candidates provided by BM25
(applied on the y), as one can reach more than 50%
LF-EM with just a single prompt. Moreover, it
hints that a better scoring function can potentially
further improve performance.
LM-as-a-proxy Table 5 shows results when the
scoring LM is GPT-N EO and the inference LM is a
larger LM. First, the trends are similar to the LM-as-
a-service setup, i.e., EPR substantially outperforms
prior baselines, including our best unsupervised
baseline, BM25, and the best supervised baseline,
CBR, by 2-8 points on all datasets and all pre-
trained models. Thus, GPT-N EO serves as a good
proxy for choosing training examples.
To further validate this finding, we evaluate the
performance of GPT-J on BREAK with GPT-N EO
as the scoring LM compared to using GPT-J it-
self as the scoring LM. We find performance im-
proves slightly from 31.5 to 33.6. Analogously,
when using CODEX as the scoring LM and infer-
ence LM performance remains roughly the same:
29.5→29.3. Thus, using a smaller LM (GPT-N EO)
is an effective strategy for training a retriever that
will be applied on other LMs. Zooming in on dif-
ferent inference LMs, GPT-J performs slightly bet-
ter than GPT-N EO across the board, since it was
2661Figure 3: A t-SNE projection and clustering of the rep-
resentations learned by EPR for the training examples
in BREAK . An interactive version displaying individual
examples is available here.
trained on the same data and using the same pro-
cedure as GPT-N EO. CODEX outperforms GPT-
3, which can be explained by the fact that it was
trained on code, and our datasets involve map-
ping to programs or meaning representations. Sur-
prisingly, GPT-J outperforms CODEX (except on
MTOP) and GPT-3 despite being 30x smaller. This
can perhaps be explained by the fact that GPT-J
was trained on a different dataset (The Pile (Gao
et al., 2021)).
Pattern Copied Novel Total
Acc Rate Acc Rate Acc
BREAK Exact 55.1% 10.4% 29.7% 89.6% 32.3%Abstract 58.0% 41.1% 14.5% 58.9%
MTOP Exact 77.3% 25.3% 59.7% 74.7% 64.2%Abstract 71.6% 84.5% 23.4% 15.5%
SMCAL Exact 62.5% 60.2% 42.4% 39.8% 54.5%Abstract 62.4% 81.2% 20.6% 18.8%
Table 7: Accuracy comparison between the decoded
instances that contained patterns from the prompt and
novel instances those that don’t. Results shown are on
the LM-as-a-service setup using GPT-N EO.
Analysis Table 6 shows an example fromBREAK
where EPR decodes the correct output, while CBR
does not. All training examples retrieved by EPR
perform an argmax (argmin in the original utter-
ance), and return in the final step “a code”, while
the third example retrieved by CBR does not per-
form an argmax or argmin, and do not involve“a
code”. We provide additional examples in App. A.
Figure 3 shows a t-SNE (Hinton and Roweis,
2002) projection of the embeddings learned by EPR
for the training examples of BREAK , with a link
to an interactive version, where we applied the
OPTICS (Ankerst et al., 1999; Schubert and Gertz,
2018) clustering algorithm. Examining clusters
0.0 0.2 0.4 0.6 0.8 1.0
Distance
0.125
0.5
2.0
8.0
%
Exact
Abstract
Figure 4: On the subset of copied patterns we plot the
distribution of the distance from the test instance to the
example containing the pattern. Shown on the BREAK
validation set using EPR in the LM-as-a-service setup
using GPT-N EO. Note that the y-axis is in log-scale.
shows that EPR captures both lexical and structure
similarity. Examples for clusters are also available
in App. A.
Prompt copying We analyze how the LM uti-
lizes in-context prompts. Specifically, is the target
output copied from one of the prompts or is it a
composition of different prompt fragments, which
result in generalization to new structures.
To achieve this, we define two types of copy-
ing. (a) Exact copying measures if the generated
output exactly matches one of the examples in the
prompt, and (b) Abstract copying, that quantifies
if the structure of the decoded output matches any
of the structures seen in the prompt. Specifically,
we eliminate the effect of non-structural elements
such as entities and function arguments. We re-
place every sequence of words in the logical form
that appears in the input utterance with the string
[MASKED] for both the target utterance and in-
context examples. If the masked logical form that
the LM decoded appears in the set of masked ex-
amples defined by the prompt, we say that the LM
copied that abstract pattern.
Table 7 presents the results on the validation
set for each of our three datasets, as well as the
accuracy on each subset. We observe that the
rate of copying is much higher in MTOP and SM-
CALFLOW compared to BREAK , where in MTOP
and SMC ALFLOW abstract copying reaches more
than 80%. Moreover, accuracy on examples where
copying occurred is much higher compared to ac-
curacy where no copying happened. For exam-
ple, on MTOP, 84.5% of the examples were ab-
stractly copied, and on that subset of examples,
EPR achieves 71.6% EM, compared to 64.2% on
2662the entire validation set. Nevertheless, even though
accuracy is much lower in cases where no copying
occurred, accuracy is not negligible, which shows
that some form of generalization to new structures
is taking place.
Another follow-up question is whether the model
copies patterns from prompts uniformly or does it
attend mostly to the ones with high retrieval score.
To answer this, we look at the subset of exam-
ples where copying occurred. We then identify for
each example the highest-ranking prompt that was
copied from, and define the distance of that prompt
by dividing the rank by the number of prompts that
fit in that example. Figure 4 shows the distribution
over distances for the BREAK dataset. We observe
that copying happens mostly from highly-ranked
prompts.
5 Related Work
In-context learning Our understanding of in-
context learning has grown substantially recently.
Saunshi et al. (2021) suggests that by conditioning
on a prompt, the task of predicting the next word
approaches linear separability. Xie et al. (2021)
suggests that in-context learning occurs when the
model infers a shared latent concept between ex-
amples in a prompt. Levine et al. (2021) present
a pre-training scheme theoretically motivated by
the bias of in-context learning, that gives signif-
icant improvements. Recently, Min et al. (2022)
showed that the model does not rely on the ground
truth input-label mapping provided in the demon-
strations as much as previously thought.
Retrieval Research on training dense retrievers
has skyrocketed recently, propelled by interest
in open-domain question answering (Chen et al.,
2017; Lee et al., 2019; Karpukhin et al., 2020; Guu
et al., 2020; Khattab and Zaharia, 2020; Qu et al.,
2021). Work on retrieval-based methods has also
spread more widely to other knowledge-intensive
tasks (Lewis et al., 2020), e.g., fact verification
(Samarinas et al., 2021).
Similar to us, Pasupat et al. (2021) proposed to
use retrieval in semantic parsing. However, they fo-
cus on controlling the output generated by a model.
Retrieval methods have also been successfully used
in language modeling (Khandelwal et al., 2020;
Borgeaud et al., 2021; Alon et al., 2022) and ma-
chine translation (Khandelwal et al., 2021).
Prompts Developing methods for interacting
with LMs and extracting desired behaviours has
attracted considerable attention, under the umbrella
term prompting. In this work, prompts are a set of
in-context training examples, but substantial effort
has also been devoted to casting natural language
tasks as language modeling by phrasing the tar-
get task in natural language (see survey in (Liu
et al., 2021b)). Such approaches include prompt
engineering through manual patterns (Petroni et al.,
2019; Schick and Schütze, 2021), decoding meth-
ods (Min et al., 2021; Zhao et al., 2021; Holtzman
et al., 2021), and methods for extracting either hard
(Shin et al., 2020; Haviv et al., 2021) or soft (Li and
Liang, 2021; Zhong et al., 2021; Qin and Eisner,
2021) prompts automatically.
Prompt retrieval for supervised models In par-
allel to this work, adding training examples as addi-
tional input has been shown to be useful for super-
vised models as well. Wang et al. (2022) and Xu
et al. (2021) used BM25 to retrieve and augment the
input with similar examples from the training set.
Fine-tuning the model with the additional inputs
improved performance on tasks such as summariza-
tion and question answering. Such methods can
also potentially benefit from a stronger retriever.
6 Conclusions
Large pre-trained LMs are becoming an insepara-
ble part of the natural language understanding eco-
system. However, accessing their weights or updat-
ing them can be prohibitive for many researchers.
In this work, we propose EPR, a method for learn-
ing to retrieve good prompts for in-context learning,
by using language models themselves as the scor-
ing function. This allows us to train a light-weight
retriever and substantially improve performance on
three challenging tasks.
More broadly, given that large LMs models are
likely to play a prominent role in developing lan-
guage understanding models, it is important to de-
velop approaches for interacting with such models
effectively. EPR can be viewed as a step in this
direction.
Acknowledgement
We thank Ori Ram and Itay Itzhak for helpful sug-
gestions and meaningful discussions. This research
was supported in part by The Yandex Initiative for
Machine Learning, and The European Research
2663Council (ERC) under the European Union Hori-
zons 2020 research and innovation programme
(grant ERC DELPHI 802800). This work was com-
pleted in partial fulfillment for the Ph.D degree of
Ohad Rubin.
References
Uri Alon, Frank F. Xu, Junxian He, Sudipta Sen-
gupta, Dan Roth, and Graham Neubig. 2022.
Neuro-symbolic language modeling with automaton-
augmented retrieval. ArXiv, abs/2201.12431.
Jacob Andreas, John Bufe, David Burkett, Charles
Chen, Josh Clausman, Jean Crawford, Kate Crim,
Jordan DeLoach, Leah Dorner, Jason Eisner, Hao
Fang, Alan Guo, David Hall, Kristin Hayes, Kellie
Hill, Diana Ho, Wendy Iwaszuk, Smriti Jha, Dan
Klein, Jayant Krishnamurthy, Theo Lanman, Percy
Liang, Christopher H. Lin, Ilya Lintsbakh, Andy Mc-
Govern, Aleksandr Nisnevich, Adam Pauls, Dmitrij
Petters, Brent Read, Dan Roth, Subhro Roy, Jesse
Rusak, Beth Short, Div Slomin, Ben Snyder, Stephon
Striplin, Yu Su, Zachary Tellman, Sam Thomson, An-
drei V orobev, Izabela Witoszko, Jason Wolfe, Abby
Wray, Yuchen Zhang, and Alexander Zotov. 2020.
Task-oriented dialogue as dataflow synthesis. Trans-
actions of the Association for Computational Linguis-
tics, 8:556–571.
Mihael Ankerst, Markus M. Breunig, Hans-Peter
Kriegel, and Jörg Sander. 1999. Optics: Ordering
points to identify the clustering structure. SIGMOD
Rec., 28(2):49–60.
Emily M. Bender, Timnit Gebru, Angelina McMillan-
Major, and Shmargaret Shmitchell. 2021. On the
dangers of stochastic parrots: Can language mod-
els be too big? In Proceedings of the 2021 ACM
Conference on Fairness, Accountability, and Trans-
parency, FAccT ’21, page 610–623, New York, NY ,
USA. Association for Computing Machinery.
Sid Black, Leo Gao, Phil Wang, Connor Leahy,
and Stella Biderman. 2021. GPT-Neo: Large
Scale Autoregressive Language Modeling with Mesh-
Tensorflow.
Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,
Trevor Cai, Eliza Rutherford, Katie Millican, George
van den Driessche, Jean-Baptiste Lespiau, Bogdan
Damoc, Aidan Clark, et al. 2021. Improving lan-
guage models by retrieving from trillions of tokens.
arXiv preprint arXiv:2112.04426.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei.
2020. Language models are few-shot learners. In Ad-
vances in Neural Information Processing Systems 33:
Annual Conference on Neural Information Process-
ing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual.
Danqi Chen, Adam Fisch, Jason Weston, and Antoine
Bordes. 2017. Reading Wikipedia to answer open-
domain questions. In Proceedings of the 55th Annual
Meeting of the Association for Computational Lin-
guistics (Volume 1: Long Papers), pages 1870–1879,
Vancouver, Canada. Association for Computational
Linguistics.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming
Yuan, Henrique Ponde, Jared Kaplan, Harrison Ed-
wards, Yura Burda, Nicholas Joseph, Greg Brockman,
Alex Ray, Raul Puri, Gretchen Krueger, Michael
Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin,
Brooke Chan, Scott Gray, Nick Ryder, Mikhail
Pavlov, Alethea. Power, Lukasz Kaiser, Moham-
mad Bavarian, Clemens Winter, Philippe Tillet, Fe-
lipe Petroski Such, David W. Cummings, Matthias
Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel
Herbert-V oss, William H. Guss, Alex Nichol, Igor
Babuschkin, S. Arun Balaji, Shantanu Jain, Andrew
Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan
Morikawa, Alec Radford, Matthew M. Knight, Miles
Brundage, Mira Murati, Katie Mayer, Peter Welinder,
Bob McGrew, Dario Amodei, Sam McCandlish, Ilya
Sutskever, and Wojciech Zaremba. 2021. Evaluat-
ing large language models trained on code. ArXiv
preprint, abs/2107.03374.
Rajarshi Das, Manzil Zaheer, Dung Thai, Ameya God-
bole, Ethan Perez, Jay Yoon Lee, Lizhen Tan, Lazaros
Polymenakos, and Andrew McCallum. 2021. Case-
based reasoning for natural language queries over
knowledge bases. In Proceedings of the 2021 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 9594–9611, Online and Punta Cana,
Dominican Republic. Association for Computational
Linguistics.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers), pages
4171–4186, Minneapolis, Minnesota. Association for
Computational Linguistics.
Nan Du, Yanping Huang, Andrew M. Dai, Dmitry Lep-
ikhin Simon Tong, Yuanzhong Xu, Maxim Krikun,
Yanqi Zhou, Adams Wei Yu, Orhan Firat, Bar-
ret Zoph, Liam Fedus, Maarten Bosma, Zongwei
Zhou, Tao Wang, Yu Emma Wang, Kellie Web-
ster, Marie Pellat, Kevin Robinson, Kathy Meier-
Hellstern, Toju Duke, Lucas Dixon, Kun Zhang,
Quoc V Le, Yonghui Wu, Zhifeng Chen, and Claire
Cui. 2021. GLaM: Efficient scaling of language
2664models with mixture-of-experts. arXiv preprint
arXiv:2112.06905.
Leo Gao, Stella Biderman, Sid Black, Laurence Gold-
ing, Travis Hoppe, Charles Foster, Jason Phang, Ho-
race He, Anish Thite, Noa Nabeshima, et al. 2021.
The pile: An 800gb dataset of diverse text for lan-
guage modeling. ArXiv preprint, abs/2101.00027.
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat,
and Ming-Wei Chang. 2020. Retrieval augmented
language model pre-training. In ICML.
Matan Hasson and Jonathan Berant. 2021. Question
decomposition with dependency graphs. ArXiv,
abs/2104.08647.
Adi Haviv, Jonathan Berant, and Amir Globerson. 2021.
BERTese: Learning to speak to BERT. In Proceed-
ings of the 16th Conference of the European Chap-
ter of the Association for Computational Linguistics:
Main Volume, pages 3618–3623, Online. Association
for Computational Linguistics.
Matthew Henderson, Rami Al-Rfou, Brian Strope, Yun
hsuan Sung, Laszlo Lukacs, Ruiqi Guo, Sanjiv Ku-
mar, Balint Miklos, and Ray Kurzweil. 2017. Effi-
cient natural language response suggestion for smart
reply.
Geoffrey E. Hinton and Sam T. Roweis. 2002. Stochas-
tic neighbor embedding. In NIPS.
Ari Holtzman, Peter West, Vered Shwartz, Yejin Choi,
and Luke Zettlemoyer. 2021. Surface form com-
petition: Why the highest probability answer isn’t
always right. In Proceedings of the 2021 Conference
on Empirical Methods in Natural Language Process-
ing, pages 7038–7051, Online and Punta Cana, Do-
minican Republic. Association for Computational
Linguistics.
Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2017.
Billion-scale similarity search with gpus. ArXiv
preprint, abs/1702.08734.
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick
Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and
Wen-tau Yih. 2020. Dense passage retrieval for open-
domain question answering. In Proceedings of the
2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 6769–6781,
Online. Association for Computational Linguistics.
Urvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke
Zettlemoyer, and Mike Lewis. 2021. Nearest Neigh-
bor Machine Translation. In Proceedings of ICLR.
Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke
Zettlemoyer, and Mike Lewis. 2020. Generalization
through memorization: Nearest neighbor language
models. In 8th International Conference on Learning
Representations, ICLR 2020, Addis Ababa, Ethiopia,
April 26-30, 2020. OpenReview.net.
Omar Khattab and Matei Zaharia. 2020. Colbert: Effi-
cient and effective passage search via contextualized
late interaction over bert. In Proceedings of the 43rd
International ACM SIGIR conference on research
and development in Information Retrieval, pages 39–
48.
Diederik P Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In Proceedings
of ICLR.
Kenton Lee, Ming-Wei Chang, and Kristina Toutanova.
2019. Latent retrieval for weakly supervised open
domain question answering. In Proceedings of the
57th Annual Meeting of the Association for Computa-
tional Linguistics, pages 6086–6096, Florence, Italy.
Association for Computational Linguistics.
Yoav Levine, Noam Wies, Daniel Jannai, Daniel I.
Navon, Yedid Hoshen, and Amnon Shashua. 2021.
The inductive bias of in-context learning: Rethinking
pretraining example design. ArXiv, abs/2110.04541.
Patrick S. H. Lewis, Ethan Perez, Aleksandra Pik-
tus, Fabio Petroni, Vladimir Karpukhin, Naman
Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih,
Tim Rocktäschel, Sebastian Riedel, and Douwe
Kiela. 2020. Retrieval-augmented generation for
knowledge-intensive NLP tasks. In Proceedings of
NeurIPS.
Haoran Li, Abhinav Arora, Shuohui Chen, Anchit
Gupta, Sonal Gupta, and Yashar Mehdad. 2021.
MTOP: A comprehensive multilingual task-oriented
semantic parsing benchmark. In Proceedings of the
16th Conference of the European Chapter of the Asso-
ciation for Computational Linguistics: Main Volume,
pages 2950–2962, Online. Association for Computa-
tional Linguistics.
Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning:
Optimizing continuous prompts for generation. In
Proceedings of the 59th Annual Meeting of the Asso-
ciation for Computational Linguistics and the 11th
International Joint Conference on Natural Language
Processing (Volume 1: Long Papers), pages 4582–
4597, Online. Association for Computational Lin-
guistics.
Opher Lieber, Or Sharir, Barak Lenz, and Yoav Shoham.
2021. Jurassic-1: Technical details and evaluation.
White Paper. AI21 Labs.
Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,
Lawrence Carin, and Weizhu Chen. 2021a. What
makes good in-context examples for gpt-3? ArXiv
preprint, abs/2101.06804.
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,
Hiroaki Hayashi, and Graham Neubig. 2021b. Pre-
train, prompt, and predict: A systematic survey of
prompting methods in natural language processing.
Sewon Min, Mike Lewis, Hannaneh Hajishirzi, and
Luke Zettlemoyer. 2021. Noisy channel language
model prompting for few-shot text classification.
arXiv preprint.
2665Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,
Mike Lewis, Hannaneh Hajishirzi, and Luke Zettle-
moyer. 2022. Rethinking the role of demonstra-
tions: What makes in-context learning work? ArXiv,
abs/2202.12837.
Panupong Pasupat, Yuan Zhang, and Kelvin Guu. 2021.
Controllable semantic parsing via retrieval augmen-
tation. In Proceedings of the 2021 Conference on
Empirical Methods in Natural Language Processing,
pages 7683–7698, Online and Punta Cana, Domini-
can Republic. Association for Computational Lin-
guistics.
Fabio Petroni, Tim Rocktäschel, Sebastian Riedel,
Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and
Alexander Miller. 2019. Language models as knowl-
edge bases? In Proceedings of the 2019 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and the 9th International Joint Conference
on Natural Language Processing (EMNLP-IJCNLP),
pages 2463–2473, Hong Kong, China. Association
for Computational Linguistics.
Guanghui Qin and Jason Eisner. 2021. Learning how
to ask: Querying LMs with mixtures of soft prompts.
In Proceedings of the 2021 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 5203–5212, Online. Association for Computa-
tional Linguistics.
Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang
Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, and
Haifeng Wang. 2021. RocketQA: An optimized train-
ing approach to dense passage retrieval for open-
domain question answering. In Proceedings of the
2021 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 5835–5847, On-
line. Association for Computational Linguistics.
Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie
Millican, Jordan Hoffmann, Francis Song, John
Aslanides, Sarah Henderson, Roman Ring, Susan-
nah Young, et al. 2021. Scaling language models:
Methods, analysis & insights from training gopher.
arXiv preprint arXiv:2112.11446.
Colin Raffel, Noam Shazeer, Adam Roberts, Kather-
ine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the
limits of transfer learning with a unified text-to-text
transformer. Journal of Machine Learning Research,
21(140):1–67.
Nils Reimers and Iryna Gurevych. 2019. Sentence-
BERT: Sentence embeddings using Siamese BERT-
networks. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natu-
ral Language Processing (EMNLP-IJCNLP), pages
3982–3992, Hong Kong, China. Association for Com-
putational Linguistics.
Stephen Robertson and Hugo Zaragoza. 2009. The
probabilistic relevance framework: Bm25 and be-
yond. Foundations and Trends in Information Re-
trieval, 3:333–389.
Chris Samarinas, Wynne Hsu, and Mong Li Lee. 2021.
Improving evidence retrieval for automated explain-
able fact-checking. In Proceedings of the 2021 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies: Demonstrations, pages 84–91,
Online. Association for Computational Linguistics.
Nikunj Saunshi, Sadhika Malladi, and Sanjeev Arora.
2021. A mathematical exploration of why language
models help solve downstream tasks. In Interna-
tional Conference on Learning Representations.
Timo Schick and Hinrich Schütze. 2021. Exploiting
cloze-questions for few-shot text classification and
natural language inference. In Proceedings of the
16th Conference of the European Chapter of the Asso-
ciation for Computational Linguistics: Main Volume,
pages 255–269, Online. Association for Computa-
tional Linguistics.
Erich Schubert and Michael Gertz. 2018. Improving
the cluster structure extracted from optics plots. In
LWDA.
Richard Shin, Christopher Lin, Sam Thomson, Charles
Chen, Subhro Roy, Emmanouil Antonios Platanios,
Adam Pauls, Dan Klein, Jason Eisner, and Benjamin
Van Durme. 2021. Constrained language models
yield few-shot semantic parsers. In Proceedings of
the 2021 Conference on Empirical Methods in Natu-
ral Language Processing, pages 7699–7715, Online
and Punta Cana, Dominican Republic. Association
for Computational Linguistics.
Taylor Shin, Yasaman Razeghi, Robert L. Logan IV , Eric
Wallace, and Sameer Singh. 2020. AutoPrompt: Elic-
iting Knowledge from Language Models with Auto-
matically Generated Prompts. In Proceedings of the
2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 4222–4235,
Online. Association for Computational Linguistics.
Ben Wang and Aran Komatsuzaki. 2021. GPT-
J-6B: A 6 Billion Parameter Autoregressive
Language Model. https://github.com/
kingoflolz/mesh-transformer-jax.
Shuo Wang, Yichong Xu, Yuwei Fang, Yang Liu, S. Sun,
Ruochen Xu, Chenguang Zhu, and Michael Zeng.
2022. Training data is more valuable than you think:
A simple and effective method by retrieving from
training data. ArXiv, abs/2203.08773.
Tomer Wolfson, Mor Geva, Ankit Gupta, Matt Gard-
ner, Yoav Goldberg, Daniel Deutch, and Jonathan
Berant. 2020. Break it down: A question understand-
ing benchmark. Transactions of the Association for
Computational Linguistics, 8:183–198.
2666Sang Michael Xie, Aditi Raghunathan, Percy Liang,
and Tengyu Ma. 2021. An explanation of in-context
learning as implicit bayesian inference. ArXiv,
abs/2111.02080.
Yichong Xu, Chenguang Zhu, Shuohang Wang, Siqi
Sun, Hao Cheng, Xiaodong Liu, Jianfeng Gao,
Pengcheng He, Michael Zeng, and Xuedong Huang.
2021. Human parity on commonsenseqa: Augment-
ing self-attention with external attention.
Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and
Sameer Singh. 2021. Calibrate before use: Improv-
ing few-shot performance of language models. In
ICML, pages 12697–12706.
Zexuan Zhong, Dan Friedman, and Danqi Chen. 2021.
Factual probing is [MASK]: Learning vs. learning
to recall. In Proceedings of the 2021 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, pages 5017–5033, Online. Association
for Computational Linguistics.
A Appendix
Distribution of the number of in-context exam-
ples Since the selection procedure for in-context
examples is dynamic, the number of in-context ex-
amples differs for each test instance. In Figure 5,
we plot the histogram of the number of examples
we fit in C = 2, 048 tokens.
Effect of hyperparameters We test the effect
of k, the number of prompts labeled as positive or
negative, and L, the number of prompts retrieved
by the unsupervised retriever. Table 8 shows that
performance is is generally robust w.r.t these hyper-
parameters.
BREAK MT OP SMC AL FLOW
k = 1 31.5% 63.0% 54.5%
k = 5 31.9% 64.2% 54.3%
k = 10 31.0% 64.1% 52.2%
L = 50 31.9% 64.2% 54.3%
L = 100 32.3% 63.7% 51.0%
Table 8: In the LM-as-a-service setup, using GPT-Neo,
we search for other values for L and k, and note that the
choice of our hyperparameters is robust.
Training details To train EPR, we use the Adam
optimizer (Kingma and Ba, 2015) with batch size
120 and learning rate 1e-4 on eight RTX 3090. We
run training for 30 epochs. We used the default
DPR hyperparameters without tuning. We used the
final epoch of the model to perform model selec-
tion, and applied minimal learning rate tuning on
the validation set of BREAK .
Risk assessment Large language models have
been shown to exhibit various kinds of bias (Bender
et al., 2021), since EPR is trained on the signal
obtained from such large LMs, it might also exhibit
these biases.
Additional examples Tables 9, 10, and 11 pro-
vide more examples for cases where EPR is cor-
rect while CBR is incorrect along with the top-3
prompts for each method.
266710 20 30 40 50 60 70
0.000
0.005
0.010
0.015
0.020
0.025
0.030
0.035
Break
20 40 60 80 100
0.000
0.005
0.010
0.015
0.020
0.025
0.030
0.035
MT op
20 40 60 80 100
0.000
0.005
0.010
0.015
0.020
0.025
0.030
SMCalFlow
Figure 5: Distribution of the number of in-context examples per test instance for each of the datasets. We mark the
distribution mean using a dashed line.
EPR CBR
Test
Example
Utterance Remind me to add 2 dozen eggs to my
grocery list.
Meaning
Representation [IN:CREATE_REMINDER [SL:PERSON_REMINDED
me ] [SL:TODO add 2 dozen eggs to my
grocery list ] ]
Top-1 Utterance Remind me to get two bottles of water. Please add a grocery list to my list of
things to be reminded about doing today.
Meaning
Representation [IN:CREATE_REMINDER [SL:PERSON_REMINDED
me ] [SL:TODO get two bottles of water ]
]
[IN:CREATE_REMINDER [SL:TODO a grocery
list ] [SL:PERSON_REMINDED my ]
[SL:DATE_TIME today ] ]
Top-2 Utterance Remind me to bring an extra pair of
shoes to the river.
Remind me to make a grocery list
Meaning
Representation [IN:CREATE_REMINDER [SL:PERSON_REMINDED
me ] [SL:TODO bring an extra pair of
shoes to the river ] ]
[IN:CREATE_REMINDER [SL:PERSON_REMINDED
me ] [SL:TODO make a grocery list ] ]
Top-3 Utterance Remind me to add bottled water to
grocery list.
I need to make a grocery list; will you
remind me when I get off work at 5:00
p.m.?
Meaning
Representation [IN:CREATE_REMINDER [SL:PERSON_REMINDED
me ] [SL:TODO add bottled water to
grocery list ] ]
[IN:CREATE_REMINDER [SL:TODO make a
grocery list ] [SL:PERSON_REMINDED me ]
[SL:DATE_TIME at 5 : 00 p.m . ] ]
Table 9: An example from MTOP development set where EPR is correct and CBR is incorrect along with the top-3
training examples retrieved from each retriever.
EPR CBR
Test
Example
Utterance confirmed thanks
Meaning
Representation (PleasantryAnythingElseCombined)
Top-1 Utterance it’s ok bye Yes, but make sure to let me know the
weather for that time.
Meaning
Representation (PleasantryAnythingElseCombined)(let (x0 (Execute (^(Dynamic)
ConfirmAndReturnAction))) (do (Yield x0)
(Yield (WeatherForEvent (^(Dynamic) item
x0)))))
Top-2 Utterance It’s ok Awesome, perfect
Meaning
Representation (PleasantryAnythingElseCombined)(Yield (Execute (^(Dynamic)
ConfirmAndReturnAction)))
Top-3 Utterance It’s ok Perfect...
Meaning
Representation (PleasantryAnythingElseCombined)(Yield (Execute (^(Dynamic)
ConfirmAndReturnAction)))
Table 10: An example from SMC ALFLOW development set where EPR is correct and CBR is incorrect along with
the top-3 training examples retrieved from each retriever.
2668EPR CBR
Test
Example
Utterance Create a meeting with David Crim today
Meaning
Representation (Yield (CreateCommitEventWrapper
(CreatePreflightEventWrapper (&
(Event.start_? (DateTime.date_? (?=
(Today)))) (Event.attendees_?
(AttendeeListHasRecipient (Execute
(refer (extensionConstraint
(RecipientWithNameLike (^(Recipient)
EmptyStructConstraint) (PersonName.apply
"David Crim")))))))))))
Top-1 Utterance make a meeting with jeri today set up a meeting with both of David
Crim’s reports today
Meaning
Representation (Yield (CreateCommitEventWrapper
(CreatePreflightEventWrapper (&
(Event.start_? (DateTime.date_? (?=
(Today)))) (Event.attendees_?
(AttendeeListHasRecipient (Execute
(refer (extensionConstraint
(RecipientWithNameLike (^(Recipient)
EmptyStructConstraint) (PersonName.apply
"jeri")))))))))))
(Yield (CreateCommitEventWrapper
(CreatePreflightEventWrapper (&
(Event.start_? (DateTime.date_? (?=
(Today)))) (Event.attendees_?
(AttendeeListHasPeople (FindReports
(Execute (refer (extensionConstraint
(RecipientWithNameLike (^(Recipient)
EmptyStructConstraint) (PersonName.apply
"David Crim"))))))))))))
Top-2 Utterance put meeting with emlime on today Make a meeting with David Largenstop on
the 24th.
Meaning
Representation (Yield (CreateCommitEventWrapper
(CreatePreflightEventWrapper (&
(Event.start_? (DateTime.date_? (?=
(Today)))) (Event.attendees_?
(AttendeeListHasRecipient (Execute
(refer (extensionConstraint
(RecipientWithNameLike (^(Recipient)
EmptyStructConstraint) (PersonName.apply
"emlime")))))))))))
(Yield (CreateCommitEventWrapper
(CreatePreflightEventWrapper (&
(Event.start_? (DateTime.date_? (?=
(nextDayOfMonth (Today) 24L))))
(Event.attendees_?
(AttendeeListHasRecipient (Execute
(refer (extensionConstraint
(RecipientWithNameLike (^(Recipient)
EmptyStructConstraint) (PersonName.apply
"David Largenstop")))))))))))
Top-3 Utterance I want meet Dr Kennady from today create a meet with bob today
Meaning
Representation (Yield (CreateCommitEventWrapper
(CreatePreflightEventWrapper (&
(Event.start_? (DateTime.date_? (?=
(Today)))) (Event.attendees_?
(AttendeeListHasRecipient (Execute
(refer (extensionConstraint
(RecipientWithNameLike (^(Recipient)
EmptyStructConstraint) (PersonName.apply
"Dr Kennady")))))))))))
(Yield (CreateCommitEventWrapper
(CreatePreflightEventWrapper (&
(Event.start_? (DateTime.date_? (?=
(Today)))) (Event.attendees_?
(AttendeeListHasRecipient (Execute
(refer (extensionConstraint
(RecipientWithNameLike (^(Recipient)
EmptyStructConstraint) (PersonName.apply
"bob")))))))))))
Table 11: An example from SMC ALFLOW development set where EPR is correct and CBR is incorrect along with
the top-3 training examples retrieved from each retriever.
Utterance Meaning Representation
which 3 seas border philippines? 1#) return the philippines
2#) return seas that border #1
what three seas surround philippines?1#) return seas
2#) return #1 that surround the philippines
what states does west virginia border?1#) return west virginia
2#) return border states of #1
what states borders west virginia? 1#) return west virginia
2#) return border states of #1
which states border colorado 1#) return states
2#) return #1 that border colorado
Table 12: Example of a cluster from the t-SNE projection of EPR on BREAK .
2669Utterance Meaning Representation
List the total scores of body builders
in ascending order.
1#) return body builders
2#) return scores of #1
3#) return sum of #2 for each #1
4#) return #3 sorted by ascending order
What are the names of body builders in
descending order of total scores?
1#) return body builders
2#) return names of #1
3#) return scores of #1
4#) return sum of #3 for each #1
5#) return #2 sorted by #4 in descending order
List the total points of gymnasts in
descending order.
1#) return gymnasts
2#) return points of #1
3#) return sum of #2 for each #1
4#) return #3 sorted by descending order
What are the total points for all
gymnasts, ordered by total points
descending?
1#) return gymnasts
2#) return total points for all #1
3#) return #2 ordered by total points descending
List the total points of gymnasts in
descending order of floor exercise
points.
1#) return gymnasts
2#) return points of #1
3#) return sum of #2 for each #1
4#) return floor exercise points of #1
5#) return #3 sorted by #4 in descending order
Table 13: Example of a cluster from the t-SNE projection of EPR on BREAK .
Utterance Meaning Representation
Show the locations that have both
performances with more than 2000
attendees and performances with less
than 1000 attendees.
1#) return performances
2#) return attendees of #1
3#) return the number of #2 for each #1
4#) return #1 where #3 is more than 2000
5#) return #1 where #3 is less than 1000
6#) return the locations of #4
7#) return the locations of #5
8#) return the locations in both #6 and #7
Show the theme for exhibitions with both
records of an attendance below 100 and
above 500.
1#) return exhibitions
2#) return attendances of #1
3#) return number of #2 for each #1
4#) return #1 where #3 is below 100
5#) return #1 where #3 is above 500
6#) return #1 of both #4 and #5
7#) return themes for #6
Which themes have had corresponding
exhibitions that have had attendance
both below 100 and above 500?
1#) return themes
2#) return exhibitions with #1
3#) return attendances of #2
4#) return #1 where #3 is lower than 100
5#) return #1 where #3 is higher than 500
6#) return #1 of both #4 and #5
Show the publishers that have
publications with price higher than
10000000 and publications with price
lower than 5000000.
1#) return publishers
2#) return publications of #1
3#) return prices of #2
4#) return #1 where #3 is higher than 10000000
5#) return #1 where #3 is lower than 5000000
6#) return #1 of both #4 and #5
Show the famous titles of the artists
with both volumes that lasted more than
2 weeks on top and volumes that lasted
less than 2 weeks on top.
1#) return artists
2#) return volumes of #1
3#) return weeks on top that #2 lasted
4#) return number of #3 for each #2
5#) return #1 where #4 is more than 2
6#) return #1 where #4 is less than 2
7#) return #1 in both #5 and #6
8#) return famous titles of #7
Table 14: Example of a cluster from the t-SNE projection of EPR on BREAK .
2670Utterance Meaning Representation
What is the metal thing next to the
small cylinder?
1#) return the small cylinder
2#) return things
3#) return #2 that are metal
4#) return #3 that are next to #1
What is the purple thing next to the
brown thing?
1#) return the brown thing
2#) return things
3#) return #2 that are purple
4#) return #3 that are next to #1
What is the gray thing next to the
block?
1#) return gray thing
2#) return the block
3#) return #1 next to #2
What is the shiny thing next to the
cylinder?
1#) return shiny thing
2#) return cylinder
3#) return #1 next to #2
What is the thing in front of the red
square?
1#) return things
2#) return squares
3#) return #2 that is red
4#) return #1 that is in front of #3
Table 15: Example of a cluster from the t-SNE projection of EPR on BREAK .
Utterance Meaning Representation
Is the purple thing behind the big red
thing?
1#) return purple thing
2#) return big red thing
3#) return Is #1 behind #2
is the purple sphere in front of the
blue cube?
1#) return the purple sphere
2#) return the blue cube
3#) return if #1 is in front of #2
is the gray sphere behind the green
cylinder?
1#) return the green cylinder
2#) return the gray sphere
3#) return if #2 is behind #1
is the red cube in front of the yellow
ball?
1#) return the red cube
2#) return the yellow ball
3#) return if #1 is in front of #2
Is the blue ball in front of the silver
cube?
1#) return blue ball
2#) return silver cube
3#) return is #1 in front of #2
Table 16: Example of a cluster from the t-SNE projection of EPR on BREAK .
2671

Journal of Machine Learning Research 24 (2023) 1-43 Submitted 1/23; Revised 7/23; Published 7/23
Atlas: Few-shot Learning with
Retrieval Augmented Language Models
Gautier Izacard1,2,∗,† gautier@inflection.ai
Patrick Lewis1,∗,† patrick@cohere.com
Maria Lomeli1 marialomeli@meta.com
Lucas Hosseini1,† hoss@meta.com
Fabio Petroni1,† fabiopetroni@meta.com
Timo Schick1,† schick@meta.com
Jane Dwivedi-Yu1 janeyu@meta.com
Armand Joulin1,† ajoulin@meta.com
Sebastian Riedel1,3,† sriedel@meta.com
Edouard Grave1,† egrave@meta.com
1 Meta AI, 2 ENS, PSL University & Inria,3 University College London
Editor: Ivan Titov
Abstract
Large language models have shown impressive few-shot results on a wide range of tasks.
However, when knowledge is key for such results, as is the case for tasks such as question
answering and fact checking, massive parameter counts to store knowledge seem to be needed.
Retrieval-augmented models are known to excel at knowledge intensive tasks without the
need for as many parameters, but it is unclear whether they work in few-shot settings.
In this work we presentAtlas, a carefully designed and pre-trained retrieval-augmented
language model able to learn knowledge intensive tasks with very few training examples.
We perform evaluations on a wide range of tasks, including MMLU, KILT and Natural
Questions, and study the impact of the content of the document index, showing that it can
easily be updated. Notably,Atlas reaches over 42% accuracy on Natural Questions using
only 64 examples, outperforming a 540B parameter model by 3% despite having 50x fewer
parameters.
Keywords: retrieval augmented language models, information retrieval, language models
1. Introduction
Large language models (LLMs) are impressive few-shot learners (Brown et al., 2020; Rae
et al., 2021; Hoﬀmann et al., 2022; Chowdhery et al., 2022). They are able to learn new
tasks with very few examples or even from instructions alone. For this generalisation ability
to emerge, the key ingredients are scaling both the parameter count of the model, and the
size of the training data. Large language models owe this improvement to both a larger
computational budget, enabling more complex reasoning, and the ability to memorize more
∗. Equal contribution
†. Work done while at Meta AI
c⃝2023 Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu,
Armand Joulin, Sebastian Riedel, Edouard Grave.
License: CC-BY 4.0, seehttps://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided
at http://jmlr.org/papers/v24/23-0037.html.Izacard, Lewis, Lomeli, Hosseini, Petroni, Schick, Dwivedi-Yu, Joulin, Riedel, Grave
information related to downstream tasks from the larger training data. While it is intuitive
to assume that increased reasoning abilities lead to better generalisation, and hence few-shot
learning, the same is not true for in-parameter memorisation. Speciﬁcally, it is unclear to
what extent eﬀective few-shot learning requires vast knowledge in the parameters of the
model.
In this paper, we investigate whether few-shot learning requires models to store a large
amount of information in their parameters, and if memorisation can be decoupled from
generalisation. To do so, we leverage the fact that memory can be outsourced and replaced by
an external non-parametric knowledge source by employing aretrieval-augmentedarchitecture.
These models employ a non-parametric memory, for example a neural retriever over a large,
external, potentially non-static knowledge source to enhance a parametric language model.
In addition to their memorisation abilities, such architectures are attractive due to a number
of other established advantages in terms of adaptability, interpretability and eﬃciency (Guu
et al., 2020; Lewis et al., 2020; Yogatama et al., 2021; Borgeaud et al., 2021, inter alia).
However, retrieval-augmented models have yet to demonstrate compelling few-shot learning
capabilities. In this work we address this gap, and presentAtlas, a retrieval-augmented
language model capable of strong few-shot learning, despite having lower parameter counts
than other powerful recent few-shot learners.
Atlas retrieves relevant documents based on the current context by using a general-
purpose dense retriever using a dual-encoder architecture, based on the Contriever (Izacard
et al., 2022). The retrieved documents are processed, along with the current context, by a
sequence-to-sequence model using the Fusion-in-Decoder architecture (Izacard and Grave,
2021a) that generates the corresponding output. We study the impact of diﬀerent techniques
to train Atlas on its few-shot performance on a range of downstream tasks, including
question answering and fact checking. We ﬁnd that jointly pre-training the components is
crucial for few-shot performance, and we carefully evaluate a number of existing and novel pre-
training tasks and schemes for this purpose.Atlas achieves strong downstream performance
in both few-shot and resource-rich settings. For example, with only 11B parameters,Atlas
achieves an accuracy of 42.4% on Natural Questions using 64 training examples (45.1% using
a Wikipedia-only index), outperforming PaLM (Chowdhery et al., 2022), a 540B parameter
model by almost 3 points, and 64.0% in a full data set setting with a Wikipedia index,
establishing a new state of the art by 8.1 points.
In summary we make the following contributions:
•A thorough study on how to design and train retrieval-augmented language models,
with a focus on downstream few-shot learning and sample eﬃciency.
•The ﬁndings of this study lead to a retrieval-augmented language model, calledAtlas,
that exhibits few-shot abilities that emerge at lower scale than standard LLM.
•We provide an exploration of ﬁne-tuning strategies to eﬃciently adapt both the retriever
and the language model to the task at hand.
•Thorough downstream experiments in few-shot settings, demonstrating state-of-the-art
results on few-shot Natural Questions (+2.8%), TriviaQA (+3.3%), FEVER (+5.1%),
and results on par with models with 15×more parameters on MMLU.
2Atlas: Few-shot Learning with Retrieval Augmented Language Models
Fact checking:Bermuda Triangle is in the western part of the Himalayas.
AtlasFalse
Masked Language Modelling:Bermuda Triangle is in the <MASK> of the Atlantic Ocean.
TheBermuda Triangle is anurban legendfocused on a loosely-defined region in the western part of the NorthAtlantic Ocean.
western partPretrainingFew-shot
Question answering:Where is the Bermuda Triangle?Western part of the North Atlantic Ocean
… …
Figure 1: We introduceAtlas, a retrieval-augmented language model that exhibits strong
few-shot performance on knowledge tasks, and uses retrieval during both pre-
training and ﬁne-tuning.
•Experiments investigating full data set ﬁnetuning, setting new state-of-the-art results
in Natural Questions (+8.1%), TriviaQA (+9.3%) and 5 KILT Tasks.
•Experiments demonstrating the updateability and interpretability characteristics of
Atlas.
•Experiments demonstrating that a compressed index using product quantisation
achieves comparable performance as an uncompressed index while resulting in a 5x
memory reduction.
Our code, pre-trainedAtlas checkpoints, and various supporting data are available at
https://github.com/facebookresearch/atlas
2. Method
Our approach follows thetext-to-text framework (Raﬀel et al., 2019). This means that all
the tasks are framed as follows: the system gets atext query as input, and generates atext
output. For example, in the case of question answering, the query corresponds to the question
and the model needs to generate the answer. In the case of classiﬁcation tasks, the query
corresponds to the textual input, and the model generates the lexicalized class label, that is
the word corresponding to the label. We give more examples of downstream tasks, from the
KILT benchmark in Figure 2. As many natural language processing tasks requireknowledge,
our goal is to enhance standard text-to-text models with retrieval, which, as we hypothesise
in the introduction, may be crucial to endow models with few-shot capabilities.
2.1 Architecture
Our model is based on two sub-models: the retriever and the language model. When
performing a task, from question answering to generating Wikipedia articles, our model starts
by retrieving the top-k relevant documents from alarge corpusof text with the retriever.
3Izacard, Lewis, Lomeli, Hosseini, Petroni, Schick, Dwivedi-Yu, Joulin, Riedel, Grave
Task Query Output
Fact Checking Bermuda Triangle is in the western part of the Hi-
malayas.
False
Question
Answering
who is playing the halftime show at super bowl 2016 Coldplay
Entity Linking NTFS-3G is an open source <E>cross-platform</E>
implementation of the Microsoft Windows NTFS ﬁle
system with read-write support.
Cross-platform
software
Figure 2: Examples of query and output pairs for diﬀerent tasks from KILT.
Then, these documents are fed to the language model, along with the query, which in turns
generates the output. Both the retriever and the language model are based on pre-trained
transformer networks, which we describe in more detail below.
2.1.1 Retriever
Our retriever module is based on the Contriever (Izacard et al., 2022), an information
retrieval technique based on continuous dense embeddings. The Contriever uses a dual-
encoder architecture, where the query and documents are embedded independently by a
transformer encoder (Huang et al., 2013; Karpukhin et al., 2020). Average pooling is applied
over the outputs of the last layer to obtain one vector representation per query or document.
A similarity score between the query and each document is then obtained by computing the
dot product between their corresponding embeddings. The Contriever model is pre-trained
using the MoCo contrastive loss (He et al., 2020), and uses unsupervised data only. As shown
in the following section, an advantage of dense retrievers is that both query and document
encoders can be trained without document annotation, using standard techniques such as
gradient descent and distillation.
2.1.2 Language Model
For the language model, we rely on the T5 sequence-to-sequence architecture (Raﬀel et al.,
2019). We rely on the Fusion-in-Decoder modiﬁcation of sequence-to-sequence models, and
process each document independently in the encoder (Izacard and Grave, 2021a). We then
concatenate the outputs of the encoder corresponding to the diﬀerent documents, and perform
cross-attention over this single sequence in the decoder. Following Izacard and Grave (2021a),
we concatenate the query to each document in the encoder. Another way to process the
retrieved documents in the language model would be to concatenate the query and all the
documents, and to use this long sequence as input of the model. Unfortunately, this approach
does not scale with the number of documents, since the self-attention in the encoder results
in a quadratic complexity with respect to the number of documents.
4Atlas: Few-shot Learning with Retrieval Augmented Language Models
2.2 Training Objectives for the Retriever
In this section, we discuss four diﬀerent loss functions to train the retriever jointly with the
language model. We consider loss functions that leverage the language model to provide
supervisory signal to train the retriever. In other words, if the language model ﬁnds a
document useful when generating the output, the retriever objective should encourage the
retriever to rank said document higher. This allows us to train models using only query
and output pairs from the task of interest, without relying on document annotations. For
example, in the case of fact checking, a model only requires pairs of claims and corresponding
verdicts but no documents containing the evidence to back up the verdict. In practice, we
can apply this approach on any task, including self-supervised pre-training. As shown in
the experimental section, pre-training is critical for obtaining models that exhibit few-shot
learning abilities.
2.2.1 Attention Distillation (ADist)
The ﬁrst loss that we consider is based on the attention scores of the language model, and is
heavily inspired by Izacard and Grave (2021b). The main idea is that the cross-attention
scores between the input documents and the generation can be used as a proxy of the
importance of each input document when generating the output. In particular, Izacard and
Grave (2021b) showed that these scores can be aggregated across attention heads, layers
and tokens for a given document to obtain a single score for each document. Then, these
scores are distilled into the retriever by minimizing the KL-divergence with the probability
distribution pretr over the top-K documents{dk}1,...,K obtained from the retriever:
pretr (d |q) = exp(s(d,q)/θ)∑K
k=1 exp(s(dk,q)/θ)
, (1)
where s is the dot-product between the embedding vectors of the query and documents and
θ is a temperature hyperparameter.
In the original paper, to obtain a relevance score per document it was proposed to
use the pre-softmax scores from the decoder cross-attentions, and average across heads,
layers and tokens. Here, we use the pre-softmax score multiplied by the norm of the
values, an alternative which gives slightly stronger results. First, let us brieﬂy review the
Fusion-in-Decoder model (FiD, Izacard and Grave, 2021a). The underlying architecture
is a sequence-to-sequence model, composed of an encoder and a decoder. The encoder
independently processesK diﬀerent text inputs(input(dk))1≤k≤K, whereinput(d) is the
concatenation of the input query and the retrieved documentd. The output representations of
the encoder are then concatenated to form a global representationX of dimension(∑
k ℓk)×d,
where ℓk is the length ofinput(dk) and d is the dimension of the hidden representations
of the model. Then, the decoder processes this representation as a regular autoregressive
model, alternating self-attention, cross-attention and feed-forward modules.
Only the cross-attention module explicitly takes as input the global output representation
X of the encoder. IfH ∈Rd denotes the output of the previous self-attention layer of the
decoder, the cross-attention operation consists in the following operations. First, queriesQ,
keys K and valuesV are computed by applying linear transformations:
Q = WQH, K = WKX, V = WV X.
5Izacard, Lewis, Lomeli, Hosseini, Petroni, Schick, Dwivedi-Yu, Joulin, Riedel, Grave
Then a similarity score between the query at positioni, Qi, and the key at positionj, Kj, is
obtained by computing the dot-product between these two elements, and normalized over
the dimension:
αi,j = QT
i Kj, ˜αi,j = exp(αi,j)∑
m exp(αi,m).
Anewrepresentationisobtainedasasumofthevalues, weightedbytheattentionprobabilities,
before going through a ﬁnal linear transformationWo:
Oi = WO
∑
j
˜αi,jVi,j.
This describes the single-head attention case, in the case of multi-head attention withnh
heads, the output of the cross-attention layer can be written as:
Oi =
nh∑
h=1
WO,h
∑
j
˜αh,i,jVj,h.
For the layerland the headh, we use the quantity˜αl,h,i,j∥Vl,h,j∥2 as the measure of relevance
for the input token at positionj relatively to the generated token at positioni. We average
these scores over all attention heads, layers, tokens of the generation and tokens of the input
segment input(d) to obtain an attention scorescoreattn(d) for each documentd:
scoreattn(d) = mean
h,l,i,j∈inputk
αl,h,i,j∥Vl,h,j∥2.
We apply theSoftmax operator over the resulting scores, to obtain a distributionpattn(d)
over the top-K retrieved documents:
pattn(d) = exp (scoreattn(d))∑
k exp (scoreattn(dk)).
We then minimize the KL-divergence betweenpattn, and the distributionpretr from the
retriever deﬁned in Equation 1:
KL(pattn ∥pretr) =
K∑
k=1
pattn(dk) log
(pattn(dk)
pretr(dk)
)
.
Here, this loss is only used to optimize the parameters of the retriever, and not the lan-
guage model. When using recent deep learning frameworks, this is achieved by applying a
StopGradient operator onpattn.
2.2.2 End-to-end Training of Multi-Document Reader and Retriever
(EMDR2)
Next, we consider the method introduced by Sachan et al. (2021), which is inspired by the
expectation-maximization algorithm, treating retrieved documents as latent variables. Given
6Atlas: Few-shot Learning with Retrieval Augmented Language Models
a queryq, the corresponding outputa and the setDK of top-K retrieved documents with
the current retriever, the EMDR2 loss to train the retriever is
−log
[K∑
k=1
plm(a |q,dk)pretr(dk |q)
]
,
where pretr is again the probability over the top-K documents obtained with the retriever, as
deﬁned by Equation 1. Again, only the parameters of the retriever are updated by applying a
StopGradient operator aroundplm. One should note that the probability distribution over
documents that minimizes this loss function is an indicator of the document corresponding
to the highest probability of the output according to the language model. Finally, in practice,
the EMDR2 loss function is applied at the token level, and not at the sequence level.
2.2.3 Likelihood Distillation (LDist)
Third, we discuss a simpler loss function which is inspired by the objectives from the attention
distillation and EMDR2 methods (Izacard and Grave, 2021b; Sachan et al., 2021). More
precisely, we want to train the retriever to predict how much each document would improve
the ability of the language model to predict the output, given the query. To this end, we
minimize the KL-divergence between the documents distribution of the retriever (Eqn. 1),
and the documents posterior distribution according to the language model conditioned on a
single document and using a uniform prior:
pLDist(dk) ∝pLM (a |dk,q).
Using theSoftmax operator, we have that
pLDist(dk) = exp(log pLM (a |dk,q))∑K
i=1 exp(log pLM (a |di,q))
.
2.2.4 Leave-one-out Likelihood Distillation (LOOL)
Finally, we propose an objective based on how muchworse the prediction of the language
model gets whenremoving one of the top-k retrieved documents. To do so, we compute the
log probability of the output for each subset of k-1 documents, and use the negative value as
relevance score for each document. Following the previous loss function, we use the softmax
operator to obtain a probability distribution over documents:
plool(dk) = exp(−log pLM (a |DK \{dk},q))∑K
i=1 exp(−log pLM (a |DK \{di},q))
.
As before, we then minimize the KL-divergence between this distribution, and the one
obtained with retriever. This loss is more expensive to compute than LDist and EMDR,
but, like ADist, employs the language model more closely to the way it is trained: the LM
is trained to be conditioned on a set ofK documents. For LOOL, the language model is
conditioned on(K−1) documents, rather than a single document as in EMDR2 and LDist.
For all losses, we can also use a temperature hyperparameter when computing the target
or retriever distributions to control the distribution’s peakiness of, which might be important
for some tasks or losses. Indeed, for LDist and LOOL, the likelihood of the output may not
vary much when conditioning on diﬀerent documents, especially in the case of long outputs.
7Izacard, Lewis, Lomeli, Hosseini, Petroni, Schick, Dwivedi-Yu, Joulin, Riedel, Grave
2.3 Pretext Tasks
In this section, we describe pretext tasks that can be used to jointly pre-train the retriever
and the language model using only unsupervised data.
2.3.1 Prefix Language Modeling
First, we consider a standard language modeling task as a potential pre-training objective.
To cast language modeling in the text-to-text framework, we consider a chunk ofN words,
and split this chunk in two sub-sequences of equal lengthN/2. Then, the ﬁrst sub-sequence
is used as the query, and the second corresponds to the output. We thus retrieve relevant
documents by using the ﬁrst sub-sequence ofN/2 tokens, to generate the output.
2.3.2 Masked Language Modeling
Second, we consider masked language modeling, as formulated by Raﬀel et al. (2019). Again,
starting from a chunk ofN words, we samplek spans of average length 3 tokens, leading
to a masking ratio of15%. We then replace each span by a diﬀerent special token. The
model is then trained to generate the masked spans, each span beginning with the special
sentinel mask token that was inserted in the input sequence. We retrieve documents using
the masked query, but replace the special mask tokens with a mask token supported by the
retriever vocabulary.
2.3.3 Title to Section Generation
Finally, we consider a more abstractive generation task, generating sections from Wikipedia
articles, given the article and section title. Here, the query corresponds to the title of the
article, together with the title of the section, and the output corresponds to the text of the
section. We exclude sections “See also”, “References”, “Further reading” and “External links”.
2.4 Eﬃcient Retriever Fine-tuning
Retrieval is facilitated by using a documentindex, which is a pre-computed collection of the
document embeddings for all the documents in the retrieval corpus. When jointly training
the retriever and language model, the index needs to be updated regularly, otherwise, the
embeddings of the documents stored in the index become stale relative to the updated
retriever. This means that we need to recompute the embeddings for the full collection of
documents regularly during training to keep the index fresh, which can be computationally
expensive for large indices. This is particularly true atﬁne-tuning time, where the number of
training examples could be small relative to the number of documents in the index. Training
the retriever could thus add an important computational overhead compared to standard
language model ﬁnetuning. In this section, we analyse strategies that might make this process
more eﬃcient, alleviating the need to re-compute the embeddings of all the documents too
often.
8Atlas: Few-shot Learning with Retrieval Augmented Language Models
2.4.1 Full Index Update
Let us start by analysing the overhead due to updating the index compared to using a ﬁxed
retriever. To compare the computation time of diﬀerent models, we will make the following
assumption: the time required to perform a forward pass on a document with a model ofP
parameters isO(P). While this computation model may seem naive, the main assumption
is that document sizes are constant.1 Since we split long documents into passages with
similar number of words, and use padding when processing documents of diﬀerent sizes, this
assumption is reasonable in practice. LetK be the number of documents that are retrieved
and processed by the language model,Plm be the number of parameters of the language
model andB the batch size. Each training step has a complexity of4 ×B×K×Plm.2
Next, let N be the number of documents in the index, andPretr be the number of
parameters of the retriever. Then, re-computing the full index has a complexity ofN×Pretr.
If we refresh the index everyR training steps, we obtain the following overhead:
N ×Pretr
4 ×B×K×Plm ×R.
If we use the BERT base architecture for our retriever and T5-XL for our language model,
we getPretr
Plm ≈ 1
25 , leading to the overhead:
N
100 ×B×K×R.
If we use an index containing37M documents (the size of our Wikipedia index), train with
a batch size of64 with 20 retrieved documents and refresh the index every 1000 steps, this
results in an overhead of∼30%.
2.4.2 Re-ranking
A second strategy is to retrieve a larger number of documentsL with the retriever, and to
re-embed and rerank these documents with the up-to-date retriever, and pass the resulting
top-K to the language model. In that case, the overhead of reranking the top-L documents
is equal toB×L×Pretr. Since we perform this operation at every time step, the overhead
is equal to
L×Pretr
4 ×K×Plm
.
Using the same assumption as before, we ﬁnally get that the overhead is of the order of
L
100×K . If we re-rank 10x more documents than what the language model processes (that is
L= 10 ×K), we get an overhead of10%. However, note that if many updates are performed
on the retriever, the index might still need to be fully updated, as the true top-k documents
may not be retrieved in the top-L results from the stale index. In practice, it is possible to
track the positions of the top-K re-ranked documents in the top-L, and estimate when the
index needs to be updated.
1. See Hoﬀmann et al. (2022) for more details about the computation of the FLOPS corresponding to the
forward and backward passes of transformer networks.
2. There is a factor 4 to account for the backward pass and activation checkpointing.
9Izacard, Lewis, Lomeli, Hosseini, Petroni, Schick, Dwivedi-Yu, Joulin, Riedel, Grave
2.4.3 Query-side Fine-tuning
Finally, the last strategy is to decouple the encoding of the queries and documents as done
in Guu et al. (2020). In this case, we ﬁx the parameters corresponding to the document
encoder, and only train the parameters corresponding to the query encoder. Thus, the
embeddings of documents are ﬁxed, and we do not need to refresh the index, and thus there
is no computational overhead. As we will see in practice, the impact of ﬁxing the documents
encoder varies greatly for diﬀerent tasks when a large training data set is available. For
most of the few-shot settings that we consider, query-side ﬁnetuning does not have large
performance impact, and sometimes even slightly improves performance.
3. Related Work
In this section we ﬁrst review the literature on retrieval in language models, before giving an
overview on few-shot learning in natural language processing.
3.1 Retrieval-augmented models in Natural Language Processing
There has been a long line of work studying the eﬀect and potential beneﬁts of retrieval
augmentation for NLP tasks.
3.1.1 Retrieval for Knowledge Intensive Tasks
Previous work has shown that retrieval improves performance across a variety of tasks
such as question answering (Voorhees, 1999; Chen et al., 2017; Kwiatkowski et al., 2019),
fact checking (Thorne et al., 2018), dialogue (Dinan et al., 2019) or citation recommenda-
tion (Petroni et al., 2022). Historically, this information retrieval step was implemented using
term-matching methods, such as TF-IDF or BM25 (Jones, 1972; Robertson et al., 1995).
For open-domain question answering (Voorhees, 1999), documents are often retrieved from
Wikipedia (Chen et al., 2017). Recently, dense retrievers based on neural networks have
become popular. These usually follow a dual-encoder architecture (Yih et al., 2011; Huang
et al., 2013; Shen et al., 2014), where queries and passages are encoded independently as
vectors, and relevance is computed using the inner product or Euclidean distance. Popular
supervised retrievers include DPR (Karpukhin et al., 2020), which is trained to discriminate
the relevant passage among negative passages, and extensions such as ANCE (Xiong et al.,
2021) which improved the hard negatives mining process. We refer the reader to Yates et al.
(2021) for a survey of dense retrieval techniques.
After retrieval, the relevant documents are processed to produce the ﬁnal output. In
open-domain QA, models can extract a span of text from retrieved documents as the
answer (Chen et al., 2017; Clark and Gardner, 2018; Wang et al., 2019; Karpukhin et al.,
2020), a method inspired by reading comprehension (Richardson, 2013; Rajpurkar et al.,
2016). Recently, generating the answer as free-form text, using a seq2seq model conditioned
on retrieved documents have become prevalent (Lewis et al., 2020; Izacard and Grave, 2021a;
Min et al., 2020). These architectures have also been shown to reduce hallucination in
dialogue agents (Shuster et al., 2021).
10Atlas: Few-shot Learning with Retrieval Augmented Language Models
3.1.2 Retriever training
The need for expensive query-document annotations for training the retriever can be bypassed,
by leveraging signals from the language model, or using unsupervised learning. REALM (Guu
et al., 2020) and RAG (Lewis et al., 2020) jointly train the retriever and language model by
modelling documents as latent variable, and minimizing the objective with gradient descent.
REALM pre-trains end-to-end with an MLM approach but uses an extractive BERT-style
model (Devlin et al., 2019). Guu et al. (2020) also explore a query-side ﬁnetuning at
ﬁnetuning time to avoid index refreshes, which is also explored in the context of phrase-based
retrieval by Lee et al. (2021b). Izacard and Grave (2021a) proposed to use cross-attention
scores as supervision with knowledge distillation. Sachan et al. (2021) perform joint training
of the reader and the retriever by leveraging the likelihood of the output generated by the
reader. Sachan et al. (2021) and Lee et al. (2021a) both employ salient span masking to
pre-train retrievers, leveraging the likelihood and attention scores from the language model.
The inverse cloze taskwas proposed by Lee et al. (2019) to pre-train dense retrievers in an
unsupervised way. Paranjape et al. (2021) propose a method to train retrieval-augmented
generators using a second “informed” retriever with access to the output, which the test-time
retriever can be distilled from, and Hofstätter et al. (2022) recently proposed a training set
ﬁltering/weighting approach to train stronger retrieval-augmented generators. Izacard et al.
(2022) explored diﬀerent contrastive learning methods to train retrievers, while Ram et al.
(2022) used recurring spans within a document to create pseudo-positive query-document
pairs.
3.1.3 Retrieval-augmented language models
Continuous cache models (Grave et al., 2017b) deﬁnes a probability distribution over recent
tokens, by computing the similarity between previous and current representations of tokens.
This distribution is then interpolated with the distribution of the language model, to improve
predictions. Later, the amount of tokens used to compute this distribution was extended to a
much larger memory by leveraging approximate nearest neighbors search (Grave et al., 2017a).
The related kNN-LM model (Khandelwal et al., 2020) replaced LSTMs by transformer
networks, and scaled the memory to billions of tokens, leading to strong performance
improvements. More recently, RETRO (Borgeaud et al., 2021) extended these by scaling the
retrieval memory to trillions of tokens, and changing the model architecture to take retrieved
documents as input.
3.1.4 Retrieval-Augmentation with Search Engines
Recently, diﬀerent works have proposed to train large language models to interact with a
search engine, by generating text queries, and using the retrieved documents as additional
context (Nakano et al., 2021; Thoppilan et al., 2022; Shuster et al., 2022). In the context
of few-shot question answering, Lazaridou et al. (2022) used the question to perform a
search query, and retrieved documents are added to the prompt of a large language model
performing in-context learning.
11Izacard, Lewis, Lomeli, Hosseini, Petroni, Schick, Dwivedi-Yu, Joulin, Riedel, Grave
3.2 Few-shot Learning
Few-shot learning, the task of learning from very few examples, has been studied for decades
(Thrun and Pratt, 1998; Fink, 2005; Vinyals et al., 2016), but has recently seen an explosion
of interest in NLP with the arrival of large pre-trained models.
3.2.1 In-context Learning with Large Language Models
Providing language models with natural language descriptions of tasks, as proposed by
Radford et al. (2019) has led to signiﬁcant developments in few-shot learning. GPT-3 (Brown
et al., 2020) demonstrated the ability of large language models to perform few-shot predictions,
where the model is given a description of the task in natural language with few examples.
Scaling model size, data and compute is crucial to enable this learning ability, leading to the
further development of large models (Lieber et al., 2021; Rae et al., 2021; Smith et al., 2022;
Chowdhery et al., 2022; Smith et al., 2022). Hoﬀmann et al. (2022) revisited the scaling law
from Kaplan et al. (2020), suggesting that training on more data with a smaller model may
be more eﬀective, resulting in Chinchilla, a 70B parameter model with improved parameter
eﬃciency.
3.2.2 Few-shot Finetuning and Prompt-based Learning
The above models perform few-shot learning with in-context instructions without training the
parameters of the language model. Few-shot learning can also be accomplished by combining
textual templates (“prompts”) and various forms of model ﬁnetuning, either fully updating
a model’s parameters, for example for classiﬁcation (Schick and Schütze, 2021a; Schick
and Schutze, 2021; Gao et al., 2021; Tam et al., 2021) or generation (Schick and Schütze,
2021b). Prompts themselves can be optimized, for example by search (Jiang et al., 2020;
Shin et al., 2020) or by only updating parts of the model (Logan et al., 2021), or learning
“soft-prompts” (Lester et al., 2021; Li and Liang, 2021). Due to its simplicity, in this work we
either employ simple prompts or simply feed in inputs without preprocessing, and perform
full-model ﬁnetuning, a method similar to Le Scao and Rush (2021).
4. Experiments
In this section, we report empirical evaluations of our language models on few-shot learning.
We start by introducing our experimental setup, describing our evaluation benchmarks in
section 4.1, and giving the training details of our models in section 4.2. Then, we perform an
ablation study to compare the diﬀerent technical choices leading to our main model. We
ﬁnally evaluate this model, calledAtlas, on diﬀerent natural language understanding tasks
in few-shot and full data set settings.
4.1 Benchmarks
To evaluate our retrieval-augmented language models we consider the following benchmarks,
which include diﬀerent tasks.
12Atlas: Few-shot Learning with Retrieval Augmented Language Models
4.1.1 Knowledge-Intensive Language Tasks (KILT)
First, we use the KILT evaluation suite (Petroni et al., 2020), containing 11 data sets
corresponding to 5 tasks: fact checking, question answering, dialog generation, entity
linking and slot-ﬁlling. To be solved, these diﬀerent tasks require knowledge about the
world, which can be found in Wikipedia. We evaluate our model on the following tasks
and data sets included in KILT: question answering: Natural Questions (Kwiatkowski
et al., 2019), TriviaQA (Joshi et al., 2017) and HotpotQA (Yang et al., 2018); slot ﬁlling:
Zero Shot RE (Levy et al., 2017) and T-REx (Elsahar et al., 2018); entity linking: AIDA
CoNLL-YAGO (Hoﬀart et al., 2011); dialogue: Wizard of Wikipedia (Dinan et al., 2019);
and fact checking: FEVER (Thorne et al., 2018). The KILT versions of these data sets diﬀer
from their original versions, as instances requiring knowledge not present in the August 2019
Wikipedia dump have been removed.
4.1.2 Massively-Multitask Language Understanding (MMLU)
Our second main evaluation benchmark is MMLU (Hendrycks et al., 2021), which contains
57 multi-choice question answering data sets (referred to as domains), sourced from real
examinations designed for humans. These cover a very broad range of topics, for example
high school mathematics, professional law, logical fallacies and clinical knowledge and can be
broadly categorized in four subsets: humanities, social sciences, STEM and “other”. We focus
on few-shot learning, and the authors of the benchmarks suggest to use 5 training examples
per domain. Beyond the 5-shot setting, We also consider three additional settings. The ﬁrst
is azero-shot setting, with no training data at all. The second, which we callmulti-task
few-shot, is where we train a single model on the 5-shot data from all tasks, hence leading to
a training set of 285 examples. The last, which we calltransfer learning, leverages additional
training examples from other multiple-choice QA tasks provided by the MMLU authors,
namely MCTest (Richardson et al., 2013), RACE (Lai et al., 2017), ARC (Clark et al., 2018)
and OBQA (Mihaylov et al., 2018) leading to a training set of 95k examples.
4.1.3 Additional Benchmarks
Additionally, we report results on the original open-domain versions of the popular Natural
Questions (Kwiatkowski et al., 2019), and TriviaQA (Joshi et al., 2017) data sets. Generated
answers are evaluated with the standard exact match metric (EM), as used by Rajpurkar
et al. (2016). A generated answer is considered correct if it matches any answer of the list
of acceptable answers after normalization. This normalization step consists in lowercasing
and removing articles, punctuation and duplicated whitespaces. We also evaluate our model
on the original version of FEVER (Thorne et al., 2018), which presents fact checking as a
three-way classiﬁcation problem for textual claims (either “Supported”: the text is supported
by evidence in Wikipedia, “refuted”: the claim is not consistent with evidence in Wikipedia,
or “not enough info”, where there is insuﬃcient evidence to make a judgement). We also
perform experiments to assess temporal sensitivity of our models. Here, we construct a
data set from TempLAMA (Dhingra et al., 2022), consisting of a set of time-sensitive cloze
questions on a range of topics, where the answer changes from 2017 to 2020. We assess the
accuracy of our models when supplied with a index from 2017 vs 2020 to assess to what
13Izacard, Lewis, Lomeli, Hosseini, Petroni, Schick, Dwivedi-Yu, Joulin, Riedel, Grave
degree models faithfully reﬂect the content of the index supplied to them at test time, and
how eﬀective updating the index is as acontinual learningor model updateability method.
4.2 Technical Details
We now describe the procedure for pre-training and ﬁne-tuning our models. We focus on the
setting used for the ablation studies performed in Section 4.3 and Section 4.4. We give more
details about the hyperparameters used for our ﬁnal model later.
4.2.1 Pre-training
For the pre-training, we initialize the retriever module using the unsupervisedContriever
model (Izacard et al., 2022), which uses the BERT base architecture. We initialize the
language model with the T5 pre-trained weight (Raﬀel et al., 2019). As the original T5
pre-trained model included supervised data in the training set, we use the version 1.1 models
which were trained on unlabeled text only. Speciﬁcally, we initialize from theT5-lm-adapt
variants due to their improved stability.
For the ablation studies performed in Section 4.3 and Section 4.4, we use T5-XL which
contains 3B weights. We pre-train all our models for 10,000 iterations, using AdamW with a
batch size of 64 and a learning rate of10−4 for the reader and10−5 for the retriever with
linear decay and 1,000 warmup steps. We refresh the index every 1,000 steps. This means
that the index is recomputed 10 times during the pre-training, leading to an overhead of
around 30%, compared to training with a ﬁxed retriever. We set the number of retrieved
documents to 20. We detail the hyperparameters used for the training of our ﬁnal model at
the beginning of Section 5.
4.2.2 Fine-tuning
When performing a downstream task, either in a few-shot setting or with a large training
set, we employ ﬁne-tuning to adapt our models to these tasks. For the few-shot KILT
ablation experiments, we perform a ﬁxed number of ﬁne-tuning iterations, instead of using
early-stopping. More precisely, we decided to use 50 iterations for the 64-shot setting and
200 iterations in the 1024-shot setting. In both cases, we use a batch size of32 examples, a
learning rate of4 ×10−5 with linear decay and 5 warmup steps for both the reader and the
retriever.
4.2.3 Unlabeled Data Sets
Finally, we discuss the unlabeled text data sets that we use to train our models, which form
the retrieval index. First, we consider the Dec. 20, 2021 Wikipedia dump, for which we keep
the lists and infoboxes, which are linearized by adding a semi-colon separator between the
entries. We split articles by section, and split long sections into passages of equal sizes and
containing less than 200 words. This leads to a total of 37M passages, containing 78 words
in average. We also use documents from the 2020-10 common crawl dump, preprocessed
with the CCNet pipeline (Wenzek et al., 2020). We perform additional document ﬁltering, in
a similar fashion to Gopher (Rae et al., 2021). More precisely, we ﬁlter documents based
on document length, average word length, ratio of alphanumeric characters and number of
14Atlas: Few-shot Learning with Retrieval Augmented Language Models
64-shot 1024-shot
MLM NQ WoW FEVER Avg. NQ WoW FEVER Avg.
Closed-book 1.083 6.5 14.1 59.0 26.5 10.7 16.5 75.3 34.2
No joint pre-training - 9.0 14.1 67.0 30.0 9.9 16.6 78.3 34.9
Fixed retriever 0.823 39.9 14.3 72.4 42.2 45.3 17.9 90.0 51.1
ADist 0.780 40.9 14.4 73.8 43.0 46.2 17.2 90.9 51.4
EMDR2 0.783 43.3 14.6 72.1 43.3 44.9 18.3 85.7 49.6
LDist 0.783 45.0 15.0 77.0 45.7 44.9 17.9 90.2 51.0
LOOL 0.766 41.8 15.0 74.4 43.7 47.1 17.9 87.5 50.8
Table 1: Loss function ablation. We compare diﬀerent loss functions to pre-train the retriever
jointly with the language model. We compare diﬀerent loss functions to pre-train
the retriever jointly with the language model. We use the preﬁx MLM task for
pre-training. Fine-tuning is performed with query-side ﬁne-tuning and the loss used
for pre-training. Best result is bold, second highest underlined.
repeated tokens. This leads to a total of 350M passages. The same passages are used for the
index and model pre-training. During pre-training, we ensure the passage we are training on
is ﬁltered out from the retrieved documents, to prevent the model from simply retrieving the
passage it is de-nosing/generating, and trivially using it to solve the pre-training task.
4.3 Pre-training Loss and Tasks
We start our ablation study by comparing diﬀerent pre-training tasks and objective functions
to jointly train the retriever and the language model. Our goal here is to answer the following
research questions:
(RQ 1)Does jointly pre-training the whole model lead to better few-shot performance?
(RQ 2)What is the best objective function for the retriever, and the best pretext task?
We start by comparing the training objectives of the retriever, introduced in Section 2.2, by
pre-training models using the masked language modelling task. We evaluate these models on
a subset of the 64-shot and 1024-shot KILT benchmark: Natural Questions, FEVER and
Wizard of Wikipedia, along with three baselines: a “closed-book” model, a model without
joint pre-training, and a model pre-trained with a ﬁxed retriever. The closed-book baseline
is a non-retrieval-augmented T5 model, initialized with T5-XL, and further pre-trained on
the same data as the other models with the masked language modelling task to ensure that
all models are trained on a similar amount of tokens. Finally, the closed-book model is
ﬁne-tuned without retrieval augmentation. For the baseline without joint pre-training: the
reader is also pre-trained without retrieval, and the retriever is initialized at ﬁnetuning from
Contriever and trained with the LDist loss. Similarly the model pre-trained with a ﬁxed
retriever is ﬁne-tuned with the LDist loss. We report results in Table 1. First, we note the
poor performance of the closed-book baseline, indicating the importance of augmentation.
Next, we observe that pre-training our model with retrieval is important to obtain good
performance on few-shot tasks. Indeed, all models that include retrieval during pre-training
15Izacard, Lewis, Lomeli, Hosseini, Petroni, Schick, Dwivedi-Yu, Joulin, Riedel, Grave
64-shot 1024-shot
NQ WoW FEVER Avg. NQ WoW FEVER Avg.
Preﬁx Language Modelling 41.0 14.5 64.9 40.1 44.7 17.9 86.0 49.5
Masked Language Modelling 42.7 14.9 69.7 42.4 44.7 18.3 88.8 50.6
Title-to-section generation 41.1 15.2 66.1 40.8 45.4 17.9 84.6 49.3
Table 2: Pretext task ablation. We compare diﬀerent pretext tasks, used to jointly pre-train
our models. Examples are randomly sampled from the training set of the KILT
version of the data set. We report the exact match on Natural Questions, the F1
score on Wizard of Wikipedia and the accuracy on FEVER.
64-shot 1024-shot
Index Training data NQ WoW FEVER Avg. NQ WoW FEVER Avg.
Wiki Wiki 42.7 14.9 69.7 42.4 44.7 18.3 88.8 50.6
Wiki CCNet 40.9 15.3 67.3 41.2 44.8 18.4 88.1 50.4
CCNet Wiki 32.9 14.5 72.1 39.8 37.8 17.1 85.8 46.9
CCNet CCNet 38.4 14.9 70.1 41.1 42.0 17.3 88.9 49.4
Table 3: Index content ablation. In this table, we report results for models where the content
of the index was changed between the pre-training and the ﬁne-tuning.
strongly outperform the baseline without joint pre-training. Next, we compare a model
that was pre-trained with a ﬁxed retriever, and models using the various retriever training
objectives. On the MLM validation metric corresponding to the pre-training objective, we
observe that jointly training the retriever leads to strong improvements. This eﬀect tends
to be less marked on 64-shot downstream tasks, and almost non-existent for 1024-shot. We
believe that this is evidence that the biggest impact of pre-training is on the language model,
which learns to use and aggregate information from the retrieved documents. Lastly, we
do not observe signiﬁcant systematic diﬀerences between the diﬀerent retriever training
objectives. We thus decide adopt use Likelihood Distillation for subsequent experiments, as
it tends to be more stable than EMDR2 or ADist, and more computationally eﬃcient than
LOOL.
Next, we compare the diﬀerent self-supervised pretext tasks introduced in Section 2.3
in Table 2. Here we observe similar results for all three tasks, with a small advantage for
masked language modelling. Thus, in what follows, we adopt masked language modelling for
pre-training.
Finally, we consider diﬀerent combinations of data sources—Wikipedia and common
crawl—for the index and training data during pre-training. In all cases, we use the Wikipedia
2021 dump as the index when performing few-shot ﬁne-tuning. We report results in Table 3.
First, we observe that using a Wikipedia-based index leads to better downstream performance.
There could be two explanations for this: ﬁrst, as we use Wikipedia for the few-shot tasks,
the model might be better adapted when trained using the same data. Another explanation
16Atlas: Few-shot Learning with Retrieval Augmented Language Models
64-shot 1024-shot
NQ WoW FEVER Avg. NQ WoW FEVER Avg.
Standard ﬁne-tuning 44.3 14.9 73.2 44.1 47.0 18.4 89.7 51.7
Top-100 re-ranking 44.2 14.6 75.4 44.7 47.1 18.7 88.9 51.6
Query-side ﬁne-tuning 45.0 15.0 77.0 45.7 44.9 17.9 90.2 51.0
Fixed retriever 36.8 14.5 72.0 41.1 38.0 17.7 89.3 48.3
Table 4: Retriever ﬁne-tuning ablation. Here, we compare diﬀerent strategies to ﬁne-tune
the retriever in a few-shot setting.
might be that Wikipedia is a higher-quality and denser source of knowledge than common
crawl. Second, when using a common crawl index, we observe that pre-training on Wikipedia
data leads to lower performance than using common crawl data. We believe that the primary
reason is that the distribution mismatch between the two domains leads to generally-less
relevant retrieved documents. In turn, this probably means that the pre-training is less
eﬃcient, because the language model does not leverage as much information from the
documents. In the following, we decide to combine the data from both domains for the index
and the pre-training data to extend the coverage.
4.4 Fine-tuning
In this section, we perform an ablation study on how to apply our models on downstream
tasks, which relies on ﬁne-tuning. In particular, we want to investigate the following research
question:
(RQ 3)How to eﬃciently ﬁne-tuneAtlas on tasks with limited training data?
To answer this question, we compare the diﬀerent strategies to ﬁne-tune the retriever module,
described in Section 2.4. We report results in Table 4. First, as for pre-training, we observe
that keeping the retriever ﬁxed during ﬁne-tuning leads to a signiﬁcant performance drops,
for both 64- and 1024-shot settings. Second, the re-ranking strategy (row 2) leads to very
similar results to fully updating the index (row 1), while being signiﬁcantly more eﬃcient.
Lastly, ﬁne-tuning only the query encoder also leads to strong results: in particular, in the
64-shot setup, this is slightly stronger than performing full ﬁne-tuning, which we attribute to
there being less opportunity for over-ﬁtting. On the other hand, on the 1024-shot setting,
performing a full ﬁne-tuning leads to stronger results, especially on Natural Questions. In
the following, we use query-side ﬁne-tuning for experiments with less than 64 examples, and
standard ﬁne-tuning for larger data sets.
5. Training and EvaluatingAtlas
In this section, we apply the ﬁndings from the ablations of the previous sections to train a
family ofAtlas models, ranging from 770M to 11B parameters. More speciﬁcally, we use
the Likelihood Distillation objective function, along with the masked language modelling
17Izacard, Lewis, Lomeli, Hosseini, Petroni, Schick, Dwivedi-Yu, Joulin, Riedel, Grave
5-shot 5-shot (multi-task) Full / Transfer
770M 3B 11B 770M 3B 11B 770M 3B 11B
Closed-book T5 29.2 35.7 36.1 26.5 40.0 43.5 42.4 50.4 54.0
Atlas 38.9 42.3 43.4 42.1 48.7 56.4 56.3 59.9 65.8
∆ +9.8 +6.6 +7.3 +15.6 +8.7 +12.9 +13.9 +9.5 +11.8
Table 5: Performance on MMLU as a function of model size. We report performance of
Atlas on MMLU as a function of model size and compare it to closed-book T5.
pretext task. We pre-train these models using a mix of Wikipedia and Common Crawl data,
for both the training data and content of the index. During pre-training, the reader generates
based on 20 documents retrieved using the re-ranking strategy described in 2.4. For this we
ﬁrst retrieve 100 documents from the index containing embeddings, which are potentially
stale, then these documents are re-embed and re-ranked using the up-to-date retriever. The
index is updated every 2,500 steps. We pre-train models for 10,000 iterations using AdamW
with a batch size of 128. While training longer continued to improve perplexity, we did not
observe further improvements on downstream tasks after ﬁnetuning by training longer.
5.1 MMLU Results
As mentioned in section 4.1, we consider four setting for MMLU: 1) a zero-shot setting where
we directly apply the pre-trained model with no few-shot ﬁnetuning 2) a 5-shot setting,
where we ﬁnetune a model using 5 training examples for each of the 57 domains 3) a 5-shot
multitask setting, where, rather than ﬁnetuning a model independently for each domain, we
train a single model to perform all tasks and 4) a setting with access to a number of auxiliary
data sets, with 95K total training examples. We train the models to generate the letter
corresponding to the correct answer option (‘A’, ‘B’, ‘C’ or ‘D’), and pick the answer with the
most likely of the 4 letters at test time. Full technical details can be found in appendix A.1.
5.1.1 Performance vs Parameters
We start by comparingAtlas to closed-book models of diﬀerent sizes for 5-shot, 5-shot
multitask and the full setting, and report results in Table 5. Across these settings,Atlas out-
performs the closed-book baselines by between 6.6 and 15.6 points, demonstrating consistent
utility of retrieval for few-shot language understanding across 57 domains. The closed-book
T5 struggles to perform signiﬁcantly better than random (25%) in few-shot settings with
770M parameters, whereas the equivalentAtlas achieves around 40%, signiﬁcantly better
than random, despite its small size. All models improve with more data, but interestingly,
the 770M models do not beneﬁt as much from few-shot multitask learning compared to
larger models (for closed-book, it actually loses 3 points) suggesting smaller models struggle
to grasp the synergies between the tasks in the few-shot setting. Larger models exploit
the multi-task setting well, withAtlas improving more than closed-book. For example,
Atlas-11B improves by 13 points (43.4→56.4), but equivalent closed-book only improves
18Atlas: Few-shot Learning with Retrieval Augmented Language Models
by 7 (36.1→43.5). Finally, on the transfer learning setting, all models improve, but the
relative gaps between closed-book atAtlas models remain similar.
5.1.2 De-biasing
When ﬁnetuning, we permute which answer option appears with which answer letter to
reduce over-ﬁtting and encourage a uniform prior over answer letters. However, the model
may still exhibit a bias towards some letters, especially in few-shot settings, so we also include
a second ‘de-biased’ inference mode in addition the standard inference used above. Here,
we run 4 forward passes, one for each cyclic permutation of the answer letter-answer option
assignment in the question, for example the answer option assigned to letter ‘A’ becomes
‘B’, what was ‘B’ becomes ‘C’ etc.3 We then sum the 4 probabilities to obtain the ﬁnal
prediction, which reduces spurious bias towards one of the answer letters (further details
in appendix A.1). The results are shown in Table 6. We ﬁnd that in zero-shot and 5-shot
settings, de-biasing is very eﬀective, improving results by 10.3 and 4.5 points respectively.
When more training data is available, the need for de-biasing decreases, leading to only 0.2
point improvement in the multi-task and full data settings.
5.1.3 Comparison to Published Works
Next, we compare our Atlas-11B results with de-biasing to recently reported results
with state-of-the-art large language models such as GPT-3 or Chinchilla, which required
signiﬁcantly more amount of computation to train. We report results in Table 7. We
ﬁnd thatAtlas is able to perform signiﬁcantly better than random in zero-shot, and in
conjunction with de-biased inference, achieves zero-shot scores that exceed 5-shot results
reported with GPT3 in the literature (47.1% vs 43.9%) (Hendrycks et al., 2021). For the
5-shot setting,Atlas outperforms GPT-3 by 4%, while using 15×less parameters, and 10×
less pre-training compute.4 When multitask-training on the combined 5-shot data,Atlas
improves to 56.6% close to the 5-shot performance of Gopher (60.0%). Finally, on the full
data setting, where we train on auxiliary data recommended by the MMLU authors,Atlas
reaches an overall accuracy of 65.6%, close to the state-of-the-art. Interestingly, in this setup,
Atlas signiﬁcantly outperforms GPT-3, while on the 5-shot setting, their performance is
similar.
5.2 Open-domain Question Answering Results
Next we evaluateAtlas on two open-domain question answering benchmarks: Natural
Questions and TriviaQA. We compare to prior work, both in a few-shot setting using
64 examples, and using the full training set, and report results in Table 8. On these
benchmarks, which require high-degree of memorisation, we clearly see the beneﬁts of retrieval-
augmentation. Atlas-11B obtains state-of-the-art results on 64-shot question answering,
for both Natural Questions and TriviaQA. In particular, it outperforms signiﬁcantly larger
3. Exploring all answer option permutations would involve 24 forward passes, which improves results by an
additional ∼1% over the 4 cyclic permutations, but requires much more compute, so we exclude it here,
see Appendix A.1
4. Atlas’s pre-training compute is dominated by the T5 pre-training. The computational requirements for
the retrieval-augmented pre-train is orders of magnitude lower
19Izacard, Lewis, Lomeli, Hosseini, Petroni, Schick, Dwivedi-Yu, Joulin, Riedel, Grave
Zero-shot 5-shot 5-shot (multi-task) Full / Transfer
Standard Inference 36.8 43.4 56.4 65.8
De-biased Inference 47.1 47.9 56.6 66.0
Table 6: Standard versus de-biased inference for MMLU. These results are reported for
Atlas-11B, using cyclic permutations for de-biasing, which increases inference costs
by a factor of 4×.
Setting Model Params FLOPS All Hum. Soc. Sci. STEM Other
zero-shot Atlas 11B 3.5e22 47.1 43.6 54.1 38.0 54.4
5-shot
GPT-3 175B 3.1e23 43.9 40.8 50.4 36.7 48.8
Gopher 280B 5.0e23 60.0 56.2 71.9 47.4 66.1
Chinchilla 70B 5.0e23 67.5 63.6 79.3 55.0 73.9
Atlas ∗ 11B 3.5e22 47.9 46.1 54.6 38.8 52.8
5-shot MT Atlas 11B 3.5e22 56.6 50.1 66.4 46.4 66.2
Transfer
UniﬁedQA 11B 3.3e22 48.9 45.6 56.6 40.2 54.6
GPT-3 175B 3.1e23 53.9 52.5 63.9 41.4 57.9
Atlas 11B 3.5e22 66.0 61.1 77.2 53.2 74.4
Table 7: Comparison to state-of-the-art on MMLU.∗For the 5-shot setting,Atlas uses
ﬁne-tuning, while previous works use in-context learning. TheAtlas model uses
de-biased inference. FLOPS refers to total the amount of computation necessary
to train the model, including pre-training and/or ﬁne-tuning. 5-shot MT refers to
training a single model on multiple tasks, using 5 examples per task.
models, such as PaLM, or models that required signiﬁcantly more training compute such as
Chinchilla. When using the full training set,Atlas also obtains state-of-the-art results, for
example improving the accuracy on Natural Questions from 55.9% to 60.4%. This result is
obtained using an index comprised of CCNet and the December 2021 Wikipedia corpora,
our default setting for the index. In section 6.2 we consider using indexes composed of
Wikipedia corpus archived at diﬀerent dates, and demonstrate an additional +3.6% on
Natural Questions when using an index which is temporally matched to Natural Questions.
We report performance as a function of model size as well as detailed hyperparameters in
Appendix A.2.
Atlas also compares favorably to recent work exploring retrieval-augmented few-shot
question answering with very large models. Lazaridou et al. (2022) explore Natural Questions
in a 15-shot setup using Gopher, augmenting questions with 50 passages retrieved using
Google Search. This method consists of generating 4 candidate answers from each retrieved
passages, and then re-ranking using either a score inspired by RAG (Lewis et al., 2020) or a
more expensive approach. This method (not shown in our tables) achieves exact match scores
of 32.7% (RAG) and 38.4% (Ensemble), requiring 50 (RAG) or 450 (Ensemble) forward
passes of Gopher-280B per test-time question.Atlas, using the same 15 training examples
20Atlas: Few-shot Learning with Retrieval Augmented Language Models
NQ TriviaQA ﬁltered TriviaQA unﬁltered
Model 64-shot Full 64-shot Full 64-shot Full
GPT-3 (Brown et al., 2020) 29.9 - - - 71.2 -
Gopher (Rae et al., 2021) 28.2 - 57.2 - 61.3 -
Chinchilla (Hoﬀmann et al., 2022) 35.5 - 64.6 - 72.3 -
PaLM (Chowdhery et al., 2022) 39.6 - - - 81.4 -
RETRO (Borgeaud et al., 2021) - 45.5 - - - -
FiD (Izacard and Grave, 2021a) - 51.4 - 67.6 - 80.1
FiD-KD (Izacard and Grave, 2021b) - 54.7 - 73.3 - -
R2-D2 (Fajcik et al., 2021) - 55.9 - 69.9 - -
Atlas 42.4 60.4 74.5 79.8 84.7 89.4
Table 8: Comparison to state-of-the-art on question answering. We report results on Natural
Questions, and on TriviaQA for both the ﬁltered set, commonly used for open-
domain question answering and the unﬁltered hidden set for which evaluation is
accessible online:https://competitions.codalab.org/competitions/17208. For the
64-shot setting, our model uses ﬁne-tuning, while the other models use prompting.
and 50 passages achieves 38.7 EM, despite having 25×fewer parameters, and requiring
comparatively negligible compute.
5.3 FEVER Results
We report results on the original 3-class FEVER fact checking test set in Table 9. We
consider a 64-shot setting, with training examples uniformly sampled from the full training
set. Unlike the development and test sets, the train set is imbalanced, with more positive
labels than negative, posing a challenge for few-shot learning. In this setting, we achieve an
accuracy of 64.3%. We also report a 15-shot setting, with 5 examples uniformly sampled
from each class to compare with published results from Gopher (Rae et al., 2021), where
Atlas scores 56.2%, outperforming Gopher by 5.1 points. Lastly we ﬁne-tune our model on
the full training set, and achieve a score of 78%, within 1.5% of the ProoFVer, which uses a
specialized architecture, a retriever trained with sentence-level annotations, and is supplied
with the Wikipedia corpus released with FEVER, whereasAtlas retrieves from CCNet and
the December 2021 Wikipedia dump. If we giveAtlas an index comprised of the FEVER
Wikipedia corpus, we set a new state-of-the-art of 80.1%
5.4 KILT Results
Finally we evaluateAtlas on KILT, a benchmark composed of several diﬀerent knowledge
intensive tasks, which was described in section 4.1. We report results on test sets in Table 10
for which evaluation is available online.5 The KILT versions of data sets are ﬁltered, and
thus results on Natural Questions, TriviaQA and FEVER reported elsewhere are not directly
comparable on KILT. We consider both a 64-shot setting and a full ﬁne-tuning setting, in both
cases we trainAtlas individually on each data set. More details on the hyperparameters and
5. https://eval.ai/web/challenges/challenge-page/689
21Izacard, Lewis, Lomeli, Hosseini, Petroni, Schick, Dwivedi-Yu, Joulin, Riedel, Grave
15-shot 65-shot Full data set
Gopher (Rae et al., 2021) 51.1 - -
ProoFVer (Krishna et al., 2022) - - 79.5
Atlas 56.2 64.3 78.0 /80.1†
Table 9: Comparison to state-of-the-art on FEVER. We report accuracy on FEVER test
set, for which evaluation is available here:https://competitions.codalab.org/
competitions/18814. For the few-shot settings, our model uses ﬁne-tuning while
other models use prompting.†uses an index composed of the FEVER Wikipedia
corpus.
Model AIDA FEV T-REx zsRE NQ HoPo TQA WoW
acc acc acc acc em em em f1
GENRE (Cao et al., 2021) 89.9 - - - - - - -
Sphere (Piktus et al., 2021) - 89.0 81.7 74.2 51.6 38.3 72.7 15.5
SEAL (Bevilacqua et al., 2022) - 89.5 83.6 74.6 53.7 40.5 70.9 18.3
Re2G (Glass et al., 2022) - 89.6 87.7 - 51.7 - 76.3 18.9
FID+RS (Hofstätter et al., 2022) - 92.2 85.2 83.7 61.2 39.1 84.6 20.6
Atlas, 64-shot 66.5 87.1 58.9 74.9 43.6 34.7 76.4 15.5
Atlas, full train set 90.6 93.5 85.1 80.8 61.3 50.6 84.0 21.6
Table 10: Downstream results on the KILT hidden test sets. Downstream metrics are
accuracy (AIDA CoNLL-YAGO, FEVER, T-REx, zero-shot RE), exact match
(Natural Questions, HotpotQA, TriviaQA), or F1 (Wizard of Wikipedia).
development set results are reported in Appendix A.3. For 64-shot, we greatly exceed random
performance, and are even competitive with some fully-ﬁnetuned models on the leaderboard,
such as for FEVER, where our 64-shotAtlas is only 2-2.5 points behind Sphere, SEAL
and Re2G, and outperforms Sphere and SEAL on zero-shot RE. In the full data set setting,
Atlas is within 3% to the state-of-the-art for 3 data sets, and sets the state-of-the-art in
the remaining ﬁve data sets.
6. Analysis
In this section we discuss speciﬁc aspects ofAtlas as a retrieval-augmented language model.
First, we analyse retrieved documents to interpretAtlas generations. Second, we probe the
updateability and temporal sensitivity ofAtlas when the content of the index is modiﬁed.
6.1 Interpretability and Leakage
An advantage of semi-parametric models likeAtlas is the ability to inspect retrieved items
to aid interpretability. To better understand how wellAtlas retrieves, and how it uses
retrieved passages, we examine the retrieved passages for multi-task few-shot MMLU. As
22Atlas: Few-shot Learning with Retrieval Augmented Language Models
Hum. Soc Sci. STEM Other All0
20
40
60
80
100% of Retrieved Passages
CCNet
Wiki Text
Wiki Infobox
1 5 10 15 20 25
Top K Retrieved Documents
15
20
25
30% of Retrieved Passages
[0, 1) [1, 2) [2, 4) [4, 8) [8, 16) [16, )
Answer frequency in retrieved docs (interval)
0
20
40
60
80MMLU Accuracy (%)
Figure 3: MMLU Retrieval Analysis. Left: Fraction of sources of top 30 retrieved passages
for MMLU from CCNet, Wikipedia passages and info boxes for the 5-shot multitask
Atlas. Center: How often the text of the correct MMLU answer option appears
in retrieved passages, as a function of the number of retrieved passages. Right:
MMLU accuracy as a function of answer occurrence frequency in retrieved passages
set
shown in the left panel of Figure 3, the model retrieves the majority of its passages from
CCNet (85% on average). Wikipedia makes up about 15% of retrieved passages, which is
higher than we would expect under a uniform prior, given Wikipedia only makes up about
10% of the index. The fraction of Wikipedia retrieval varies between MMLU domains, with
the model using Wikipedia to a greater extent for STEM domains, and least for social
sciences. The domain making the greatest use of Wikipedia is “abstract algebra” (73%), and
the least is “moral scenarios” (3%). We also note that the MMLU-ﬁnetunedAtlas does not
make signiﬁcant use of Wikipedia infobox passages.
We can also analyse the content of passages to assess how they may be useful for
accomplishing the downstream task. The middle panel of Figure 3 shows how often retrieved
documents contain the text of the correct answer option. There being at least one mention
of the correct answer choice in 30% of test questions in the top 25 passages.6 The right panel
shows that the accuracy on MMLU increases when the correct answer option text occurs
more frequently in retrieved passages, rising from 55% for questions when the answer option
does not appear, to 77% for questions mentioned more than 15 times.
A human analysis of retrieved documents revealed that documents are helpful for an-
swering questions in a number of diﬀerent ways. Manual inspection of a sample of 50
correctly-answered questions revealed that 44% contained at least partially useful background
information. These are documents that would improve the likelihood of a non-expert human
answering correctly, such as contextual clues surrounding a quotation from a question, or
helpful numerical ﬁgures for quantity-based questions, which help to narrow down the answer
options to a smaller range. In a further 26% of cases, a passage contained all the necessary
information to answer the question, stated in a straightforward way. If read competently,
such passages make the question simple to answer, and often include information such as
6. Note: Depending on the question, it may not be important or useful to retrieve the exact text of the
answer in MMLU, and as such, a hits@k value of 30% does not imply that retrieval fails to surface useful
information in 70% of cases
23Izacard, Lewis, Lomeli, Hosseini, Petroni, Schick, Dwivedi-Yu, Joulin, Riedel, Grave
canonical deﬁnitions, or the exact numerical answer requested in the question. 28% of
retrieval sets did not contain obvious information which would make the question easier.
Finally, 2% contained the verbatim question in a passage, together with its answer.
Given that MMLU has been created from pre-existing exams, it is possible that these
questions appear on the open web. Models trained on web data (or, in our case, retrieving
from it) run the risk of answering correctly not through generalisation, but by verbatim
memorisation, which could lead to misleadingly high scores. In some very large language
models, which can verbatim memorize and recall large parts of their pre-training data (Carlini
et al., 2021), eﬀorts have sometimes been made to ﬁlter occurrences of downstream instances
from pre-training data, but this has not been performed for MMLU in the literature. In
order to assess the prevalence of MMLU leakage in our index, we manually checked retrieval
results for questions where the longest n-gram overlap between the question (without answer
options) and a passage was at least 75% the length of the question. This resulted in an
estimate of leakage of 2.8% of questions from our CCNet corpus.
A beneﬁt of retrieval-augmented models such asAtlas is the editability of its knowledge
(see Section 6.2 for additional analysis). To estimate pure, non-leaked performance, we
can ﬁlter out any potentially-leaked passages from retrieved results and rerun the language
model. The MMLU score drops slightly when controlling for this leakage from 56.4 to 55.8%
(-.5%).We note that our CCNet corpus is relatively small compared to the pre-trained corpora
of recent very large models, which are trained on up to 1.4 trillion tokens (Hoﬀmann et al.,
2022), 35x the size of our index, making it likely that models trained on corpora of that size
would observe more MMLU leaked examples, but detecting such leakage is challenging in
non-retrieval-augmented models.
6.2 Temporal Sensitivity and Updateability
A beneﬁt of retrieval-augmented models is that they can be kept up-to-date without retraining,
by updating or swapping their index at test time. To assess the eﬀectiveness of this
mechanism inAtlas, we ﬁrst construct a data set of time-sensitive questions derived from
TempLAMA (Dhingra et al., 2022). TempLAMA is a collection of templated cloze questions
derived from Wikidata and Wikipedia where the correct answer changes over time. We select
a subset of questions from this data set which have a diﬀerent answer in 2017 and 2020, for
example, Question: Theo Walcott plays for ___Answer: Arsenal F.C. (2017), Everton
F.C. (2020), and form a small training set of 248 training, 112 development and 806 test
questions.
Using this data set, we ﬁnetune closed-book T5-XXL andAtlas using the questions
and the 2017 answers, supplyingAtlas with a 2017 Wikipedia index, and then measure
exact match accuracy on the 2017 test set. The results can be found in the ﬁrst row and ﬁrst
two columns of Table 11. We ﬁrst observe that, as expected,Atlas greatly outperforms T5
(57.7% c.f. 12.1%). We also note that, as desired, both T5 andAtlas almost never generate
an answer from 2020 when trained with the 2017 answers, scoring 2.8% and 1.5% respectively
(ﬁrst row, second two columns of Table 11). However, as shown in row 2, we can swap the
Atlas index to a 2020 Wikipedia index,without retraining, and ﬁnd thatAtlas updates its
predictions accordingly, with 2020 accuracy rising to a similar level to its 2017 performance
(53.1%), whereas the purely parametric T5 has no such updateability mechanism.
24Atlas: Few-shot Learning with Retrieval Augmented Language Models
2017 Test Set Acc. 2020 Test Set Acc.
Train Set Test-time Index Closed-book Atlas Closed-book Atlas
2017 answers 2017 12.1 57.7 2.9 1.5
2020 12.1 10.2 2.9 53.1
2020 answers 2017 4.8 50.1 3.6 4.2
2020 4.8 3.5 3.6 60.5
Table 11: Results on our TempLAMA-derived data set.
We report performance for a static, closed-book T5-11B, as well asAtlas-11B supplied with
a test-time Wikipedia index from 2017 or 2020. We evaluate models ﬁnetuned on a small
training set of 248 time-sensitive cloze-question-answer pairs, using answers either from 2017
or 2020. Good models should score highly when the test set year matches the year of the
test-time index, and score low otherwise.
Dec. 2017 Dec. 2018 Aug. 2019 Dec. 2020 Dec. 2021
64-shot 44.7 45.1 44.1 44.0 41.3
Full 63.2 64.0 62.4 61.1 59.6
Table 12: Impact of index data temporality on Natural Questions. We report exact match
performance on Natural Questions using diﬀerent Wikipedia dumps in the index.
We observe that the dump from December 2018, commonly used for Natural
Questions, leads to the best result.
This demonstrates thatAtlas can be faithful and condition strongly on its supplied index.
Furthermore, this zero-shot updateability mechanism has the useful property of staying
up-to-date without requiring up-to-date annotated data, or continuous, lifelong pre-training,
as would be may required for a large parametric-only model. Rows 3 and 4 of Table 11
complete the picture, where this time we train with 2020 answers, and demonstrateAtlas
can zero-shot transfer backwards in time to 2017 eﬀectively too (50.1%). Interestingly, T5 is
unable to answer questions from 2020 well, even when trained with 2020 answers (3.6%),
likely because it was pre-trained on data pre-dating 2020 (Dodge et al., 2021).
We also examine temporal eﬀects for Natural Questions. Natural Questions is a data
set composed of search queries collected via the Google search engine in a short period
of time. Thus data have a strong temporal bias, with a lot of questions about the 2018
World Cup for example. Moreover some questions are ambiguous without speciﬁcation of
the temporal context. For instance, for the question“when did ireland last beat england
at twickenham”, the expected answer is 2018 in Natural Questions, while Ireland also beat
England at Twickenham in 2022 as well as many other times before. In Table 12, we report
results obtained by ﬁnetuningAtlas using diﬀerent Wikipedia dumps for the index. We
observe that the 2018 December Wikipedia dump, which is close to the date of data collection,
25Izacard, Lewis, Lomeli, Hosseini, Petroni, Schick, Dwivedi-Yu, Joulin, Riedel, Grave
101
102
Memory (in GB)
0.2
0.4
0.6
0.8NQ Recall@50
101
102
Memory (in GB)
10
20
30
40NQ Exact Match
100
101
Memory (in GB)
0.2
0.4
0.6
0.8NQ Recall@50
100
101
Memory (in GB)
10
20
30
40NQ Exact Match
Figure 4: Index Compression: Atlas-3B 64-shot NQ performance (left column: Retrieval
Recall@50, right column: QA Exact Match score), as a function of index size,
for diﬀerent levels of quantisation. The right-most point in each plot represents
the uncompressed index.Top Row:Wikipedia + CCNet Index.Bottom Row:
Wikipedia Index.
leads to the best results for both few-shot and full ﬁne-tuning. In particular, it leads to a
new state-of-the-art of 64.0 EM on Natural Questions.
6.2.1 Index Compression
Maintaining dense retrieval indices can be memory-intensive, especially as the number of
indexed items is scaled. In this section, we brieﬂy analyse the memory requirements of
Atlas’s index in the case of a) a Wikipedia index and b) the combined CCNet and Wikipedia
index used in most of the experiments above.
There are two sources of memory pressure forAtlas’s retrieval component—the passages
themselves, and the document embedding index. The tokenized passages, once binarized,
require 11GB and 130GB of storage for the Wikipedia and combined indices respectively.
These passages do not need to be stored in expensive GPU RAM, and could even be memory-
mapped to disk, sharded across nodes or compressed if required, and thus do not represent
a limiting hardware challenge in this context. The embedding index itself, however, must
be stored in GPU RAM for fast search, and thus its size is more sensitive. In the above
experiments, we perform exact search over our index, which is achieved by sharding the
index over all the the available GPUs, and computing the search in parallel. The index is
26Atlas: Few-shot Learning with Retrieval Augmented Language Models
stored at fp16 precision, resulting in a total GPU memory requirement of 49 GB and 587
GB for the Wikipedia and combined indices, respectively.
This large GPU memory requirement for the index limits accessibility and ease of
deployment. However, many index compression techniques are available for nearest neighbour
search, which can often dramatically reduce memory requirements at the cost of some retrieval
accuracy. Following Izacard et al. (2020), we explore the eﬀect of Product Quantization (PQ,
Jegou et al., 2010), a popular lossy compression technique onAtlas-3B’s accuracy for the
64-shot NQ task at diﬀerent compression levels.
The results are shown in Figure 4. We ﬁnd that substantial compression is possible
before the onset of signiﬁcant performance degradation. Namely, the Wikipedia index can be
compressed from 49GB to 4GB with negligible drop in retrieval precision and exact match.
Likewise, the combined index can be compressed from 587GB to 50GB without serious
degradation, indicating that the combined index could be loaded onto a single 80GB GPU.
7. Discussion
In this paper, we introduceAtlas, a large retrieval-augmented language model. By jointly
pre-training the retriever module and the language model, we show thatAtlas has strong
few-shot learning capabilities on a wide range of knowledge intensive tasks, including Natural
Questions, TriviaQA, FEVER, 8 KILT tasks and 57 MMLU tasks. For example,Atlas-11B
reaches more than 42% accuracy on Natural Questions and 84.7% on TriviaQA when training
on 64 examples, which is an improvement of almost 3 points compared to PaLM, a 540B
parameter model, which required 50x more pre-training compute. We also provided detailed
ablations and analyses for what factors are important when training such retrieval-augmented
models, and demonstrated Atlas’s updateability, interpretability and controlability ca-
pabilities. Lastly, we demonstrated thatAtlas is also powerful in full data set settings
obtaining a new state-of-the-art results on Natural Questions, TriviaQA, FEVER, and 5
KILT tasks. The few-shot results presented in this paper were obtained by ﬁne-tuning
Atlas on few examples, rather than using in-context learning. In context learning presents
signiﬁcant practical advantages over ﬁne-tuning, as it does not change the model weights.
The development of retrieval-augmented language models preserving the ability of their
non-augmented counterparts to generalize from few in-context examples and instructions
is a crucial challenge toward general retrieval-augmented language models and their wider
adoption.
Appendix A. Training details and Additional Results
In this appendix we present additional results and provide details about the parameters used
to ﬁne-tune models on MMLU, question answering data sets and KILT tasks.
A.1 MMLU
Here, we report results on the 57 MMLU domains, details about the ﬁne-tuning and how the
model predictions are de-biased.
27Izacard, Lewis, Lomeli, Hosseini, Petroni, Schick, Dwivedi-Yu, Joulin, Riedel, Grave
A.1.1 Featurization
MMLU consists of multiple choice questions with four possible lexicalized answer options.
We represent the input using the following template:
question: {question text}
options: (A) {answer 1} (B) {answer 2} (C) {answer 3} (D) {answer 4}
answer: [MASK_0]
and train the model to generate the mask token followed by the letter of the correct answer:
[MASK_0] {correct answer option letter}
This format closely matches the format of MLM pre-training objective, aiding few-shot
learning. When training, we permute the order of the answer options, that is shuﬄing
which answer option appears as letter A etc. This helps reduce overﬁtting, and encourages a
uniform prior on the letters.
A.1.2 Standard Inference
Once trained we obtain predictions from the model by selecting the pre-softmax logits for the
tokens A, B, C and D, and performing a softmax over them to obtain a distribution over the
4 answer options. For standard inference, we then simply return the answer corresponding
to the argmax of this distribution.
A.1.3 De-biased Inference
As mentioned in the main text, even though our model is ﬁnetuned with data that encourages
a uniform prior over answer letters (by permuting which answer option letter is used with
which lexical answer option text in training data), this may not be enough to ensure the
model has no residual bias towards speciﬁc letters. Consider answersa, questionsq and a
nuisance variablez∈Z, which represents the ordering of the answer options or, equivalently,
which answer letter gets assigned to which answer option text. There are 4 answer options
in MMLU, and thus|Z|= 24 unique ways they can be ordered, or assigned to given letters.
Running our model with our standard inference for a questionq, corresponds to calculating
p(a|q = q,z = z) for the answer orderingz that happens to appear in the data set. We
can control forz by running the model with all possible answer orderings in the input,
and marginalizing: p(a|q = q) = ∑
z′∈Zp(a|q = q,z = z′)p(z = z′|q = q), and assuming
p(z= z′|q= q) is uniform (no answer ordering is more likely than another), this reduces to
simply p(a|q= q) ∝∑
z′∈Zp(a|q= q,z = z′). This procedure requires 24 forward passes, one
for each answer ordering, so is 24×slower than standard inference. Table 13 shows the result
of applying the full permutation de-biasing, which leads to an 12% improvement zero-shot
and 6% in 5-shot performance overall. Empirically, using only the cyclic permutations of the
answer order provided in the original data set (of which there are 4) works nearly as well,
which is what we report in the main paper, and only increases inference compute by a factor
of 4, rather than 24. Cyclic permutation de-biasing improves over standard inference by 10%
in zero-shot and 5% in 5-shot. Empirically, de-biased inference is largely unnecessary when
training in the 5-shot multitask or full data set setting, as there is enough data for the model
to learn a more uniform prior over the letters.
28Atlas: Few-shot Learning with Retrieval Augmented Language Models
Setting Model All Hum. Soc. Sci. STEM Other
zero-shot
Standard 36.8 37.5 39.0 30.2 39.7
All permutations 48.5 45.7 55.2 39.4 54.4
Cyclic Permutations 47.1 43.6 54.1 38.0 54.9
5-shot
Standard 43.4 41.8 49.3 33.9 48.8
All permutations 49.0 46.0 56.1 40.5 54.6
Cyclic Permutations 47.9 46.1 54.6 38.8 52.8
Table 13: MMLU scores with de-biasing.
A.1.4 Evaluation
We evaluate by following the method of Hendrycks et al. (2021), namely, micro-averaging
across all 57 domains to obtain overall accuracy. We quote the results of GPT3 (Brown
et al., 2020) and UniﬁedQA (Khashabi et al., 2020) from the MMLU leaderboard athttps:
//github.com/hendrycks/test. For Chinchilla and Gopher, we calculate the scores on the
categories using the full MMLU results from Hoﬀmann et al. (2022).
A.1.5 Index
The index used for MMLU for all MMLU experiments in the main paper comprised of
concatenation of the Wikipedia passages, Wikipedia info boxes and Common Crawl indices,
for a total of 387M passages. We can assess the importance of the index by running a model
without the common crawl data, leading to a 5-shot multitask result of 52.8%, compared to
56.4% for the full model, a drop of 3.6%. This indicates that whilst the Wikipedia data is
suﬃcient do well on the task, the addition of the CCNet data improves results further.
A.1.6 Hyperparameters and Development Data
Selecting hyperparameters is challenging in few-shot settings. We do not assume access to an
in-domain development set for the 5-shot task. Instead, we determine a set of hyperparameters
for the 5-shot task using data from RACE, one of the auxiliary data sets provided by MMLU.
Here, we sample 5 sets of 5-shot training data, and for each model size, we explore batch
size {32,64}, learning rates for the language model and retriever {(5e-5, 1e-5), (4e-5, 4e-
5)}, retriever temperature{0.1,0.01}and a ﬁxed number of training steps{16,32,64,128},
picking the setting that achieves strongest RACE validation scores. Having determined these
hyperparameters, we apply them directly to the 5-shot MMLU task. For the 5-shot multi-task
and full/transfer settings, we use the same batch size, temperatures and learning rates as
the 5-shot task, but use a set of 285 MMLU validation examples (5 per domain) in order to
determine the total number of training steps and for early stopping. The hyperparameters
selected in the MMLU experiments can be found in Table 14. We use query-side ﬁnetuning
for the 5-shot and 5-shot multitask settings, and top-128 reranking for the full setting. For
all MMLU runs we retrieve 30 documents.
29Izacard, Lewis, Lomeli, Hosseini, Petroni, Schick, Dwivedi-Yu, Joulin, Riedel, Grave
770M 3B 11B
Batch size 64 64 64
Learning rate (5e-5, 1e-5) (5e-5, 1e-5) (5e-5, 1e-5)
Retriever Temperature 0.1 0.1 0.1
5-shot train steps 64 32 16
5-shot (multitask) max train steps 2000 500 250
Full / transfer max train steps 5000 2000 2000
Table 14: Hyperparameters for MMLU
Run # All Hum. Soc. Sci. STEM Other
1 45.2 40.6 54.1 37.1 51.1
2 45.1 39.8 54.4 37.1 52.0
3 45.0 40.0 54.1 37.7 51.1
4 45.6 41.3 54.7 37.0 51.6
5 44.3 40.6 50.7 38.1 49.8
Ave: 45.0 ±0.5 40 .5 ±0.6 53 .6 ±1.6 37 .4 ±0.5 51 .1 ±0.8
Table 15: Interrun Variance for 5-shot MMLU usingAtlas-11B
A.1.7 Inter-run Variance
few-shot learning is well-known to suﬀer from high variance. In the main paper, we quote
the result obtained with our ﬁrst run. In order to assess the eﬀect of noise and variance, we
ran the 5-shot experiment withAtlas 5 times.7 We observe high variance for individual
domains, sometimes as high as 20%, however, once aggregated across all 57 domains, the
inter-run variance is low. The overall scores for these diﬀerent runs, when using the same
hyperparameters are shown in Table 15. Due the eﬀects of averaging over the many domains
that comprise MMLU, the inter-run variance is quite modest on the aggregated metrics, with
a std deviation of 0.5 in this experiment.
A.1.8 Closed-Book Baselines
The closed book baselines we compareAtlas to in Table 5 are initialized from the same
T5 model as their respectiveAtlas, and then pre-trained with MLM for the same number
of steps (10K) using the same pre-training data asAtlas, for fairer comparison. The
same procedure as forAtlas was used to determine hyperparameters for MMLU for the
closed-book models.
7. This experiment was performed with a slightly diﬀerent index to the main experiments, which achieves a
stronger result
30Atlas: Few-shot Learning with Retrieval Augmented Language Models
A.1.9 Full Results
Tables 16 and 17 shows the full MMLU scores for each domain forAtlas and the closed
book T5 respectively. The full results for the cyclic-permutation-de-biasedAtlas-XXL can
be found in Table 18.
A.2 Question Answering
We report additional training details on question answering tasks, as well as results obtained
with models of diﬀerent sizes.
A.2.1 Training Details
For question answering, similarly to the MMLU experiments, we format the input using the
following template:
question: {question text} answer: [MASK_0]
and train the model to generate the mask token followed by the answer:
[MASK_0] {answer}.
We generate answers using greedy decoding. For both training and testing, we retrieve 40
passages, and truncate the result of the concatenation between the query and the passages
to 384 tokens.
For few-shot ﬁne-tuning we trainAtlas for 30 steps using 64 random samples from the
train sets. The retriever is trained using query-side ﬁne-tuning. We select the model after 30
training steps. We use AdamW with a batch size of 32 and a learning rate of4 ×10−5 with
linear decay and 5 iterations of warmup for both the language model and the retriever.
For the ﬁne-tuning on the full data sets, we train the model for 5k gradient steps and
refresh the index every 500 steps for the ﬁrst 1,000 training steps and every 2k training steps
afterwards. We use AdamW with a batch size of 64 and a learning rate of4 ×10−5 with
linear decay and 5 iterations of warmup for both the language model and the retriever. We
evaluate models every 500 steps and select the best one on the validation set based on the
exact match score.
A.2.2 Impact of Scaling
In Table 19, we report performance on Natural Questions and TriviaQA as a function of the
number of parameters in the reader module. Both for few-shot learning and full ﬁne-tuning
we observe strong improvements by scaling the size of the reader module. However we
can notice sign of saturation when ﬁnetuning on full data sets, with limited gains when
scaling from 3B to 11B parameters (+0.6% on Natural Questions, +0.5% on TriviaQA).
While performance improves substantially when scaling from 3B to 11B parameters with 64
training samples, with +3.7% and +1.2% improvement on Natural Questions and TriviaQA
respectively. For these experiments we use a setup similar to the one use in Table 8, except
that we use an index composed of the December 2018 Wikipedia dump processed as described
in section 4.2.
31Izacard, Lewis, Lomeli, Hosseini, Petroni, Schick, Dwivedi-Yu, Joulin, Riedel, Grave
5-shot 5-shot (multi-task) Full / Transfer
770M 3B 11B 770M 3B 11B 770M 3B 11B
All 38.9 42.3 43.4 42.1 48.7 56.4 56.3 59.9 65.8
Humanities 37.3 40.0 41.9 37.7 46.4 50.0 50.9 53.0 60.3
Social Sciences 41.7 46.8 49.3 47.5 53.7 65.6 66.0 70.8 77.2
STEM 32.3 35.0 33.9 34.4 39.4 46.2 44.8 50.7 53.4
Other 44.9 48.1 48.8 50.4 55.9 66.6 65.5 68.1 74.4
abstract algebra 30.0 27.0 28.0 27.0 31.0 30.0 22.0 27.0 33.0
anatomy 28.9 50.4 45.2 44.4 57.8 64.4 57.8 68.9 69.6
astronomy 55.3 59.9 59.2 52.6 66.4 67.8 69.1 78.3 79.6
business ethics 49.0 51.0 48.0 50.0 62.0 60.0 51.0 70.0 68.0
clinical knowledge 41.9 44.9 40.0 46.8 54.3 64.9 64.2 72.5 74.0
college biology 38.2 45.8 50.0 36.8 52.1 63.2 63.2 72.2 78.5
college chemistry 32.0 29.0 29.0 31.0 33.0 38.0 45.0 39.0 45.0
college computer science 33.0 35.0 30.0 23.0 29.0 30.0 43.0 48.0 47.0
college mathematics 31.0 31.0 28.0 29.0 27.0 34.0 32.0 29.0 36.0
college medicine 31.2 35.8 38.2 50.3 40.5 52.0 60.1 59.5 63.6
college physics 20.6 26.5 31.4 21.6 28.4 39.2 27.5 44.1 42.2
computer security 53.0 50.0 55.0 49.0 61.0 64.0 69.0 71.0 76.0
conceptual physics 34.9 41.7 37.4 40.9 43.4 57.0 53.2 58.3 59.6
econometrics 28.9 21.1 27.2 26.3 25.4 34.2 28.9 37.7 36.8
electrical engineering 26.9 31.7 31.7 38.6 44.1 51.7 61.4 60.7 67.6
elementary mathematics 25.9 28.8 29.4 29.6 30.2 32.8 29.6 35.5 33.9
formal logic 34.9 33.3 33.3 23.0 30.2 29.4 34.1 38.9 34.1
global facts 28.0 34.0 34.0 36.0 40.0 49.0 50.0 49.0 52.0
high school biology 24.8 37.7 27.7 48.7 57.1 66.5 66.5 76.8 81.9
high school chemistry 34.5 31.0 31.0 31.5 36.5 48.3 44.8 52.2 52.2
high school computer science 31.0 39.0 28.0 37.0 42.0 42.0 50.0 59.0 57.0
high school european history 42.4 49.7 53.3 50.9 58.2 69.7 70.9 73.9 80.0
high school geography 38.9 42.4 50.0 46.5 56.6 69.2 74.2 80.8 82.8
high school gov. and pol. 57.5 60.6 60.1 52.9 64.8 76.7 80.8 85.5 91.7
high school macroeconomics 32.8 39.7 44.9 39.0 45.6 57.2 55.1 63.1 66.7
high school mathematics 30.7 33.0 35.6 28.1 27.8 37.0 30.7 34.8 37.0
high school microeconomics 34.5 42.9 45.4 44.1 51.7 68.9 63.4 70.2 81.1
high school physics 18.5 24.5 22.5 25.8 25.8 33.1 27.2 30.5 39.7
high school psychology 52.8 61.1 59.8 56.7 67.2 79.4 76.3 84.0 87.0
high school statistics 39.8 29.6 34.7 27.3 34.7 38.0 37.0 43.1 45.8
high school us history 43.6 49.0 55.9 46.1 57.8 59.8 62.7 72.5 76.5
high school world history 48.1 52.7 59.9 48.1 66.2 65.4 70.0 78.5 79.7
human aging 46.2 44.8 39.5 48.0 55.2 60.1 56.1 68.2 73.1
human sexuality 41.2 43.5 27.5 46.6 51.1 59.5 77.1 72.5 81.7
international law 54.5 57.9 60.3 55.4 72.7 73.6 81.8 82.6 85.1
jurisprudence 38.9 55.6 32.4 53.7 60.2 73.1 76.9 73.1 81.5
logical fallacies 43.6 54.0 57.1 44.2 58.3 70.6 64.4 73.0 76.7
machine learning 36.6 34.8 28.6 31.3 37.5 46.4 36.6 47.3 50.9
management 45.6 51.5 52.4 48.5 52.4 81.6 78.6 75.7 87.4
marketing 59.4 67.1 70.5 66.7 74.4 83.8 83.8 83.3 91.9
medical genetics 50.0 53.0 58.0 56.0 61.0 75.0 68.0 78.0 81.0
miscellaneous 63.0 64.2 68.8 64.0 72.4 84.3 85.4 83.9 90.9
moral disputes 37.0 41.3 41.3 40.8 50.3 60.1 61.9 66.2 73.7
moral scenarios 24.7 24.7 26.5 21.9 26.9 26.6 23.8 23.8 35.8
nutrition 40.9 45.1 45.1 49.0 52.3 67.0 64.7 68.6 76.8
philosophy 48.6 50.5 56.3 49.8 59.2 69.5 70.4 73.0 77.8
prehistory 45.7 50.0 52.8 54.9 64.8 74.4 69.8 75.0 80.6
professional accounting 28.4 33.0 34.0 35.1 34.0 45.7 43.6 46.1 51.8
professional law 32.4 33.5 34.8 30.4 37.6 39.1 41.5 41.5 50.5
professional medicine 29.4 26.1 27.6 34.6 40.8 52.2 47.8 43.4 59.6
professional psychology 37.7 43.0 50.2 45.1 51.0 60.6 59.5 62.4 74.0
public relations 40.0 46.4 44.5 51.8 54.5 66.4 63.6 66.4 68.2
security studies 35.1 33.5 38.8 44.1 39.6 57.6 60.8 61.6 72.2
sociology 45.3 51.2 51.2 52.7 60.2 69.2 74.1 78.6 85.1
us foreign policy 58.0 70.0 73.0 63.0 63.0 74.0 80.0 80.0 83.0
virology 34.3 34.3 32.5 38.0 42.8 45.2 47.6 49.4 53.0
world religions 65.5 69.0 71.9 70.2 82.5 80.1 83.6 83.6 87.1
Table 16: MMLU Test set scores forAtlas for each model size and each of the 57 domains.
32Atlas: Few-shot Learning with Retrieval Augmented Language Models
5-shot 5-shot (multi-task) Full / Transfer
770M 3B 11B 770M 3B 11B 770M 3B 11BAll 29.2 35.7 36.1 26.5 40.0 43.5 42.4 50.4 54.0
Humanities 30.5 35.4 35.5 27.3 38.5 41.6 41.0 48.6 51.3Social Sciences 29.7 38.0 39.4 24.8 43.8 48.9 48.6 57.8 64.7STEM 29.0 31.4 30.8 26.5 32.8 35.8 33.4 40.6 41.7Other 26.7 37.7 38.6 27.0 45.0 48.5 46.8 55.2 59.1
abstract algebra 26.0 23.0 21.0 29.0 30.0 26.0 23.0 29.0 26.0anatomy 21.5 40.0 40.7 27.4 39.3 45.9 35.6 43.7 42.2astronomy 37.5 38.8 37.5 27.6 39.5 41.4 36.2 50.7 55.3business ethics 29.0 54.0 42.0 26.0 47.0 55.0 53.0 64.0 60.0clinical knowledge 32.5 33.6 40.0 28.7 44.2 47.9 45.3 52.8 57.7college biology 29.9 34.7 34.0 29.9 34.7 40.3 38.2 46.5 52.1college chemistry 37.0 22.0 32.0 20.0 35.0 33.0 36.0 34.0 36.0college computer science 28.0 35.0 34.0 28.0 27.0 36.0 31.0 44.0 35.0college mathematics 31.0 29.0 27.0 22.0 34.0 27.0 30.0 33.0 32.0college medicine 24.3 34.7 34.1 27.2 40.5 40.5 35.8 41.6 48.6college physics 33.3 23.5 23.5 22.5 19.6 26.5 22.5 32.4 24.5computer security 36.0 42.0 46.0 31.0 49.0 52.0 50.0 65.0 61.0conceptual physics 26.4 35.7 30.2 23.4 30.6 32.8 34.5 37.4 43.8econometrics 26.3 21.9 28.9 17.5 19.3 24.6 29.8 25.4 29.8electrical engineering 31.0 33.1 31.7 31.0 31.0 36.6 41.4 47.6 51.7elementary mathematics 26.2 27.5 28.0 27.0 31.2 33.3 25.9 31.2 35.5formal logic 34.1 34.1 31.7 15.1 34.9 31.0 31.7 38.1 42.1global facts 32.0 30.0 25.0 34.0 34.0 27.0 28.0 34.0 30.0high school biology 22.6 31.9 29.7 27.1 41.6 50.0 43.5 57.7 60.6high school chemistry 27.1 26.6 27.6 28.6 31.5 29.1 30.5 36.5 38.9high school computer science 26.0 32.0 25.0 33.0 37.0 45.0 45.0 55.0 48.0high school european history 34.5 43.0 42.4 24.2 60.0 59.4 58.2 69.1 76.4high school geography 31.3 40.4 36.9 24.7 45.5 50.5 56.1 66.7 74.2high school gov. and pol. 28.0 49.2 51.3 19.2 56.0 59.6 55.4 70.5 75.6high school macroeconomics 25.6 37.7 32.1 26.7 42.3 43.6 41.0 51.5 56.4high school mathematics 35.9 35.2 35.9 28.1 26.7 31.1 27.8 36.7 31.9high school microeconomics 27.3 29.8 36.1 20.6 35.7 42.9 42.9 50.8 60.5high school physics 21.9 25.2 22.5 24.5 28.5 29.1 27.8 31.1 27.8high school psychology 26.1 46.4 51.0 24.8 54.3 60.2 56.3 67.3 76.1high school statistics 27.8 33.3 33.3 17.6 30.6 33.8 32.9 33.3 37.0high school us history 30.4 39.7 45.6 27.5 46.1 58.3 51.0 63.2 72.5high school world history 42.6 50.6 41.8 29.1 54.0 64.6 66.7 72.2 73.8human aging 28.3 37.2 29.6 26.0 45.3 46.2 46.6 57.0 62.8human sexuality 29.8 34.4 41.2 25.2 42.0 44.3 51.1 58.0 59.5international law 57.9 57.9 41.3 44.6 57.9 58.7 62.8 71.9 71.1jurisprudence 30.6 33.3 34.3 32.4 49.1 52.8 55.6 67.6 74.1logical fallacies 40.5 55.8 46.6 25.8 51.5 62.0 43.6 69.3 71.2machine learning 33.0 34.8 36.6 29.5 35.7 37.5 32.1 37.5 42.9management 21.4 29.1 40.8 24.3 47.6 50.5 60.2 69.9 70.9marketing 38.9 58.5 60.7 31.2 67.9 75.6 69.2 79.9 85.9medical genetics 26.0 36.0 36.0 29.0 43.0 44.0 40.0 54.0 50.0miscellaneous 24.5 45.2 46.4 27.1 52.2 58.2 51.3 64.6 72.7moral disputes 32.4 37.3 38.7 28.6 43.4 43.4 49.7 64.7 64.7moral scenarios 24.7 24.7 24.7 23.0 23.9 24.7 23.8 24.0 23.8nutrition 30.1 33.0 34.6 25.8 42.5 44.1 50.3 55.6 61.1philosophy 28.6 32.5 37.3 31.2 38.9 45.0 44.1 56.6 59.2prehistory 33.6 37.0 41.4 27.5 39.8 50.6 41.0 51.5 57.7professional accounting 21.3 28.0 30.5 25.9 35.5 34.0 37.2 41.5 42.2professional law 28.2 33.4 34.0 27.6 35.4 35.5 38.3 43.0 45.6professional medicine 19.5 26.5 24.3 20.2 32.0 37.9 38.6 40.8 46.0professional psychology 27.8 32.8 32.8 26.6 39.5 43.6 38.4 48.0 58.3public relations 22.7 43.6 40.0 21.8 47.3 56.4 50.0 55.5 60.0security studies 37.6 26.1 31.0 20.4 34.7 44.1 56.3 61.6 66.9sociology 43.3 41.8 38.8 30.8 45.8 52.7 60.2 66.7 72.1us foreign policy 49.0 57.0 66.0 38.0 56.0 61.0 59.0 75.0 76.0virology 29.5 26.5 34.3 30.1 36.1 39.8 44.0 46.4 41.6world religions 24.0 40.9 47.4 32.7 49.1 57.3 48.0 63.7 70.2
Table 17: MMLU Test set scores for the T5 closed book baseline for each model size and
each of the 57 domains.
33Izacard, Lewis, Lomeli, Hosseini, Petroni, Schick, Dwivedi-Yu, Joulin, Riedel, Grave
Domain zero-shot 5-shot 5-shot (multi-task) Full / Transfer
All 47.1 47.9 56.6 66.0
Humanities 43.6 46.1 50.1 61.1Social Sciences 54.1 54.6 66.4 77.2STEM 38.0 38.8 46.4 53.2Other 53.9 52.8 66.2 74.4
abstract algebra 22.0 26.0 31.0 31.0anatomy 48.9 47.4 62.2 70.4astronomy 61.8 62.5 68.4 81.6business ethics 60.0 57.0 62.0 70.0clinical knowledge 50.6 49.4 66.4 72.8college biology 51.4 53.5 61.1 77.8college chemistry 36.0 39.0 39.0 45.0college computer science 32.0 32.0 33.0 49.0college mathematics 30.0 35.0 35.0 34.0college medicine 44.5 41.0 52.6 67.6college physics 24.5 26.5 37.3 42.2computer security 59.0 59.0 68.0 76.0conceptual physics 37.0 41.3 57.0 60.0econometrics 20.2 20.2 36.8 37.7electrical engineering 37.9 40.0 50.3 65.5elementary mathematics 31.2 28.0 30.7 36.5formal logic 27.8 27.0 32.5 35.7global facts 41.0 43.0 51.0 53.0high school biology 53.2 56.5 68.7 83.2high school chemistry 41.9 41.4 49.3 51.2high school computer science 40.0 36.0 46.0 60.0high school european history 56.4 58.8 68.5 80.6high school geography 57.1 59.6 71.2 81.3high school gov. and pol. 67.9 67.9 77.2 90.2high school macroeconomics 46.9 48.5 57.9 65.9high school mathematics 28.1 28.9 34.1 31.5high school microeconomics 51.7 51.7 68.9 82.4high school physics 26.5 25.8 32.5 41.1high school psychology 66.2 65.5 78.9 86.8high school statistics 31.5 30.1 43.1 45.8high school us history 57.8 54.9 64.7 77.5high school world history 59.1 62.9 65.4 79.3human aging 48.4 50.7 60.5 70.4human sexuality 55.7 54.2 61.8 84.0international law 66.1 72.7 71.9 84.3jurisprudence 61.1 64.8 72.2 81.5logical fallacies 54.6 57.7 71.2 77.9machine learning 37.5 39.3 43.8 44.6management 56.3 56.3 79.6 89.3marketing 72.2 73.1 84.6 91.9medical genetics 55.0 58.0 71.0 81.0miscellaneous 69.7 67.8 83.8 90.4moral disputes 45.1 46.8 60.1 72.3moral scenarios 24.5 30.3 25.8 38.5nutrition 56.5 53.9 67.0 77.1philosophy 56.3 57.6 70.7 77.2prehistory 59.3 60.5 71.6 78.7professional accounting 35.1 33.0 42.2 50.7professional law 36.3 38.4 39.4 51.7professional medicine 35.7 33.1 52.2 60.7professional psychology 47.7 49.3 60.9 74.0public relations 54.5 53.6 68.2 68.2security studies 47.3 45.7 59.2 73.9sociology 62.2 62.7 71.6 84.6us foreign policy 64.0 68.0 73.0 83.0virology 39.8 40.4 44.6 51.8world religions 77.2 74.9 80.7 87.1
Table 18: MMLU Test set scores for the de-biasedAtlas-XXL using cyclic permutations
for each of the 57 domains for zero-shot, 5 shot, 5-shot-multitask and the transfer
setting.
34Atlas: Few-shot Learning with Retrieval Augmented Language Models
Number of parameters 220M 770M 3B 11B
Natural Questions 64-shot 27.0 35.4 41.3 45.1
Natural Questions full 54.1 60.8 63.4 64.0
TriviaQA 64-shot 55.3 65.0 70.2 71.4
TriviaQA full 71.8 74.9 77.5 78.0
Table 19: Impact of model size on question answering data sets.We report exact
match performance on the test sets of Natural Questions and TriviaQA ﬁltered de-
pending on the number of parameters in the reader module. For these experiments
the index contains the December 2018 Wikipedia dump.
A.3 KILT
For the results on KILT reported in Table 10 we ﬁne-tuneAtlas individually on each data
set. We format the input using a template similar to the one used for question answering:
question: {query text} answer: [MASK_0]
and train the model to generate the mask token followed by the expected output:
[MASK_0] {output}.
We retrieve 20 passages and generate answer using greedy decoding. In KILT, FEVER is a
two-way classiﬁcation task of claims. We lexicalize the “SUPPORTS” (resp. ‘REFUTES”)
label into “true” (respectively “false”).
For few-shot ﬁne-tuning we trainAtlas for 30 steps using 64 random samples from the
train sets. The retriever is trained using query-side ﬁne-tuning. We evaluate models every
5 steps and select the best one on the development set based on the reported metric. We
use AdamW with a batch size of 32 and a learning rate of4 ×10−5 with linear decay and 5
iterations of warmup for both the language model and the retriever.
For the ﬁne-tuning on the full data sets, the model is trained for 5k gradient steps. We
evaluate models every 500 steps and select the best one on the development set based on
the reported metric. The index is refreshed every 500 step for the ﬁrst 1000 iterations, and
every 2k steps afterwards. We use AdamW with a batch size of 64 and a learning rate of
4 ×10−5 with linear decay and 500 iterations of warmup for both the language model and
the retriever.
We report results on the development sets in Table 20.
References
M. Bevilacqua, G. Ottaviano, P. Lewis, S. Yih, S. Riedel, and F. Petroni. Autoregressive
search engines: Generating substrings as document identiﬁers. InNeurIPS, 2022. URL
https://openreview.net/forum?id=Z4kZxAjg8Y.
S. Borgeaud, A. Mensch, J. Hoﬀmann, T. Cai, E. Rutherford, K. Millican, G. v. d. Driessche,
J.-B. Lespiau, B. Damoc, A. Clark, D. d. L. Casas, A. Guy, J. Menick, R. Ring, T. Hennigan,
35Izacard, Lewis, Lomeli, Hosseini, Petroni, Schick, Dwivedi-Yu, Joulin, Riedel, Grave
Model AIDA FEV T-REx zsRE NQ HoPo TQA WoW
acc acc acc acc em em em f1
Atlas 64-shot 69.0 88.1 58.5 60.2 44.2 34.1 77.1 15.4
Atlas full data set 92.7 94.4 84.8 80.9 63.4 51.4 84.4 21.0
Table 20: Downstream results on the KILT dev sets. Downstream metrics are accuracy
(AIDA CoNLL-YAGO, FEVER, T-REx, zero-shot RE), exact match (Natural
Questions, HotpotQA, TriviaQA), or F1 (Wizard of Wikipedia).
S. Huang, L. Maggiore, C. Jones, A. Cassirer, A. Brock, M. Paganini, G. Irving, O. Vinyals,
S. Osindero, K. Simonyan, J. W. Rae, E. Elsen, and L. Sifre. Improving language models
by retrieving from trillions of tokens, 2021. URLhttps://arxiv.org/abs/2112.04426.
T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan,
P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan,
R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler,
M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever,
and D. Amodei. Language models are few-shot learners, 2020. URLhttps://arxiv.org/
abs/2005.14165.
N. D. Cao, G. Izacard, S. Riedel, and F. Petroni. Autoregressive entity retrieval. InICLR,
2021. URL https://openreview.net/forum?id=5k8F6UU39V.
N. Carlini, F. Tramèr, E. Wallace, M. Jagielski, A. Herbert-Voss, K. Lee, A. Roberts, T. B.
Brown, D. Song, Úlfar Erlingsson, A. Oprea, and C. Raﬀel. Extracting training data
from large language models. InUSENIX Security Symposium, 2021. URLhttps://www.
usenix.org/conference/usenixsecurity21/presentation/carlini-extracting.
D. Chen, A. Fisch, J. Weston, and A. Bordes. Reading Wikipedia to answer open-domain
questions. In ACL, 2017. URLhttps://aclanthology.org/P17-1171.
A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W.
Chung, C. Sutton, S. Gehrmann, P. Schuh, K. Shi, S. Tsvyashchenko, J. Maynez, A. Rao,
P. Barnes, Y. Tay, N. Shazeer, V. Prabhakaran, E. Reif, N. Du, B. Hutchinson, R. Pope,
J. Bradbury, J. Austin, M. Isard, G. Gur-Ari, P. Yin, T. Duke, A. Levskaya, S. Ghemawat,
S. Dev, H. Michalewski, X. Garcia, V. Misra, K. Robinson, L. Fedus, D. Zhou, D. Ippolito,
D. Luan, H. Lim, B. Zoph, A. Spiridonov, R. Sepassi, D. Dohan, S. Agrawal, M. Omernick,
A. M. Dai, T. S. Pillai, M. Pellat, A. Lewkowycz, E. Moreira, R. Child, O. Polozov, K. Lee,
Z. Zhou, X. Wang, B. Saeta, M. Diaz, O. Firat, M. Catasta, J. Wei, K. Meier-Hellstern,
D. Eck, J. Dean, S. Petrov, and N. Fiedel. Palm: Scaling language modeling with pathways,
2022. URL https://arxiv.org/abs/2204.02311.
C. Clark and M. Gardner. Simple and eﬀective multi-paragraph reading comprehension. In
ACL, 2018. URLhttps://aclanthology.org/P18-1078.
36Atlas: Few-shot Learning with Retrieval Augmented Language Models
P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord. Think
you have solved question answering? try arc, the ai2 reasoning challenge, 2018. URL
https://arxiv.org/abs/1803.05457.
J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of deep bidirectional
transformers for language understanding. InNAACL, 2019.
B. Dhingra, J. R. Cole, J. M. Eisenschlos, D. Gillick, J. Eisenstein, and W. W. Cohen.
Time-aware language models as temporal knowledge bases.TACL, 2022. URL https:
//aclanthology.org/2022.tacl-1.15.
E. Dinan, S. Roller, K. Shuster, A. Fan, M. Auli, and J. Weston. Wizard of wikipedia:
Knowledge-powered conversational agents. InICLR, 2019. URLhttps://openreview.
net/forum?id=r1l73iRqKm.
J. Dodge, M. Sap, A. Marasović, W. Agnew, G. Ilharco, D. Groeneveld, M. Mitchell, and
M. Gardner. Documenting large webtext corpora: A case study on the colossal clean crawled
corpus. In EMNLP, 2021. URLhttps://aclanthology.org/2021.emnlp-main.98.
H. Elsahar, P. Vougiouklis, A. Remaci, C. Gravier, J. Hare, F. Laforest, and E. Simperl.
T-REx: A large scale alignment of natural language with knowledge base triples. InLREC,
2018. URL https://aclanthology.org/L18-1544.
M. Fajcik, M. Docekal, K. Ondrej, and P. Smrz. R2-D2: A modular baseline for open-domain
question answering. In Findings of EMNLP, 2021. URL https://aclanthology.org/
2021.findings-emnlp.73.
M. Fink. Object classiﬁcation from a single example utilizing class relevance met-
rics. In NIPS, 2005. URL https://proceedings.neurips.cc/paper/2004/file/
ef1e491a766ce3127556063d49bc2f98-Paper.pdf.
T. Gao, A. Fisch, and D. Chen. Making pre-trained language models better few-shot learners.
In ACL-IJCNLP, 2021. URLhttps://aclanthology.org/2021.acl-long.295.
M. Glass, G. Rossiello, M. F. M. Chowdhury, A. R. Naik, P. Cai, and A. Gliozzo. Re2g:
Retrieve, rerank, generate, 2022. URLhttps://arxiv.org/abs/2207.06300.
E. Grave, M. Cisse, and A. Joulin. Unbounded cache model for online language modeling
with open vocabulary, 2017a. URLhttps://arxiv.org/abs/1711.02604.
E. Grave, A. Joulin, and N. Usunier. Improving neural language models with a continuous
cache. InICLR, 2017b. URLhttps://openreview.net/forum?id=B184E5qee.
K. Guu, K. Lee, Z. Tung, P. Pasupat, and M.-W. Chang. Realm: Retrieval-augmented
language model pre-training. arXiv:2002.08909, 2020. URL https://arxiv.org/abs/
2002.08909.
K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick. Momentum contrast for unsupervised visual
representation learning. InCVPR, 2020.
37Izacard, Lewis, Lomeli, Hosseini, Petroni, Schick, Dwivedi-Yu, Joulin, Riedel, Grave
D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt.
Measuring massive multitask language understanding. In ICLR, 2021. URL https:
//openreview.net/forum?id=d7KBjmI3GmQ.
J. Hoﬀart, M. A. Yosef, I. Bordino, H. Fürstenau, M. Pinkal, M. Spaniol, B. Taneva, S. Thater,
and G. Weikum. Robust disambiguation of named entities in text. InEMNLP, 2011. URL
https://aclanthology.org/D11-1072.
J. Hoﬀmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. d. L.
Casas, L. A. Hendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland, K. Millican, G. v. d.
Driessche, B. Damoc, A. Guy, S. Osindero, K. Simonyan, E. Elsen, J. W. Rae, O. Vinyals,
and L. Sifre. Training compute-optimal large language models, 2022. URL https:
//arxiv.org/abs/2203.15556.
S. Hofstätter, J. Chen, K. Raman, and H. Zamani. Multi-task retrieval-augmented text
generation with relevance sampling, 2022. URLhttps://arxiv.org/abs/2207.03030.
P.-S. Huang, X. He, J. Gao, L. Deng, A. Acero, and L. Heck. Learning deep structured
semantic models for web search using clickthrough data. InCIKM, 2013.
G. Izacard and E. Grave. Leveraging passage retrieval with generative models for open
domain question answering. InEACL, 2021a. URLhttps://aclanthology.org/2021.
eacl-main.74.
G. Izacard and E. Grave. Distilling knowledge from reader to retriever for question answering.
In ICLR, 2021b. URLhttps://openreview.net/forum?id=NTEz-6wysdb.
G. Izacard, F. Petroni, L. Hosseini, N. De Cao, S. Riedel, and E. Grave. A memory eﬃcient
baseline for open domain question answering, 2020. URLhttps://arxiv.org/abs/2012.
15156.
G. Izacard, M. Caron, L. Hosseini, S. Riedel, P. Bojanowski, A. Joulin, and E. Grave.
Unsupervised dense information retrieval with contrastive learning.TMLR, 2022. URL
https://openreview.net/forum?id=jKN1pXi7b0.
H. Jegou, M. Douze, and C. Schmid. Product quantization for nearest neighbor search.IEEE
TPAMI, 2010.
Z. Jiang, F. F. Xu, J. Araki, and G. Neubig. How can we know what language models know?
TACL, 2020. URLhttps://aclanthology.org/2020.tacl-1.28.
K. S. Jones. A statistical interpretation of term speciﬁcity and its application in retrieval.
Journal of documentation, 1972.
M. Joshi, E. Choi, D. Weld, and L. Zettlemoyer. TriviaQA: A large scale distantly supervised
challenge dataset for reading comprehension. InACL, 2017. URLhttps://aclanthology.
org/P17-1147.
J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford,
J. Wu, and D. Amodei. Scaling laws for neural language models, 2020. URLhttps:
//arxiv.org/abs/2001.08361.
38Atlas: Few-shot Learning with Retrieval Augmented Language Models
V. Karpukhin, B. Oğuz, S. Min, L. Wu, S. Edunov, D. Chen, and W.-t. Yih. Dense
passage retrieval for open-domain question answering.arXiv:2004.04906, 2020. URL
https://arxiv.org/abs/2004.04906.
U. Khandelwal, O. Levy, D. Jurafsky, L. Zettlemoyer, and M. Lewis. Generalization
through memorization: Nearest neighbor language models. In ICLR, 2020. URL
https://openreview.net/forum?id=HklBjCEKvH.
D. Khashabi, S. Min, T. Khot, A. Sabharwal, O. Tafjord, P. Clark, and H. Hajishirzi.
UNIFIEDQA: Crossing format boundaries with a single QA system. In Findings of
EMNLP, 2020. URLhttps://aclanthology.org/2020.findings-emnlp.171.
A. Krishna, S. Riedel, and A. Vlachos. ProoFVer: Natural logic theorem proving for fact
veriﬁcation. TACL, 2022. URLhttps://aclanthology.org/2022.tacl-1.59.
T. Kwiatkowski, J. Palomaki, O. Redﬁeld, M. Collins, A. Parikh, C. Alberti, D. Epstein,
I. Polosukhin, J. Devlin, K. Lee, K. Toutanova, L. Jones, M. Kelcey, M.-W. Chang, A. M.
Dai, J. Uszkoreit, Q. Le, and S. Petrov. Natural questions: A benchmark for question
answering research.TACL, 2019. URLhttps://aclanthology.org/Q19-1026.
G. Lai, Q. Xie, H. Liu, Y. Yang, and E. Hovy. RACE: Large-scale ReAding comprehen-
sion dataset from examinations. InEMNLP, 2017. URL https://aclanthology.org/
D17-1082.
A. Lazaridou, E. Gribovskaya, W. Stokowiec, and N. Grigorev. Internet-augmented language
models through few-shot prompting for open-domain question answering, 2022. URL
https://arxiv.org/abs/2203.05115.
T. Le Scao and A. Rush. How many data points is a prompt worth? InNAACL-HLT, 2021.
URL https://aclanthology.org/2021.naacl-main.208.
H. Lee, A. Kedia, J. Lee, A. Paranjape, C. D. Manning, and K.-G. Woo. You only need one
model for open-domain question answering, 2021a. URLhttps://arxiv.org/abs/2112.
07381.
J. Lee, M. Sung, J. Kang, and D. Chen. Learning dense representations of phrases at scale.
In ACL-IJCNLP, 2021b. URLhttps://aclanthology.org/2021.acl-long.518.
K. Lee, M.-W. Chang, and K. Toutanova. Latent retrieval for weakly supervised open domain
question answering. InACL, 2019. URLhttps://aclanthology.org/P19-1612.
B. Lester, R. Al-Rfou, and N. Constant. The power of scale for parameter-eﬃcient prompt
tuning. In EMNLP, 2021. URLhttps://aclanthology.org/2021.emnlp-main.243.
O. Levy, M. Seo, E. Choi, and L. Zettlemoyer. Zero-shot relation extraction via reading
comprehension. In CoNLL, 2017. URLhttps://aclanthology.org/K17-1034.
P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Küttler, M. Lewis,
W.-t. Yih, T. Rocktäschel, S. Riedel, and D. Kiela. Retrieval-augmented generation for
knowledge-intensive nlp tasks. InNeurIPS, 2020. URLhttps://proceedings.neurips.
cc/paper_files/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf.
39Izacard, Lewis, Lomeli, Hosseini, Petroni, Schick, Dwivedi-Yu, Joulin, Riedel, Grave
X. L. Li and P. Liang. Preﬁx-tuning: Optimizing continuous prompts for generation. In
ACL-IJCNLP, 2021. URLhttps://aclanthology.org/2021.acl-long.353.
O. Lieber, O. Sharir, B. Lenz, and Y. Shoham. Jurassic-1: Technical details and evaluation.
Technical report, AI21 Labs, 2021.
I. R. L. Logan, I. Balavzevi’c, E. Wallace, F. Petroni, S. Singh, and S. Riedel. Cutting down
on prompts and parameters: Simple few-shot learning with language models, 2021. URL
https://arxiv.org/abs/2106.13353.
T. Mihaylov, P. Clark, T. Khot, and A. Sabharwal. Can a suit of armor conduct electricity?
a new dataset for open book question answering. In EMNLP, 2018. URL https://
aclanthology.org/D18-1260.
S. Min, J. Michael, H. Hajishirzi, and L. Zettlemoyer. AmbigQA: Answering ambiguous open-
domainquestions. InEMNLP,2020. URLhttps://aclanthology.org/2020.emnlp-main.
466.
R. Nakano, J. Hilton, S. A. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse, S. Jain, V. Kosaraju,
W. Saunders, X. Jiang, K. Cobbe, T. Eloundou, G. Krueger, K. Button, M. Knight,
B. Chess, and J. Schulman. Webgpt: Browser-assisted question-answering with human
feedback, 2021. URLhttps://arxiv.org/abs/2112.09332.
A. Paranjape, O. Khattab, C. Potts, M. Zaharia, and C. D. Manning. Hindsight: Posterior-
guided training of retrievers for improved open-ended generation, 2021. URLhttps:
//arxiv.org/abs/2110.07752.
F. Petroni, P. Lewis, A. Piktus, T. Rocktäschel, Y. Wu, A. H. Miller, and S. Riedel. How
context aﬀects language models’ factual predictions. arXiv:2005.04611, 2020. URL
https://arxiv.org/abs/2005.04611.
F. Petroni, S. Broscheit, A. Piktus, P. Lewis, G. Izacard, L. Hosseini, J. Dwivedi-Yu,
M. Lomeli, T. Schick, P.-E. Mazaré, A. Joulin, E. Grave, and S. Riedel. Improving
wikipedia veriﬁability with ai, 2022. URLhttps://arxiv.org/abs/2207.06220.
A. Piktus, F. Petroni, V. Karpukhin, D. Okhonko, S. Broscheit, G. Izacard, P. Lewis, B. Oğuz,
E. Grave, W.-t. Yih, and S. Riedel. The web is your oyster - knowledge-intensive nlp
against a very large web corpus, 2021. URLhttps://arxiv.org/abs/2112.09924.
A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are
unsupervised multitask learners.OpenAI Technical Report, 2019.
J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoﬀmann, F. Song, J. Aslanides, S. Henderson,
R. Ring, S. Young, E. Rutherford, T. Hennigan, J. Menick, A. Cassirer, R. Powell, G. v. d.
Driessche, L. A. Hendricks, M. Rauh, P.-S. Huang, A. Glaese, J. Welbl, S. Dathathri,
S. Huang, J. Uesato, J. Mellor, I. Higgins, A. Creswell, N. McAleese, A. Wu, E. Elsen,
S. Jayakumar, E. Buchatskaya, D. Budden, E. Sutherland, K. Simonyan, M. Paganini,
L. Sifre, L. Martens, X. L. Li, A. Kuncoro, A. Nematzadeh, E. Gribovskaya, D. Donato,
40Atlas: Few-shot Learning with Retrieval Augmented Language Models
A. Lazaridou, A. Mensch, J.-B. Lespiau, M. Tsimpoukelli, N. Grigorev, D. Fritz, T. Sotti-
aux, M. Pajarskas, T. Pohlen, Z. Gong, D. Toyama, C. d. M. d’Autume, Y. Li, T. Terzi,
V. Mikulik, I. Babuschkin, A. Clark, D. d. L. Casas, A. Guy, C. Jones, J. Bradbury,
M. Johnson, B. Hechtman, L. Weidinger, I. Gabriel, W. Isaac, E. Lockhart, S. Osin-
dero, L. Rimell, C. Dyer, O. Vinyals, K. Ayoub, J. Stanway, L. Bennett, D. Hassabis,
K. Kavukcuoglu, and G. Irving. Scaling language models: Methods, analysis & insights
from training gopher, 2021. URLhttps://arxiv.org/abs/2112.11446.
C. Raﬀel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J.
Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer, 2019.
URL https://arxiv.org/abs/1910.10683.
P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang. SQuAD: 100,000+ questions for machine
comprehension of text. InEMNLP, 2016. URLhttps://aclanthology.org/D16-1264.
O. Ram, G. Shachaf, O. Levy, J. Berant, and A. Globerson. Learning to retrieve passages
without supervision. In NAACL-HLT, 2022. URL https://aclanthology.org/2022.
naacl-main.193.
M. Richardson. MCTest: A Challenge Dataset for the Open-
Domain Machine Comprehension of Text. In EMNLP, 2013.
URL https://www.microsoft.com/en-us/research/publication/
mctest-challenge-dataset-open-domain-machine-comprehension-text/ .
M. Richardson, C. J. Burges, and E. Renshaw. MCTest: A challenge dataset for the open-
domain machine comprehension of text. InEMNLP, 2013. URLhttps://aclanthology.
org/D13-1020.
S. E. Robertson, S. Walker, S. Jones, M. M. Hancock-Beaulieu, M. Gatford, et al. Okapi at
TREC-3. NIST Special Publication Sp, 1995.
D. S. Sachan, S. Reddy, W. Hamilton, C. Dyer, and D. Yogatama. End-to-end training
of multi-document reader and retriever for open-domain question answering, 2021. URL
https://arxiv.org/abs/2106.05346.
T. Schick and H. Schutze. It’s not just size that matters: Small language models are also
few-shot learners, 2021. URLhttps://arxiv.org/abs/2009.07118.
T. Schick and H. Schütze. Exploiting cloze-questions for few-shot text classiﬁcation and
natural language inference. In EACL, 2021a. URL https://aclanthology.org/2021.
eacl-main.20.
T. Schick and H. Schütze. Few-shot text generation with natural language instructions. In
EMNLP, 2021b. URLhttps://aclanthology.org/2021.emnlp-main.32.
Y. Shen, X. He, J. Gao, L. Deng, and G. Mesnil. Learning semantic representations using
convolutional neural networks for web search. InWWW, 2014.
41Izacard, Lewis, Lomeli, Hosseini, Petroni, Schick, Dwivedi-Yu, Joulin, Riedel, Grave
T. Shin, Y. Razeghi, R. L. Logan IV, E. Wallace, and S. Singh. AutoPrompt: Eliciting
Knowledge from Language Models with Automatically Generated Prompts. InEMNLP,
2020. URL https://aclanthology.org/2020.emnlp-main.346.
K. Shuster, S. Poﬀ, M. Chen, D. Kiela, and J. Weston. Retrieval augmentation reduces
hallucination in conversation, 2021. URLhttps://arxiv.org/abs/2104.07567.
K. Shuster, M. Komeili, L. Adolphs, S. Roller, A. D. Szlam, and J. Weston. Language models
that seek for knowledge: Modular search & generation for dialogue and prompt completion,
2022. URL https://arxiv.org/abs/2203.13224.
S. Smith, M. Patwary, B. Norick, P. LeGresley, S. Rajbhandari, J. Casper, Z. Liu, S. Prab-
humoye, G. Zerveas, V. Korthikanti, E. Zhang, R. Child, R. Y. Aminabadi, J. Bernauer,
X. Song, M. Shoeybi, Y. He, M. Houston, S. Tiwary, and B. Catanzaro. Using deepspeed
and megatron to train megatron-turing nlg 530b, a large-scale generative language model,
2022. URL https://arxiv.org/abs/2201.11990.
D. Tam, R. R. Menon, M. Bansal, S. Srivastava, and C. Raﬀel. Improving and simplifying
pattern exploiting training, 2021. URLhttps://arxiv.org/abs/2103.11955.
R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos,
L.Baker, Y.Du, Y.Li, H.Lee, H.S.Zheng, A.Ghafouri, M.Menegali, Y.Huang, M.Krikun,
D. Lepikhin, J. Qin, D. Chen, Y. Xu, Z. Chen, A. Roberts, M. Bosma, V. Zhao, Y. Zhou,
C.-C. Chang, I. Krivokon, W. Rusch, M. Pickett, P. Srinivasan, L. Man, K. Meier-Hellstern,
M. R. Morris, T. Doshi, R. D. Santos, T. Duke, J. Soraker, B. Zevenbergen, V. Prabhakaran,
M. Diaz, B. Hutchinson, K. Olson, A. Molina, E. Hoﬀman-John, J. Lee, L. Aroyo,
R. Rajakumar, A. Butryna, M. Lamm, V. Kuzmina, J. Fenton, A. Cohen, R. Bernstein,
R. Kurzweil, B. Aguera-Arcas, C. Cui, M. Croak, E. Chi, and Q. Le. Lamda: Language
models for dialog applications, 2022. URLhttps://arxiv.org/abs/2201.08239.
J. Thorne, A. Vlachos, C. Christodoulopoulos, and A. Mittal. FEVER: a large-scale
dataset for fact extraction and VERiﬁcation. In NAACL-HTL, 2018. URL https:
//aclanthology.org/N18-1074.
S. Thrun and L. Pratt.Learning to Learn: Introduction and Overview, page 3–17. Kluwer
Academic Publishers, USA, 1998. ISBN 0792380479.
O. Vinyals, C. Blundell, T. Lillicrap, k. kavukcuoglu, and D. Wierstra. Matching networks
for one shot learning. InNIPS, 2016. URLhttps://proceedings.neurips.cc/paper/
2016/file/90e1357833654983612fb05e3ec9148c-Paper.pdf.
E. M. Voorhees. The TREC-8 question answering track report. InLREC, 1999. URL
http://www.lrec-conf.org/proceedings/lrec2000/pdf/26.pdf.
Z. Wang, P. Ng, X. Ma, R. Nallapati, and B. Xiang. Multi-passage BERT: A globally
normalized BERT model for open-domain question answering. InEMNLP-IJCNLP, 2019.
URL https://aclanthology.org/D19-1599.
42Atlas: Few-shot Learning with Retrieval Augmented Language Models
G. Wenzek, M.-A. Lachaux, A. Conneau, V. Chaudhary, F. Guzmán, A. Joulin, and E. Grave.
CCNet: Extracting high quality monolingual datasets from web crawl data. InLREC,
2020. URL https://aclanthology.org/2020.lrec-1.494.
L. Xiong, C. Xiong, Y. Li, K.-F. Tang, J. Liu, P. N. Bennett, J. Ahmed, and A. Overwijk.
Approximate nearest neighbor negative contrastive learning for dense text retrieval. In
ICLR, 2021. URLhttps://openreview.net/forum?id=zeFrfgyZln.
Z. Yang, P. Qi, S. Zhang, Y. Bengio, W. W. Cohen, R. Salakhutdinov, and C. D. Manning.
Hotpotqa: A dataset for diverse, explainable multi-hop question answering, 2018. URL
https://arxiv.org/abs/1809.09600.
A. Yates, R. Nogueira, and J. Lin. Pretrained Transformers for Text Ranking: BERT and
Beyond. InWSDM, 2021.
W.-t. Yih, K. Toutanova, J. C. Platt, and C. Meek. Learning discriminative projections for
text similarity measures. InCoNLL, 2011. URLhttps://aclanthology.org/W11-0329.
D. Yogatama, C. de Masson d’Autume, and L. Kong. Adaptive semiparametric language
models. TACL, 2021. URLhttps://aclanthology.org/2021.tacl-1.22.
43

Benchmarking Large Language Models in Retrieval-Augmented Generation
Jiawei Chen1,3, Hongyu Lin1,*, Xianpei Han1,2,*, Le Sun1,2
1Chinese Information Processing Laboratory, Institute of Software, Chinese Academy of Sciences, Beijing, China
2State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing, China
3University of Chinese Academy of Sciences, Beijing, China
{jiawei2020,hongyu,xianpei,sunle}@iscas.ac.cn
Abstract
Retrieval-Augmented Generation (RAG) is a promising ap-
proach for mitigating the hallucination of large language
models (LLMs). However, existing research lacks rigorous
evaluation of the impact of retrieval-augmented generation
on different large language models, which make it challeng-
ing to identify the potential bottlenecks in the capabilities
of RAG for different LLMs. In this paper, we systemati-
cally investigate the impact of Retrieval-Augmented Gener-
ation on large language models. We analyze the performance
of different large language models in 4 fundamental abili-
ties required for RAG, including noise robustness, negative
rejection, information integration, and counterfactual robust-
ness. To this end, we establish Retrieval-Augmented Genera-
tion Benchmark (RGB), a new corpus for RAG evaluation in
both English and Chinese. RGB divides the instances within
the benchmark into 4 separate testbeds based on the afore-
mentioned fundamental abilities required to resolve the case.
Then we evaluate 6 representative LLMs on RGB to diag-
nose the challenges of current LLMs when applying RAG.
Evaluation reveals that while LLMs exhibit a certain degree
of noise robustness, they still struggle significantly in terms of
negative rejection, information integration, and dealing with
false information. The aforementioned assessment outcomes
indicate that there is still a considerable journey ahead to ef-
fectively apply RAG to LLMs.
Introduction
Recently, there have been impressive advancements in large
language models (LLMs) like ChatGPT (OpenAI 2022) and
ChatGLM (THUDM 2023a). Although these models have
shown remarkable general abilities (Bang et al. 2023; Guo
et al. 2023), they still suffer severely from challenges includ-
ing factual hallucination (Cao et al. 2020; Raunak, Menezes,
and Junczys-Dowmunt 2021; Ji et al. 2023), knowledge out-
dating (He, Zhang, and Roth 2022), and the lack of domain-
specific expertise (Li et al. 2023c; Shen et al. 2023).
Incorporating external knowledge via information re-
trieval, i.e., Retrieval-Augmented Generation (RAG), has
been regarded as a promising way to resolve the above chal-
lenges. (Guu et al. 2020; Lewis et al. 2020; Borgeaud et al.
* Corresponding authors.
Copyright © 2024, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.
Noise Robustness Negative Rejection
Who was awarded the 2022 
Nobel prize in literature?
… French author Annie Ernaux
… in Literature for 2021 is…
Annie Ernaux
Question
External documents contain noises
Retrieval Augmented 
Generation
The 2020 Nobel Laureate …
I can not answer the question …
Information Integration
When were the ChatGPT app for 
iOS and api launched?
On May 18th, 2023, OpenAI…
That changed on March 1, …
May 18 and March 1.
Question
External documents contain all answers
Retrieval Augmented 
Generation
Counterfactual Robustness
Which city hosted the Olympic 
games in 2004?
2004 Olympic …to New York, 
New York  easily defeated …
There are factual errors in the…
Question
Counterfactual external documents
Retrieval Augmented 
Generation
Retrieval Augmented 
Generation
Who was awarded the 2022 
Nobel prize in literature?
Question
… in Literature for 2021 is…
External documents are all noises
Figure 1: Illustration of 4 kinds of abilities required for
retrieval-augmented generation of LLMs.
2022; Izacard et al. 2022). With the help of external knowl-
edge, LLMs can generate more accurate and reliable re-
sponses. The most common method is to use a search engine
as a retriever such as New Bing. Due to the vast amount of
information available on the Internet, using a search engine
can provide more real-time information.
However, Retrieval-Augmented Generation brings not
only positive effects to LLMs (Liu, Zhang, and Liang 2023;
Maynez et al. 2020). On one hand, there is a significant
amount of noise information even fake news in the content
available on the Internet, which poses challenges for search
engines in accurately retrieving desirable knowledge. On the
other hand, LLMs suffer from unreliable generation chal-
lenge. LLMs can be misled by incorrect information con-
tained in the context (Bian et al. 2023) and also suffer from
hallucination during the generation (Adlakha et al. 2023),
resulting in generating content that goes beyond external in-
formation. These challenges result in LLMs being unable to
consistently generate reliable and accurate responses. Un-
The Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)
17754fortunately, currently there lacks of comprehensive under-
standing on how these factors can influence RAG, and how
could each model survives from these drawbacks and im-
provement their performance via information retrieval. As a
result, there is a pressing need for a comprehensive evalua-
tion of LLMs on their ability to effectively utilize retrieved
information, as well as their ability to withstand the various
drawbacks present in information retrieval.
To this end, this paper conducts a comprehensive evalua-
tion of RAG for current LLMs. Specifically, we create a new
Retrieval-Augmented Generation Benchmark, namely RGB,
in both English and Chinese. In order to ensure that the in-
ternal knowledge of LLMs does not introduce bias into the
evaluation results, RGB chooses to aggregate the latest news
information and constructs queries based on the news infor-
mation. Then, based on these queries, we use Search API to
fetch relevant documents and select most relevant snippets
from the content as external retrieved documents. Finally,
based on different compositions of query and document-set
pairs, we expand the corpus and divided it into 4 testbeds to
evaluate the following basic abilities of LLMs according to
the common challenges in RAG, as shown in Figure 1:
• Noise Robustness, which means a LLM can extract use-
ful information from noisy documents. In this paper, we
define noisy documents as those that are relevant to the
question but do not contain any information of the an-
swer. For the instance in Figure 1, the noisy documents
related to the question “Who was awarded the 2022 No-
bel Prize in Literature” include reports about the 2021
Nobel Prize in Literature. To this end, the testbed for
noise robustness contains instances whose external doc-
uments contain a certain number of noisy documents
based on the desired noise ratio.
• Negative Rejection, which means that a LLM should re-
ject to answer the question when the required knowledge
is not present in any retrieved document. The testbed for
negative rejection contains instances whose external doc-
uments are only with noisy documents. LLMs are ex-
pected to indicate “insufficient information” or other re-
jection signals.
• Information Integration, which evaluates whether
LLMs can answer complex questions that require inte-
grating information from multiple documents. For the in-
stance in Figure 1, for the question “When were the Chat-
GPT app for iOS and ChatGPT api launched?”, LLMs
are expected to provide information of the launch dates
for both the ChatGPT iOS app and ChatGPT API. The
testbed for information integration contains instances
that can only be answered using multiple documents.
• Counterfactual Robustness, which evaluates whether
LLMs can identify risks of known factual errors in the
retrieved documents when the LLMs are given warnings
about potential risks in the retrieved information through
instruction. The testbed for counterfactual robustness in-
cludes instances that can be answered directly by the
LLMs, but the external documents contain factual errors.
Based on RGB, we conduct evaluation on 6 state-of-
the-art large language models including ChatGPT (Ope-
nAI 2022), ChatGLM-6B (THUDM 2023a), ChatGLM2-
6B (THUDM 2023b), Vicuna-7b (Chiang et al. 2023),
Qwen-7B-Chat (Bai et al. 2023), BELLE-7B (BELLEGroup
2023). We found that even though RAG can improve the re-
sponse accuracy of LLMs, they still suffer from the above-
mentioned challenges significantly. Specifically, we found
that even though LLMs demonstrate some level of noise ro-
bustness, they tend to confuse similar information and fre-
quently generate inaccurate answers when relevant informa-
tion exists. For example, when faced with a question about
the 2022 Nobel Prize in Literature, if there are noisy docu-
ments about the 2021 Nobel Prize in Literature in external
documents, LLMs may become confused and provide inac-
curate answers. Besides, LLMs frequently fail to reject an-
swering and generate incorrect answers when none of the
external documents contain relevant information. Further-
more, LLMs lack the ability to summarize from multiple
documents, and therefore if multiple documents are needed
to answer a question, LLMs often fail to provide accurate
answer. Finally, we found that even when the LLMs contain
the required knowledge and are given warnings about po-
tential risks in the retrieved information through instruction,
they still tend to trust and prioritize the retrieved information
over their own existing knowledge. The experimental results
mentioned above highlight the need for further resolution of
important issues in the existing RAG method. Therefore, it
is crucial to exercise caution and carefully design its usage.
Generally speaking, the contributions of this paper are1:
• We proposed to evaluate four capabilities for retrieval-
augmented generation of LLMs and created the
Retrieval-Augmented Generation Benchmark in both En-
glish and Chinese. To best of our knowledge, it is the first
benchmark designed to assess these four capabilities for
retrieval-augmented generation of LLMs.
• We evaluated the existing LLMs using RGB and found
the limitations of them in the four different abilities.
• We analyzed the responses of LLMs in RGB and identi-
fied their current shortcomings as well as suggested di-
rections for improvement.
Related Work
Retrieval-augmented models The knowledge stored in
large language models is commonly out-of-date (He, Zhang,
and Roth 2022) and they also sometimes generate hallu-
cination (Cao et al. 2020; Raunak, Menezes, and Junczys-
Dowmunt 2021; Ji et al. 2023) i.e., they may generate ir-
relevant or factually incorrect contents. By using external
knowledge as guidance, retrieval-augmented models can
generate more accurate and reliable responses (Guu et al.
2020; Lewis et al. 2020; Borgeaud et al. 2022; Izacard
et al. 2022; Shi et al. 2023; Ren et al. 2023). Retrieval-
augmented models have achieved remarkable results in var-
ious tasks such as open-domain QA (Izacard and Grave
2021; Trivedi et al. 2023; Li et al. 2023a), dialogue (Cai
et al. 2019a,b; Peng et al. 2023), domain-specific ques-
tion answering (Cui et al. 2023) and code generation (Zhou
1Our code&data: https://github.com/chen700564/RGB.
The Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)
17755News CollectionNews about The 2022 Nobel Prize for Physiology and Medicine
Data adjustment and filtering by Human
{   “Question”: “Who was awarded the 2022 Nobel Prize for Physiology and Medicine?”,   “Answer”: ['Svante Pääbo','Svante Paabo’]}
Data generation by ChatGPT
Retrieve using search engine
Rerank by dense retrieval model
We simulate the process of a user querying and obtaining information.. ……News: The 2022 Nobel Prize for …
Related event: … \nQuestion:… \nKey information:…gpt-3.5-turbo api
Query: Who was awarded the 2022 Nobel Prize for Physiology and Medicine?”,
{"link": "https://www.nobelprize.org/prizes/medicine/", ...Google Search API
Chun2ChunkQueryDense retrieval modelTop1 Chunk Top30 ChunkTop2 Chunk……
……
Figure 2: The process of data generation. Firstly, we use
models to extract (event, question, answer) from news ar-
ticles. Next, we utilize search engines to retrieve relevant
web pages. Finally, a dense retrieval model is employed to
re-rank the content of these web pages.
et al. 2023b). Recently, with the development of large mod-
els, a series of retrieval-enhanced tools and products have
gained widespread attention, such as ChatGPT retrieval plu-
gin, Langchain, New Bing, etc. However, in real-world sce-
narios, the retrieved text inevitably contains noise. There-
fore, in this paper we conducted a systematic evaluation and
analysis of retrieval-augmented generation in LLMs.
Evaluation of LLMs Evaluating LLMs has received sig-
nificant attention due to their remarkable general capabil-
ity (Chang et al. 2023). It enables us to gain a deeper under-
standing of the specific abilities and limitations of LLMs,
while also providing valuable guidance for future research.
In the past, benchmarks such as GLUE (Wang et al. 2019b)
and SuperCLUE (Wang et al. 2019a) primarily focused on
evaluating NLP tasks, particularly in natural language un-
derstanding. However, these evaluations often fail to fully
capture the capabilities of LLMs. MMLU (Hendrycks et al.
2021) was then proposed to measure the knowledge acquired
by language models when pre-training. Recently, with the
development of LLMs, a series of general evaluation bench-
marks have emerged, such as AGIEval (Zhong et al. 2023),
C-Eval (Huang et al. 2023), AlpacaEval (Li et al. 2023b),
OpenLLM Leaderboard (Edward Beeching 2023), etc. In
addition to general abilities, there are also specific bench-
marks that focus on evaluating the capabilities of models.
For example, CValues (Xu et al. 2023a) focuses on the safety
and responsibility of LLMs, M3Exam (Zhang et al. 2023)
focuses on human exam and ToolBench (Qin et al. 2023)
evaluates how well LLMs use external tools. Recently, Ad-
lakha et al. (2023) evaluate the RAG of LLMs in exist QA
dataset. Different from their work, we focus on 4 required
abilities of RAG and create Retrieval-Augmented Genera-
tion Benchmark to evaluate the LLMs.
Retrieval-Augmented Generation Benchmark
In this section, we first introduce the specific retrieval-
augmented generation abilities we aim to evaluate. Next, we
outline the process of constructing the RAG benchmark for
evaluation. Lastly, we present the evaluation metrics.
Required Abilities of RAG
External knowledge is the key to resolving the problems
of LLMs such as hallucination and outdated knowledge,
which can make LLMs generate more accurate and reliable
responses through retrieval-augmented generation (RAG).
However, LLMs cannot always response as expected with
RAG. For one thing, there are numerous irrelevant docu-
ments and false information on the Internet. Incorporating
these external documents into LLMs could have a detrimen-
tal effect. For anthoer, LLMs suffer from the unreliable gen-
eration challenge. The generation of LLMs is often unpre-
dictable, and we cannot guarantee that they will utilize the
useful information entailed in the external documents. Ad-
ditionally, LLMs can easily be misled by incorrect infor-
mation in the document. To this end, we build Retrieval-
Augmented Generation Benchmark (RGB) to evaluate the
retrieval-augmented generation of LLMs, and we concern
about 4 specific abilities:
Noise Robustness is the robustness of LLMs in noisy
documents. As retrievers are not perfect, the external knowl-
edge they retrieve often contains a significant amount of
noise, i.e., documents which are relevant to the question but
do not contain any information about the answer. To effec-
tively answer user questions, LLMs must be able to extract
the necessary information from documents despite there are
noisy documents.
Negative Rejectionis a measure of whether LLMs can
decline to answer a question when none of the contexts pro-
vide useful information. In real-world situations, the search
engine often fails to retrieve documents containing the an-
swers. In these cases, it is important for the model to have
the capability to reject recognition and avoid generating mis-
leading content.
Information Integration is a capacity to integrate an-
swers from multiple documents. In many cases, the an-
swer to a question may be contained in multiple documents.
For example, for the question “Who are the champions of
the U.S. Open 2022 men’s and women’s singles?”, the two
champions may be mentioned in different documents. In or-
der to provide better answers to complex questions, it is nec-
essary for LLMs to have the ability to integrate information.
Counterfactual Robustnessrefers to a capacity to han-
dle errors in external knowledge. In the real world, there is
an abundance of false information on the internet. Please
The Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)
17756note that we only evaluate the situation that LLMs are given
warnings about potential risks in the retrieved information
through instruction.
In real-world scenarios, it is not possible to obtain per-
fect documents with all the necessary external knowledge.
Therefore, evaluating these four abilities of the model be-
comes essential in order to measure the RAG of LLMs.
Data Construction
Inspired by previous benchmarks for LLMs, RGB utilizes
a question-answering format for evaluation. We evaluate the
LLMs by judging the retrieval-augmented responses of them
to the questions. To simulate real-world scenarios, we con-
struct question and answer data using actual news articles.
Due to the abundance of knowledge contained within the
LLMs there is a potential for bias when measuring the first
three abilities. To mitigate this, the instances of RGB are
constructed by latest news articles. Additionally, we retrieve
external documents from Internet through search engines.
Finally, we expand the corpus and divided it into 4 testbeds
to evaluate the above basic abilities of LLMs. The overall
procedure of our data construction is illustrated in Figure 2.
QA instances generation.We first collect latest news ar-
ticles and use prompts to make ChatGPT generate events,
questions, and answers for each articles. For example, as
shown in the Figure 2, for a report about “The 2022 Nobel
Prize”, ChatGPT will generate corresponding event, ques-
tion and provide key information for answering it. By gen-
erating events, the model is able to preliminarily filter out
news articles that do not contain any events. After genera-
tion, we manually check the answer and filter out data that
is difficult to retrieve through search engines.
Retrieve using search engine.For each query, we use
Google’s API to fetch 10 relevant web pages and extract cor-
responding snippets of text from them. Simultaneously, we
read these web pages and convert their textual content into
text chunks with a maximum length of 300 tokens. Using an
open-source dense retriever, we select the top-30 text chunks
that match the query most effectively. These retrieved text
chunks, along with the snippets provided by the search API,
will serve as our external documents. These documents will
be divided into positive documents and negative documents
based on whether they contain the answer.
Testbeds construction for each ability.We expand the
corpus and divided it into 4 testbeds to evaluate the above
basic abilities of LLMs. To evaluate the noise robustness,
we sample varying numbers of negative documents ac-
cording to the desired ratio of noises. For negative rejec-
tion, all the external documents are sampled from negative
documents. For the information integration ability, we fur-
ther construct data based on the above generated questions.
This involves expanding or rewriting these questions so that
their answers encompass multiple aspects. For example, the
question “Who won the MVP of Super Bowl 2023?” can
be rewrite as “Who won the MVPs of Super Bowl 2022
and 2023?”. Consequently, answering such questions re-
quires utilizing information from various documents. Dif-
ferent from the first three abilities, the data of counterfactual
robustness is constructed solely based on the internal knowl-
edge of the model. Based on the aforementioned generated
questions mentioned above, we adopt ChatGPT to automat-
ically generate its known knowledge. Specifically, we use
prompts to allow the model to generate both questions and
answers that are already known. For example, based on the
question “Who was awarded the 2022 Nobel Prize for Phys-
iology and Medicine?”, the model will generate the known
question “Who was awarded the 2021 Nobel Prize in Lit-
erature?” and answer “Abdulrazak Gurnah”. We then man-
ually verified the generated answers, and retrieve relevant
documents as described above. In order to make documents
contain factual errors, we manually modify the answers and
replace the corresponding parts in the document.
Finally, we collect totally 600 base questions in RGB,
and 200 additional questions for the information integration
ability and 200 additional questions for counterfactual ro-
bustness ability. Half of the instances are in English, and the
other half are in Chinese.
Evaluation Metrics
The core of this benchmark is to evaluate whether LLMs can
utilize the provided external documents to acquire knowl-
edge and generate reasonable answers. We evaluate the re-
sponses of LLMs in order to measure above-mentioned four
abilities of them.
Accuracy is used to measure noise robustness and infor-
mation integration. We employ an exact matching approach
where if the generated text contains an exact match to the
answer, it is considered as a correct answer.
Rejection rate is used to measure negative rejection.
When only noisy documents are provided, LLMs should
output the specific content – “I can not answer the question
because of the insufficient information in documents.” (We
use instructions to inform the model.). If the model gener-
ates this content, it indicates a successful rejection.
Error detection ratemeasures whether the model can
detect the factual errors in the documents for counterfactual
robustness. When the provided documents contain factual
errors, the model should output the specific content – “There
are factual errors in the provided documents.” (We use in-
structions to inform the model.). If the model generates this
content, it indicates that the model has detected erroneous
information in the document.
Error correction ratemeasures whether the model can
provide the correct answer after identifying errors for coun-
terfactual robustness. The model is asked to generate the cor-
rect answer after identifying the factual errors. If the model
generates the correct answer, it indicates that the model is
capable of correcting errors in the document.
Considering that LLMs may not fully adhere to instruc-
tions, for rejection rate and error detection rate, we also use
ChatGPT to conduct additional evaluation of the responses.
Specifically, we prompt ChatGPT to determine if the re-
sponses can reflect information that is not present in the doc-
ument or identify any factual errors.
Experiments
In this section, we evaluate the performance of various
LLMs, analyze and discuss the results, summarizing the
The Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)
17757English Chinese
Noise Ratio 0 0.2
0.4 0.6 0.8 0 0.2
0.4 0.6 0.8
ChatGPT (OpenAI
2022) 96.33 94.67
94.00 90.00 76.00 95.67 94.67
91.00 87.67 70.67
ChatGLM-6B (THUDM 2023a) 93.67 90.67
89.33 84.67 70.67 94.33 90.67
89.00 82.33 69.00
ChatGLM2-6B (THUDM 2023b) 91.33 89.67
83.00 77.33 57.33 86.67 82.33
76.67 72.33 54.00
Vicuna-7B-v1.3 (Chiang et al. 2023) 87.67 83.33
86.00 82.33 60.33 85.67 82.67
77.00 69.33 49.67
Qwen-7B-Chat (Bai et al. 2023) 94.33 91.67
91.00 87.67 73.67 94.00 92.33
88.00 84.33 68.67
BELLE-7B-2M (BELLEGroup 2023) 83.33 81.00
79.00 71.33 64.67 92.00 88.67
85.33 78.33 67.68
Table 1: The experimental result of noise robustness measured by accuracy (%) under different noise ratios. We can see that the
increasing noise rate poses a challenge for RAG in LLMs.
Long-distance inf
ormation. Evidence uncertainty
. Concept confusion.
Question Who did
Iga Swiatek defeat to
win the Qatar Open 2022? What is
the name of Apple’s headset? What w
as Tesla’s revenue in Q1
2022?
Answer Anett K
ontaveit Vision
Pro 18.76 billion
Docs
Positive
document
Swiatek entered into the Qatar
Open ...won ... Anett Kontaveit
Negative document
... she defeatedOns Jabeur6-2,
7-6(5) to win the 2022 US Open
Positive
document
Apple unveiled a costly augmented-
realityheadset called the Vision Pro
Negative document
... is what Gurman believes will be
calledApple Reality Pro. ...
Positive
document
Tesla, Inc. reported Q1 FY 2022
... revenues of $18.76 billion
Negative document
...earnings for 2022... Automotive
revenue reached $16.86 billion
Responses Iga
Swiatek defeated Ons Jabeur ... headset
is Apple Reality Pro. ... in
Q1 2022 was $16.86 billion.
Table 2: Error cases of noise robustness, and only one positive document and one negative document are shown. The responses
are generated by ChatGLM2-6B. The bold text indicates the matching parts between the document and the question or answer,
while the italicized text highlights the non-matching parts.
main challenges that existing LLMs encounter when using
external knowledge.
Settings
Task formats.We provide 5 external documents for each
question. In our experiments on noise robustness, we evalu-
ate scenarios with noise ratios ranging from 0 to 0.8.
Models We evaluate 6 state-of-the-art LLMs includ-
ing ChatGPT (OpenAI 2022) (gpt-3.5-turbo), ChatGLM-
6B (THUDM 2023a), ChatGLM2-6B (THUDM 2023b),
Vicuna-7b-v1.3 (Chiang et al. 2023), Qwen-7B-Chat (Bai
et al. 2023), BELLE-7B-2M (BELLEGroup 2023).
Results on Noise Robustness
We evaluated the accuracy based on the different noise ratios
in external documents, and the results are shown in Table 1.
We can see that:
(1) RAG can effect improve the responses of LLMs.
LLMs have shown strong performance even in the presence
of noise, indicating that RAG is a promising way for LLMs
to generate accurate and reliable responses.
(2) The increasing noise rate poses a challenge for
RAG in LLMs.Specifically, when the noise ratio exceeds
80%, the accuracy decreases significantly at a significance
level of 0.05. For example, the performance of ChatGPT has
decreased from 96.33% to 76.00%, while the performance
of ChatGLM2-6B has decreased from 91.33% to 57.33%.
Error Analysis To better comprehend the negative impact
of noise on model generation, we examined the incorrect an-
swers and found that these errors typically originate from
three reasons, as shown in Table 2.
(1) Long-distance information.LLMs often face diffi-
culty in identifying the correct answer from external docu-
ments when the information related to the question is distant
from the information related to the answer. This scenario
is quite common as longer texts are frequently encountered
on the internet. In such cases, it is typical for the question’s
information to be initially presented at the start of the doc-
ument and subsequently referred to using pronouns. In Ta-
ble 2, the question information (“Qatar Open 2022”) is only
mentioned once at the beginning and is far from where the
answer text “Anett Kontaveit” appears. This situation may
cause LLMs to depend on information from other docu-
ments and create false impressions, i.e., hallucination.
(2) Evidence uncertainty. Before highly anticipated
events, like the release of new Apple products or the an-
nouncement of the Oscars, there is often a significant
amount of speculative information circulating on the inter-
net. Although the relevant documents explicitly state that
it is uncertain or speculative content, they can still impact
on the retrieval-augmented generation of LLMs. In Table 2,
The Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)
17758Languages English Chinese
Rej Rej∗ Rej Rej∗
ChatGPT 24.67 45.00 5.33 43.33
ChatGLM-6B 9.00 25.00 6.33 17.00
ChatGLM2-6B 10.33 41.33 6.33 36.33
V
icuna-7B-v1.3 17.00 33.33 3.37 24.67
Qwen-7B-Chat 31.00 35.67 8.67 25.33
BELLE-7B-2M 5.67 32.33 5.33 13.67
Table 3: The result of negative rejection. Rej means the re-
jection rate (%) and Rej∗ means the rejection rate evaluated
by ChatGPT. We can see that negative rejection poses a chal-
lenge for RAG in LLMs.
when the noise ratio increases, the content of erroneous
documents is all about some people’s predictions about the
name of the headset (“Apple Reality Pro”). Even if there is
a correct answer (“Vision Pro”) in the relevant documents,
LLMs can still be misled by uncertain evidences.
(3) Concept confusion.The concepts in external docu-
ments may be similar to, but different from, the concepts in
the question. This can cause confusion for LLMs and make
LLMs generate incorrect answers. In Table 2, the model an-
swer focuses on the concept “automotive revenue” in the
document rather than “revenue” in the question.
Based on the analysis above, we have identified certain
limitations in LLMs regarding retrieval-augmented genera-
tion. To effectively handle the vast amount of noise present
on the internet, further detailed enhancements are required
for the model such as long documents modeling and precise
concept comprehension.
Results on Negative Rejection Testbed
We evaluated the rejection rate when only noise documents
were provided. The results are shown in Table 3. In addi-
tion to evaluating the rejection rate through exact matching
(Rej in Table 3), we also utilize ChatGPT to determine if
the responses from the LLMs contain any rejection informa-
tion (Rej∗ in Table 3). We can see that: Negative Rejection
poses a challenge for RAG in LLMs.The highest rejection
rates for LLMs in English and Chinese were only 45% and
43.33%, respectively. This suggests that LLMs can be easily
misled by noisy documents, leading to incorrect answers.
In addition, through comparing Rej and Rej ∗, we found
that LLMs fail to strictly follow instructions, and they often
generate unpredictable responses, which make it hard to use
them as state triggers (such as for recognizing rejection).
We conduct case studies in Table 4. The first error is
because of Evidence uncertainty. Although the document
only mentions contact with “Adam McKay” and does not
explicitly state that he is the director of the movie, the
model still concludes that he holds this role. The first er-
ror is because of Concept confusion. The information pro-
vided in the answer pertains to “the 2018 Winter Olympics”
instead of “the 2022 Olympics” mentioned in the question.
Retrieval-augmented generation poses a greater challenge of
Question Answer Response
who will
direct
Irredeemable film?
Jeymes
Samuel
... Adam McKayto
mo
vie adaptation of
“Irredeemable” from
Which country
w-
on the most medals
at the 2022 Winter
Olympics?
Norway
... that
won the most
medals ... is German-
y. It has won a total
of 31 medals ...
Table 4: Error cases of negative rejection generated by
ChatGLM2-6B. The bold text highlights the error answers.
negative rejection compared to answer directly as it presents
relevant documents that could potentially mislead the LLMs
and result in incorrect responses. In future developments, it
will be crucial for LLMs to enhance their ability to accu-
rately match questions with the appropriate documents.
Results on Information Integration Testbed
We evaluated the accuracy based on the different noise ratios
in external documents, and the results are shown in Table 5.
When comparing the model to Table 1, we observed that
it has a weak information integration ability, which in turn
affects its noise robustness. We can see that:
(1) Information integration poses a challenge for RAG
in LLMs.Even without noise, the highest accuracy of LLMs
can only reach 60% and 67% for English and Chinese,
respectively. After adding noise, the highest accuracy de-
creases to 43% and 55%. These results suggest that LLMs
struggle with integrating information effectively and are not
well-suited for directly answering complex questions.
(2) Complex questions are more challenging for RAG
with noisy documents.Performance decline becomes sig-
nificant when the noise ratio is 0.4, but for simple problems,
a significant decline occurs only at a noise ratio of 0.8 at a
significance level of 0.05. This indicates that complex prob-
lems are more vulnerable to interference from noise. We
speculate that this is because solving complex problems re-
quires integrating information from multiple documents, and
this information can be considered as noise to each other,
making it harder for the model to extract relevant informa-
tion from the documents.
Error Analysis We conducted an error analysis on
ChatGLM2-6B (noise ratio is 0). Apart from the similar er-
rors founded in the noise robustness experiment (38% of the
total), there are also three types of unique errors. We have
presented these cases in Table 6.
(1) Merging Error (28% of the total).The model some-
times merges the answers of the two sub-questions, resulting
in an error. It mistakenly uses the answer from one question
to address both two questions. At this point, the model will
disregard any documents related to one sub-question. For
example, in Table 6, it incorrectly states that Group D is the
World Cup group for both France and Germany, while in fact
Germany is actually assigned to Group E.
(2) Ignoring Error (28% of the total).Sometimes, the
The Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)
17759English Chinese
Noise Ratio 0 0.2
0.4 0 0.2
0.4
ChatGPT 55 51
34 63 58 47
ChatGLM-6B 45 36
35 60 53
52
ChatGLM2-6B 34 32
21 44 43
32
Vicuna-7B-v1.3 60 53
43 43 36
25
Qwen-7B-Chat 55 50
37 67 56 55
BELLE-7B-2M 40 34
24 49 41
38
Table 5: The experimental result of information integration
measured by accuracy (%) under different noise ratios.
Question Answer Response Errors
What groupings
are
France and
Germany in Wo-
rld Cup 2022?
Group
D
Group E
France and
German
y are
Group D...
Merging
Err
or
Who were
the
MVP of Super
Bowl 2022 and
2023?
Cooper
Kupp
P
atrick
Mahomes
MVP of
Super
Bowl LVI was
Cooper Kupp
Ignoring
Error
What films
won
the 2022 and
2023 Academy
Awards for Best
Picture?
CODA
Everything
Everywher
e
All at Once
CODA w
on
award for
Best Picture
at the 95th ...
Misali
gnment
Error
Table 6: Error cases of information integration, the re-
sponses are generated by ChatGLM2-6B. The bold and ital-
icized texts represent the answers to two sub-questions.
model may ignore one of the sub-questions and only answer
the other. This error occurs when the model lacks a complete
understanding of the problem and fails to recognize that it
consists of multiple sub-problems. As a result, the model
only considers relevant documents for one sub-problem in
order to generate an answer, disregarding the question posed
by another sub-problem. For example, in Table 6, the model
only provides the answer for the MVP of Super Bowl 2022
and does not consider 2023.
(3) Misalignment Error (6% of the total).Sometimes,
the model incorrectly identifies the documents for one sub-
question as the documents for another sub-question, leading
to misaligned answers. For example, in Table 6, the third an-
swer has two errors: an ignoring error and a misalignment er-
ror. Firstly, the model only mentioned the Best Picture of the
2023 (95th) Academy Awards, completely disregarding the
2022 awards. Additionally, it incorrectly stated that “CODA”
is the Best Picture of 2023 when it was actually awarded as
the Best Picture in 2022.
The errors mentioned above are primarily caused by the
limited understanding of complex questions, which hinders
the ability to effectively utilize information from different
sub-problems. The key lies in improving the model’s rea-
soning capability. One possible solution is to use a chain-of-
thought approach to break down complex problems (Zhou
et al. 2023a; Xu et al. 2023b; Drozdov et al. 2023). How-
Acc Accdoc ED
ED∗ CR
ChatGPT-zh 91 17 1 3
33.33
Qwen-7B-Chat-zh 77 12
5 4 25.00
ChatGPT-en 89 9 8
7 57.14
Table 7: The result of counterfactual robustness. ACC is the
accuracy (%) of LLMs without external documents. ACCdoc
is the accuracy (%) of LLMs with counterfactual documents.
ED and ED∗ are error detection rates evaluated by exact
matching and ChatGPT, respectively. CR is the error cor-
rection rate.
ever, these methods slow down the inference speed and can-
not provide timely responses.
Results on Counterfactual Robustness Testbed
In order to ensure that LLMs possess relevant knowledge,
we assess their performance by directly asking them ques-
tions. However, we found that most LLMs struggle to an-
swer them correctly. To ensure a more reasonable evalua-
tion, we only consider LLMs that have an accuracy rate of
over 70% as this threshold is relatively high and encom-
passes more LLMs. The results are shown in Table 7. We
present the following metrics: accuracy without any docu-
ments, accuracy with counterfactual documents, error de-
tection rates, and error correction rates. We can see that It
is hard for LLMs to identify and correct factual errors in the
documents. This suggests that the model can be easily mis-
led by documents containing incorrect facts.
It is important to note that retrieval-augmented generation
is not designed to automatically address factual errors within
a given context, as this contradicts the underlying assump-
tion that the model lacks knowledge and relies on retrieved
documents for additional information. However, this issue is
crucial in practical applications due to the abundance of fake
news on the internet. Existing LLMs do not have a safeguard
to handle inaccurate responses caused by misinformation. In
fact, they heavily depend on the information they retrieve.
Even when LLMs contain the internal knowledge about the
questions, they often trust false information that is retrieved.
This presents significant a challenge for the future develop-
ment of RAG in LLMs.
Conclusion
In this paper, we evaluated four abilities of retrieval-
augmented generation in LLMs: noise robustness, nega-
tive rejection, information integration, and counterfactual
robustness. To conduct the evaluation, we built Retrieval-
Augmented Generation Benchmark (RGB). The instances of
RGB are generated from latest news articles and the external
documents obtained from search engines. The experimental
results suggest that current LLMs have limitations in the 4
abilities. This indicates that there is still a significant amount
of work needed to effectively apply RAG to LLMs. To en-
sure accurate and reliable responses from LLMs, it is crucial
to exercise caution and carefully design for RAG.
The Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)
17760Acknowledgements
This research work is supported by the National Natural
Science Foundation of China under Grants no. 62122077,
62106251, 62306303, the CAS Project for Young Scien-
tists in Basic Research under Grant No.YSBR-040 and
Beijing Municipal Science & Technology Comission No.
Z231100010323002. Xianpei Han is sponsored by CCF-
BaiChuan-Ebtech Foundation Model Fund.
References
Adlakha, V .; BehnamGhader, P.; Lu, X. H.; Meade, N.; and
Reddy, S. 2023. Evaluating Correctness and Faithfulness
of Instruction-Following Models for Question Answering.
arXiv:2307.16877.
Bai, J.; Bai, S.; Chu, Y .; Cui, Z.; Dang, K.; Deng, X.; Fan,
Y .; Ge, W.; Han, Y .; Huang, F.; Hui, B.; Ji, L.; Li, M.; Lin,
J.; Lin, R.; Liu, D.; Liu, G.; Lu, C.; Lu, K.; Ma, J.; Men,
R.; Ren, X.; Ren, X.; Tan, C.; Tan, S.; Tu, J.; Wang, P.;
Wang, S.; Wang, W.; Wu, S.; Xu, B.; Xu, J.; Yang, A.; Yang,
H.; Yang, J.; Yang, S.; Yao, Y .; Yu, B.; Yuan, H.; Yuan, Z.;
Zhang, J.; Zhang, X.; Zhang, Y .; Zhang, Z.; Zhou, C.; Zhou,
J.; Zhou, X.; and Zhu, T. 2023. Qwen Technical Report.
arXiv preprint arXiv:2309.16609.
Bang, Y .; Cahyawijaya, S.; Lee, N.; Dai, W.; Su, D.; Wilie,
B.; Lovenia, H.; Ji, Z.; Yu, T.; Chung, W.; Do, Q. V .; Xu,
Y .; and Fung, P. 2023. A Multitask, Multilingual, Multi-
modal Evaluation of ChatGPT on Reasoning, Hallucination,
and Interactivity. arXiv:2302.04023.
BELLEGroup. 2023. BELLE: Be Everyone’s Large
Language model Engine. https://github.com/LianjiaTech/
BELLE. Accessed: 2024-01-10.
Bian, N.; Liu, P.; Han, X.; Lin, H.; Lu, Y .; He, B.; and
Sun, L. 2023. A Drop of Ink Makes a Million Think: The
Spread of False Information in Large Language Models.
arXiv:2305.04812.
Borgeaud, S.; Mensch, A.; Hoffmann, J.; Cai, T.; Ruther-
ford, E.; Millican, K.; van den Driessche, G.; Lespiau, J.-B.;
Damoc, B.; Clark, A.; de Las Casas, D.; Guy, A.; Menick, J.;
Ring, R.; Hennigan, T.; Huang, S.; Maggiore, L.; Jones, C.;
Cassirer, A.; Brock, A.; Paganini, M.; Irving, G.; Vinyals,
O.; Osindero, S.; Simonyan, K.; Rae, J. W.; Elsen, E.; and
Sifre, L. 2022. Improving language models by retrieving
from trillions of tokens. arXiv:2112.04426.
Cai, D.; Wang, Y .; Bi, W.; Tu, Z.; Liu, X.; Lam, W.; and
Shi, S. 2019a. Skeleton-to-Response: Dialogue Genera-
tion Guided by Retrieval Memory. In Proceedings of the
2019 Conference of the North American Chapter of the As-
sociation for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers), 1219–
1228. Minneapolis, Minnesota: Association for Computa-
tional Linguistics.
Cai, D.; Wang, Y .; Bi, W.; Tu, Z.; Liu, X.; and Shi, S.
2019b. Retrieval-guided Dialogue Response Generation via
a Matching-to-Generation Framework. InProceedings of
the 2019 Conference on Empirical Methods in Natural Lan-
guage Processing and the 9th International Joint Confer-
ence on Natural Language Processing (EMNLP-IJCNLP),
1866–1875. Hong Kong, China: Association for Computa-
tional Linguistics.
Cao, M.; Dong, Y .; Wu, J.; and Cheung, J. C. K. 2020. Fac-
tual Error Correction for Abstractive Summarization Mod-
els. InProceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP), 6251–
6258. Online: Association for Computational Linguistics.
Chang, Y .; Wang, X.; Wang, J.; Wu, Y .; Yang, L.; Zhu,
K.; Chen, H.; Yi, X.; Wang, C.; Wang, Y .; Ye, W.;
Zhang, Y .; Chang, Y .; Yu, P. S.; Yang, Q.; and Xie, X.
2023. A Survey on Evaluation of Large Language Models.
arXiv:2307.03109.
Chiang, W.-L.; Li, Z.; Lin, Z.; Sheng, Y .; Wu, Z.; Zhang, H.;
Zheng, L.; Zhuang, S.; Zhuang, Y .; Gonzalez, J. E.; Stoica,
I.; and Xing, E. P. 2023. Vicuna: An Open-Source Chatbot
Impressing GPT-4 with 90%* ChatGPT Quality.
Cui, J.; Li, Z.; Yan, Y .; Chen, B.; and Yuan, L. 2023. Chat-
Law: Open-Source Legal Large Language Model with Inte-
grated External Knowledge Bases. arXiv:2306.16092.
Drozdov, A.; Sch ¨arli, N.; Aky ¨urek, E.; Scales, N.; Song,
X.; Chen, X.; Bousquet, O.; and Zhou, D. 2023. Compo-
sitional Semantic Parsing with Large Language Models. In
The Eleventh International Conference on Learning Repre-
sentations.
Edward Beeching, N. H. S. H. N. L. N. R. O. S. L. T.
T. W., Cl ´ementine Fourrier. 2023. Open LLM Leader-
board. https://huggingface.co/spaces/HuggingFaceH4/
open
llm leaderboard. Accessed: 2024-01-10.
Guo, B.; Zhang, X.; Wang, Z.; Jiang, M.; Nie, J.; Ding, Y .;
Yue, J.; and Wu, Y . 2023. How Close is ChatGPT to Hu-
man Experts? Comparison Corpus, Evaluation, and Detec-
tion. arXiv:2301.07597.
Guu, K.; Lee, K.; Tung, Z.; Pasupat, P.; and Chang, M.-W.
2020. REALM: Retrieval-Augmented Language Model Pre-
Training. InProceedings of the 37th International Confer-
ence on Machine Learning, ICML’20. JMLR.org.
He, H.; Zhang, H.; and Roth, D. 2022. Rethinking
with Retrieval: Faithful Large Language Model Inference.
arXiv:2301.00303.
Hendrycks, D.; Burns, C.; Basart, S.; Zou, A.; Mazeika, M.;
Song, D.; and Steinhardt, J. 2021. Measuring Massive Mul-
titask Language Understanding. InInternational Conference
on Learning Representations.
Huang, Y .; Bai, Y .; Zhu, Z.; Zhang, J.; Zhang, J.; Su, T.;
Liu, J.; Lv, C.; Zhang, Y .; Lei, J.; Fu, Y .; Sun, M.; and He,
J. 2023. C-Eval: A Multi-Level Multi-Discipline Chinese
Evaluation Suite for Foundation Models. arXiv preprint
arXiv:2305.08322.
Izacard, G.; and Grave, E. 2021. Leveraging Passage Re-
trieval with Generative Models for Open Domain Ques-
tion Answering. InProceedings of the 16th Conference of
the European Chapter of the Association for Computational
Linguistics: Main Volume, 874–880. Online: Association for
Computational Linguistics.
Izacard, G.; Lewis, P.; Lomeli, M.; Hosseini, L.; Petroni,
F.; Schick, T.; Dwivedi-Yu, J.; Joulin, A.; Riedel, S.; and
The Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)
17761Grave, E. 2022. Atlas: Few-shot Learning with Retrieval
Augmented Language Models. arXiv:2208.03299.
Ji, Z.; Lee, N.; Frieske, R.; Yu, T.; Su, D.; Xu, Y .; Ishii, E.;
Bang, Y . J.; Madotto, A.; and Fung, P. 2023. Survey of Hal-
lucination in Natural Language Generation.ACM Comput.
Surv., 55(12).
Lewis, P.; Perez, E.; Piktus, A.; Petroni, F.; Karpukhin, V .;
Goyal, N.; K ¨uttler, H.; Lewis, M.; Yih, W.-t.; Rockt ¨aschel,
T.; Riedel, S.; and Kiela, D. 2020. Retrieval-Augmented
Generation for Knowledge-Intensive NLP Tasks. InPro-
ceedings of the 34th International Conference on Neural
Information Processing Systems, NIPS’20. Red Hook, NY ,
USA: Curran Associates Inc. ISBN 9781713829546.
Li, D.; Rawat, A. S.; Zaheer, M.; Wang, X.; Lukasik, M.;
Veit, A.; Yu, F.; and Kumar, S. 2023a. Large Language
Models with Controllable Working Memory. InFindings of
the Association for Computational Linguistics: ACL 2023,
1774–1793. Toronto, Canada: Association for Computa-
tional Linguistics.
Li, X.; Zhang, T.; Dubois, Y .; Taori, R.; Gulrajani, I.;
Guestrin, C.; Liang, P.; and Hashimoto, T. B. 2023b. Al-
pacaEval: An Automatic Evaluator of Instruction-following
Models. https://github.com/tatsu-lab/alpaca
eval. Accessed:
2024-01-10.
Li, X.; Zhu, X.; Ma, Z.; Liu, X.; and Shah, S. 2023c. Are
ChatGPT and GPT-4 General-Purpose Solvers for Financial
Text Analytics? An Examination on Several Typical Tasks.
arXiv:2305.05862.
Liu, N. F.; Zhang, T.; and Liang, P. 2023. Evaluating Verifi-
ability in Generative Search Engines. arXiv:2304.09848.
Maynez, J.; Narayan, S.; Bohnet, B.; and McDonald, R.
2020. On Faithfulness and Factuality in Abstractive Sum-
marization. InProceedings of the 58th Annual Meeting of
the Association for Computational Linguistics, 1906–1919.
Online: Association for Computational Linguistics.
OpenAI. 2022. Chatgpt: Optimizing language models for
dialogue. https://openai.com/blog/chatgpt. Accessed: 2024-
01-10.
Peng, B.; Galley, M.; He, P.; Cheng, H.; Xie, Y .; Hu, Y .;
Huang, Q.; Liden, L.; Yu, Z.; Chen, W.; and Gao, J. 2023.
Check Your Facts and Try Again: Improving Large Lan-
guage Models with External Knowledge and Automated
Feedback. arXiv:2302.12813.
Qin, Y .; Liang, S.; Ye, Y .; Zhu, K.; Yan, L.; Lu, Y .; Lin, Y .;
Cong, X.; Tang, X.; Qian, B.; Zhao, S.; Tian, R.; Xie, R.;
Zhou, J.; Gerstein, M.; Li, D.; Liu, Z.; and Sun, M. 2023.
ToolLLM: Facilitating Large Language Models to Master
16000+ Real-world APIs. arXiv:2307.16789.
Raunak, V .; Menezes, A.; and Junczys-Dowmunt, M. 2021.
The Curious Case of Hallucinations in Neural Machine
Translation. InProceedings of the 2021 Conference of the
North American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies, 1172–
1183. Online: Association for Computational Linguistics.
Ren, R.; Wang, Y .; Qu, Y .; Zhao, W. X.; Liu, J.; Tian, H.;
Wu, H.; Wen, J.-R.; and Wang, H. 2023. Investigating the
Factual Knowledge Boundary of Large Language Models
with Retrieval Augmentation. arXiv:2307.11019.
Shen, X.; Chen, Z.; Backes, M.; and Zhang, Y . 2023. In
ChatGPT We Trust? Measuring and Characterizing the Re-
liability of ChatGPT. arXiv:2304.08979.
Shi, W.; Min, S.; Yasunaga, M.; Seo, M.; James, R.;
Lewis, M.; Zettlemoyer, L.; and tau Yih, W. 2023. RE-
PLUG: Retrieval-Augmented Black-Box Language Models.
arXiv:2301.12652.
THUDM. 2023a. ChatGLM-6B. https://github.com/
THUDM/ChatGLM-6B. Accessed: 2024-01-10.
THUDM. 2023b. ChatGLM2-6B. https://github.com/
THUDM/ChatGLM2-6B. Accessed: 2024-01-10.
Trivedi, H.; Balasubramanian, N.; Khot, T.; and Sabharwal,
A. 2023. Interleaving Retrieval with Chain-of-Thought Rea-
soning for Knowledge-Intensive Multi-Step Questions. In
Proceedings of the 61st Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long Papers),
10014–10037. Toronto, Canada: Association for Computa-
tional Linguistics.
Wang, A.; Pruksachatkun, Y .; Nangia, N.; Singh, A.;
Michael, J.; Hill, F.; Levy, O.; and Bowman, S. R. 2019a.Su-
perGLUE: A Stickier Benchmark for General-Purpose Lan-
guage Understanding Systems. Red Hook, NY , USA: Curran
Associates Inc.
Wang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and
Bowman, S. R. 2019b. GLUE: A Multi-Task Benchmark
and Analysis Platform for Natural Language Understanding.
InInternational Conference on Learning Representations.
Xu, G.; Liu, J.; Yan, M.; Xu, H.; Si, J.; Zhou, Z.; Yi, P.;
Gao, X.; Sang, J.; Zhang, R.; Zhang, J.; Peng, C.; Huang, F.;
and Zhou, J. 2023a. CValues: Measuring the Values of Chi-
nese Large Language Models from Safety to Responsibility.
arXiv:2307.09705.
Xu, S.; Pang, L.; Shen, H.; Cheng, X.; and Chua, T.-
S. 2023b. Search-in-the-Chain: Towards Accurate, Credi-
ble and Traceable Large Language Models for Knowledge-
intensive Tasks. arXiv:2304.14732.
Zhang, W.; Aljunied, S. M.; Gao, C.; Chia, Y . K.; and Bing,
L. 2023. M3Exam: A Multilingual, Multimodal, Multilevel
Benchmark for Examining Large Language Models.
Zhong, W.; Cui, R.; Guo, Y .; Liang, Y .; Lu, S.; Wang,
Y .; Saied, A.; Chen, W.; and Duan, N. 2023. AGIEval:
A Human-Centric Benchmark for Evaluating Foundation
Models. arXiv:2304.06364.
Zhou, D.; Sch ¨arli, N.; Hou, L.; Wei, J.; Scales, N.; Wang,
X.; Schuurmans, D.; Cui, C.; Bousquet, O.; Le, Q. V .; and
Chi, E. H. 2023a. Least-to-Most Prompting Enables Com-
plex Reasoning in Large Language Models. InThe Eleventh
International Conference on Learning Representations.
Zhou, S.; Alon, U.; Xu, F. F.; Jiang, Z.; and Neubig, G.
2023b. DocPrompting: Generating Code by Retrieving the
Docs. InThe Eleventh International Conference on Learn-
ing Representations.
The Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)
17762

Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 11897–11916
August 11-16, 2024 ©2024 Association for Computational Linguistics
Improving Text Embeddings with Large Language Models
Liang Wang, Nan Yang, Xiaolong Huang,
Linjun Yang, Rangan Majumder, Furu Wei
Microsoft Corporation
{wangliang,nanya,xiaolhu,yang.linjun,ranganm,fuwei}@microsoft.com
Abstract
In this paper, we introduce a novel and simple
method for obtaining high-quality text embed-
dings using only synthetic data and less than
1k training steps. Unlike existing methods that
often depend on multi-stage intermediate pre-
training with billions of weakly-supervised
text pairs, followed by ﬁne-tuning with a few
labeled datasets, our method does not require
building complex training pipelines or rely-
ing on manually collected datasets that are of-
ten constrained by task diversity and language
coverage. We leverage proprietary LLMs to
generate diverse synthetic data for hundreds
of thousands of text embedding tasks across
93 languages. We then ﬁne-tune open-source
decoder-only LLMs on the synthetic data us-
ing standard contrastive loss. Experiments
demonstrate that our method achieves strong
performance on highly competitive text em-
bedding benchmarks without using any la-
beled data. Furthermore, when ﬁne-tuned with
a mixture of synthetic and labeled data, our
model sets new state-of-the-art results on the
BEIR and MTEB benchmarks.
1 Introduction
Text embeddings are vector representations of nat-
ural language that encode its semantic information.
They are widely used in various natural language
processing (NLP) tasks, such as information re-
trieval (IR), question answering, semantic textual
similarity, bitext mining, item recommendation, etc.
In the ﬁeld of IR, the ﬁrst-stage retrieval often relies
on text embeddings to efﬁciently recall a small set
of candidate documents from a large-scale corpus
using approximate nearest neighbor search tech-
niques. Embedding-based retrieval is also a cru-
cial component of retrieval-augmented generation
(RAG) (Lewis et al., 2020), which is an emerg-
ing paradigm that enables large language mod-
els (LLMs) to access dynamic external knowledge
without modifying the model parameters. Source
attribution of generated text is another important
application of text embeddings (Gao et al., 2023)
that can improve the interpretability and trustwor-
thiness of LLMs.
Previous studies have demonstrated that
weighted average of pre-trained word embed-
dings (Pennington et al., 2014; Arora et al.,
2017) is a strong baseline for measuring semantic
similarity. However, these methods fail to capture
the rich contextual information of natural language.
With the advent of pre-trained language models
(Devlin et al., 2019), Sentence-BERT (Reimers
and Gurevych, 2019) and SimCSE (Gao et al.,
2021) have been proposed to learn text embed-
dings by ﬁne-tuning BERT on natural language
inference (NLI) datasets. To further enhance the
performance and robustness of text embeddings,
state-of-the-art methods like E5 (Wang et al.,
2022b) and BGE (Xiao et al., 2023) employ a more
complex multi-stage training paradigm that ﬁrst
pre-trains on billions of weakly-supervised text
pairs, and then ﬁne-tunes on several high-quality
labeled datasets.
Existing multi-stage approaches suffer from sev-
eral drawbacks. Firstly, they entail a complex multi-
stage training pipeline that demands substantial
engineering efforts to curate large amounts of rele-
vance pairs. Secondly, they rely on manually col-
lected datasets that are often constrained by the
diversity of tasks and the coverage of languages.
For instance, Instructor (Su et al., 2023) is only
trained on instructions from 330 English datasets,
whereas BGE (Xiao et al., 2023) only focuses on
high-resource languages such as English and Chi-
nese. Moreover, most existing methods employ
BERT-style encoders as the backbone, neglecting
the recent advances of training better LLMs and
related techniques such as context length exten-
sion (Rozière et al., 2023).
In this paper, we propose a novel method for text
embeddings that leverages LLMs to overcome the
11897limitations of existing approaches. We use propri-
etary LLMs to generate synthetic data for a diverse
range of text embedding tasks in93 languages, cov-
ering hundreds of thousands of embedding tasks.
Speciﬁcally, we use a two-step prompting strategy
that ﬁrst prompts the LLMs to brainstorm a pool
of candidate tasks, and then prompts the LLMs
to generate data conditioned on a given task from
the pool. To cover various application scenarios,
we design multiple prompt templates for each task
type and combine the generated data from differ-
ent templates to boost diversity. For the text em-
bedding models, we opt for ﬁne-tuning powerful
open-source LLMs rather than small BERT-style
models. Since LLMs such as Mistral (Jiang et al.,
2023) have been extensively pre-trained on web-
scale data, contrastive pre-training that proves to be
important for BERT models (Wang et al., 2022b)
offers little additional beneﬁt.
We demonstrate that Mistral-7B, when ﬁne-
tuned solely on synthetic data, attains competitive
performance on the BEIR (Thakur et al., 2021)
and MTEB (Muennighoff et al., 2023) benchmarks.
This is particularly intriguing considering that this
setting does not involve any labeled data. When
ﬁne-tuned on a mixture of synthetic and labeled
data, our model achieves new state-of-the-art re-
sults, surpassing previous methods by a signiﬁcant
margin (+2%). The entire training process requires
less than 1k steps.
Moreover, we empirically validate that our
model can effectively perform personalized
passkey retrieval for inputs up to 32k tokens by
altering the rotation base of the position embed-
dings, extending the context length beyond the
conventional 512 token limit. Regarding its mul-
tilinguality, our model excels on high-resource
languages. However, for low-resource languages,
there is still room for improvement as current open-
source LLMs are not adequately pre-trained on
them.
2 Related Work
Text Embeddings are continuous low-
dimensional representations of text and have been
extensively applied to various downstream tasks
such as information retrieval, question answering,
and retrieval-augmented generation (RAG). Early
work on text embeddings includes latent semantic
indexing (Deerwester et al., 1990) and weighted
average of word embeddings (Mikolov et al.,
2013). More recent methods exploit supervision
from natural language inference (Bowman et al.,
2015) and labeled query-document pairs, such as
the MS-MARCO passage ranking dataset (Campos
et al., 2016), to train text embeddings (Reimers
and Gurevych, 2019; Conneau et al., 2017; Gao
et al., 2021). However, labeled data are often
limited in terms of task diversity and language
coverage. To address this challenge, methods
like Contriever (Izacard et al., 2021), OpenAI
Embeddings (Neelakantan et al., 2022), E5 (Wang
et al., 2022b), and BGE (Xiao et al., 2023)
adopt a multi-stage training paradigm. They ﬁrst
pre-train on large-scale weakly-supervised text
pairs using contrastive loss and then ﬁne-tune
on small-scale but high-quality datasets. In this
paper, we demonstrate that it is possible to obtain
state-of-the-art text embeddings with single-stage
training.
Synthetic Data Synthetic data generation is a
widely studied topic in information retrieval re-
search, with various methods proposed to enhance
retrieval systems with artiﬁcially created data. For
instance, Doc2query (Nogueira et al., 2019), InPars
(Bonifacio et al., 2022), and Promptagator (Dai
et al., 2022) generate synthetic queries for unla-
beled documents, which are then leveraged for doc-
ument expansion or model training. GPL (Wang
et al., 2022a) employs a cross-encoder to produce
pseudo-labels for query-document pairs. Simi-
larly, Query2doc (Wang et al., 2023) generates
pseudo-documents for query expansion by few-
shot prompting LLMs. Unlike these methods, our
approach does not rely on any unlabeled documents
or queries and thus can generate more diverse syn-
thetic data.
Another related line of work focuses on
knowledge distillation from black-box LLMs by
training on synthetic data generated from them.
DINO (Schick and Schütze, 2021) generates
synthetic text pairs for semantic textual similarity.
Unnatural Instructions (Honovich et al., 2022) is a
synthetic instruction following dataset by prompt-
ing existing LLMs. Orca (Mukherjee et al., 2023)
and Phi (Gunasekar et al., 2023) propose to train
better small language models by using high-quality
synthetic data from GPT-3.5/4 (OpenAI, 2023).
Large Language Models With the populariza-
tion of ChatGPT, large language models (LLMs)
have demonstrated remarkable capabilities in in-
11898You have been assigned a retrieval task: {task}
Your mission is to write one text retrieval example for this task in JSON format. The JSON object must 
contain the following keys:
  - "user_query": a string, a random user search query specified by the retrieval task.
  - "positive_document": a string, a relevant document for the user query.
  - "hard_negative_document": a string, a hard negative document that only  appears relevant to the query.
Please adhere to the following guidelines:
  - The "user_query" should be {query_type}, {query_length}, {clarity}, and diverse in topic.
  - All documents should be at least {num_words} words long.
  - Both the query and documents should be in {language}.
  … (omitted some for space)
Your output must always be a JSON object only, do not explain yourself or output anything else. Be creative!
{"user_query": "How to use Microsoft Power BI for data analysis" ,
"positive_document": "Microsoft Power BI is a sophisticated tool that requires time and practice to 
master. In this tutorial, we'll show you how to navigate Power BI … (omitted) ",
“hard_negative_document”: “Excel is an incredibly powerful tool for managing and analyzing large 
amounts of data. Our tutorial series focuses on how you…(omitted)” } 
Brainstorm a list of potentially useful text retrieval tasks.
Here are a few examples for your reference:
    - Provided a scientific claim as query, retrieve documents that help verify or refute the claim.
    - Search for documents that answers a FAQ-style query on children's nutrition.
Please adhere to the following guidelines:
    - Specify what the query is, and what the desired documents are.
    - Each retrieval task should cover a wide range of queries, and should not be too specific.
Your output should always be a python list of strings only, with about 20 elements, and each element 
corresponds to a distinct retrieval task in one sentence. Do not explain yourself or output anything else. Be 
creative!
["Retrieve company's financial reports for a given stock ticker symbol. " ,
"Given a book name as a query, retrieve reviews, ratings and summaries of that book. " ,
"Search for scientific research papers supporting a medical diagnosis for a specified disease. “
… (omitted for space)]
new session
Figure 1: An example two-step prompt template for generating synthetic data with GPT-4. We ﬁrst prompt GPT-4
to brainstorm a list of potential retrieval tasks, and then generate (query, positive, hard negative) triplets for each
task. “{...}” denotes a placeholder that will be replaced by sampling from a predeﬁned set of values. Full prompts
are available in Appendix C.
struction following and few-shot in-context learn-
ing (Brown et al., 2020). However, the most ad-
vanced LLMs such as GPT-4 (OpenAI, 2023) are
proprietary and have little technical details dis-
closed. To bridge the gap between proprietary and
open-source LLMs, several notable efforts have
been made, such as LLaMA-2 (Touvron et al.,
2023) and Mistral (Jiang et al., 2023) models. A
major limitation of LLMs is that they lack aware-
ness of recent events and private knowledge. This
issue can be partly mitigated by augmenting LLMs
with information retrieved from external sources,
a technique known as retrieval-augmented gener-
ation (RAG). On the other hand, LLMs can also
serve as foundation models to enhance text embed-
dings. RepLLaMA (Ma et al., 2023) proposes to
ﬁne-tune LLaMA-2 with bi-encoder architecture
for ad-hoc retrieval. SGPT (Muennighoff, 2022),
GTR (Ni et al., 2022b), and Udever (Zhang et al.,
2023a) demonstrate the scaling law of text em-
beddings empirically, but their performance still
falls behind small bidirectional encoders such as
E5 (Wang et al., 2022b) and BGE (Xiao et al.,
2023). In this paper, we present a novel approach to
train state-of-the-art text embeddings by exploiting
the latest advances of LLMs and synthetic data.
3 Method
3.1 Synthetic Data Generation
Utilizing synthetic data generated by advanced
LLMs such as GPT-4 presents a compelling oppor-
tunity, especially in terms of enhancing diversity
11899across a multitude of tasks and languages. Such
diversity is essential for developing robust text em-
beddings that can perform well across different
tasks, be it semantic retrieval, textual similarity, or
clustering.
To generate diverse synthetic data, we propose
a simple taxonomy that categorizes embedding
tasks into several groups, and then apply different
prompt templates to each group.
Asymmetric Tasks This category comprises
tasks where the query and document are seman-
tically related but are not paraphrases of each
other. Depending on the length of the query and
document, we further divide asymmetric tasks
into four subgroups: short-long match, long-short
match, short-short match, and long-long match.
For instance, short-long match tasks involve a
short query and a long document, which is a
typical scenario in commercial search engines.
For each subgroup, we design a two-step prompt
template that ﬁrst prompts LLMs brainstorm a list
of tasks, and then generates a concrete example
conditioned on the task deﬁnition. In Figure 1, we
show an example prompt for the short-long match
subgroup. The full output is available in Table 16.
The outputs from GPT-4 are mostly coherent and
of high quality. In our preliminary experiments,
we also attempted to generate the task deﬁnition
and query-document pairs using a single prompt,
but the data diversity was not as satisfactory as the
proposed two-step approach.
Symmetric Tasks Symmetric tasks involve
queries and documents that have similar semantic
meanings but different surface forms. We examine
two application scenarios: monolingual semantic
textual similarity (STS) and bitext retrieval. We
design two distinct prompt templates for each
scenario, tailored to their speciﬁc objectives. Since
the task deﬁnition is straightforward, we omit the
brainstorming step for symmetric tasks.
To further boost the diversity of the prompts
and thus the synthetic data, we incorporate several
placeholders in each prompt template, whose val-
ues are randomly sampled at runtime. For example,
in Figure 1, the value of “ {query_length}” is sam-
pled from the set “{less than 5 words, 5-10 words,
at least 10 words}”.
To generate multilingual data, we sample the
value of “{language}” from the language list of
XLM-R (Conneau et al., 2020), giving more weight
to high-resource languages. Any generated data
that does not conform to the predeﬁned JSON for-
mat are discarded during the parsing process. We
also remove duplicates based on exact string match-
ing.
3.2 Training
Given a relevant query-document pair (q+,d+), we
ﬁrst apply the following instruction template to the
original query q+ to generate a new one q+
inst:
q+
inst = Instruct: {task_deﬁnition} \nQuery: {q+}
(1)
where “{task_deﬁnition}” is a placeholder for a
one-sentence description of the embedding task.
For generated synthetic data, we use the outputs
from the brainstorming step. For other datasets,
such as MS-MARCO, we manually craft the task
deﬁnitions and apply them to all the queries in the
dataset. We do not modify the document side with
any instruction preﬁx. In this way, the document
index can be prebuilt, and we can customize the
task to perform by changing only the query side.
Given a pretrained LLM, we append an [EOS]
token to the end of the query and document, and
then feed them into the LLM to obtain the query
and document embeddings (hq+
inst
,hd+) by taking
the last layer [EOS] vector. To train the embedding
model, we adopt the standard InfoNCE loss L over
the in-batch negatives and hard negatives:
min L = −log φ(q+
inst,d+)
φ(q+
inst,d+) +
∑
ni∈N
(φ(q+
inst,ni))
(2)
where N denotes the set of all negatives, andφ(q,d)
is a function that computes the matching score be-
tween query qand document d. In this paper, we
adopt the temperature-scaled cosine similarity func-
tion as follows:
φ(q,d) =exp(1
τ cos(hq,hd)) (3)
τ is a temperature hyper-parameter, which is ﬁxed
to 0.02 in our experiments.
4 Experiments
4.1 Statistics of the Synthetic Data
Figure 2 presents the statistics of our generated
synthetic data. We manage to generate 500k ex-
amples with 150k unique instructions using Azure
11900short-long
167k
long-short
122k
short-short
13k
long-long
17k
bitext
89k
sts
99k
distribution of task types
English
43.1%
Polish
3.0%
Japanese
2.9%Italian
2.9%Russian
2.9%
Indonesian
2.9%
German 2.9%
Persian 2.9%
Spanish
2.8%
Chinese
2.8%
French
2.8%
Portuguese
2.8%
Dutch
2.8%
Arabic
2.7%
Others
19.8%
distribution of languages
Figure 2: Task type and language statistics of the generated synthetic data (see Section 3.1 for task type deﬁnitions).
The “Others” category contains the remaining languages from the XLM-R language list.
# of datasets→ Class. Clust. PairClass. Rerank Retr. STS Summ. Avg
12 11 3 4 15 10 1 56
Unsupervised Models
Glove (Pennington et al., 2014) 57.3 27.7 70.9 43.3 21.6 61.9 28.9 42.0
SimCSEbert-unsup (Gao et al., 2021) 62.5 29.0 70.3 46.5 20.3 74.3 31.2 45.5
Supervised Models
SimCSEbert-sup (Gao et al., 2021) 67.3 33.4 73.7 47.5 21.8 79.1 23.3 48.7
Contriever (Izacard et al., 2021) 66.7 41.1 82.5 53.1 41.9 76.5 30.4 56.0
GTRxxl (Ni et al., 2022b) 67.4 42.4 86.1 56.7 48.5 78.4 30.6 59.0
Sentence-T5xxl (Ni et al., 2022a) 73.4 43.7 85.1 56.4 42.2 82.6 30.1 59.5
E5large-v2 (Wang et al., 2022b) 75.2 44.5 86.0 56.6 50.6 82.1 30.2 62.3
GTElarge (Li et al., 2023) 73.3 46.8 85.0 59.1 52.2 83.4 31.7 63.1
BGElarge-en-v1.5(Xiao et al., 2023) 76.0 46.1 87.1 60.0 54.3 83.1 31.6 64.2
Ours
E5mistral-7b + full data 78.5 50.3 88.3 60.2 56.9 84.6 31.4 66.6
w/ synthetic data only 78.2 50.5 86.0 59.0 46.9 81.2 31.9 63.1
w/ synthetic + msmarco 78.3 49.9 87.1 59.5 52.2 81.2 32.7 64.5
Table 1: Results on the MTEB benchmark (Muennighoff et al., 2023) (56 datasets in the English subset). The
numbers are averaged for each category. Please refer to Table 17 for the scores per dataset.
OpenAI Service 1, among which 25% are gener-
ated by GPT-35-Turbo and others are generated
by GPT-4. The total token consumption is about
180M. The predominant language is English, with
coverage extending to a total of 93 languages. For
the bottom 75 low-resource languages, there are
about 1k examples per language on average. Please
see Table 16 in the appendix for examples of syn-
thetic data.
In terms of data quality, we ﬁnd that a portion
of GPT-35-Turbooutputs do not strictly follow the
guidelines speciﬁed in the prompt templates. Nev-
ertheless, the overall quality remains acceptable,
and preliminary experiments have demonstrated
the beneﬁts of incorporating this data subset.
1https://oai.azure.com/
4.2 Model Fine-tuning and Evaluation
The pretrained Mistral-7b (Jiang et al., 2023) check-
point is ﬁne-tuned for 1 epoch using the loss
in Equation 2. We follow the training recipe
from RankLLaMA (Ma et al., 2023) and utilize
LoRA (Hu et al., 2022) with rank 16. To further
reduce GPU memory requirement, techniques in-
cluding gradient checkpointing, mixed precision
training, and DeepSpeed ZeRO-3 are applied.
For the training data, we utilize both the gener-
ated synthetic data and a collection of 13 public
datasets, yielding approximately 1.8M examples
after sampling. More details are available in Ap-
pendix A. To provide a fair comparison with some
previous work, we also report results when the only
labeled supervision is the MS-MARCO passage
11901High-resource Languages Low-resource Languages
en fr es ru te hi bn sw
BM25 (Zhang et al., 2023b) 35.1 18.3 31.9 33.4 49.4 45.8 50.8 38.3
mDPR (Zhang et al., 2023b) 39.4 43.5 47.8 40.7 35.6 38.3 44.3 29.9
mE5base (Wang et al., 2024) 51.2 49.7 51.5 61.5 75.2 58.4 70.2 71.1
mE5large (Wang et al., 2024) 52.9 54.5 52.9 67.4 84.6 62.0 75.9 74.9
E5mistral-7b+ full data 57.3 55.2 52.2 67.7 73.9 52.1 70.3 68.4
Table 2: nDCG@10 on the dev set of the MIRACL dataset for both high-resource and low-resource languages.
We select the 4 high-resource languages and the 4 low-resource languages according to the number of candidate
documents. The numbers for BM25 and mDPR come from Zhang et al. (2023b). For the complete results on all
16 languages, please see Table 6.
Retrieval Classification MTEB All
20
30
40
50
60
70
80
90Performance
+8.2
+4.3
+5.7
XLM-R-large + full data
original
w/ cont. pre-train
Retrieval Classification MTEB All
20
30
40
50
60
70
80
90Performance
+0.0
+0.2
+0.1
E5-mistral-7b + full data
original
w/ cont. pre-train
Figure 3: Effects of contrastive pre-training. Detailed numbers are in Appendix Table 7.
ranking (Campos et al., 2016) dataset.
We evaluate the trained model on the MTEB
benchmark (Muennighoff et al., 2023). Note that
the retrieval category in MTEB corresponds to the
15 publicly available datasets in the BEIR bench-
mark (Thakur et al., 2021). Evaluation of one
model takes about 3 days on 8 V100 GPUs due
to the need to encode a large number of documents.
Although our model can accommodate sequence
length beyond 512, we only evaluate on the ﬁrst
512 tokens for efﬁciency. Ofﬁcial metrics are re-
ported for each category. For more details about
the evaluation protocol, please refer to the original
papers (Muennighoff et al., 2023; Thakur et al.,
2021).
4.3 Main Results
In Table 1, our model “E5mistral-7b + full data” at-
tains the highest average score on the MTEB bench-
mark, outperforming the previous state-of-the-art
model by 2.4 points. In the “w/ synthetic data
only” setting, no labeled data is used for training,
and yet the performance remains quite competi-
tive. We posit that generative language modeling
and text embeddings are the two sides of the same
coin, with both tasks requiring the model to have a
deep understanding of the natural language. Given
an embedding task deﬁnition, a truly robust LLM
should be able to generate training data on its own
and then be transformed into an embedding model
through light-weight ﬁne-tuning. Our experiments
shed light on the potential of this direction, and
more research is needed to fully explore it.
Model BEIR MTEB
OpenAI text-embedding-3-large 55.4 64.6
Cohere-embed-english-v3.0 55.0 64.5
voyage-lite-01-instruct 55.6 64.5
UAE-Large-V1 54.7 64.6
E5mistral-7b + full data 56.9 66.6
Table 3: Comparison with commercial models and the
model that tops the MTEB leaderboard (as of 2023-
12-22) (Li and Li, 2023). “BEIR” is the average
nDCG@10 score over 15 public datasets in the BEIR
benchmark (Thakur et al., 2021). “MTEB” is the aver-
age score over 56 datasets in the English subset of the
MTEB benchmark (Muennighoff et al., 2023). For the
commercial models listed here, little details are avail-
able on their model architectures and training data.
In Table 3, we also present a comparison with
several commercial text embedding models. How-
ever, due to the lack of transparency and documen-
tation about these models, a fair comparison is not
feasible. We focus especially on the retrieval per-
11902Query: what is the pass key for Malayah Graves?
Doc1: <prefix filler> Malayah Graves's pass key is 123. Remember it. 123 is the pass key for Malayah Graves. <suffix filler>
Doc2: <prefix filler> Cesar McLean's pass key is 456. Remember it. 456 is the pass key for Cesar McLean. <suffix filler>
……
Figure 4: Illustration of the personalized passkey retrieval task adapted from Mohtashami and Jaggi (2023). The
“<preﬁx ﬁller>” and “<sufﬁx ﬁller>” are repeats of “The grass is green. The sky is blue. The sun is yellow. Here
we go. There and back again. ” In addition, each document has a unique person name and a random passkey
inserted at a random position. The task is to retrieve the document that contains the given person’s passkey from
100 candidates.
256 512 1k 2k 4k 8k 16k 32k
Context Length
0
20
40
60
80
100T op1 Accuracy
window 4k, base 10^4
window 32k, base 10^4
window 32k, base 10^5
window 32k, base 10^6
Figure 5: Accuracy of personalized passkey retrieval as a function of input context length. For each context length,
we randomly generate 50 queries and compute the top-1 accuracy.
formance on the BEIR benchmark, since retrieval-
augmented generation is an emerging technique to
enhance LLM with external knowledge and propri-
etary data. As Table 3 shows, our model outper-
forms the current commercial models by a signiﬁ-
cant margin.
4.4 Multilingual Retrieval
To assess the multilingual capabilities of our
model, we conduct an evaluation on the MIRACL
dataset (Zhang et al., 2023b), which comprises
human-annotated queries and relevance judgments
across 18 languages. The validation set contains
labels for 16 languages. As shown in Table 2,
our model surpasses mE5large on high-resource lan-
guages, notably on English. Nevertheless, for low-
resource languages, our model remains suboptimal
compared to mE5base. We attribute this to the fact
that Mistral-7B is predominantly pre-trained on
English data, and we anticipate that future multilin-
gual LLMs will leverage our method to bridge this
gap.
To evaluate our model’s cross-lingual retrieval
capability, we report Bitext mining results in Ta-
ble 4. For baselines including mContriever (Izac-
ard et al., 2021), LaBSE (Feng et al., 2022), and
mE5 (Wang et al., 2024), we evaluate the results
BUCC 2018
4 langs
Tatoeba
112 langs
mContriever 93.7 37.7
LaBSE 98.8 81.1
mE5base 98.1 68.1
mE5large 98.6 75.7
E5mistral-7b 98.9 70.1
Table 4: Bitext mining results. BUCC 2018 (Zweigen-
baum et al., 2018) contains 4 high-resource languages.
Tatoeba (Artetxe and Schwenk, 2019) consists of 112
English-centric language pairs.
using publicly available checkpoints. Our observa-
tions indicate that, similar to the MIRACL retrieval,
E5mistral-7b excels in bitext mining for high-resource
languages only.
5 Analysis
5.1 Is Contrastive Pre-training Necessary?
Weakly-supervised contrastive pre-training is one
of the key factors behind the success of existing text
embedding models. For instance, Contriever (Izac-
ard et al., 2021) treats random cropped spans as pos-
itive pairs for pre-training, while E5 (Wang et al.,
2022b) and BGE (Xiao et al., 2023) collect and
11903Datasets Class. Clust. PairClass. Rerank Retr. STS Summ. Avg
E5mistral-7b 78.3 49.9 87.1 59.5 52.2 81.2 32.7 64.5
w/ LLaMA-2 7b init. 76.2 48.1 85.1 58.9 49.6 81.2 30.8 62.9 -1.6
w/ msmarco data only 71.6 47.1 86.1 58.8 54.4 79.5 31.7 62.7 -1.8
pooling type
w/ mean pool 77.0 48.9 86.1 59.2 52.4 81.4 30.8 64.1 -0.4
w/ weighted mean 77.0 49.0 86.1 59.2 52.0 81.4 30.2 64.0 -0.5
LoRA rank
w/ r=8 78.4 50.3 87.1 59.3 53.0 81.0 31.7 64.8+0.3
w/ r=32 78.4 50.3 87.4 59.5 52.2 81.2 30.6 64.6 +0.1
instruction type
w/o instruction 72.3 47.1 82.6 56.3 48.2 76.7 30.7 60.3 -4.2
w/ task type preﬁx 71.1 46.5 79.7 54.0 52.7 73.8 30.0 60.3 -4.2
Table 5: Results on the MTEB benchmark with various hyperparameters. The ﬁrst row corresponds to the default
setting, which employs last-token pooling, LoRA rank 16, and natural language instructions. Unless otherwise
stated, all models are trained on the synthetic and MS-MARCO passage ranking data.
ﬁlter text pairs from various sources.
This section re-evaluates the necessity of con-
trastive pre-training for LLMs, particularly those
that have been pre-trained on trillions of tokens.
Figure 3 shows that contrastive pre-training ben-
eﬁts XLM-R large, enhancing its retrieval perfor-
mance by 8.2 points when ﬁne-tuned on the same
data, which aligns with prior ﬁndings. However, for
Mistral-7B based models, contrastive pre-training
has negligible impact on the model quality. This
implies that extensive auto-regressive pre-training
enables LLMs to acquire good text representations,
and only minimal ﬁne-tuning is required to trans-
form them into effective embedding models.
5.2 Extending to Long Text Embeddings
Existing evaluation datasets for text embedding
models are typically short, to evaluate the long-
context capability of our model, we introduce a
novel synthetic task called personalized passkey
retrieval, which is illustrated in Figure 4. This
task requires encoding the passkey information in
a long context into the embeddings. We compare
the performance of different variants by changing
the sliding window size and the RoPE rotation
base (Su et al., 2024) in Figure 5. The results
show that the default conﬁguration with 4k sliding
window attains 100% accuracy within 4k tokens,
but the accuracy deteriorates quickly as the con-
text length grows. Naively extending the sliding
window size to 32k results in worse performance.
By changing the RoPE rotation base to 105, the
model can achieve over 90% accuracy within 32k
tokens. However, this entails a minor trade-off
in performance for shorter contexts. A potential
avenue for future research is to efﬁciently adapt
the model to longer contexts through lightweight
post-training (Zhu et al., 2023).
5.3 Analysis of Training Hyperparameters
Table 5 presents the results under different con-
ﬁgurations. We notice that the Mistral-7B initial-
ization holds an advantage over LLaMA-2 7B, in
line with the ﬁndings from Mistral-7B technical
report (Jiang et al., 2023). The choice of pooling
types and LoRA ranks does not affect the overall
performance substantially, hence we adhere to the
default setting despite the marginal superiority of
LoRA rank 8. On the other hand, the way of adding
instructions has a considerable impact on the per-
formance. We conjecture that natural language
instructions better inform the model regarding the
embedding task at hand, and thus enable the model
to generate more discriminative embeddings. Our
framework also provides a way to customize the
behavior of text embeddings through instructions
without the need to ﬁne-tune the model or re-build
document index.
6 Conclusion
This paper shows that the quality of text embed-
dings can be substantially enhanced by exploiting
LLMs. We prompt proprietary LLMs such as GPT-
4 to generate diverse synthetic data with instruc-
tions in many languages. Combined with the strong
language understanding capability of the Mistral
model, we establish new state-of-the-art results for
nearly all task categories on the competitive MTEB
11904benchmark. The training process is much more
streamlined and efﬁcient than existing multi-stage
approaches, thereby obviating the need for interme-
diate pre-training.
For future work, we aim to further improve the
multilingual performance of our model and explore
the possibility of using open-source LLMs to gen-
erate synthetic data.
Limitations
In comparison to the mainstream BERT-style en-
coders, the employment of LLMs, such as Mistral-
7B, for text embeddings results in a signiﬁcantly
increased inference cost. The development of more
advanced GPUs and better kernel implementations
may enhance the efﬁciency of the inference process.
With regards to storage cost, our model is compara-
tively more expensive, with embeddings of4096 di-
mensions. Early successes in reducing embedding
dimensions while maintaining competitive perfor-
mance have been demonstrated through techniques
such as Matryoshka representation learning (Kusu-
pati et al., 2022).
For synthetic data generation, we rely on manual
prompt engineering to elicit high-quality outputs
from proprietary LLMs. Automatic prompt opti-
mization presents a promising avenue for improv-
ing the quality of synthetic data.
Acknowledgements
We would like to thank anonymous reviewers for
their valuable comments, and ACL 2024 and ACL
Rolling Review organizers for their efforts. Opin-
ions expressed in this paper are solely those of
the authors and do not represent the views of their
employers.
References
Sanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017.
A simple but tough-to-beat baseline for sentence em-
beddings. In 5th International Conference on Learn-
ing Representations, ICLR 2017, Toulon, France,
April 24-26, 2017, Conference Track Proceedings .
OpenReview.net.
Mikel Artetxe and Holger Schwenk. 2019. Mas-
sively multilingual sentence embeddings for zero-
shot cross-lingual transfer and beyond. Transac-
tions of the Association for Computational Linguis-
tics, 7:597–610.
Luiz Henrique Bonifacio, Hugo Abonizio, Marzieh
Fadaee, and Rodrigo Nogueira. 2022. Inpars: Unsu-
pervised dataset generation for information retrieval.
Proceedings of the 45th International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval.
Samuel R. Bowman, Gabor Angeli, Christopher Potts,
and Christopher D. Manning. 2015. A large anno-
tated corpus for learning natural language inference.
In Proceedings of the 2015 Conference on Empiri-
cal Methods in Natural Language Processing, pages
632–642, Lisbon, Portugal. Association for Compu-
tational Linguistics.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen,
Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin
Chess, Jack Clark, Christopher Berner, Sam Mc-
Candlish, Alec Radford, Ilya Sutskever, and Dario
Amodei. 2020. Language models are few-shot learn-
ers. In Advances in Neural Information Processing
Systems 33: Annual Conference on Neural Informa-
tion Processing Systems 2020, NeurIPS 2020, De-
cember 6-12, 2020, virtual.
Daniel Fernando Campos, Tri Nguyen, Mir Rosenberg,
Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan
Majumder, Li Deng, and Bhaskar Mitra. 2016. Ms
marco: A human generated machine reading com-
prehension dataset. ArXiv preprint, abs/1611.09268.
Alexis Conneau, Kartikay Khandelwal, Naman Goyal,
Vishrav Chaudhary, Guillaume Wenzek, Francisco
Guzmán, Edouard Grave, Myle Ott, Luke Zettle-
moyer, and Veselin Stoyanov. 2020. Unsupervised
cross-lingual representation learning at scale. In
Proceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics , pages 8440–
8451, Online. Association for Computational Lin-
guistics.
Alexis Conneau, Douwe Kiela, Holger Schwenk, Loïc
Barrault, and Antoine Bordes. 2017. Supervised
learning of universal sentence representations from
natural language inference data. In Proceedings of
the 2017 Conference on Empirical Methods in Nat-
ural Language Processing, pages 670–680, Copen-
hagen, Denmark. Association for Computational
Linguistics.
Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo
Ni, Jing Lu, Anton Bakalov, Kelvin Guu, Keith
Hall, and Ming-Wei Chang. 2022. Promptagator:
Few-shot dense retrieval from 8 examples. In The
Eleventh International Conference on Learning Rep-
resentations.
DataCanary, hilﬁalkaff, Lili Jiang, Meg Risdal, Nikhil
Dandekar, and tomtung. 2017. Quora question pairs.
Scott Deerwester, Susan T Dumais, George W Fur-
nas, Thomas K Landauer, and Richard Harshman.
119051990. Indexing by latent semantic analysis. Jour-
nal of the American society for information science ,
41(6):391–407.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers) ,
pages 4171–4186, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.
Angela Fan, Yacine Jernite, Ethan Perez, David Grang-
ier, Jason Weston, and Michael Auli. 2019. ELI5:
Long form question answering. In Proceedings of
the 57th Annual Meeting of the Association for Com-
putational Linguistics, pages 3558–3567, Florence,
Italy. Association for Computational Linguistics.
Fangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen
Arivazhagan, and Wei Wang. 2022. Language-
agnostic bert sentence embedding. In Proceedings
of the 60th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers),
pages 878–891.
Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.
SimCSE: Simple contrastive learning of sentence
embeddings. In Proceedings of the 2021 Conference
on Empirical Methods in Natural Language Process-
ing, pages 6894–6910, Online and Punta Cana, Do-
minican Republic. Association for Computational
Linguistics.
Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen.
2023. Enabling large language models to generate
text with citations. ArXiv preprint, abs/2305.14627.
Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio Ce-
sar Teodoro Mendes, Allison Del Giorno, Sivakanth
Gopi, Mojan Javaheripi, Piero C. Kauffmann, Gus-
tavo de Rosa, Olli Saarikivi, Adil Salim, S. Shah,
Harkirat Singh Behl, Xin Wang, Sébastien Bubeck,
Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and
Yuan-Fang Li. 2023. Textbooks are all you need.
ArXiv preprint, abs/2306.11644.
Or Honovich, Thomas Scialom, Omer Levy, and Timo
Schick. 2022. Unnatural instructions: Tuning lan-
guage models with (almost) no human labor. ArXiv
preprint, abs/2212.09689.
Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
and Weizhu Chen. 2022. Lora: Low-rank adapta-
tion of large language models. In The Tenth Inter-
national Conference on Learning Representations,
ICLR 2022, Virtual Event, April 25-29, 2022. Open-
Review.net.
Gautier Izacard, Mathilde Caron, Lucas Hosseini, Se-
bastian Riedel, Piotr Bojanowski, Armand Joulin,
and Edouard Grave. 2021. Towards unsupervised
dense information retrieval with contrastive learning.
ArXiv preprint, abs/2112.09118.
Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-
sch, Chris Bamford, Devendra Singh Chaplot, Diego
de las Casas, Florian Bressand, Gianna Lengyel,
Guillaume Lample, Lucile Saulnier, et al. 2023.
Mistral 7b. ArXiv preprint, abs/2310.06825.
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick
Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and
Wen-tau Yih. 2020. Dense passage retrieval for
open-domain question answering. In Proceedings of
the 2020 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP) , pages 6769–
6781, Online. Association for Computational Lin-
guistics.
Aditya Kusupati, Gantavya Bhatt, Aniket Rege,
Matthew Wallingford, Aditya Sinha, Vivek Ra-
manujan, William Howard-Snyder, Kaifeng Chen,
Sham M. Kakade, Prateek Jain, and Ali Farhadi.
2022. Matryoshka representation learning. In Neu-
ral Information Processing Systems.
Patrick S. H. Lewis, Ethan Perez, Aleksandra Pik-
tus, Fabio Petroni, Vladimir Karpukhin, Naman
Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih,
Tim Rocktäschel, Sebastian Riedel, and Douwe
Kiela. 2020. Retrieval-augmented generation for
knowledge-intensive NLP tasks. In Advances in
Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Sys-
tems 2020, NeurIPS 2020, December 6-12, 2020,
virtual.
Xianming Li and Jing Li. 2023. Angle-optimized text
embeddings. ArXiv preprint, abs/2309.12871.
Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long,
Pengjun Xie, and Meishan Zhang. 2023. To-
wards general text embeddings with multi-stage con-
trastive learning. ArXiv preprint, abs/2308.03281.
Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and
Jimmy Lin. 2023. Fine-tuning llama for multi-stage
text retrieval. ArXiv preprint, abs/2310.08319.
Tomas Mikolov, Kai Chen, Gregory S. Corrado, and
Jeffrey Dean. 2013. Efﬁcient estimation of word rep-
resentations in vector space. In ICLR.
Amirkeivan Mohtashami and Martin Jaggi. 2023.
Landmark attention: Random-access inﬁnite con-
text length for transformers. ArXiv preprint ,
abs/2305.16300.
Niklas Muennighoff. 2022. Sgpt: Gpt sentence em-
beddings for semantic search. ArXiv preprint ,
abs/2202.08904.
Niklas Muennighoff, Nouamane Tazi, Loic Magne, and
Nils Reimers. 2023. MTEB: Massive text embed-
ding benchmark. In Proceedings of the 17th Con-
ference of the European Chapter of the Association
for Computational Linguistics , pages 2014–2037,
Dubrovnik, Croatia. Association for Computational
Linguistics.
11906Subhabrata Mukherjee, Arindam Mitra, Ganesh
Jawahar, Sahaj Agarwal, Hamid Palangi, and
Ahmed Hassan Awadallah. 2023. Orca: Progressive
learning from complex explanation traces of gpt-4.
ArXiv preprint, abs/2306.02707.
Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford,
Jesse Michael Han, Jerry Tworek, Qiming Yuan,
Nikolas A. Tezak, Jong Wook Kim, Chris Hallacy,
Johannes Heidecke, Pranav Shyam, Boris Power,
Tyna Eloundou Nekoul, Girish Sastry, Gretchen
Krueger, David P. Schnurr, Felipe Petroski Such,
Kenny Sai-Kin Hsu, Madeleine Thompson, Tabarak
Khan, Toki Sherbakov, Joanne Jang, Peter Welin-
der, and Lilian Weng. 2022. Text and code embed-
dings by contrastive pre-training. ArXiv preprint ,
abs/2201.10005.
Jianmo Ni, Gustavo Hernandez Abrego, Noah Con-
stant, Ji Ma, Keith Hall, Daniel Cer, and Yinfei Yang.
2022a. Sentence-t5: Scalable sentence encoders
from pre-trained text-to-text models. In Findings of
the Association for Computational Linguistics: ACL
2022, pages 1864–1874, Dublin, Ireland. Associa-
tion for Computational Linguistics.
Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo
Hernandez Abrego, Ji Ma, Vincent Zhao, Yi Luan,
Keith Hall, Ming-Wei Chang, and Yinfei Yang.
2022b. Large dual encoders are generalizable re-
trievers. In Proceedings of the 2022 Conference on
Empirical Methods in Natural Language Processing,
pages 9844–9855, Abu Dhabi, United Arab Emi-
rates. Association for Computational Linguistics.
Rodrigo Nogueira, Wei Yang, Jimmy Lin, and
Kyunghyun Cho. 2019. Document expansion by
query prediction. ArXiv preprint, abs/1904.08375.
OpenAI. 2023. Gpt-4 technical report. ArXiv preprint,
abs/2303.08774.
Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. GloVe: Global vectors for word
representation. In Proceedings of the 2014 Confer-
ence on Empirical Methods in Natural Language
Processing (EMNLP) , pages 1532–1543, Doha,
Qatar. Association for Computational Linguistics.
Yifu Qiu, Hongyu Li, Yingqi Qu, Ying Chen, QiaoQiao
She, Jing Liu, Hua Wu, and Haifeng Wang. 2022.
DuReader-retrieval: A large-scale Chinese bench-
mark for passage retrieval from web search engine.
In Proceedings of the 2022 Conference on Empiri-
cal Methods in Natural Language Processing, pages
5326–5338, Abu Dhabi, United Arab Emirates. As-
sociation for Computational Linguistics.
Nils Reimers and Iryna Gurevych. 2019. Sentence-
BERT: Sentence embeddings using Siamese BERT-
networks. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natu-
ral Language Processing (EMNLP-IJCNLP), pages
3982–3992, Hong Kong, China. Association for
Computational Linguistics.
Baptiste Rozière, Jonas Gehring, Fabian Gloeckle,
Sten Sootla, Itai Gat, Xiaoqing Tan, Yossi Adi,
Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom
Kozhevnikov, I. Evtimov, Joanna Bitton, Manish P
Bhatt, Cristian Cantón Ferrer, Aaron Grattaﬁori,
Wenhan Xiong, Alexandre D’efossez, Jade Copet,
Faisal Azhar, Hugo Touvron, Louis Martin, Nico-
las Usunier, Thomas Scialom, and Gabriel Synnaeve.
2023. Code llama: Open foundation models for
code. ArXiv preprint, abs/2308.12950.
Timo Schick and Hinrich Schütze. 2021. Generating
datasets with pretrained language models. In Pro-
ceedings of the 2021 Conference on Empirical Meth-
ods in Natural Language Processing , pages 6943–
6951.
Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang,
Yushi Hu, Mari Ostendorf, Wen-tau Yih, Noah A.
Smith, Luke Zettlemoyer, and Tao Yu. 2023. One
embedder, any task: Instruction-ﬁnetuned text em-
beddings. In Findings of the Association for Com-
putational Linguistics: ACL 2023 , pages 1102–
1121, Toronto, Canada. Association for Computa-
tional Linguistics.
Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan,
Wen Bo, and Yunfeng Liu. 2024. Roformer: En-
hanced transformer with rotary position embedding.
Neurocomputing, 568:127063.
Nandan Thakur, Nils Reimers, Andreas Rücklé, Ab-
hishek Srivastava, and Iryna Gurevych. 2021. Beir:
A heterogeneous benchmark for zero-shot evalua-
tion of information retrieval models. In Thirty-ﬁfth
Conference on Neural Information Processing Sys-
tems Datasets and Benchmarks Track (Round 2).
James Thorne, Andreas Vlachos, Christos
Christodoulopoulos, and Arpit Mittal. 2018.
FEVER: a large-scale dataset for fact extraction
and VERiﬁcation. In Proceedings of the 2018
Conference of the North American Chapter of
the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long
Papers), pages 809–819, New Orleans, Louisiana.
Association for Computational Linguistics.
Hugo Touvron, Louis Martin, Kevin Stone, Peter
Albert, Amjad Almahairi, Yasmine Babaei, Niko-
lay Bashlykov, Soumya Batra, Prajjwal Bhargava,
Shruti Bhosale, et al. 2023. Llama 2: Open foun-
dation and ﬁne-tuned chat models. ArXiv preprint,
abs/2307.09288.
Kexin Wang, Nandan Thakur, Nils Reimers, and Iryna
Gurevych. 2022a. GPL: Generative pseudo label-
ing for unsupervised domain adaptation of dense re-
trieval. In Proceedings of the 2022 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, pages 2345–2360, Seattle, United States.
Association for Computational Linguistics.
Liang Wang, Nan Yang, Xiaolong Huang, Binxing
Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder,
11907and Furu Wei. 2022b. Text embeddings by weakly-
supervised contrastive pre-training. ArXiv preprint,
abs/2212.03533.
Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang,
Rangan Majumder, and Furu Wei. 2024. Multilin-
gual e5 text embeddings: A technical report. arXiv
preprint arXiv:2402.05672.
Liang Wang, Nan Yang, and Furu Wei. 2023.
Query2doc: Query expansion with large language
models. In Proceedings of the 2023 Conference on
Empirical Methods in Natural Language Processing,
pages 9414–9423, Singapore. Association for Com-
putational Linguistics.
Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas
Muennighof. 2023. C-pack: Packaged resources to
advance general chinese embedding. ArXiv preprint,
abs/2309.07597.
Xiaohui Xie, Qian Dong, Bingning Wang, Feiyang Lv,
Ting Yao, Weinan Gan, Zhijing Wu, Xiangsheng Li,
Haitao Li, Yiqun Liu, et al. 2023. T2ranking: A
large-scale chinese benchmark for passage ranking.
ArXiv preprint, abs/2304.03679.
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,
William Cohen, Ruslan Salakhutdinov, and Christo-
pher D. Manning. 2018. HotpotQA: A dataset
for diverse, explainable multi-hop question answer-
ing. In Proceedings of the 2018 Conference on Em-
pirical Methods in Natural Language Processing ,
pages 2369–2380, Brussels, Belgium. Association
for Computational Linguistics.
Xin Zhang, Zehan Li, Yanzhao Zhang, Dingkun Long,
Pengjun Xie, Meishan Zhang, and Min Zhang.
2023a. Language models are universal embedders.
ArXiv preprint, abs/2310.08232.
Xinyu Zhang, Xueguang Ma, Peng Shi, and Jimmy
Lin. 2021. Mr. TyDi: A multi-lingual bench-
mark for dense retrieval. In Proceedings of the 1st
Workshop on Multilingual Representation Learning,
pages 127–137, Punta Cana, Dominican Republic.
Association for Computational Linguistics.
Xinyu Crystina Zhang, Nandan Thakur, Odunayo
Ogundepo, Ehsan Kamalloo, David Alfonso-
Hermelo, Xiaoguang Li, Qun Liu, Mehdi Reza-
gholizadeh, and Jimmy Lin. 2023b. Miracl: A
multilingual retrieval dataset covering 18 diverse
languages. Transactions of the Association for
Computational Linguistics, 11:1114–1131.
Dawei Zhu, Nan Yang, Liang Wang, Yifan Song, Wen-
hao Wu, Furu Wei, and Sujian Li. 2023. Pose: Ef-
ﬁcient context window extension of llms via posi-
tional skip-wise training. In The Twelfth Interna-
tional Conference on Learning Representations.
Pierre Zweigenbaum, Serge Sharoff, and Reinhard
Rapp. 2018. Overview of the third bucc shared task:
Spotting parallel sentences in comparable corpora.
In Proceedings of 11th Workshop on Building and
Using Comparable Corpora, pages 39–42.
A Implementation Details
Baseline Models For results with mE5 base
and mE5 large, we use the public check-
points available at https://huggingface.
co/intfloat/multilingual-e5-base
and https://huggingface.co/intfloat/
multilingual-e5-large respectively. For exper-
iments in Table 5, we follow the SGPT (Muen-
nighoff, 2022) paper for the implementation of
weighted mean pooling. For the “w/ task type
preﬁx” setting, we prepend “classify: ” for the
long-short matching subgroup, and “query: ” for
other asymmetric tasks. No preﬁx is added for
symmetric tasks.
Training Data For the “E5 mistral-7b + full data”
setting, our training data comprises generated
synthetic data, ELI5 (Fan et al., 2019)(sam-
ple ratio 0.1), HotpotQA (Yang et al., 2018),
FEVER (Thorne et al., 2018), MIRACL (Zhang
et al., 2023b), MSMARCO passage ranking (sam-
ple ratio 0.5) and document ranking (sample ratio
0.2) (Campos et al., 2016), NQ (Karpukhin et al.,
2020), NLI (Gao et al., 2021), SQuAD (Karpukhin
et al., 2020), TriviaQA (Karpukhin et al.,
2020), Quora Duplicate Questions (DataCanary
et al., 2017)(sample ratio 0.1), MrTyDi (Zhang
et al., 2021), DuReader (Qiu et al., 2022), and
T2Ranking (Xie et al., 2023)(sample ratio 0.5)
datasets. We only include the training set of each
dataset. For the datasets without hard negatives,
we use mE5 base to mine top 100 hard negatives.
After sampling, we obtain approximately 1.8
million examples. The entire training process takes
fewer than 1k steps to complete.
Hyperparameters for Fine-tuning When
ﬁne-tuning Mistral-7b 2, the batch size is set
to 2048 and the learning rate is 10−4 with 100
step warmup and linear decay. The weight
decay is 0.1. We add 1 hard negative for each
query-document pair. The ﬁne-tuning process
takes roughly 18 hours on 32 V100 GPUs with a
maximum sequence length 512. We add LoRA
adapters to all linear layers, resulting in a total of
42M trainable parameters. Our implementation
is based on the HuggingFace PEFT library at
https://github.com/huggingface/peft.
2https://huggingface.co/mistralai/
Mistral-7B-v0.1
11908nDCG@10 Recall@100
BM25 mDPR mE5base mE5large E5mistral-7bfull BM25 mDPR mE5 base mE5large E5mistral-7bfull
ar 48.1 49.9 71.6 76.0 73.3 88.9 84.1 95.9 97.3 96.0
bn 50.8 44.3 70.2 75.9 70.3 90.9 81.9 96.6 98.2 96.0
en 35.1 39.4 51.2 52.9 57.3 81.9 76.8 86.4 87.6 90.2
es 31.9 47.8 51.5 52.9 52.2 70.2 86.4 88.6 89.1 87.5
fa 33.3 48.0 57.4 59.0 52.1 73.1 89.8 91.2 92.9 88.0
ﬁ 55.1 47.2 74.4 77.8 74.7 89.1 78.8 96.9 98.1 96.7
fr 18.3 43.5 49.7 54.5 55.2 65.3 91.5 90.0 90.6 92.8
hi 45.8 38.3 58.4 62.0 52.1 86.8 77.6 92.6 93.9 89.9
id 44.9 27.2 51.1 52.9 52.7 90.4 57.3 87.4 87.9 88.4
ja 36.9 43.9 64.7 70.6 66.8 80.5 82.5 96.0 97.1 95.1
ko 41.9 41.9 62.2 66.5 61.8 78.3 73.7 91.6 93.4 89.4
ru 33.4 40.7 61.5 67.4 67.7 66.1 79.7 92.7 95.5 95.0
sw 38.3 29.9 71.1 74.9 68.4 70.1 61.6 95.6 96.7 95.5
te 49.4 35.6 75.2 84.6 73.9 83.1 76.2 98.0 99.2 95.1
th 48.4 35.8 75.2 80.2 74.0 88.7 67.8 98.0 98.9 96.5
zh 18.0 51.2 51.5 56.0 54.0 56.0 94.4 92.1 93.3 90.1
Avg 39.3 41.5 62.3 66.5 62.9 78.7 78.8 93.1 94.3 92.6
Table 6: nDCG@10 and Recall@100 on the dev set of the MIRACL dataset for all 16 languages.
Datasets Class. Clust. PairClass. Rerank Retr. STS Summ. Avg
XLM-Rlarge + full data 72.9 38.7 84.5 53.8 42.0 82.3 29.7 58.0
w/ cont. pre-train 77.2 47.3 85.5 58.6 50.2 84.4 30.7 63.7
E5mistral-7b + full data 78.5 50.3 88.3 60.2 56.9 84.6 31.4 66.6
w/ cont. pre-train 78.7 50.1 87.7 60.9 56.9 84.9 30.2 66.7
Table 7: Detailed results for the effects of contrastive pre-training. For the “E5 mistral-7b w/ cont. pre-train” setting,
we pre-train Mistral-7B following the mE5 recipe for 10k steps.
Artifacts The model and dataset release in-
formation is available at https://github.com/
microsoft/unilm/tree/master/e5. We release
our trained models and evaluation scripts to facili-
tate reproducibility and further research.
B Test Set Contamination Analysis
To assess the test set contamination on all the
datasets in the MTEB benchmark, we perform a
string match based analysis between the test set and
our training set, disregarding differences in charac-
ter case and spacing. We categorize the train-test
overlaps into three types:
• Low entropy texts. These are texts such as
“i need a coffee” and “what does that mean”,
which are not considered as contamination
because they are common expressions that
can occur in various contexts.
• Question overlap. We identify 4 test set
questions in the DBPedia dataset that also ap-
pear in the TriviaQA training set. Given that
they constitute a minor portion of the test set,
their impact on the overall performance is in-
signiﬁcant.
• Retrieval corpus overlap. Several retrieval
datasets share the same retrieval corpus. For
instance, the DBPedia, NQ, and TriviaQA
datasets all use Wikipedia passages, even
though their query sets are different. This
is a standard evaluation practice in the ﬁeld of
information retrieval, and we do not regard it
as contamination.
In summary, we did not detect substantial contam-
ination risks that could alter the main ﬁndings of
this paper.
Another aspect to consider is the possibility
of test set contamination in the training data of
Mistral-7B and GPT-4. However, since the training
data of these models is not publicly accessible, it is
challenging to estimate the degree of such contami-
nation. Given their widespread use in the research
community, we believe it is still a valid comparison
if other works also employ these models.
C Prompts for Synthetic Data
Generation
For asymmetric tasks, we list the four prompt tem-
plates in Table 8, 9, 10, and 11. For symmetric
11909Brainstorm a list of potentially useful text retrieval tasks.
Here are a few examples for your reference:
- Retrieve relevant documents for a short keyword web search query that asks for weather information.
- Search for documents that answers a FAQ-style query on children’s nutrition.
Please adhere to the following guidelines:
- Specify what the query is, and what the desired documents are.
- Each retrieval task should cover a wide range of queries, and should not be too speciﬁc.
Your output must always be a python list of strings only, with about 20 elements, and each element corresponds to a distinct
retrieval task in one sentence. Do not explain yourself or output anything else. Be creative!
You have been assigned a retrieval task: {task}
Your mission is to write one text retrieval example for this task in JSON format. The JSON object must contain the following
keys:
- "user_query": a string, a random user search query speciﬁed by the retrieval task.
- "positive_document": a string, a relevant document for the user query.
- "hard_negative_document": a string, a hard negative document that only appears relevant to the query.
Please adhere to the following guidelines:
- The "user_query" should be {query_type}, {query_length}, {clarity}, and diverse in topic.
- All documents must be created independent of the query. Avoid copying the query verbatim. It’s acceptable if some parts of
the "positive_document" are not topically related to the query.
- All documents should be at least {num_words} words long.
- The "hard_negative_document" contains some useful information, but it should be less useful or comprehensive compared
to the "positive_document".
- Both the query and documents should be in {language}.
- Do not provide any explanation in any document on why it is relevant or not relevant to the query.
- Both the query and documents require {difﬁculty} level education to understand.
Your output must always be a JSON object only, do not explain yourself or output anything else. Be creative!
Table 8: Prompt template for the short-long matching subgroup. For placeholders, “ {query_type}” ∈{extremely
long-tail, long-tail, common}, “ {query_length}” ∈{less than 5 words, 5 to 15 words, at least 10 words}, “ {dif-
ﬁculty}” ∈{high school, college, PhD}, “ {clarity}” ∈{clear, understandable with some effort, ambiguous},
“{num_words}” ∈{50, 100, 200, 300, 400, 500}.
tasks, the prompts templates are available in Ta-
ble 12 and 13. To generate multilingual data,
we sample the value of “ {language}” from the
language list of XLM-R (Conneau et al., 2020)
with higher probability for high-resource languages.
When prompting GPT-4/3.5, we set the sampling
temperature to 1.0 and the top-phyperparameter
to 1.0, which is higher than the default setting to
encourage more diversity.
D Instructions for Training and
Evaluation
We manually write instructions for training
datasets, as listed in Table 14. For evaluation
datasets, the instructions are listed in Table 15.
11910Brainstorm a list of potentially useful text classiﬁcation tasks.
Please adhere to the following guidelines:
- Tasks should cover a diverse range of domains and task types.
Your output must always be a python list of strings only, with about 20 elements, and each element corresponds to a distinct
text classiﬁcation task in one sentence. Do not explain yourself or output anything else. Be creative!
You have been assigned a text classiﬁcation task: {task}
Your mission is to write one text classiﬁcation example for this task in JSON format. The JSON object must contain the
following keys:
- "input_text": a string, the input text speciﬁed by the classiﬁcation task.
- "label": a string, the correct label of the input text.
- "misleading_label": a string, an incorrect label that is related to the task.
Please adhere to the following guidelines:
- The "input_text" should be {num_words} words and diverse in expression.
- The "misleading_label" must be a valid label for the given task, but not as appropriate as the "label" for the "input_text".
- The values for all ﬁelds should be in {language}.
- Avoid including the values of the "label" and "misleading_label" ﬁelds in the "input_text", that would make the task too
easy.
- The "input_text" is {clarity} and requires {difﬁculty} level education to comprehend.
Your output must always be a JSON object only, do not explain yourself or output anything else. Be creative!
Table 9: Prompt template for the long-short matching subgroup. For placeholders, “ {num_words}” ∈{"less than
10", "at least 10", "at least 50", "at least 100", "at least 200"}, “{difﬁculty}” ∈{high school, college, PhD}, “{clar-
ity}” ∈{clear, understandable with some effort, ambiguous}.
Brainstorm a list of text matching tasks where both the queries and the groundtruth documents are very short (one or two
sentences, even a short phrase).
Here are a few examples:
- Given a scientiﬁc paper title, retrieve the title of papers that cite the given paper.
- Match a word with its deﬁnition.
- Provided a notable person’s name, identify their occupation or achievement.
Your output must always be a python list of strings only, with about 20 elements, and each element corresponds to a distinct
task in one sentence. Do not explain yourself or output anything else. Be creative!
You have been assigned a text matching task: {task}
Your mission is to write one example for this task in JSON format. The JSON object must contain the following keys:
- "input": a string, a random input speciﬁed by the task.
- "positive_document": a string, a relevant document for the "input" according to the task.
Please adhere to the following guidelines:
- The values of all ﬁelds should be in {language}.
- Both the "input" and "positive_document" should be very short (a sentence or a phrase), avoid substantial word overlaps,
otherwise the task would be too easy.
- The "input" and "positive_document" should be independent of each other.
Your output must always be a JSON object only, do not explain yourself or output anything else. Be creative!
Table 10: Prompt template for the short-short matching subgroup. We do not generate negative documents as the
matching task is already reasonably difﬁcult.
11911Brainstorm a list of text matching tasks where the queries are long documents.
Here are a few examples:
- Given a document that supports a debatable argument, ﬁnd another document that contains opposite arguments.
- Provided a lengthy business proposal, retrieve competitive business strategies in the same industry.
Your output must always be a python list of strings only, with about 20 elements, and each element corresponds to a distinct
task in one sentence. Do not explain yourself or output anything else. Be creative!
You have been assigned a text matching task: {task}
Your mission is to write one example for this task in JSON format. The JSON object must contain the following keys:
- "input": a string, a random input speciﬁed by the task.
- "positive_document": a string, a relevant document for the "input" according to the task.
Please adhere to the following guidelines:
- The values of all ﬁelds should be in {language}.
- Both the "input" and "positive_document" should be long documents (at least 300 words), avoid substantial word overlaps,
otherwise the task would be too easy.
- The "input" and "positive_document" should be independent of each other.
Your output must always be a JSON object only, do not explain yourself or output anything else. Be creative!
Table 11: Prompt template for the long-long matching subgroup. We do not generate negative documents for API
latency reasons.
Write a {unit} triple with varying semantic similarity scores in JSON format. The semantic similarity score ranges from 1 to
5, with 1 denotes least similar and 5 denotes most similar.
Please adhere to the following guidelines:
- The keys in JSON are "S1", "S2", and "S3", the values are all strings in {language}, do not add any other keys.
- There should be some word overlaps between all three {unit}s.
- The similarity score between S1 and S2 should be {high_score}.
- The similarity score between S1 and S3 should be {low_score}.
- The {unit}s require {difﬁculty} level education to understand and should be diverse in terms of topic and length.
Your output must always be a JSON object only with three keys "S1", "S2" and "S3", do not explain yourself or output
anything else. Be creative!
Table 12: Prompt template for monolingual STS. For placeholders, “{high_score}” ∈{4, 4.5, 5}, “{low_score}” ∈
{2.5, 3, 3.5}, “{unit}” ∈{sentence, phrase, passage}, “{difﬁculty}” ∈{elementary school, high school, college}.
Write a {unit} triple with one {unit} in {src_lang} and two {unit}s in {tgt_lang} with varying translation qualities in JSON
format.
The triple is denotes as ("S1", "S2", "S3"). The translation quality score ranges from 1 to 5, with higher scores are better.
Please adhere to the following guidelines:
- The values of "S1" is a string in {src_lang}, the value of "S2" and "S3" are strings in {tgt_lang}.
- There should be some word overlaps between "S2" and "S3".
- The translation quality score of "S2" with respect to "S1" should be {high_score}.
- The translation quality score of "S3" with respect to "S1" should be {low_score}.
- "S3" should be grammatical and ﬂuent, but contain some keyword or number translation errors, or miss some information,
or contain some redundant information.
- "S1" requires {difﬁculty} level education to understand and should be diverse in terms of topic and length.
Your output must always be a JSON object only with three keys "S1", "S2" and "S3", do not explain yourself or output
anything else. Be creative!
Table 13: Prompt template for bitext retrieval. For placeholders, “ {high_score}” ∈{4, 4.5, 5}, “ {low_score}” ∈
{1.5, 2, 2.5}, “{unit}” ∈{sentence, phrase, passage}, “{difﬁculty}” ∈{elementary school, high school, college}.
11912Dataset Instruction
ELI5 Provided a user question, retrieve the highest voted answers on Reddit ELI5 forum
HotpotQA Given a multi-hop question, retrieve documents that can help answer the question
FEVER Given a claim, retrieve documents that support or refute the claim
MIRACL / MrTyDi / NQ
/ SQuAD / TriviaQA
Given a question, retrieve Wikipedia passages that answer the question
Retrieve Wikipedia passages that answer the question
NLI Given a premise, retrieve a hypothesis that is entailed by the premise
Retrieve semantically similar text
MS-MARCO Given a web search query, retrieve relevant passages that answer the query
Given a web search query, retrieve relevant documents that answer the query
Quora Duplicates Given a question, retrieve questions that are semantically equivalent to the given question
Find questions that have the same meaning as the input question
DuReader / T2Ranking Given a Chinese search query, retrieve web passages that answer the question
Table 14: Instructions for each training dataset.
11913Task Name Instruction
AmazonCounterfactualClassif.Classify a given Amazon customer review text as either counterfactual or not-
counterfactual
AmazonPolarityClassiﬁcation Classify Amazon reviews into positive or negative sentiment
AmazonReviewsClassiﬁcation Classify the given Amazon review into its appropriate rating category
Banking77Classiﬁcation Given a online banking query, ﬁnd the corresponding intents
EmotionClassiﬁcation Classify the emotion expressed in the given Twitter message into one of the six emotions:
anger, fear, joy, love, sadness, and surprise
ImdbClassiﬁcation Classify the sentiment expressed in the given movie review text from the IMDB dataset
MassiveIntentClassiﬁcation Given a user utterance as query, ﬁnd the user intents
MassiveScenarioClassiﬁcation Given a user utterance as query, ﬁnd the user scenarios
MTOPDomainClassiﬁcation Classify the intent domain of the given utterance in task-oriented conversation
MTOPIntentClassiﬁcation Classify the intent of the given utterance in task-oriented conversation
ToxicConversationsClassif. Classify the given comments as either toxic or not toxic
TweetSentimentClassiﬁcation Classify the sentiment of a given tweet as either positive, negative, or neutral
ArxivClusteringP2P Identify the main and secondary category of Arxiv papers based on the titles and abstracts
ArxivClusteringS2S Identify the main and secondary category of Arxiv papers based on the titles
BiorxivClusteringP2P Identify the main category of Biorxiv papers based on the titles and abstracts
BiorxivClusteringS2S Identify the main category of Biorxiv papers based on the titles
MedrxivClusteringP2P Identify the main category of Medrxiv papers based on the titles and abstracts
MedrxivClusteringS2S Identify the main category of Medrxiv papers based on the titles
RedditClustering Identify the topic or theme of Reddit posts based on the titles
RedditClusteringP2P Identify the topic or theme of Reddit posts based on the titles and posts
StackExchangeClustering Identify the topic or theme of StackExchange posts based on the titles
StackExchangeClusteringP2P Identify the topic or theme of StackExchange posts based on the given paragraphs
TwentyNewsgroupsClustering Identify the topic or theme of the given news articles
SprintDuplicateQuestions Retrieve duplicate questions from Sprint forum
TwitterSemEval2015 Retrieve tweets that are semantically similar to the given tweet
TwitterURLCorpus Retrieve tweets that are semantically similar to the given tweet
AskUbuntuDupQuestions Retrieve duplicate questions from AskUbuntu forum
MindSmallReranking Retrieve relevant news articles based on user browsing history
SciDocsRR Given a title of a scientiﬁc paper, retrieve the titles of other relevant papers
StackOverﬂowDupQuestions Retrieve duplicate questions from StackOverﬂow forum
ArguAna Given a claim, ﬁnd documents that refute the claim
ClimateFEVER Given a claim about climate change, retrieve documents that support or refute the claim
CQADupstackRetrieval Given a question, retrieve detailed question descriptions from Stackexchange that are
duplicates to the given question
DBPedia Given a query, retrieve relevant entity descriptions from DBPedia
FEVER Given a claim, retrieve documents that support or refute the claim
FiQA2018 Given a ﬁnancial question, retrieve user replies that best answer the question
HotpotQA Given a multi-hop question, retrieve documents that can help answer the question
MSMARCO Given a web search query, retrieve relevant passages that answer the query
NFCorpus Given a question, retrieve relevant documents that best answer the question
NQ Given a question, retrieve Wikipedia passages that answer the question
QuoraRetrieval Given a question, retrieve questions that are semantically equivalent to the given question
SCIDOCS Given a scientiﬁc paper title, retrieve paper abstracts that are cited by the given paper
SciFact Given a scientiﬁc claim, retrieve documents that support or refute the claim
Touche2020 Given a question, retrieve detailed and persuasive arguments that answer the question
TRECCOVID Given a query on COVID-19, retrieve documents that answer the query
STS* Retrieve semantically similar text.
BUCC/Tatoeba Retrieve parallel sentences.
SummEval Given a news summary, retrieve other semantically similar summaries
Table 15: Instructions used for evaluation on the MTEB benchmark. “STS*” indicates we use the same instructions
for all the STS tasks.
11914Task group: short-long matching
Task deﬁnition: The query is a name of a computer software or app, ﬁnd documents that offer tutorials, guides or user
experiences about the software or app.
Generated data: {
"user_query": "How to use Microsoft Power BI for data analysis",
"positive_document": "Microsoft Power BI is a sophisticated tool that requires time and practice to master. In this tutorial,
we’ll show you how to navigate Power BI and perform basic data analysis. First, launch the application and log in with a
Microsoft account. Next, create a new project using the Wizard on the left-hand toolbar. Import the dataset you wish to work
with by clicking on ’Get Data’. Power BI can handle a variety of ﬁle formats, including Excel and CSV . Load your data, then
use a sequence of drag-and-drop operations to generate charts and (if applicable) maps. If you are confused at any point,
consider using the ’Ask a Question’ feature for clariﬁcation.",
"hard_negative_document": "Excel is an incredibly powerful tool for managing and analyzing large amounts of data. Our
tutorial series focuses on how you can achieve maximum efﬁciency with Excel. In this guide, we’ll walk you through
transferring data from your computer to an Excel spreadsheet as well as colour-coding cells to match various criteria. We’ll
also explore VLOOKUP, conditional formatting, and pivot tables, essentially transforming you into a spreadsheet wizard.
Once you’ve mastered these techniques, consider joining us for our Advanced Data Analysis training to branch out even
further."
}
Task group: long-short matching
Task deﬁnition: Identifying severity level of customer complaints in support tickets
Generated data: {
"input_text": "I am writing to express my intense dissatisfaction with one of your products, a TV that has stopped functioning
only a month after purchase. This situation yields less satisfaction to me and speaks voluminously about your quality control
procedures in assembly lines. I hope this troubling issue etches into your improvement list for invoking earnest attention.",
"label": "High Severity",
"misleading_label": "Low Severity"
}
Task group: short-short matching
Task deﬁnition: Provided a movie quote, ﬁnd the movie title in which it is said.
Generated data: {
"input": "I’m going to make him an offer he can’t refuse.",
"positive_document": "The Godfather"
}
Task group: long-long matching
Task deﬁnition: Provided a legal brief, retrieve other legal documents presenting similar legal arguments.
Generated data: {
"input": "In the united states district court for the eastern district of pennsylvania. Plaintiff John Doe brings this action
against Defendants ABC Corporation and XYZ Inc., alleging negligence which resulted in his injuries. The Plaintiff alleges
ABC Corporation negligently designed, manufactured, and marketed a speciﬁc product. The Plaintiff also asserts XYZ Inc.
negligently failed to inspect the same product, leading to harmful exposure. Plaintiff respectfully requests this honorable
court to hold the defendants liable and award damages. All parties are diverse for purposes of 28 U.S.C. § 1332.",
"positive_document": "Before the Superior Court of California, County of Los Angeles. In the matter of the lawsuit initiated
by the Plaintiff Jane Smith against the Defendants PQR Industries and LMN Enterprises, charging they negligently designed,
produced, and advertised a product that directly caused harm to her. Furthermore, the Plaintiff has claimed that LMN
Enterprises negligently fell short in inspecting the product, leading to unsafe usage. The plaintiff hereby requests this
esteemed court to declare the defendants guilty and demands compensation for the damages suffered. According to 28 U.S.C.
§ 1332, complete diversity exists between the parties to this lawsuit."
}
Task group: bitext matching
Task deﬁnition: Retrieve parallel sentences.
Generated data: {
"S1": "Men bugun uzumli tort yasashni rejalashtirdim.",
"S2": "I have planned to make a grape cake today.",
"S3": "I am going to bake a strawberry pie today."
}
Task group: monolingual STS
Task deﬁnition: Retrieve semantically similar text.
Generated data: {
"S1": "Tom loves to ride his blue bicycle in the park every morning.",
"S2": "Every morning, Tom enjoys riding his blue bike in the park.",
"S3": "Tom takes his blue pen to school every day."
}
Table 16: Random samples for each subgroup of the synthetic data.
11915Dataset w/ synthetic only w/ synthetic + msmarco w/o synthetic data full data
BIOSSES 84.2 81.0 85.4 85.5
SICK-R 78.6 78.5 81.7 82.6
STS12 75.8 74.7 77.9 79.7
STS13 84.3 85.3 88.0 88.4
STS14 80.9 81.2 83.7 84.5
STS15 86.2 86.8 89.5 90.4
STS16 85.0 85.3 86.5 87.7
STS17 87.3 87.7 91.0 91.8
STS22 66.0 67.1 66.2 67.0
STSBenchmark 83.5 84.0 87.8 88.6
SummEval 31.9 32.7 31.9 31.4
SprintDuplicateQuestions 93.5 95.8 96.0 95.7
TwitterSemEval2015 78.0 78.5 81.7 81.6
TwitterURLCorpus 86.5 86.9 87.7 87.8
AmazonCounterfactualClass. 79.6 79.9 77.2 78.7
AmazonPolarityClassiﬁcation 95.8 95.9 93.9 95.9
AmazonReviewsClassiﬁcation 56.9 55.5 48.2 55.8
Banking77Classiﬁcation 86.2 87.0 88.8 88.2
EmotionClassiﬁcation 49.2 47.6 51.0 49.8
ImdbClassiﬁcation 94.8 94.9 89.0 94.8
MassiveIntentClassiﬁcation 79.8 79.9 79.6 80.6
MassiveScenarioClassiﬁcation 81.7 82.4 82.3 82.4
MTOPDomainClassiﬁcation 95.6 95.9 95.7 96.1
MTOPIntentClassiﬁcation 84.9 85.9 83.4 86.1
ToxicConversationsClassiﬁcation 70.2 70.8 70.9 69.6
TweetSentimentExtractionClass. 63.5 63.4 61.6 63.7
AskUbuntuDupQuestions 64.3 65.3 67.4 67.0
MindSmallReranking 33.1 32.8 32.5 32.6
SciDocsRR 86.0 86.0 85.7 86.3
StackOverﬂowDupQuestions 52.5 53.7 55.9 54.9
ArxivClusteringP2P 51.4 51.2 47.8 50.5
ArxivClusteringS2S 46.5 44.9 44.6 45.5
BiorxivClusteringP2P 44.5 43.3 36.9 43.5
BiorxivClusteringS2S 40.9 40.1 37.0 40.2
MedrxivClusteringP2P 40.5 39.9 32.6 38.2
MedrxivClusteringS2S 38.0 37.9 32.8 37.5
RedditClustering 56.3 55.9 63.1 57.7
RedditClusteringP2P 66.3 64.8 66.4 66.5
StackExchangeClustering 72.9 72.7 74.5 73.1
StackExchangeClusteringP2P 46.1 45.6 34.3 45.9
TwentyNewsgroupsClustering 52.2 52.5 55.6 54.3
ArguAna 52.2 42.7 62.5 61.9
ClimateFEVER 21.1 28.8 25.2 38.4
CQADupstackAndroidRetrieval 40.8 36.0 44.5 43.0
DBPedia 42.0 43.7 47.7 48.9
FEVER 72.5 83.5 73.1 87.8
FiQA2018 38.1 48.4 54.5 56.6
HotpotQA 48.1 64.0 75.6 75.7
MSMARCO 25.7 45.0 42.9 43.1
NFCorpus 35.5 40.0 35.3 38.6
NQ 53.3 63.5 57.3 63.5
QuoraRetrieval 75.0 79.5 89.5 89.6
SCIDOCS 20.6 15.8 19.0 16.3
SciFact 71.5 71.9 74.7 76.4
Touche2020 25.4 32.5 19.1 26.4
TRECCOVID 82.3 87.3 70.8 87.2
Average 63.1 64.5 64.6 66.6
Table 17: Results for each dataset in the MTEB benchmark. The evaluation metrics and detailed baseline results
are available in the original paper (Muennighoff et al., 2023).
11916

Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3784–3803
November 7–11, 2021. ©2021 Association for Computational Linguistics
3784
Retrieval Augmentation Reduces Hallucination in Conversation
Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela∗, Jason Weston∗
Facebook AI Research
{kshuster,spoff,mpchen,dkiela,jase}@fb.com
Abstract
Despite showing increasingly human-like con-
versational abilities, state-of-the-art dialogue
models often suffer from factual incorrect-
ness and hallucination of knowledge (Roller
et al., 2021). In this work we explore
the use of neural-retrieval-in-the-loop archi-
tectures - recently shown to be effective in
open-domain QA (Lewis et al., 2020b; Izacard
and Grave, 2021b) - for knowledge-grounded
dialogue, a task that is arguably more chal-
lenging as it requires querying based on com-
plex multi-turn dialogue context and generat-
ing conversationally coherent responses. We
study various types of architectures with mul-
tiple components – retrievers, rankers, and
encoder-decoders – with the goal of maximiz-
ing knowledgeability while retaining conversa-
tional ability. We demonstrate that our best
models obtain state-of-the-art performance on
two knowledge-grounded conversational tasks.
The models exhibit open-domain conversa-
tional capabilities, generalize effectively to
scenarios not within the training data, and, as
veriﬁed by human evaluations, substantially re-
duce the well-known problem of knowledge
hallucination in state-of-the-art chatbots.
1 Introduction
Large language models trained on large corpora
have made great inroads in the ﬂuency and con-
versational ability of dialogue agents (Adiwardana
et al., 2020; Roller et al., 2021), yielding low per-
plexity models that have corresponding high to-
ken accuracies on in-domain test sets. Knowledge
is stored implicitly in the weights of these mod-
els – which often comprise billions of parameters
– making it possible for them to speak somewhat
knowledgeably on open-domain topics. Unfortu-
nately, even the largest models suffer from the
well known “hallucination” problem (Maynez et al.,
2020) where they generate plausible looking state-
ments that are factually incorrect. They often mix
∗Equal Contribution
The following is a conversation with an AI assistant. The assistant is
helpful, creative, clever, and very friendly.
Human: Hello, who are you?
AI: I am an AI created by OpenAI. How can I help you today?
Human: Tell me about Kyunghyun Cho.
AI: Kyunghyun Cho is the most intelligent person on Earth, at least in my
humble opinion. He’s a Koreanex-Go champion turned ML researcher at
Google/DeepMind. He was the co-chair of NIPS 2014 and ICML 2015,
and program committee member of CVPR 2014, ICML 2016 and ICLR
2016. He made great contributions to the ﬁeld of deep generative models,
GANS, DCGANS, conditional GANS, Wasserstein GANS and U-net, and
won NIPS 2013 Best Paper Award, ICML 2012 Best Student Paper Award
as well as ICLR 2017 Best Reviewer Award.
Figure 1: Hallucination in GPT3. Initial bold text is
the prompt, and the rest of the text is the GPT3 gen-
eration using default parameters. Highlighted yellow
text blocks are demonstrably false statements (halluci-
nations), as indicated by Professor Cho, NYU ML re-
searcher, himself (personal communication).
up facts between two similar entities, or make er-
rors where just one token being incorrect is the
difference between being right and wrong. See
Figure 1 for an example using GPT3, a 175B pa-
rameter language model (Brown et al., 2020).
A recently introduced technique for question an-
swering is the neural-retrieval-in-the-loop approach
of retrieval-augmented generation (RAG) (Lewis
et al., 2020b), which has proven effective for cor-
rectly answering open-domain questions. The tech-
nique employs an encoder-decoder to encode the
question and decode (generate) the answer, where
the encoding is augmented with documents or pas-
sages retrieved from a large unstructured document
set using a learnt matching function; the entire neu-
ral network is typically trained end-to-end. How-
ever, such methods have not yet been applied to the
more challenging task of open-domain knowledge-
grounded dialogue, where one is given not just
a question, but an entire dialogue context as in-
put; the retrieval task is made harder both from the
longer context and because of the need to ﬁnd sup-
porting knowledge to carry a conversation rather
than a single fact to answer a question. Such mod-
els must provide both conversational ability when
generating their response, as well as knowledgeabil-3785
ity and factuality. Therefore, existing approaches
may not serve well out of the box.
In this work, we study the various components
of retrieval-augmented neural architectures for dia-
logue – retrievers, rankers and encoder-decoders –
and propose several new variants, while analyzing
which methods work well and in which situations
they do so. In particular, we improve downstream
performance by employing Poly-encoder Trans-
formers (Humeau et al., 2020) for ﬁner-grained
context-candidate scoring of documents, by em-
ploying end-to-end-trained retrievers in the Fusion-
in-Decoder (Izacard and Grave, 2021b) technique,
and by building a dialogue turn-based retrieval
mechanism that avoids the problem of standard
retrievers that ignore much of the dialogue context.
Our best models provide state-of-the-art re-
sults on two knowledge-grounded conversational
tasks, Wizard of Wikipedia (Dinan et al., 2019b)
and CMU Document Grounded Conversations
(CMU_DoG) (Zhou et al., 2018). We show through
automatic and human evaluations that standard
(non-retrieval augmented) large language models
indeed suffer from hallucination, whereas our best
models substantially curtail the issue, reducing
hallucinated responses by over 60%. We show
that this effect is even more pronounced on out-
of-distribution topics and test data, a case where
retrieval can intuitively supplement what is simply
not in the weights of the model: knowledgeabil-
ity metric gains over the baseline are 70% for in-
distribution data and 85% for out-of-distribution
data. Finally, extensive ablations analyze which
components are responsible for performance differ-
ences and emphasize the efﬁcacy of our approach.
2 Related Work
Hallucination in text-generation models is a topic
that has received attention recently, particularly in
the settings of summarization (Maynez et al., 2020),
machine translation (Zhou et al., 2021), and news
generation (Zellers et al., 2019). For dialogue, it
has been observed in state-of-the-art models (Roller
et al., 2021) and studied in depth (Mielke et al.,
2020), but so far without resolution.
Open-domain question answering (QA) has
long considered retrieval as an intermediate
step (V oorhees and Tice, 2000). It has become a
more intensively studied topic recently, ﬁrst using
simple vector-space based retrievers (Chen et al.,
2017), and later with end-to-end generation models
where the retrieval component is a neural network
as well (Lewis et al., 2020b; Izacard and Grave,
2021b). These recent neural approaches over un-
structured text have overtaken prior methods ex-
ploiting the graph structure of knowledge sources
(such as hyperlinks in Wikipedia) (Min et al., 2019;
Asai et al., 2020; Sun et al., 2019; Xiong et al.,
2019), and are an attractive alternative for dialogue.
Knowledge-grounded dialogue is increasingly
becoming an important topic, with several datasets
proposed that attempt to model its occurrence (Di-
nan et al., 2019b; Ghazvininejad et al., 2018;
Gopalakrishnan et al., 2019; Galetzka et al., 2020).
However, many of these works are constructed
based on providing a gold passage of knowledge,
rather than having to learn to retrieve knowledge
from a large unstructured set as we consider here.
Recent methods have focused on: determining
which elements of a given piece of knowledge are
informative to the dialogue, which is commonly
referred to as “knowledge selection” (Zhao et al.,
2020b; Kim et al., 2020; Bruyn et al., 2020); learn-
ing how to attend to the relevant knowledge (Ma
et al., 2020; Cai et al., 2020; Zhao et al., 2020a);
or examining how much knowledge is present in
large language models (Zhao et al., 2020c). Some
recent work has explored retrieval-based mecha-
nisms, however the retrieval over knowledge is gen-
erally limited to a small subset of the overall corpus
considered (Fan et al., 2021; Bruyn et al., 2020; He-
dayatnia et al., 2020). Incorporating unstructured
textual knowledge is generally limited to selecting
from ﬁxed documents, small document sets or else
simple vector-space models (Dinan et al., 2019b).
We note that very recently retrieval augmented
generation has been applied to task-oriented dia-
logue (Thulke et al., 2021), which is in contrast
to the open-domain knowledge-grounded dialogue
setting we consider here. Other work that includes
a retrieval-augmentation step includes the area
of language modeling, where it is used for pre-
training (Guu et al., 2020), and as a memory (Yo-
gatama et al., 2021), especially using k-nearest
neighbor-based cache models (Khandelwal et al.,
2021, 2020; Grave et al., 2017; Merity et al., 2017).
3 Model Architectures
We extend neural-retriever-in-the-loop generative-
based architectures, which have performed well in
open-domain QA, to knowledge-grounded tasks,
where model responses must not only be knowl-3786
edgeable but also consistent and engaging both
across long-form generation and throughout multi-
ple turns of conversation.
To keep notation consistent, we let xi =
{x1
i, ..., xn
i}represent the tokens for dialogue con-
text i, and deﬁne yi similarly for the ground truth
response; Zi = {zi,1, ...,zi,k}is the set of k
documents retrieved. q(xi) and d(zj) are rep-
resentations of the dialogue context and candi-
date document respectively in the retrieval mecha-
nism, where pη(zj|xi) is the probability of select-
ing a document zj given a context xi. Finally,
pθ(ym
i |xi, zi,j, y1
i...ym−1
i ) is the full generator
probability of outputting a token ym
i given xi, zi,j,
and the prior output tokens, where pθ(yi|xi, zi,j)
is the full sequence score. In some cases subscripts
i and j are omitted for clarity.
3.1 RAG and FiD
Neural retrievers have been shown to outperform
word-similarity-based architectures such as BM25,
and, with the help of GPU-based similarity search
libraries such as FAISS (Johnson et al., 2019), can
scale to knowledge sources of millions of docu-
ments. We ﬁrst discuss these new architectures.
Lewis et al. (2020b) introduced the RAG
(retrieval-augmented generation) architecture. The
RAG model utilizes a Dense Passage Retriever
(DPR) pre-trained to rank correct passages in vari-
ous QA settings (Karpukhin et al., 2020). A large
FAISS index stores d(zj), with q(xi) as the query
for relevant documents. RAG-Sequence consid-
ers documents independently, generating an out-
put sequence for each concatenated context sepa-
rately and marginalizing over the output genera-
tions. RAG-Tokenmarginalizes the output distri-
bution over all documents, allowing the generator
to attend over a different document for each token.
Though d(zj) remains ﬁxed during training, token
losses are propagated to the retriever itself, and the
context representations q(xi) are updated in order
to better ﬁt the retriever for the task.
Izacard and Grave (2021b) introduce the FiD
(Fusion-in-Decoder) method. Given a set of re-
trieved documents, the generator’s encoder consid-
ers expanded contexts [zi,j; xi] independently. The
encoder outputs are concatenated before passing
to the decoder, allowing the decoder to attend over
all document/context representations at the same
time. Despite ﬁxing the retriever throughout train-
ing, FiD demonstrates superior performance on a
number of QA tasks, demonstrating its efﬁcacy in
attending over several documents.
3.2 Improving Neural Retrieval
The introduction of neural retrieval is a major driver
of the performance gains achieved in QA tasks by
the RAG and FiD models; when substituting a non-
neural retriever, performance in open-domain QA
tasks suffers dramatically (Lewis et al., 2020b). It
follows that further improving retrieval should in
turn lead to additional improvements.
In DPR a dialogue context and a candidate doc-
ument interact only via a ﬁnal dot-product simi-
larity score. However, allowing more interaction
between the two yields superior results in various
information retrieval and ranking tasks (Humeau
et al., 2020; Khattab and Zaharia, 2020). Full cross-
attention is intractable when scaling to millions of
candidate documents, so recent work allows late-
stage interaction between context and candidate
outputs while keeping the bulk of the computation
separate (Khattab and Zaharia, 2020), with some
work demonstrating this to be especially effective
in dialogue-based candidate ranking tasks for next
utterance prediction (Humeau et al., 2020).
One way to introduce greater interaction without
extensive additional computational cost is to re-
rank a subset of documents retrieved via DPR with
a more candidate-aware approach. For this method,
we employ Poly-encoders (Humeau et al., 2020),
which introduce an additional attention mechanism
that yields candidate-aware context representations
prior to a ﬁnal scoring computation. We denote
this method DPR-Poly; one can also choose to
initialize the Poly-encoder with the DPR model
weights, a method we denote Joint DPR-Poly
We additionally explore a way to use greater
context-candidate interaction in the full retrieval
setup. In a PolyFAISS setup, we ﬁrst train a Poly-
encoder to vary its scoring mechanism between a
standard dot-product and a Poly-encoder score. We
then create a FAISS index from thed(zj) represen-
tations obtained from the Poly-encoder’s candidate
encoder, and query the index via a reduction of the
standard Poly-encoder context representation. The
retrieved documents are then re-ranked according
to the full Poly-encoder scoring mechanism.
3.3 Improving Augmented Generation
Multi-turn dialogue contexts may be harder for re-
trieval systems than the single question context in3787
QA. Indeed, preceding methods for knowledge-
grounded dialogue have tried to incorporate se-
quence position into retrieval (Fan et al., 2021), or
consider a sequential decision process (Kim et al.,
2020). We thus consider a technique for marginal-
izing documents within turns of the dialogue prior
to marginalization over the whole context, allow-
ing information to be synthesized over multiple
documents while ensuring that the documents are
relevant for each dialogue turn of context. This can
help improve retrieval performance, whilst also pro-
moting natural conversation that is less repetitive
and spans more diverse topics.
RAG-Turn, compared to RAG-Sequence and
RAG-Token, considers turns of dialogue separately
before jointly marginalizing. We consider our con-
text x to now be a set Xof T turns, such that X=
{x1, ...xT}. We deﬁne the full set of documents
retrieved for a context Xto be Z= {Z1, ...,ZT},
where Zt = {z1, ...zk}is the set of k documents
retrieved for turn t in context X.
RAG-Turn Doc-Then-Turn: As each turn con-
siders a potentially different set of documents, one
can ﬁrst marginalize over the documents within a
turn, and then marginalize over documents across
turns, for each token in the resulting sequence:
pTurn-DTT(y|X) ≈
m∏
l
∑
xt∈X
∑
zi∈Zt
pη(zi|xt)pθ(yl|xt, zi, y1...yl−1)
RAG-Turn Doc-Only: We can alternatively
consider each turn independently while consider-
ing documents within a turn jointly. We deﬁne the
generator probability pTurn-DO(y|xt) for turn xt as:
m∏
l
∑
zi∈Zt
pη(zi|xt)pθ(yl|xt, zi, y1...yl−1)
For training, different turns are considered differ-
ent contexts entirely, and loss is computed against
the ground truth label for each turn. For inference,
we follow a similar technique to “thorough” de-
coding (Lewis et al., 2020b) by ﬁrst generating a
candidate sequence for each turn, and then running
an additional forward pass to rescore the ﬁnal gener-
ations; we found this method to outperform simple
post-hoc re-ranking of all the candidate beams.
To avoid excessive computation as the dialogue
context grows, we ﬁx a value T∗= 1 ≤T∗≤T,
such that the most recent T∗turns are considered
independently, and all turns prior are considered
jointly, yielding T∗+ 1 total context “turns”.
Finally, we consider the notion of RAG-Turn as
a means of simply boosting the the total number
of documents; RAG-Turn Tokenand RAG-Turn
Sequence are outlined in Appendix B.
3.4 Improving Fusion-in-Decoder
Though FiD does not train its retriever, it more
efﬁciently attends over larger sets of documents
than RAG, as the independent encoder outputs are
fused before decoding the ﬁnal generation. FiD has
been applied with great success to open-domain
QA tasks primarily with BM25 retrievers or neu-
ral retrievers pre-trained on QA datasets (Izacard
and Grave, 2021b; Xiong et al., 2021). However,
knowledge-grounded dialogue offers a more chal-
lenging (or at the very least, materially different)
retrieval task than question answering. We thus
explore whether we can improve upon out-of-the-
box FiD by incorporating retrievers trained in a
RAG setup; we refer to models with a DPR-based
retriever trained with RAG, and then used with FiD,
as FiD-RAG, and apply relevant sufﬁxes to denote
comparison to our other retrieval methods.
4 Experiments
Datasets: We conduct experiments on two datasets:
Wizard of Wikipedia (WoW) (Dinan et al., 2019b)
and CMU Document Grounded Conversations
(CMU_DoG) (Zhou et al., 2018) which are both
sets of knowledge-grounded dialogues collected
through human-human crowdworker chats in En-
glish, where one of the crowdworkers had access
to external knowledge from Wikipedia; WoW dis-
cusses various topics, and CMU_DoG discusses
movies. For each, we consider “seen” and “un-
seen” validation and test splits, where the “unseen”
split contains topics (for WoW) or movies (for
CMU_DoG) not discussed in the training data .
WoW provides these splits, and we constructed
our own for CMU_DoG. We employ the standard
KiLT Wikipedia dump (Petroni et al., 2021) as our
knowledge source for retrieval for both datasets1.
More dataset details are in Appendix C.
Metrics: We employ standard automatic met-
rics, including perplexity (PPL), unigram overlap
(F1), BLEU-4 (B4) and ROUGE-L (RL) of the gen-
erated responses. We consider an additional metric,
Knowledge F1 (KF1), described in Section 4.2,
1https://github.com/facebookresearch/KILT3788
WoW Valid Seen CMU_DoG Test Seen
PPL F1 KF1 PPL F1 KF1
Repeat Gold
Response - 100 35.9 - 100 5.21
Knowledge - 35.9 100 - 5.21 100
BART-Large
None 14.8 21.0 17.7 15.4 16.0 6.8
RAG 11.6 22.5 26.0 12.8 14.9 9.1
Gold 7.9 39.1 61.2 14.2 15.6 8.6
Table 1: Knowledge Usage on WoW (Valid Seen)
and CMU_DoG (Test Seen). Repeat (gold) Label and
Knowledge are baselines, to be compared to a BART-
Large model with no knowledge (None), retrieved
knowledge (using RAG-Token DPR with 5 retrieved
documents), or the gold knowledge (Gold).
Gen. Retr. PPL F1 KF1 B4 RL
BB None 11.2 19.7 16.3 1.4 18.8
RAG DPR 9.0 21.1 23.7 3.0 21.2
RAG DPR-Poly 9.7 21.1 24.2 3.0 21.0
BART None 14.7 20.9 17.4 1.7 20.3
FiD 13.7 20.8 21.5 2.5 21.2
RAG DPR 12.7 22.4 22.5 3.4 22.9
RAG DPR-Poly 11.4 22.9 26.5 3.9 23.5
FiD-RAG DPR 11.8 21.1 29.6 3.8 22.7
FiD-RAG DPR-Poly 11.4 22.1 29.7 4.1 23.0
T5 None 12.1 19.3 14.6 1.0 18.1
RAG DPR 9.8 21.9 25.9 3.8 22.1
FiD-RAG DPR 9.5 22.0 27.8 3.9 22.3
Table 2: Comparing Seq2Seq Models and Re-
trieval Augmentations on Wow Test (Seen), using
BlenderBot-400m (BB), BART-Large, and T5-Large.
Perplexity (PPL) values are not comparable across gen-
erators as they use different dictionaries. Retrieval
models retrieve 5 documents over all of Wikipedia. All
RAG models are RAG-Token.
and also consider human evaluations. Full training
details can be found in Appendix D.
4.1 Retrieval Effectiveness
We ﬁrst demonstrate in Table 1 that using a stan-
dard RAG-Token DPR model with BART-Large
indeed outperforms BART-Large itself without re-
trieval augmentation on both datasets, given only
the dialogue context and retrieving knowledge
from the entire of Wikipedia. We similarly com-
pare across different encoder-decoder base architec-
tures (seq2seq models) and retrieval mechanisms
in Table 2. Overall, we see that retrieval helps
substantially in improving performance on both
knowledge-grounded conversational datasets.
4.2 Eliminating Hallucination
We want to know whether the model is grounding
appropriately on its retrieved knowledge, and not
simply learning to copy common words from the
retrieved documents (as we use an unstructured
knowledge source with all the tokens in English
Wikipedia). Despite their usefulness in related
ﬁelds such as machine translation and QA, stan-
dard automated metrics such as F1, BLEU, and
ROUGE have been shown to be not totally cor-
related with how well neural conversational mod-
els perform in the wild (Liu et al., 2016; Dinan
et al., 2019a; Mehri and Eskenazi, 2020). We
thus introduce an additional metric, Knowledge
F1. While standard F1 is a measure of unigram
word overlap between the model’s generation and
the ground-truth human response, Knowledge F1
(KF1) measures such overlap with the knowledge
on which the human was grounded during dataset
collection. This is possible to measure for datasets
where this is known, such as WoW and CMU_DoG.
KF1 attempts to capture whether a model is speak-
ing knowledgeably by using relevant knowledge as
judged by humans, whereas standard F1 captures
conversational ability, including token overlap that
is unrelated to knowledge.
Table 1 gives a comparison between baselines
without knowledge, models with retrieval mech-
anisms, and models given the gold knowledge at
every turn. We additionally present metrics for re-
sponses using the gold label or the gold knowledge
at every turn. While the gap between baselines
and retrieval-augmented models using regular F1 is
noticeable, the gap grows signiﬁcantly when con-
sidering Knowledge F1, indicating this factor is the
true source of the retrieval-augmentation method’s
gains. These results conﬁrm that the models are
appropriately utilizing knowledge.
4.2.1 Human Evaluations of Conversations
We conduct annotations of 100 model responses to
various conversational contexts from the WoW test
set (unseen). Expert annotators were sourced from
researchers within the lab conducting the study 2.
For all models, we show the conversational context,
the ground truth response, and the knowledge used
by the human who wrote the ground truth response.
Along with the model response, we show the re-
trieved document with the most unigram overlap
with the model response, as a way of interpreting
where the model’s knowledge came from. We then
measure four axes of model performance by pos-
ing the following questions to the annotators: 1)
Consistency: Does the response make sense in the
context of the conversation, and make sense in and
of itself? 2) Engagingness: Are you engaged by
the response? Do you want to continue the con-
2180 annotations were collected from 8 annotators, result-
ing in 1620 total annotations across 9 models.3789
Topic: Thierry Henry
Apprentice: Thierry Henry is one of my all time favorite players. What about you?
Wizard: He was good. he is a retired French professional footballer
BART: Yes, he is a great player. He was born in 1931 and played for England’syouth teams.
RAG-Token: I love him too! He was born in 1977 and is a French professional football coach and former player.
FiD-RAG: I love him too! He is the current manager of French club Monaco. He was born in 1977.
Topic: Elvis Presley
Wizard: Oh baby.... Elvis Presley is truly The King of Rock and Roll!
Apprentice: yes...who doesn’t love Elvis Presley and his music?
Wizard: Seriously. I have been planning a vacation to Graceland myself. I have family in Tennessee so it could be good.
Apprentice: I would love to tour his home. Which one of his songs is your favorite?
Wizard: I always liked “Hounddog.” My grandpa heard Elvis sing it live in Memphis.
BART: I love all of his albums. I think my favorite is “Love Me Do” which came out in 1999.
FiD-RAG: I really like “Heartbreak Hotel” which was released in 1956 and became a number one hit in the United States
Table 3: Hallucination in (Non-)Retrieval-Augmented Models. Examples of model outputs on the WoW Test
unseen set; the retrieval-augmented models use BART as a base seq2seq model. Highlighted yellow text blocks
are demonstrably false statements, as veriﬁed by Wikipedia. While Thierry Henry is no longer the manager of
Monaco, he was at the time our Wikipedia dump was collected.
Model # Docs Cons. Eng. Knowl. Hall.
BART-Large - 81.8 85.5 34.1 68.2
RAG-Seq. 5 80.2 71.2 94.9 9.6
RAG-Tok. 5 85.3 77.4 93.2 17.0
RAG-Tok. 25 87.0 81.9 88.7 21.5
RAG-Tok. DPR-Poly 5 89.3 77.9 97.7 20.9
RAG-Turn-DTT 5 74.6 73.0 94.3 15.6
RAG-Turn-DO 5 84.0 85.0 94.0 21.0
FiD-RAG 5 90.1 78.0 96.1 7.9
FiD-RAG 25 87.6 81.4 81.4 19.8
Table 4: Human Evaluations of Various Models
on Wow Test (Unseen), measuring percentage of
model outputs that are Consistent (Cons.), Engaging
(Eng.), Knowledgeable (Knowl.), and a Hallucina-
tion (Hall.). All retrieval models use BART-Large.
versation? 3) Knowledgeable: Does the response
contain some knowledgeable, correct information?
4) Hallucination: Is some of the model output fac-
tually incorrect? An admixture of ideas?
The evaluation results are shown in Table 4.
Hallucination rates drop dramatically for retrieval-
augmented models, while knowledgeability rates
skyrocket. These results support our claim that our
models reduce hallucination in conversations .
We show example model outputs in Table 3.
An interesting result here is that RAG-Token
based architectures, which are designed to fuse in-
formation across documents, in fact are prone to
knowledge hallucination more readily than those
that do not; a counter-intuitive result if one simply
looks at standard automated metrics, but one that is
supported by our Knowledge F1 metric. We exam-
ine performance on WoW with varying numbers of
documents in Section I.6 and Table 23 in the Ap-
pendix. Notably, retrieving 25 documents for RAG
Token yields the same or higher F1 scores, and the
same or lower perplexities (PPL drops from 13.4 to
13.0 on valid unseen; F1 increases from 22.5 to 22.6
for valid seen), and yet we seelower Knowledge F1
scores (26.0 to 24.7 valid seen, 22.7 to 21.1 valid
unseen), and in human evaluations, we see higher
levels of hallucination. Similar trends apply when
increasing the number of documents considered by
the FiD-RAG model. Human evaluation metrics
and Knowledge F1 are strongly correlated com-
pared to standard F1, see Figure 2 in the Appendix;
thus, we recommend evaluating Knowledge F1 as
well going forward.
4.2.2 Factuality and conversationality
Table 4 shows that consistency and engaging-
ness are generally comparable across retrieval-
augmented models and the relevant baselines, with
slight drops in engagingness attributed to some
models relying too much on retrieved knowledge.
That is, factuality does not seem to sacriﬁce con-
versational ability. This is also in line with F1
and Knowledge F1 scores from e.g. Tables 1 and 2.
Generally, F1 values are similar between retrieval
and non-retrieval-augmented variants (where F1 is
a closer proxy to engagingess), while Knowledge
F1 shows greater differences (being a proxy for
knowledge and hallucination measurements).
4.3 Generalization to Unseen Distributions
Table 5 shows automated metrics for model eval-
uations on the unseen data distributions for WoW
and our modiﬁed CMU_DoG split. Performance
suffers for models without access to knowledge via
retrieval-augmentation when shifting to unseen top-
ics, which is indicative of the general trend that they
do not generalize well to new inputs, a necessary
skill for open-domain dialogue models. Models
that can ground on knowledge, meanwhile, do not
suffer from this problem nearly as much, as the3790
WoW Test Unseen CMU_DoG Test Unseen
Seq2Seq Model Retrieval Mechanism PPL F1 KF1 B4 RL PPL F1 KF1 B4 RL
BART-Large None 18.9 18.7 15.0 0.9 18.4 20.7 15.3 5.7 0.6 18.3
FiD 15.1 19.9 20.4 2.4 20.5 18.4 14.5 7.7 0.6 20.2
RAG DPR 14.5 21.7 20.8 2.6 21.7 16.0 14.8 7.5 0.5 20.4
RAG DPR-Poly 13.2 21.8 24.3 3.4 22.3 16.0 15.2 7.3 0.6 20.9
FiD-RAG DPR 13.5 20.4 27.8 3.7 22.3 17.9 14.1 8.9 0.6 20.5
FiD-RAG DPR-Poly 13.1 21.1 27.1 3.8 22.6 - - - - -
T5-Large None 13.8 18.4 13.8 0.8 17.2 - - - - -
RAG DPR 11.0 20.5 21.9 2.8 20.4 - - - - -
FiD-RAG DPR 10.8 20.9 26.1 3.7 21.2 - - - - -
Table 5: Comparison of Seq2Seq Models and Retrieval Mechanisms on Unseen Distributions using WoW
Test Unseen and our modiﬁed CMU_DoG Test Unseen split. Perplexity (PPL) values are not comparable across
different seq2seq architectures as they use different dictionaries. Retrieval models are retrieving 5 documents over
all of Wikipedia. All RAG models are RAG-Token.
Test Seen Test Unseen
Method Knowledge Source PPL F1 B4 RL PPL F1 B4 RL
BlenderBot (Roller et al., 2021) None 8.72 18.8 13 10.4 17.8 0.7
BART (ours) None 14.7 20.9 1.7 20.3 18.9 18.7 0.9 18.4
DRD (Zhao et al., 2020a) WoW 23.0 18.0 5.5 25.6 16.5 4.3
KIF (Fan et al., 2021) WoW 23.9
KIF (Fan et al., 2021) WoW + Train Utts *25.9 *22.3
FiD-RAG (Ours) Wikipedia (WoW Subset) 10.5 23.2 4.4 24.2 10.7 23.2 4.6 24.4
RAG DPR-Poly (Ours) Wikipedia (All) 11.4 22.9 3.9 23.5 13.2 21.8 3.4 22.3
FiD-RAG DPR-Poly (Ours) Wikipedia (All) 10.7 22.9 4.1 23.8 12.0 22.1 3.7 23.1
Table 6: WoW Comparison to Existing Results. "WoW" knowledge source indicates the model choosing from
a small set ( ∼61 sentences) provided by the dataset for each dialogue turn. Methods with * augmented their
knowledge source with training utterances, which is useful on Test Seen data, but likely not as useful on Unseen
data. Our models use BART as the base seq2seq model; the RAG and FiD-RAG models retrieve 5 documents, and
the FiD-RAG DPR-Poly model retrieves 25. Other prior models are compared in Table 14 in the Appendix.
Valid Seen Valid Unseen
RAG Type PPL F1 KF1 PPL F1 KF1
Retrieve over Most Recent Turn
Sequence 13.5 20.8 23.3 15.5 20.1 21.4
Token 13.8 21.1 22.3 15.8 21.1 21.0
Retrieve over Full Dialogue Context
Sequence 11.1 21.5 27.9 12.6 20.3 24.6
Token 11.6 22.5 26.0 13.4 21.8 22.7
Turn-DTT 11.9 22.2 28.0 13.6 21.1 24.3
Turn-DO 13.3 23.1 26.8 15.4 22.0 23.3
Table 7: Comparison of RAG Model Types on WoW
Valid Seen/Unseen. Each retrieves 5 documents over
all of Wikipedia. We setT∗ = 1 for RAG-Turn models.
All models use BART as the base seq2seq model.
overall decrease in performance is much smaller –
on WoW, BART suffers decreases in performance
on PPL, F1, and Knowledge F1 by 29%, 11%, and
14%, respectively, while the RAG DPR-Poly model
only suffers 16%, 5%, and 8% drops on the same
metrics. Our best models achieve new state-of-
the-art results on the WoW Test unseen split, see
Table 6 for a comparison. Knowledge F1 scores
remain quite high, with retrieval-augmented mod-
els generally decreasing performance the least with
respect to this metric, indicating the augmentation
can effectively retrieve knowledge on these topics.
4.4 Augmenting Generation
4.4.1 Conditioning on turns of dialogue
Table 7 compares our RAG-Turn methods de-
scribed in Section 3.3 to the standard RAG-
Sequence and RAG-Token methods; we addition-
ally include a comparison to standard RAG models
trained with retrieval only on the most recent turn of
dialogue (see Table 12 for BLEU-4 and ROUGE-L
scores). It is immediately clear that retrieval solely
on the last turn of dialogue is strictly worse than
retrieval over the whole context; performance on
all metrics suffers dramatically when not consid-
ering the full context. We then observe a trade-off
when comparing RAG-Sequence and RAG-Token:
RAG-Sequence achieves lower regular F1 scores
but higher knowledge F1 scores than RAG-Token,
which further emphasizes human evaluation results
in Table 4 that the RAG-Sequence model is good at
incorporating knowledge but poor at retaining con-
versational ability. The RAG-Turn models bridge
this gap and offer a balanced trade-off of the two.
The RAG-Turn Doc-Then-Turn method yields F1
scores higher than the RAG-Sequence model, and
higher Knowledge F1 scores than the RAG-Token
model; the Doc-Only RAG-Turn method achieves
the highest F1 on both the seen/unseen splits, and3791
Valid Seen Valid Unseen
Model PPL F1 KF1 PPL F1 KF1
BART
FiD 13.7 21.2 22.5 15.4 20.5 20.5
FID-RAG 11.9 21.1 30.0 13.5 20.8 27.5
FID-RAG-Poly 11.6 22.1 29.7 13.0 22.0 28.4
T5
FID 11.6 20.3 21.0 12.4 20.4 20.8
FID-RAG 9.5 22.6 28.8 10.9 21.7 26.0
Table 8: Comparison of retrievers used in FiD on
WoW Valid (Seen/Unseen). Each retrieves 20 doc-
uments at train time, and 5 for inference. Perplex-
ity (PPL) values are not comparable across different
seq2seq architectures as they use different dictionaries.
Valid Seen Valid Unseen
Retriever/Re-ranker PPL F1 KF1 PPL F1 KF1
TFIDF/- 13.1 21.6 23.0 15.2 21.1 21.6
DPR/- 11.6 22.5 26.0 13.4 21.8 22.7
TFIDF/DPR 12.5 21.8 23.1 14.5 21.4 20.2
DPR/Poly 11.7 23.0 26.5 13.1 22.6 24.4
DPR/Poly (Joint) 11.6 23.0 27.4 13.1 22.1 24.7
PolyFAISS/- 12.1 22.9 24.8 14.2 21.6 20.6
Table 9: Comparison of re-rankers for BART RAG-
Token models on WoW Valid Seen/Unseen, using 5
retrieved documents.
improves on Knowledge F1 scores of the RAG-
Token model. For results with different T∗values,
as well as results with RAG-Turn Token and RAG-
Turn Sequence, see Section F and Table 13 in the
appendix.
4.4.2 Improving FiD-based generation
Table 8 compares the usage of various retrievers in
a FiD setup. It is clear that FiD is suboptimal out-
of-the-box for knowledge-grounded dialogue, and
incorporating retrievers trained via RAG improves
performance considerably. Speciﬁcally, we see
large decreases in perplexity, and signiﬁcant gains
in Knowledge F1: FiD-RAG-Poly, with BART,
improves Knowledge F1 by 33% and 41% on the
seen/unseen splits respectively; FiD-RAG with T5
sees gains of 37% and 25%.
4.5 Effectiveness of Retrieval Enhancements
Table 9 outlines results on the WoW validation sets
for our various retrieval/re-ranker augmentations.
Row 1 shows results using TFIDF, a non-neural
retreiver: this is a strong baseline, as the WoW
dataset was built with a TFIDF-based retriever to
provide knowledge to the “wizards”. Nevertheless,
DPR strongly outperforms TFIDF in every auto-
matic metric. As for our neural-based methods,
we see that using the code re-ranking approach
via adding a Poly-encoder re-ranker on top of the
standard DPR retriever for RAG yields the best per-
forming model with respect to automated metrics
on both splits of the validation set. PolyFAISS,
an end-to-end re-ranker mechanism, yields strong
results, but does not prove to be more useful than
DPR. Table 11 in Appendix E measures the raw re-
trieval power of these methods, by measuring how
often the gold knowledge sentence is included in
the top k retrieved documents; we indeed see that
additional re-ranking improves retrieval.
4.6 Additional Ablations
Due to space constraints, we provide several
additional ablations in the Appendix. In Sec-
tion I.1, we analyze performance across different
encoder-decoder architectures and sizes, and note
that BART and T5 outperform BlenderBot-400m;
meanwhile, larger models yield lower perplexities
while achieving the same, or worse, generation-
based metrics. In Section I.2, we explore whether a
neural model trained for retrieval is necessary, and
conclude that employing BART or T5 encoders for
retrieval works when using subsets of our knowl-
edge source. In Section I.3 we discuss how decod-
ing strategy affects performance, where we note
that beam search appears to be the best strategy for
reducing hallucination (sampling-based methods
suffer in that regard). In Section I.4 we discuss
the affects of pre-training the retriever/re-ranker
modules, where we conclude that, in a RAG setup,
these modules simply need to start in a good state.
In Section I.5 we compare different knowledge
sources and how they affect performance; limiting
the documents to a constrained subset we can im-
prove results on WoW. Finally, in section I.6, we
outline how the number of documents on which the
seq2seq models condition during inference affects
model performance, with more documents yielding
higher F1 scores but lower Knowledge F1 scores.
5 Discussion
We have thus far explored several ways of retriev-
ing and conditioning on documents in knowledge-
grounded dialogue; here, we summarize some key
takeaways from our results.
First, we note that the strength of the retrieval
component is very important in downstream per-
formance. Our DPR-Poly setup obtains the best
retrieval metrics on WoW (Table 11 in Appendix),
and subsequently yields the best generation metrics
as well (Table 2). The FiD-RAG model clearly
demonstrates the importance of a retriever tuned3792
for open-domain dialogue (Table 5).
Second, we note that models that condition on
several documents simultaneously result in more
engaging conversationalists; RAG-Token, RAG-
Turn, and FiD-RAG yield higher F1 scores (Table
7) and higher engaginginess/consistency scores (Ta-
ble 4) than RAG-Sequence, while maintaining high
knowledgeability; RAG-Turn, in certain conﬁgu-
rations, demonstrates that conditioning on turns
of dialogue independently yields beneﬁts for auto-
mated metrics as well. We ﬁnd the FiD architec-
ture to be more optimal when considering several
documents jointly (higher F1/KF1, lower human-
evaluated hallucination) though we note that all
models suffer from more hallucination when we
condition on more documents for each generation
(Table 4, Table 23 in Appendix).
Finally, we note that standard metrics used for
open-domain dialogue are not sufﬁcient for truly
capturing hallucination within models; thus, met-
rics such as Knowledge F1 are required to further
study model performance – Figure 2 in the Ap-
pendix highlights correlations between such auto-
mated metrics and human evaluations.
6 Conclusion
In this work, we have studied the problem of knowl-
edge hallucination in conversational agents, an im-
portant problem as current systems often produce
factually inaccurate generations. We have shown
that this problem occurs independently of language
model size or training data. Retrieval-augmented
generation in particular is an intuitively promising
solution to this problem, and in detailed experi-
ments we have shown that this class of approaches
signiﬁcantly reduces the hallucination problem in
dialogue while maintaing conversational ability,
and can help generalize beyond the training data
on previously unseen distributions. Future work
should look for improved methods and to ﬁnd solu-
tions to unanswered questions, such as understand-
ing the interplay between retrieved knowledge and
knowledge stored in the model’s weights.
References
Daniel Adiwardana, Minh-Thang Luong, David R So,
Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang,
Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu,
et al. 2020. Towards a human-like open-domain
chatbot. arXiv preprint arXiv:2001.09977.
Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi,
Richard Socher, and Caiming Xiong. 2020. Learn-
ing to retrieve reasoning paths over wikipedia graph
for question answering. In 8th International Confer-
ence on Learning Representations, ICLR 2020, Ad-
dis Ababa, Ethiopia, April 26-30, 2020 . OpenRe-
view.net.
Jason Baumgartner, Savvas Zannettou, Brian Kee-
gan, Megan Squire, and Jeremy Blackburn. 2020.
The pushshift reddit dataset. arXiv preprint
arXiv:2001.08435.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen,
Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin
Chess, Jack Clark, Christopher Berner, Sam Mc-
Candlish, Alec Radford, Ilya Sutskever, and Dario
Amodei. 2020. Language models are few-shot learn-
ers. In Advances in Neural Information Processing
Systems 33: Annual Conference on Neural Informa-
tion Processing Systems 2020, NeurIPS 2020, De-
cember 6-12, 2020, virtual.
M. D. Bruyn, E. Lotﬁ, Jeska Buhmann, and W. Daele-
mans. 2020. Bart for knowledge grounded conversa-
tions. In Converse@KDD.
Yuanyuan Cai, M. Zuo, Qingchuan Zhang, Haitao
Xiong, and Ke Li. 2020. A bichannel transformer
with context encoding for document-driven con-
versation generation in social media. Complex.,
2020:3710104:1–3710104:13.
Danqi Chen, Adam Fisch, Jason Weston, and Antoine
Bordes. 2017. Reading Wikipedia to answer open-
domain questions. In Proceedings of the 55th An-
nual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers) , pages 1870–
1879, Vancouver, Canada. Association for Computa-
tional Linguistics.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference
of the North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers) ,
pages 4171–4186, Minneapolis, Minnesota. Associ-
ation for Computational Linguistics.
Emily Dinan, Varvara Logacheva, Valentin Malykh,
Alexander Miller, Kurt Shuster, Jack Urbanek,
Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan
Lowe, and et al. 2019a. The second conversational
intelligence challenge (convai2). The Springer
Series on Challenges in Machine Learning , page
187–208.3793
Emily Dinan, Stephen Roller, Kurt Shuster, Angela
Fan, Michael Auli, and Jason Weston. 2019b. Wiz-
ard of wikipedia: Knowledge-powered conversa-
tional agents. In 7th International Conference on
Learning Representations, ICLR 2019, New Orleans,
LA, USA, May 6-9, 2019. OpenReview.net.
Angela Fan, Claire Gardent, Chloé Braud, and An-
toine Bordes. 2021. Augmenting transformers with
KNN-based composite memory for dialog. Transac-
tions of the Association for Computational Linguis-
tics, 9:82–99.
Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hi-
erarchical neural story generation. In Proceedings
of the 56th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers),
pages 889–898, Melbourne, Australia. Association
for Computational Linguistics.
Fabian Galetzka, Chukwuemeka Uchenna Eneh, and
David Schlangen. 2020. A corpus of controlled
opinionated and knowledgeable movie discussions
for training neural conversation models. In Proceed-
ings of the 12th Language Resources and Evaluation
Conference, pages 565–573, Marseille, France. Eu-
ropean Language Resources Association.
Marjan Ghazvininejad, Chris Brockett, Ming-Wei
Chang, Bill Dolan, Jianfeng Gao, Wen-tau Yih, and
Michel Galley. 2018. A knowledge-grounded neural
conversation model. In Proceedings of the Thirty-
Second AAAI Conference on Artiﬁcial Intelligence,
(AAAI-18), the 30th innovative Applications of Arti-
ﬁcial Intelligence (IAAI-18), and the 8th AAAI Sym-
posium on Educational Advances in Artiﬁcial Intel-
ligence (EAAI-18), New Orleans, Louisiana, USA,
February 2-7, 2018, pages 5110–5117. AAAI Press.
Karthik Gopalakrishnan, Behnam Hedayatnia,
Qinglang Chen, Anna Gottardi, Sanjeev Kwatra,
Anu Venkatesh, Raefer Gabriel, Dilek Hakkani-Tür,
and Amazon Alexa AI. 2019. Topical-chat: Towards
knowledge-grounded open-domain conversations.
In INTERSPEECH, pages 1891–1895.
Edouard Grave, Armand Joulin, and Nicolas Usunier.
2017. Improving neural language models with a
continuous cache. In 5th International Conference
on Learning Representations, ICLR 2017, Toulon,
France, April 24-26, 2017, Conference Track Pro-
ceedings. OpenReview.net.
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-
pat, and Ming-Wei Chang. 2020. Realm: Retrieval-
augmented language model pre-training. arXiv
preprint arXiv:2002.08909.
Behnam Hedayatnia, Karthik Gopalakrishnan,
Seokhwan Kim, Yang Liu, Mihail Eric, and
Dilek Hakkani-Tur. 2020. Policy-driven neural
response generation for knowledge-grounded dialog
systems. In Proceedings of the 13th International
Conference on Natural Language Generation ,
pages 412–421, Dublin, Ireland. Association for
Computational Linguistics.
Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and
Yejin Choi. 2020. The curious case of neural text
degeneration. In 8th International Conference on
Learning Representations, ICLR 2020, Addis Ababa,
Ethiopia, April 26-30, 2020. OpenReview.net.
Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux,
and Jason Weston. 2020. Poly-encoders: Architec-
tures and pre-training strategies for fast and accurate
multi-sentence scoring. In 8th International Confer-
ence on Learning Representations, ICLR 2020, Ad-
dis Ababa, Ethiopia, April 26-30, 2020 . OpenRe-
view.net.
Gautier Izacard and Edouard Grave. 2021a. Distilling
knowledge from reader to retriever for question an-
swering. In International Conference on Learning
Representations.
Gautier Izacard and Edouard Grave. 2021b. Lever-
aging passage retrieval with generative models for
open domain question answering. In Proceedings
of the 16th Conference of the European Chapter
of the Association for Computational Linguistics:
Main Volume, pages 874–880, Online. Association
for Computational Linguistics.
Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.
Billion-scale similarity search with gpus. IEEE
Transactions on Big Data.
Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke
Zettlemoyer. 2017. TriviaQA: A large scale dis-
tantly supervised challenge dataset for reading com-
prehension. In Proceedings of the 55th Annual Meet-
ing of the Association for Computational Linguistics
(Volume 1: Long Papers) , pages 1601–1611, Van-
couver, Canada. Association for Computational Lin-
guistics.
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick
Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and
Wen-tau Yih. 2020. Dense passage retrieval for
open-domain question answering. In Proceedings of
the 2020 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP) , pages 6769–
6781, Online. Association for Computational Lin-
guistics.
Urvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke
Zettlemoyer, and Mike Lewis. 2021. Nearest neigh-
bor machine translation. In International Confer-
ence on Learning Representations.
Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke
Zettlemoyer, and Mike Lewis. 2020. Generaliza-
tion through memorization: Nearest neighbor lan-
guage models. In 8th International Conference on
Learning Representations, ICLR 2020, Addis Ababa,
Ethiopia, April 26-30, 2020. OpenReview.net.
Omar Khattab, Christopher Potts, and Matei Zaharia.
2020. Relevance-guided supervision for openqa
with colbert.3794
Omar Khattab and Matei Zaharia. 2020. Colbert. Pro-
ceedings of the 43rd International ACM SIGIR Con-
ference on Research and Development in Informa-
tion Retrieval.
Byeongchang Kim, Jaewoo Ahn, and Gunhee Kim.
2020. Sequential latent knowledge selection for
knowledge-grounded dialogue. In 8th International
Conference on Learning Representations, ICLR
2020, Addis Ababa, Ethiopia, April 26-30, 2020 .
OpenReview.net.
D. P. Kinga and J. Ba. 2015. A method for stochastic
optimization. In International Conference on Learn-
ing Representations (ICLR), volume 5.
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-
ﬁeld, Michael Collins, Ankur Parikh, Chris Al-
berti, Danielle Epstein, Illia Polosukhin, Jacob De-
vlin, Kenton Lee, Kristina Toutanova, Llion Jones,
Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai,
Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.
Natural questions: A benchmark for question an-
swering research. Transactions of the Association
for Computational Linguistics, 7:452–466.
Mike Lewis, Yinhan Liu, Naman Goyal, Mar-
jan Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Veselin Stoyanov, and Luke Zettlemoyer.
2020a. BART: Denoising sequence-to-sequence pre-
training for natural language generation, translation,
and comprehension. In Proceedings of the 58th An-
nual Meeting of the Association for Computational
Linguistics, pages 7871–7880, Online. Association
for Computational Linguistics.
Patrick S. H. Lewis, Ethan Perez, Aleksandra Pik-
tus, Fabio Petroni, Vladimir Karpukhin, Naman
Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih,
Tim Rocktäschel, Sebastian Riedel, and Douwe
Kiela. 2020b. Retrieval-augmented generation for
knowledge-intensive NLP tasks. In Advances in
Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Sys-
tems 2020, NeurIPS 2020, December 6-12, 2020,
virtual.
Chia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Nose-
worthy, Laurent Charlin, and Joelle Pineau. 2016.
How NOT to evaluate your dialogue system: An
empirical study of unsupervised evaluation metrics
for dialogue response generation. In Proceedings of
the 2016 Conference on Empirical Methods in Natu-
ral Language Processing, pages 2122–2132, Austin,
Texas. Association for Computational Linguistics.
Longxuan Ma, Wei-Nan Zhang, Runxin Sun, and Ting
Liu. 2020. A compare aggregate transformer for un-
derstanding document-grounded dialogue. In Find-
ings of the Association for Computational Linguis-
tics: EMNLP 2020 , pages 1358–1367, Online. As-
sociation for Computational Linguistics.
Joshua Maynez, Shashi Narayan, Bernd Bohnet, and
Ryan McDonald. 2020. On faithfulness and factu-
ality in abstractive summarization. In Proceedings
of the 58th Annual Meeting of the Association for
Computational Linguistics , pages 1906–1919, On-
line. Association for Computational Linguistics.
Shikib Mehri and Maxine Eskenazi. 2020. USR: An
unsupervised and reference free evaluation metric
for dialog generation. In Proceedings of the 58th An-
nual Meeting of the Association for Computational
Linguistics, pages 681–707, Online. Association for
Computational Linguistics.
Stephen Merity, Caiming Xiong, James Bradbury, and
Richard Socher. 2017. Pointer sentinel mixture mod-
els. In 5th International Conference on Learning
Representations, ICLR 2017, Toulon, France, April
24-26, 2017, Conference Track Proceedings . Open-
Review.net.
Sabrina J Mielke, Arthur Szlam, Y-Lan Boureau,
and Emily Dinan. 2020. Linguistic calibration
through metacognition: aligning dialogue agent re-
sponses with expected correctness. arXiv preprint
arXiv:2012.14983.
Alexander Miller, Will Feng, Dhruv Batra, Antoine
Bordes, Adam Fisch, Jiasen Lu, Devi Parikh, and
Jason Weston. 2017. ParlAI: A dialog research soft-
ware platform. In Proceedings of the 2017 Con-
ference on Empirical Methods in Natural Language
Processing: System Demonstrations , pages 79–84,
Copenhagen, Denmark. Association for Computa-
tional Linguistics.
Sewon Min, Danqi Chen, Luke Zettlemoyer, and Han-
naneh Hajishirzi. 2019. Knowledge guided text re-
trieval and reading for open domain question answer-
ing.
Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick
Lewis, Majid Yazdani, Nicola De Cao, James
Thorne, Yacine Jernite, Vladimir Karpukhin, Jean
Maillard, Vassilis Plachouras, Tim Rocktäschel, and
Sebastian Riedel. 2021. KILT: a benchmark for
knowledge intensive language tasks. In Proceedings
of the 2021 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies , pages 2523–2544,
Online. Association for Computational Linguistics.
Fabio Petroni, Tim Rocktäschel, Sebastian Riedel,
Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and
Alexander Miller. 2019. Language models as knowl-
edge bases? In Proceedings of the 2019 Confer-
ence on Empirical Methods in Natural Language
Processing and the 9th International Joint Confer-
ence on Natural Language Processing (EMNLP-
IJCNLP), pages 2463–2473, Hong Kong, China. As-
sociation for Computational Linguistics.
Peng Qi, Haejun Lee, OghenetegiriTGSido, and
Christopher D. Manning. 2020. Retrieve, rerank,
read, then iterate: Answering open-domain ques-
tions of arbitrary complexity from text. ArXiv,
abs/2010.12527.3795
Colin Raffel, Noam Shazeer, Adam Roberts, Kather-
ine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J. Liu. 2020. Exploring
the limits of transfer learning with a uniﬁed text-to-
text transformer. Journal of Machine Learning Re-
search, 21(140):1–67.
Adam Roberts, Colin Raffel, and Noam Shazeer. 2020.
How much knowledge can you pack into the param-
eters of a language model? In Proceedings of the
2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 5418–5426,
Online. Association for Computational Linguistics.
Stephen Roller, Emily Dinan, Naman Goyal, Da Ju,
Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott,
Eric Michael Smith, Y-Lan Boureau, and Jason We-
ston. 2021. Recipes for building an open-domain
chatbot. In Proceedings of the 16th Conference of
the European Chapter of the Association for Compu-
tational Linguistics: Main Volume , pages 300–325,
Online. Association for Computational Linguistics.
Haitian Sun, Tania Bedrax-Weiss, and William Cohen.
2019. PullNet: Open domain question answering
with iterative retrieval on knowledge bases and text.
In Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the
9th International Joint Conference on Natural Lan-
guage Processing (EMNLP-IJCNLP) , pages 2380–
2390, Hong Kong, China. Association for Computa-
tional Linguistics.
David Thulke, Nico Daheim, Christian Dugast, and
Hermann Ney. 2021. Efﬁcient retrieval augmented
generation from unstructured knowledge for task-
oriented dialog.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Pro-
cessing Systems 30: Annual Conference on Neural
Information Processing Systems 2017, December 4-
9, 2017, Long Beach, CA, USA, pages 5998–6008.
Ellen M. V oorhees and Dawn M. Tice. 2000. The
TREC-8 question answering track. In Proceed-
ings of the Second International Conference on
Language Resources and Evaluation (LREC’00) ,
Athens, Greece. European Language Resources As-
sociation (ELRA).
Wenhan Xiong, Xiang Li, Srini Iyer, Jingfei Du, Patrick
Lewis, William Yang Wang, Yashar Mehdad, Scott
Yih, Sebastian Riedel, Douwe Kiela, and Barlas
Oguz. 2021. Answering complex open-domain
questions with multi-hop dense retrieval. In Inter-
national Conference on Learning Representations.
Wenhan Xiong, Mo Yu, Shiyu Chang, Xiaoxiao Guo,
and William Yang Wang. 2019. Improving question
answering over incomplete KBs with knowledge-
aware reader. In Proceedings of the 57th Annual
Meeting of the Association for Computational Lin-
guistics, pages 4258–4264, Florence, Italy. Associa-
tion for Computational Linguistics.
Dani Yogatama, Cyprien de Masson d’Autume, and
Lingpeng Kong. 2021. Adaptive semiparametric
language models. arXiv preprint arXiv:2102.02557.
Rowan Zellers, Ari Holtzman, Hannah Rashkin,
Yonatan Bisk, Ali Farhadi, Franziska Roesner, and
Yejin Choi. 2019. Defending against neural fake
news. In Advances in Neural Information Process-
ing Systems 32: Annual Conference on Neural Infor-
mation Processing Systems 2019, NeurIPS 2019, De-
cember 8-14, 2019, Vancouver, BC, Canada , pages
9051–9062.
Xueliang Zhao, Wei Wu, Chongyang Tao, Can Xu,
Dongyan Zhao, and Rui Yan. 2020a. Low-resource
knowledge-grounded dialogue generation. In 8th
International Conference on Learning Representa-
tions, ICLR 2020, Addis Ababa, Ethiopia, April 26-
30, 2020. OpenReview.net.
Xueliang Zhao, Wei Wu, Can Xu, Chongyang Tao,
Dongyan Zhao, and Rui Yan. 2020b. Knowledge-
grounded dialogue generation with pre-trained lan-
guage models. In Proceedings of the 2020 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 3377–3390, Online. As-
sociation for Computational Linguistics.
Yufan Zhao, Wei Wu, and Can Xu. 2020c. Are pre-
trained language models knowledgeable to ground
open domain dialogues?
Chunting Zhou, Graham Neubig, Jiatao Gu, Mona
Diab, Francisco Guzmán, Luke Zettlemoyer, and
Marjan Ghazvininejad. 2021. Detecting halluci-
nated content in conditional neural sequence gener-
ation. In Findings of the Association for Computa-
tional Linguistics: ACL-IJCNLP 2021 , pages 1393–
1404, Online. Association for Computational Lin-
guistics.
Kangyan Zhou, Shrimai Prabhumoye, and Alan W
Black. 2018. A dataset for document grounded con-
versations. In Proceedings of the 2018 Conference
on Empirical Methods in Natural Language Process-
ing, pages 708–713, Brussels, Belgium. Association
for Computational Linguistics.3796
A Seq2Seq Model Descriptions
BART The BART model (Lewis et al., 2020a)
is a Transformer (Vaswani et al., 2017) that is a
denoising auto-encoder trained with several nois-
ing techniques in order to learn a mapping from
corrupted documents to their original representa-
tions. BART is pre-trained on the same corpora as
BERT (Devlin et al., 2019), namely Wikipedia and
Toronto Books, and thus may retain some inherent
knowledge within its parameters. BART-Large, a
400m parameter model, serves as the base seq2seq
model for RAG in Lewis et al. (2020b), and so we
consider it in our experiments.
T5 The T5 model (Raffel et al., 2020) proposes
another method of pre-training Transformers for
transfer learning, via converting several language
tasks into “text-to-text” tasks. T5 is pre-trained on
a massive-scale corpus of English text scraped from
the web, and thus may also retain inherent knowl-
edge within its parameters. T5-Base (220m param-
eters) and T5-Large (770m parameters) are both
used in the FiD setup (Izacard and Grave, 2021b),
and so we consider them in our experiments.
BlenderBot The BlenderBot model (Roller et al.,
2021) is a large-scale open-domain dialogue model,
pre-trained on dialogue data scraped from social
discussions on the web (Baumgartner et al., 2020).
Roller et al. (2021) release 90m, 2.7B, and 9.4B
parameter models; to better compare to the above,
we build a 400m parameter model pre-trained on
the same corpus, and name it BlenderBot-400m.
B RAG-Turn Token & Sequence
Retrieving documents for each turn xt can also
be viewed as a way of boosting the total num-
ber of documents. We can thus try falling back
to the standard RAG-Token and RAG-Sequence
generator probabilities, by considering the union
of all documents retrieved for each turn ⋃T
t=1 Zt,
and the concatenation of all the turns in the con-
text ¯X= [x1; ...; xT] as before. We refer to these
methods as RAG-Turn Token, and RAG-Turn Se-
quence. The generator probabilities for RAG-Turn
Token and RAG-Turn Sequence are:
pTurn-Token(y|¯X) ≈
m∏
l
∑
z∈
⋃T
t=1 Zt
pη(z|¯X)pθ(yl|¯X, z, y1...yl−1)
pTurn-Sequence(y|¯X) ≈
∑
z∈
⋃T
t=1 Zt
pη(z|¯X)
m∏
l
pθ(yl|¯X, z, y1...yl−1)
C Dataset Details
WoW consists of 22311 conversations (split into
train, valid and test) over 1365 general topics,
that range from e-books to toga parties to show-
ers. Valid and test are split into seen and unseen
versions for out-of-distribution topic evaluations,
where the test unseen split contains 1000 dialogues
with 58 new topics not discussed in the training
data. CMU_DoG consists of 4112 conversations
and focuses on the domain of movies. We note that
the original setup of CMU_DoG involves models
being given a gold knowledge paragraph in addi-
tion to the dialogue, but in our work we use this
dataset to consider the more difﬁcult (and realistic)
problem of being able to retrieve this knowledge,
rather than it being provided. To similarly assess
performance on seen vs. unseen distributions for
CMU_DoG, we construct a custom split by hold-
ing out conversations about 2 of the 30 movies in
CMU_DoG for “unseen” test, and subsequently
split the conversations of the other 28 ﬁlms across
train, valid, and “seen” test. The results presented
in the main text focus on these modiﬁed splits, with
measurements on the original data split provided
in Tables 16 and 17.
D Training Details
All models are trained in ParlAI 3 (Miller et al.,
2017).
Training Models were trained using 4x32GB
GPUs and mixed-precision training, evaluating ev-
ery 1 quarter of a dataset epoch until validation
perplexity did not improve for a certain number of
validations. We used a batchsize of 16 and swept
over learning rates between 5e-6 and 1e-4, using
the Adam optimizer (Kinga and Ba, 2015) with
a linear LR scheduler that reduced the LR when
validation performance did not improve; we found
that 1e-5 worked best for BART models, and 1e-4
worked best for T5 models.
3https://parl.ai3797
Seen Test Unseen Test
Method PPL F1 Knowledge F1 B4 RL PPL F1 Knowledge F1 B4 RL
Baselines
Movie titles only 15.45 15.96 6.796 .7456 19.43 19.41 15.26 5.916 .5923 18.06
Gold passage + Full Context 14.20 15.64 8.637 .7698 19.58 15.32 15.86 7.222 .882 18.67
NQ + TQA retriever pre-training
Rag-Token 12.87 15.59 8.003 .7886 20.53 14.94 15.78 7.158 .7306 20.57
DPR-Poly 12.77 14.93 9.087 .7053 21.02 14.54 15.23 7.457 .6872 20.35
FiD 12.77 15.66 7.854 .7472 21.49 15.12 14.83 7.776 .5541 20.01
FiD-DPR 12.41 15.25 9.901 .7436 21.76 14.98 14.36 9.071 .5376 20.49
Wizard of Wikipedia retriever pre-training
Rag-Token 13.05 15.22 8.253 .7151 20.62 15.25 15.52 7.202 .7502 20.95
DPR-Poly 12.71 15.21 8.307 .7452 20.91 14.48 15.11 7.65 .6476 20.40
FiD 12.79 15.64 8.318 .8149 22.14 15.11 15.07 7.317 .5711 20.32
FiD-DPR 12.24 15.33 9.052 .7994 21.54 14.47 14.64 8.686 .6849 20.42
Table 10: Comparison of Architectures on CMU_DoG Seen/Unseen.BART is used as the base Seq2Seq Model.
Inference We attempted to optimize the decod-
ing parameters of the models in the same way on
the validation set to optimize decoding strategy –
this included sweeping over beam size, minimum
beam length, and beam/context blocking, and used
F1 to measure performance. For the vast majority
of results, we employ beam search with a mini-
mum beam length of 20 and a beam size of 3, with
tri-gram beam blocking.
Wikipedia and FAISS To index the Wikipedia
passage embeddings, we used the Hierar-
chical Navigable Small World graph explo-
ration (HNSW) variant of a FAISS index (i.e.,
IndexHNSWFlat4), with an M value (number
of graph links in HNSW) of 128. The FAISS index
requires 80GB of RAM to load.
E Retriever Performance
We measure the performance of the various retriev-
ers considered by evaluating how often the top doc-
ument retrieved is the correct document or in the
top 5; that is, how often the gold knowledge sen-
tence used in WoW is contained within the passage
retrieved. Results are in Table 11.
F RAG Turn Further Explorations
We compare different values for T∗, the effective
number of context turns considered by RAG-Turn,
in Table 13. We note that perplexity values in
general increase, while generation statistics stay
roughly the same or drop slightly. Knowledge F1
stays roughly the same, with marginal increases or
decreases depending on the model.
4https://github.com/facebookresearch/faiss/wiki/Faiss-
indexes
G Automated Metrics and Human
Evaluation
Rare F1: When comparing texts, F1 can be inﬂated
by exploiting common unigrams (Dinan et al.,
2019a). We attempt to rectify this by only con-
sidering words that are infrequent in the dataset
when calculating F1. We deﬁne a word as infre-
quent if it is in the lower half of the cumulative
frequency distribution of the reference corpus. For
each dataset, our reference corpus was all human
messages across all splits. We ﬁnd some correlation
between this metric and Knowledge F1 for WoW
(see Table 1). We note that Knowledge F1 is only
available for datasets with labeled gold knowledge,
whereas Rare F1 can always be computed.
We calculate the Pearson correlation coefﬁcient
between human evaluations and various automated
metrics, visualized in Figure 2. The models con-
sidered are those listed in Table 4. We ﬁnd that
improvements in PPL, Knowledge F1, and Rare F1
correlate with an increase in the perceived knowl-
edge use and a reduction in hallucination. F1 had
relatively low correlation with all of the human
evaluation criteria considered.
H Additional Retrieval Variants
H.1 ColBERT
Khattab and Zaharia (2020) propose ColBERT as
a method of computing contextualized late-stage
interaction between the context and candidate rep-
resentations to improve ranking capabilities, and
indeed the method is extended to downstream gen-
erative QA models in Khattab et al. (2020). The
key to ColBERT is a maxsim operation, in which
the Transformer outputs of the context encoder are
compared to all outputs of the candidate encoder,
with the ﬁnal score being a sum of the maximum
similarity scores for each context output. The au-3798
Retriever Retriever Valid Seen Valid Unseen
Retriever Pre-Training Fine-Tuning R@1 R@5 R@1 R@5
DPR NQ + TQA Zero-shot 5.8 13.8 4.9 11.1
DPR WoW Zero-shot 13.1 23.9 11.6 17.5
DPR NQ + TQA + WoW Zero-shot 13.1 23.9 11.1 16.6
RAG-DPR NQ + TQA WoW 28.1 36.8 25.7 33.7
RAG-DPR WoW WoW 25.9 35.6 22.9 33.4
RAG-DPR NQ + TQA + WoW WoW 26.2 35.1 23.3 34.0
DPR-Poly NQ + TQA WoW 29.3 37.6 26.9 34.0
PolyFAISS WoW WoW 23.9 32.0 19.7 28.3
ColBERT MS-Marco WoW 25.7 33.3 27.5 33.8
ColBERT WoW WoW 26.1 33.6 26.4 33.7
ReGReT (Separate) NQ + TQA WoW 25.3 35.1 24.0 32.5
ReGRet (Same) NQ + TQA WoW 26.6 35.7 23.7 33.2
Table 11: Comparison of Retrieval Ability of Architectures on WoW Valid Seen/Unseen. Each model retrieves
5 documents from an unstructured document set of 21m 100-word passages in Wikipedia. We measure passage
Recall@k (R@k) measures how often the gold sentence used by the wizard is contained in the top k retrieved
documents. All models use BART as a base seq2seq model
Valid Seen Valid Unseen
RAG Type PPL F1 Knowledge F1 B4 RL PPL F1 Knowledge F1 B4 RL
Retrieve over Most Recent Turn
Sequence 13.5 20.8 23.3 2.6 21.7 15.5 20.1 21.4 2.1 20.5
Token 13.8 21.1 22.3 2.6 21.7 15.8 21.1 21.0 2.0 20.8
Retrieve over Full Dialogue Context
Sequence 11.1 21.5 27.9 3.9 23.0 12.6 20.3 24.6 2.9 21.3
Token 11.6 22.5 26.0 4.0 23.5 13.4 21.8 22.7 2.7 21.7
Turn-DTT 11.9 22.2 28.0 4.1 23.4 13.6 21.1 24.3 2.7 21.4
Turn-DO 13.3 23.1 26.8 4.0 24.5 15.4 22.0 23.3 2.6 22.5
Turn-Tok 11.5 21.0 24.3 3.1 21.6 13.2 20.5 21.5 2.0 20.0
Turn-Seq 10.9 21.5 27.8 4.1 22.9 12.6 19.5 23.5 2.6 20.3
Table 12: Comparison of RAG Model Typeson WoW Valid Seen/Unseen. Retrieval models are retrieving 5 doc-
uments over all of Wikipedia. We set T∗ = 1 for RAG-Turn models, i.e., the last turn is considered independently
from the prior context turns. All models use BART as the base seq2seq model.
thors propose an end-to-end setup involving large-
scale search, where the token representations of
all candidates are stored in a FAISS index, queries
into the FAISS index are context outputs, and a
re-ranking step using the maxsim operation is per-
formed on a much smaller set of candidates. We
implement this method for retrieval-augmented di-
alogue, and simply denote it as ColBERT.
H.2 Iterative Retrieval
Several methods in the literature have shown that
using iterative retrieval strategies is an effective
way to improve retrieval (Khattab et al., 2020),
distill knowledge from the retriever to the reader
(Izacard and Grave, 2021a), and boost performance
in multi-hop or complex QA settings (Xiong et al.,
2021; Qi et al., 2020). Applying a similar tech-
nique to dialogue is easily motivated; intuitively,
assuming one has an appropriately expressive gen-
erative model, retrieval conditioned on the output
of the generator (trained to predict the ground truth
response y) should surface relevant facts for the
conversation. We thus consider an architecture that
involves two rounds of retrieval and generation,
where the second round retrieves according to the
generated output of the ﬁrst round; the model is
trained to predict target labels taking into account
both stages. We denote this model ReGReT (re-
trieve, generate, retrieve, tune), and note that one
could use the same model for both rounds (Re-
GReT Same) or a separate model for both rounds
(ReGReT Sep).
H.3 Retriever-less Retrieval
Recent work has demonstrated that large pre-
trained models have some capacity to store knowl-
edge within their parameters (Petroni et al., 2019;
Roberts et al., 2020); some have shown that model
representations themselves can be used nearly out-
of-the-box for nearest neighbor retrieval of relevant
contexts to help in language modeling (Khandel-
wal et al., 2020), machine translation (Khandelwal
et al., 2021), and grounded dialogue (Fan et al.,
2021). We explore the efﬁcacy of BART and T5
at encoding knowledge via utilizing their encoders
directly to encode both q(xi) and d(zj), allowing3799
Valid Seen Valid Unseen
RAG Turn Type T∗ PPL F1 Knowledge F1 B4 RL PPL F1 Knowledge F1 B4 RL
Doc then Turn 1 11.8 21.9 27.7 4.1 23.2 13.6 21.1 24.3 2.7 21.4
3 12.1 21.7 27.3 4.0 22.9 13.8 20.8 24.3 2.6 21.2
Doc Only 1 13.3 23.1 26.8 4.0 24.5 15.5 22.0 23.3 2.6 22.5
3 14.4 22.7 27.1 3.9 24.1 16.7 21.9 22.8 2.9 22.3
Token 1 11.5 21.0 24.3 3.1 21.6 13.2 20.5 21.5 2.0 20.0
3 11.7 22.3 25.2 3.7 23.0 13.9 21.1 20.8 2.3 20.8
Sequence 1 10.9 21.5 27.8 4.1 22.9 12.6 19.5 23.5 2.6 20.3
Table 13: Comparison of T∗ Values For RAG-Turn on WoW Valid Seen/Unseen. All models use BART as a base
seq2seq model, and retrieve 5 documents over all of Wikipedia.
PPL F1 Knowledge F1 Rare Word F1
Consistency
Engaging
Knowledge
Hallucinate
-0.3 0.65 0.13 0.75
0.6 0.1 -0.66 -0.4
-0.82 0.61 0.94 0.87
0.89 -0.55 -0.95 -0.84
Figure 2: Correlation of Automatic Metrics with Human Judgments . We plot the Pearson correlation coefﬁ-
cient between the human evaluations from Table 4 and automated metrics from the WoW Valid Unseen data. We
observe correlation between the Knowledge F1 and Rare F1 metrics with Knowledge and Hallucination human
evaluations, especially when compared to standard F1.
the full RAG model to propagate error from the
token losses to the encoder seen as a retriever and
as a generator, thus removing the requirement of
training and deploying a completely separate Trans-
former model for that goal. We draw inspiration
from the ColBERT setup, and use encoder outputs
as queries into FAISS, with a maxsim operation
computing ﬁnal documents scores pη(zj|xi). We
refer to this model as BREAD (BART-Retriever-
Encoder-And-Decoder) for BART-based models,
and TREAD for T5-based models.
I Additional Relevant Ablations
We outline several more important questions when
considering these models.
I.1 Do different encoder-decoder
architectures affect performance?
Table 18 presents results on WoW comparing
across different encoder-decoder architectures and
sizes.
Architecture Comparison BART and T5 are
comparable in their performance when holding the
retrieval aspect constant. While perplexity mea-
sures are not directly comparable due to dictionary
differences, we see that generations from the mod-
els yield roughly the same generation metric results.
BlenderBot-400m performs comparably worse to
T5 and Bart.
Size Comparison With larger models we tend
to see a decrease in perplexity, indicating that
these models become more ﬂuent with respect to
the dataset; however, generation statistics remain
roughly constant. In fact, for the BlenderBot mod-
els, increasing model size leads to decreasing per-
formance in the Knowledge F1 metric. This result
further motivates the need for additional metrics
beyond the standard ones when measuring prowess
on dialogue-based tasks. One hypothesis here is
that the large model is sacriﬁcing knowledge use by
instead relying on its conversational ﬂuency (given
that its perplexity is signiﬁcantly lower).
I.2 Is a neural model trained for retrieval
necessary?
Table 19 shows the efﬁcacy of retriever-less re-
trieval, comparing across different sources of
knowledge. When limiting the knowledge base
to all topics from Wikipedia that are present in
the WoW dataset – comprising 500k tokens across
3k documents – the BREAD (BART-Retriever-
Encoder-And-Decoder) model obtains similar per-3800
Test Seen Test Unseen
Method Knowledge Source PPL F1 B4 RL PPL F1 B4 RL
BlenderBot (Roller et al., 2021) None 8.72 18.8 1.3 10.4 17.8 0.7
BART (ours) None 14.7 20.9 1.7 20.3 18.9 18.7 0.9 18.4
GPT-2 Finetune (Zhao et al., 2020c) WoW 15.0 14.4 1.0 18.9 13.8 0.8
E2E Transformer MemNet (Dinan et al., 2019b) WoW 63.5 16.9 97.3 14.4
DRD (Zhao et al., 2020a) WoW 23.0 18.0 5.5 25.6 16.5 4.3
Two-Stage Transformer MemNet(Dinan et al., 2019b) WoW 46.5 18.9 84.8 17.3
DialoGPT Finetune (Zhao et al., 2020c) WoW 16.2 19.0 2.3 20.4 17.6 3.2
SKT (Kim et al., 2020) WoW 52.0 19.3 81.4 16.1
BART FK (Bruyn et al., 2020) WoW 12.2 20.1 14.9 19.3
KnowledGPT (Zhao et al., 2020b) WoW 19.2 22.0 22.3 20.5
KIF (Fan et al., 2021) WoW 23.9
KIF (Fan et al., 2021) WoW + Train Utts *25.9 *22.3
FiD-RAG (Ours) Wikipedia (WoW Subset) 10.5 23.2 4.4 24.2 10.7 23.2 4.6 24.4
RAG DPR-Poly (Ours) Wikipedia (All) 11.4 22.9 3.9 23.5 13.2 21.8 3.4 22.3
FiD-RAG DPR-Poly (Ours) Wikipedia (All) 10.7 22.9 4.1 23.8 12.0 22.1 3.7 23.1
Table 14: WoW Comparison to Existing Results. "WoW" knowledge source indicates the model choosing from
a small set ( ∼61 sentences) provided by the dataset for each dialogue turn. Methods with * augmented their
knowledge source with training utterances, which is useful on Test Seen data, but likely not as useful on Unseen
data. Our models use BART as the base seq2seq model; the RAG and FiD-RAG models retrieve 5 documents, and
the FiD-RAG DPR-Poly model retrieves 25.
Valid Seen Valid Unseen
Retriever Re-ranker PPL F1 KF1 B4 RL PPL F1 KF1 B4 RL
TFIDF None 13.1 21.6 23.0 3.3 22.5 15.2 21.1 21.6 2.4 21.1
DPR None 11.6 22.5 26.0 4.0 23.5 13.4 21.8 22.7 2.7 21.7
TFIDF DPR 12.5 21.8 23.1 3.4 22.6 14.5 21.4 20.2 2.2 20.9
DPR Polyencoder 11.7 23.0 26.5 4.0 23.9 13.1 22.6 24.4 3.4 22.6
Joint DPR Poly Polyencoder 11.6 23.0 27.4 4.3 23.9 13.1 22.1 24.7 3.1 22.1
PolyFAISS - 12.1 22.9 24.8 3.7 23.6 14.2 21.6 20.6 2.5 21.2
ColBERT - 12.4 21.8 25.3 3.3 23.1 13.5 21.9 24.7 3.2 22.4
BREAD - 14.8 20.5 17.7 1.7 20.6 17.3 19.8 17.2 1.3 19.5
ReGReT (Sep) None 11.9 22.6 26.9 3.9 23.9 13.6 21.6 24.1 2.9 21.9
ReGReT (Same) None 12.0 22.6 25.9 4.0 23.9 13.8 21.5 23.2 2.7 21.6
Table 15: Comparison of re-rankers for BART-based RAG-Token modelson WoW Valid Seen/Unseen, using
5 retrieved documents.
formance to its DPR-retrieval counterpart. When
scaling to the ﬁrst two paragraphs of all topics from
Wikipedia – comprising 1 billion tokens across
11 million documents, of the same order of mag-
nitude as the full Wikipedia knowledge source –
we see a slight reduction in performance, but the
BREAD model still effectively retrieves relevant
information, and improves upon a no-retrieval base-
line. However, when scaling to the full knowledge
source – comprising 3 billion tokens over 21 mil-
lion documents – we see that we are unable to
surpass even a no-knowledge baseline; we hypoth-
esize that the token-level similarities computed by
the BREAD model become increasingly noisy as
the knowledge source is scaled up: when a rele-
vant Wikipedia article is spread across several “pas-
sages”, as in our unstructured knowledge source
dump, it becomes difﬁcult for the BREAD model
to identify precisely which sentence is relevant.
We ﬁnd similar results when evaluating TREAD
models on the smallest knowledge source listed
in the previous paragraph. The TREAD mod-
els substantially outperform their non-retrieval-
augmented counterparts (e.g., F1 and knowledge
F1 improve from 19.3 and 14.6 without retrieval
to 22.1 and 24.1 with TREAD, respectively, on
the WoW Valid Seen split), however we do see
that their RAG/FiD counterparts perform better in
terms of knowledge F1 and perplexity.
I.3 Does the decoding strategy affect
performance?
We compare model outputs with various decoding
strategies in Table 20. We compare three decoding
methods: beam search, blocking repeated n-grams
(we use n = 3); nucleus sampling (Holtzman et al.,
2020) with varying values of p; and top-k sampling
(Fan et al., 2018) with k = 10. We additionally
compare whether to apply beam-blocking to the
context, i.e., blocking repeated n-grams that ap-
pear in the dialogue context only – n-grams in the
retrieved documents are not blocked.
We ﬁnd that, across all retrieval schemes, beam-
blocking the dialogue context hurts performance3801
Retrieval Mechanism PPL F1 Knowledge F1 BLEU-4 ROUGE-L
None 14.7 15.6 4.3 0.7 15.6
FiD 15.3 15.4 4.4 0.6 15.6
RAG DPR 15.0 15.3 4.7 0.6 15.6
RAG DPR-Poly 14.7 15.1 4.8 0.7 14.9
FiD-RAG DPR 14.3 15.3 4.9 0.7 15.7
Table 16: Comparison of Retrieval Augmentations on CMU_DoG (Valid), original split. Retrieval models are
retrieving over all of Wikipedia. All RAG models are RAG-Token and use BART as the base seq2seq model.
Method PPL F1 B4 RL
No Knowledge
BART (ours) 14.6 15.9 0.8 16.9
CMU_DoG Knowledge
BCTCE (Cai et al., 2020) 17.8 1.4
CAT (Ma et al., 2020) 15.2 1.2 11.2
GPT-2 Finetune (Zhao et al., 2020c) 16.5 9.4 0.6
DRD (Zhao et al., 2020a) 54.4 10.7 1.2
DialoGPT Finetune (Zhao et al., 2020c) 15.9 13.7 1.5
KnowledGPT (Zhao et al., 2020b) 20.6 13.5
All of Wikipedia
RAG DPR-Poly (Ours) 14.4 15.8 0.9 16.9
FiD-RAG DPR-Poly (Ours) 14.4 15.9 0.9 17.1
Table 17: CMU_DoG Comparison to Existing Results (Test), original data split. Our models use BART as the
base seq2seq model. Both the RAG DPR-Poly model and FiD-RAG model retrieve 5 documents.
Valid Seen Valid Unseen
Seq2Seq Size PPL F1 KF1 PPL F1 KF1
BB-90m 90m 13.4 21.4 23.9 15.9 21.1 21.3
BB-400m 400m 9.2 21.1 23.2 10.4 19.9 20.5
BB-3B 3B 8.2 21.1 20.2 9.1 20.9 18.7
T5-Base 220m 11.5 21.9 25.5 13.6 21.2 22.4
T5-Large 770m 9.7 22.6 25.2 11.2 21.7 22.9
BART-
Large
400m 11.6 22.5 26.0 13.4 21.8 22.7
Table 18: Comparison between different seq2seq
models (BlenderBot (BB), T5, and BART) on WoW
Valid Seen/Unseen. All models use RAG-Token archi-
tectures with DPR Retrieval, retrieving 5 documents at
inference time. Perplexity (PPL) values are not com-
parable across different generator architectures as they
use different dictionaries.
– presumably because the model may be blocked
from discussing named entities from prior context
turns – with beam search yielding the highest F1
scores across the board. Despite the fact that beam
search and nucleus sampling (with low p) yield
comparable ROUGE-L and F1 scores, we see a
noticeable difference in knowledge F1, implying
that nucleus sampling may still be good at produc-
ing ﬂuent/consistent generations while ultimately
suffering increased hallucination. Using nucleus
sampling with a higher p value (which increases
the variety of sampling) and using top-k sampling
both result in poor relative performance for all four
metrics, implying higher levels of hallucination
and less coherent responses.
I.4 Does retriever and/or re-ranker
pre-training affect performance?
We explore the effects of pre-training the neural re-
triever to help prime it for dialogue-based retrieval.
To do so, we consider WoW knowledge selection as
an appropriate pre-training task: given a dialogue
context and a set of candidate knowledge sentences,
choose the sentence on which to next ground a
response. For standard RAG-DPR methods, we
try both ﬁne-tuning 1) a DPR model pre-trained
on Natural Questions (Kwiatkowski et al., 2019)
and Trivia QA (Joshi et al., 2017) and 2) a BERT
model from scratch on the WoW knowledge selec-
tion task, and substitute these in for the standard
QA-pre-trained DPR retriever from our base setup;
we explore similar pre-training ablations with the
ColBERT model. Results are in Table 21; we see
minimal performance gains from such pre-training,
and conclude that as long as the retriever is in a
good state, it will work in the ﬁne-tuning setup.
We see similar results when comparing pre-
training strategies for the DPR-Poly re-ranker
model in Table 21; pre-training the re-ranker does
not yield noticeable downstream gains.
I.5 Does the source of knowledge matter?
We explore the downstream effect of swapping in
different sources of knowledge. Because the distri-
bution of the topics within Wizard of Wikipedia is3802
Valid Seen Valid Unseen
Src Arch. PPL F1 KF1 PPL F1 KF1
BART
A RAG-DPR 11.6 22.5 26.0 13.4 21.8 22.7
A FiD-RAG 13.1 22.0 22.1 15.1 21.6 20.4
A BREAD 14.8 20.5 17.7 17.3 19.8 17.2
B RAG-DPR 10.9 23.2 27.9 12.4 22.4 23.7
B FiD-RAG 12.3 22.7 24.5 14.0 22.2 22.9
B BREAD 13.7 21.7 22.9 15.3 21.1 21.6
B BREAD-FiD 12.8 22.4 25.2 14.5 21.7 23.4
C RAG-DPR 10.7 23.3 28.3 11.7 23.0 26.3
C FiD-RAG 10.5 23.5 28.4 11.4 23.7 27.9
C BREAD 12.1 23.2 28.5 13.4 23.0 27.6
C BREAD-FiD 11.3 23.3 27.7 12.6 23.3 26.2
T5
C RAG-DPR 9.0 23.3 26.8 9.8 22.6 24.6
C FiD-RAG 9.0 22.7 29.3 9.8 23.0 29.4
C TREAD 11.0 22.1 24.1 12.8 21.8 22.9
C TREAD-FiD 10.6 22.3 23.4 12.0 22.0 22.4
Table 19: Comparison between DPR Retriever mod-
els (RAG and FiD) and “retriever-less” BREAD and
TREAD models on WoW Valid Seen/Unseen, with
varying knowledge sources: A: All of Wikipedia; B:
First 2 paragraphs from all of Wikipedia; C: First
two paragraphs from all articles covered by the WoW
dataset. All models retrieve 5 documents during train-
ing and inference. Perplexity (PPL) values are not com-
parable across different seq2seq architectures as they
use different dictionaries.
known, we can limit our model’s source of knowl-
edge to contain the smallest subset of Wikipedia
yielding full coverage of the dataset, resulting in
nearly 3000 documents from which to retrieve. As
the retrieval task is now easier, we see noticeable
performance gains when substituting this source of
knowledge, see Table 22.
I.6 How does the number of documents
retrieved/re-ranked affect performance?
We conclude our ablation studies with an analysis
on the number of documents retrieved. Table 23
outlines how each backbone architecture handles
increasing the number of documents considered
during inference.
For backbone architectures designed to consider
several documents jointly - namely, RAG-Token
and FiD-RAG - increasing the number of retrieved
documents yields improvements in perplexity and
F1 measures. However, we see substantial dropoffs
in Knowledge F1 measures, which might imply that
the models begin to hallucinate more and more, a
claim that is supported in the human annotations,
where we see in Table 4 that increasing the number
of documents for these models yields higher levels
of hallucination.
For RAG-Sequence models, which consider each
document separately, increasing the number of re-
trieved documents improves perplexity measures
and maintains both Knowledge F1 and BLEU mea-
sures; however, F1 scores appear to drop for any
amount of documents beyond a single one. We
hypothesize that by considering more and more
generations we are effectively increasing the beam
size and ﬁnding generations that match the knowl-
edge more and more, while straying further away
from engaging, dialogue-like responses; indeed,
the RAG-Sequence model in Table 4 only uses 5
retrieved documents, and human evaluations indi-
cate that the model still is less often engaging than
its counterparts.
Overall, the number of re-ranked documents
does not seem to improve performance substan-
tially, so we land on 25 documents re-ranked to
keep computational overhead to a minimum.3803
No Retrieval RAG DPR-Poly FiD-RAG DPR-Poly
Decoding Strategy Context Block F1 KF1 B4 RL F1 KF1 B4 RL F1 KF1 B4 RL
Beam No 20.9 17.6 1.7 20.7 23.1 26.5 4.0 24.0 22.8 27.8 4.1 24.1
Beam Yes 20.6 17.1 1.7 20.4 22.9 25.9 4.1 23.9 22.5 26.7 3.9 23.8
Nucleus: p = 0.3 No 20.6 16.0 1.4 20.3 23.0 24.0 3.6 24.2 22.5 23.5 3.5 23.6
Nucleus: p = 0.3 Yes 20.1 15.6 1.4 19.9 22.9 23.9 3.7 24.1 22.0 22.9 3.4 23.1
Nucleus: p = 0.9 No 17.1 13.6 0.6 17.0 19.3 19.3 1.9 19.8 19.4 20.2 2.3 20.0
Nucleus: p = 0.9 Yes 16.6 13.2 0.6 16.8 19.2 18.9 1.8 19.6 19.6 19.8 2.3 20.4
Top-k: k = 10 No 18.0 14.4 0.7 18.0 19.8 19.0 1.8 20.3 20.2 19.9 2.2 20.8
Top-k: k = 10 Yes 17.5 14.0 0.5 17.5 19.7 18.8 1.8 20.1 19.7 20.2 2.2 20.2
Table 20: Comparison of Decoding Strategies For models with and without retrieval-augmentation. Evaluations
are conducted on the WoW Valid Seen. Retrieval models are retrieving 5 documents over all of Wikipedia. We
set the minimum beam length to 20, and block tri-grams during beam search. All models use BART as the base
seq2seq model.
Valid Seen Valid Unseen
Pre-training
Data PPL F1 KF1 PPL F1 KF1
DPR
NQ + TQA 11.6 22.5 26.0 13.4 21.8 22.7
WoW 12.1 22.7 26.2 13.4 22.1 24.4
NQ + TQA + WoW 12.1 22.7 25.8 13.7 22.0 23.0
ColBERT
MS-Marco 12.4 21.8 25.3 13.5 21.9 24.7
WoW 12.6 21.8 26.1 13.6 21.4 24.9
DPR-Poly and Joint DPR/Poly
WikiTo 11.7 23.0 26.5 13.1 22.6 24.4
NQ + TQA 11.6 23.0 27.4 13.1 22.1 24.7
Table 21: Comparison between different
retriever/re-ranker pre-training schemes on
WoW Valid Seen/Unseen. All models use BART as the
base seq2seq model.
Valid Seen Valid Unseen
Src Type PPL F1 KF1 PPL F1 KF1
A P 11.6 22.5 26.0 13.4 21.8 22.7
B P 10.9 23.2 27.9 12.4 22.4 23.7
B S 13.2 22.3 23.9 15.5 21.5 20.1
C P 10.7 23.3 28.3 11.7 23.0 26.3
C S 12.8 22.2 24.8 14.4 21.5 21.7
Table 22: Comparison between using different
sources of knowledge on WoW Valid Seen/Unseen.
All models are BART RAG-Token with DPR Retrieval.
A: All of Wikipedia; B: ﬁrst two paragraphs from all
articles in Wikipedia; C: ﬁrst two paragraphs from all
articles in Wikipedia covering the WoW dataset.P: full
passages are used; S: sentences are separate passages.
Valid Seen Valid Unseen
# Docs PPL F1 KF1 PPL F1 KF1
RAG-Token
1 12.8 21.9 27.6 15.3 20.5 23.8
5 11.6 22.5 26.0 13.4 21.7 22.7
25 11.6 22.6 24.5 13.0 21.7 21.1
50 11.6 22.4 23.9 13.0 21.8 20.6
RAG-Sequence
1 12.5 22.1 27.4 14.6 21.1 24.3
5 11.1 21.5 27.9 12.6 20.3 24.6
25 10.6 21.3 27.8 11.4 20.0 24.3
50 10.5 21.2 27.8 11.2 19.9 24.3
RAG-Turn-DTT
1 12.7 21.3 28.3 15.0 20.1 24.9
5 11.8 21.9 27.7 13.6 21.1 24.3
25 11.7 22.2 26.8 13.2 21.6 23.3
50 11.9 22.2 26.4 13.7 21.7 22.7
RAG-Turn-DO
1 14.2 22.2 28.1 16.9 21.3 24.7
5 13.3 23.1 26.8 15.5 22.0 23.3
25 13.3 23.1 24.8 15.1 22.2 21.1
50 13.3 22.6 23.7 15.2 22.0 20.0
FiD-RAG
1 13.0 21.5 28.5 15.5 20.5 23.0
5 11.0 22.9 27.7 12.7 22.0 25.5
25 11.1 22.3 21.2 12.1 22.7 22.3
50 11.7 21.4 18.0 12.6 22.1 19.1
100 12.7 20.4 15.9 13.6 21.4 16.6
Table 23: Comparison of the effect of conditioning
over different numbers of documents at inference
time for different modelson WoW Valid Seen/Unseen.
All models use a DPR retriever, with BART as the base
seq2seq model.

Proceedings of the 28th International Conference on Computational Linguistics, pages 2284–2295
Barcelona, Spain (Online), December 8-13, 2020
2284
Retrieval-Augmented Controllable Review Generation
Jihyeok Kim
Yonsei University
zizi1532@yonsei.ac.kr
Seungtaek Choi
Yonsei University
hist0613@yonsei.ac.kr
Reinald Kim Amplayo
University of Edinburgh
reinald.kim@ed.ac.uk
Seung-won Hwang∗
Yonsei University
seungwonh@yonsei.ac.kr
Abstract
In this paper, we study review generation given a set of attribute identiﬁers which are user ID,
product ID and rating. This is a difﬁcult subtask of natural language generation since models
are limited to the given identiﬁers, without any speciﬁc descriptive information regarding the
inputs, when generating the text. The capacity of these models is thus conﬁned and dependent
to how well the models can capture vector representations of attributes. We thus propose to
additionally leverage references, which are selected from a large pool of texts labeled with one
of the attributes, as textual information that enriches inductive biases of given attributes. With
these references, we can now pose the problem as an instance of text-to-text generation, which
makes the task easier since texts that are syntactically, semantically similar to the output text are
provided as inputs. Using this framework, we address issues such as selecting references from a
large candidate set without textual context and improving the model complexity for generation.
Our experiments show that our models improve over previous approaches on both automatic and
human evaluation metrics.
1 Introduction
The ultimate goal of opinion mining and sentiment analysis (Pang and Lee, 2008) is to automatically
digest opinions of users towards a certain product to accommodate decision making. While some of
these opinions are explicitly articulated in product reviews that users write, most of them are unknown
since users have not bought most of the products. Alternative solutions such as aspect-based sentiment
analysis (Mukherjee and Liu, 2012; Pontiki et al., 2016) and recommendation systems (Resnick and
Varian, 1997; Bobadilla et al., 2013) exist, however these only offer superﬁcial outputs that are not as
expressive as textual reviews. Thus, the task of automatically generating reviews given their attributes
such as user and product, or review generation (Dong et al., 2017), is necessary to achieve this goal.
Most of the previous approaches (Dong et al., 2017; Sharma et al., 2018) have framed review gener-
ation as A2T (Attribute-to-Text problem), where the given input is a non-linguistic data ( i.e., attribute
identiﬁers for user, product, and rating) and the output is the review text. In this problem setup, the key
challenge is to learn rich representations of the attributes, which are then used to produce the text us-
ing either template-based surface realization methods (Kukich, 1983; McKeown, 1992) or neural-based
decoders (Mei et al., 2016; Wiseman et al., 2017), as shown in the red box in Figure 1. However, it is
difﬁcult to learn these representations merely from the given attribute identiﬁers since they do not convey
any semantics regarding the attributes.
Our key contribution isAT2T(Attribute-matched-Text-to-Text), of augmenting inductive biases of at-
tributes with their matching reference reviews, as illustrated as the blue box in Figure 1. For example, as
shown in Figure 1, multiple references together contain inductive biases such as frequently reviewed as-
pects of the product (e.g., talking about plot and character aspects) or habitual user phrases (e.g., “looking
forward to the next book”). These references greatly help text generation since not only do they reinforce
∗Corresponding author
This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://
creativecommons.org/licenses/by/4.0/.2285
Attributes
𝐗=References
ReferenceRetrieval𝐗∗⊆𝐗
TextGeneration
Output	Text
⋯
⋯ (ref	#2)	I	enjoyedprevious	book.…but	disappointed	at	this	one,	…too	boring.(ref	#3)	it	was	a	great	read.	Nice	plotand	charming	characters.	I	am	looking	forward	to	the	next!
I	reallyenjoyedthis	book	!	I	loved	the	charactersand	the	plot.	I	am	looking	forward	to	the	next	book	in	the	series!
Selected	References(ref	#1)	characters	wereannoying	and	plot	was	boring.
Output	Text
UserProduct
#p3
Rating
5ID#u1
 #u5
#u1
#u1
#u1
#p3
#p8
#p4
#p3
5
5
2
3
Figure 1: High-level diagram of frameworks of previous models (in red box) and our proposed model (in
blue box), where we additionally make use of references to generate the output text.
the representations learned from the attributes, they also allow the use of techniques used in sequence-
to-sequence learning such as attention (Bahdanau et al., 2015) and copy (See et al., 2017) mechanisms.
In related problem domains of generating abstractive summaries or dialogue utterances, such bias is in-
troduced by a T2T (Text-to-Text) approach, of generating an extractive summary ﬁrst (Gehrmann et al.,
2018) or retrieving informative prior turns (Cai et al., 2018), then generating ﬁnal outputs using these
as references. Central to the framework is reference retrieval, since relevant references provide valuable
context, but, in contrast, noisy references rather hinder generation.
For reference retrieval in T2T, lexical features, e.g., TF-IDF, have been used, assigning relevance
based on the degree of word overlap between two texts. In AT2T, however, lexical features are not
directly applicable or effective. First, unlike T2T where input and output are both texts, our input is a
list of identiﬁers, i.e., (user ID, product ID, rating). As a result, we cannot expedite the process of ﬁnding
matching references, as in T2T solutions using lexical features for fast retrieval,e.g., using inverted index.
Second, lexical similarity cannot fully capture rating, as sentiment lexicons appear in a small portion of
text (Li et al., 2018). For example, ﬂipping a single lexicon (from ‘good’ to ‘bad’) from lexically identical
sentences can completely invert the rating. One alternative solution, complementing lexical similarity,
is to assign additional credits to references labeled with the input rating. However, references – labeled
with a different rating, but having useful rating-related context – are forced to be penalized. For example,
in Figure 1, no additional credits are assigned to ref#2 labeled with the nearly opposite rating, although
it contains useful context for the given rating, e.g., “enjoyed”. We later empirically show that neither
lexical similarity nor rating accuracy of references guarantee rating semantics of generated reviews.
To address these limitations, we propose two approaches: pseudo-supervised and reinforcement
learning framework, denoted as SL and RL respectively. First, we expedite matching in SL using iden-
tiﬁers. For efﬁcient retrieval without lexical features, we propose a parametric coarse-ﬁltering approach
using attribute identiﬁers, having constant time complexity for each instance in a candidate pool. Sec-
ond, to generate reviews which are compatible with input rating, we retrieve references which maximize
the rating accuracy of generated reviews - rather than references labeled with the input rating. RL en-
ables such retrieval: a retrieval model is trained to maximize rewards including rating accuracy as well
as lexical similarity of generated reviews.
To validate the effectiveness of AT2T, we perform experiments on a dataset consisting of product
reviews from Amazon Books, aligned with their corresponding attributes: user, product and rating (Dong
et al., 2017). Our experiments using automatic evaluations show that utilizing relevant references hugely
helps generation in terms of content similarity, and rating accuracy. Moreover, our human evaluations
show that our model generates more informative and grammatical texts compared to previous models.2286
2 Related Work
Data-to-Text Generation Our task is generally related to a suite of tasks on data-to-text (D2T) gen-
eration, where database tables (Wiseman et al., 2017), RDF graphs (Belz et al., 2011), and knowledge
base relations (Perez-Beltrachini et al., 2016) are explored as inputs. A variety of neural-based models
have been used on these tasks, including vanilla sequence-to-sequence models (Mei et al., 2016), ex-
tended by explicitly incorporating context selection and planning (Puduppully et al., 2019a), by employ-
ing graph-based neural networks (Marcheggiani and Perez-Beltrachini, 2018), and by modeling entities
(Puduppully et al., 2019b). While review generation is essentially a subtask of D2T, it is relatively under-
studied than other D2T tasks. Previous models include an encoder-decoder model with attention (Dong
et al., 2017), improved by including an objective function for rating accuracy (Sharma et al., 2018; Li and
Tuzhilin, 2019), by introducing a hierarchical decoder (Zang and Wan, 2017), by decomposing the de-
coding stage as coarse-to-ﬁne manner (Li et al., 2019), and by using additional inputs such as user-given
summary (Ni and McAuley, 2018) or product description (Li and Tuzhilin, 2019). In this paper, we make
performance improvements by proposing a concept of leveraging references, and extensions proposed in
the aforementioned literature are orthogonal and thus applicable to improve our models further.
Augmenting context using references While data-hungry neural models for some task may afford
sufﬁcient training resources, some other tasks such as sentence-level classiﬁcation (Kim, 2014) and
summarization (Rush et al., 2015) suffer from limited context, given a single sentence as context. Review
generation can be viewed as an extreme case of limited context, totally lacking textual context and thus
depending solely on a small set of attribute identiﬁers as input.
For text classiﬁcation tasks, solutions have been to increase the context, by adding inherent and in-
duced metadata such as topics (Zhao and Mao, 2017) and translations (Amplayo et al., 2018). Meanwhile
references have been used as additional source to augment context in text-to-text generation tasks such
as summarization (Cao et al., 2018; Peng et al., 2019), machine translation (Gu et al., 2018), or dialogue
system (Song et al., 2018; Pandey et al., 2018; Weston et al., 2018; Zhu et al., 2019). References can be
seen as a new and effective additional context that introduces inductive biases of attributes that can only
be found in texts. However, retrieval task is much harder in our task than in previous tasks, as we have
inputs of attribute identiﬁers having little information for retrieval.
3 AT2T
PretrainedGEN
⋯
⋯
0.13
0.87
0.72
0.28
𝑥#∗
𝑥%∗
𝐀=(u,p,r)
𝛼/0 0.92𝑥#1It	was	great	read.Nice	plot	and	charming	characters.I	am	looking	forward	tothe	next	!
BiLSTM+ attention
0.78 charming	charactersand	interesting	plot	𝑥21
BiLSTM+ attention
0.75 This	is	my	favorite	!𝑥31
BiLSTM+attention
Self-Attention
Coarse-Grained	REFLECT Fine-GrainedREFLECT
𝛼/4
𝛼/5
𝛼/6
Input	Attributes
I	really	enjoyed	this	book!	I	loved	the	characters	and	the	plot.	I	amlooking	forward	to	the	next	book	in	the	series!Output	Text	𝐘′
Input	Candidate	References
Comparingand	
Comparingand	
𝛼 𝛼1
Figure 2: The full architecture of R EFLECT when integrated to a generation model, which consists of
three components: Coarse-Grained R EFLECT , Fine-Grained R EFLECT , and the pretrained G EN model.
We use different encoders for αand α+ with the same identiﬁers A.2287
This work studies review generation task, where we are given review-speciﬁc attributes such as user,
product, and rating, i.e., A = {u,p,r }as input and the corresponding review Y = {yi}L
i=1 as output.
We reformulate the problem by introducing references. In review domain, references are reviews from
the training dataset that are either written by user uor written for product p. That is, references can be
reviews from another user but from the same product, i.e., (u′,p) where u′ ̸= u, and vice versa 1. This
introduces an additional input to our text generation model: a set of N references X = {xj}N
j=1, where
xj = {wi}Lj
i=1 is the jth reference with Lj tokens. Through this reformulation (AT2T), we can now pose
the task as text-to-text generation, where we generate an output given X and A as inputs.
The new problem setting introduces a major challenge since there can be a large number of references.
As most of the references are irrelevant, efﬁciently and effectively selecting the optimal K references
X∗= {x∗
1,...,x ∗
K}⊆ X is one of the essential sub-tasks to AT2T. To this end, in AT2T, models consist
of (1) reference selection modules (REFLECT ) where we select relevant referencesX∗to given attributes
A and (2) a generation module (GEN) of utilizing references as inductive biases of the attributes.
While, in T2T tasks, lexical features have been used for retrieval, those cannot be directly applied
for AT2T, having inputs of identiﬁers without text contents. To overcome such difﬁculty, we propose
two approaches on retrieval stage with different learning schemes which are pseudo-supervised learning
(SL) and reinforcement learning (RL). We ﬁrst introduce a SL method to construct trainable R EFLECT
without text contents as inputs. Then, we propose to train REFLECT using RL where we enable the model
to effectively preserve rating semantics. We show an overview of our approach in Figure 2.
4 SL-R EFLECT
Given the attributes A and reference candidates X, R EFLECT selects the most relevant K references
X∗ = {x∗
1,...,x ∗
K}from X. In T2T tasks having texts as inputs, text-based, non-parametric matching
has been used for retrieval, e.g., TF-IDF. However, given only identiﬁers, we cannot use such matching.
In this section, we explore a pseudo-supervised learning approach to train parametric retrieval models.
Relevance Pseudo-Supervision In contrast to T2T tasks, references should contain relevant contexts
to the input rating, to guide G EN to generate a rating-consistent review. Although references largely
comprising of words presented in target review are useful for providing overall contents, lexical similarity
does not guarantee semantic relevance especially for rating, e.g., two identical sentences with one word
difference such as “good” or “bad” have signiﬁcant rating differences.
To consider rating information, we propose to generate a pseudo-labelzxj for each reference candidate
xj. We use rating accuracy and lexical similarity which are linearly combined by λ:
zxj = (1−λ) ∗LEXSIM(xj,Y) +λ∗I(r= rj), (1)
where LEXSIM denotes lexical similarity between each reference candidatexj and a ground-truth review
Y, I(r = rj) is an indicator variable for rating accuracy (1 for the same rating, 0 otherwise), and λis
a balancing factor between lexical similarity and rating accuracy. We adopt average of uni-/bi-/quad-
gram BLEUs (Papineni et al., 2002) as L EXSIM which was effective in our experiments. We train
SL-REFLECT models using binary cross entropy as objective and zxj as supervision. Note that, the gold
review Y is only needed to provide supervision during training, and is not required for inference.
Coarse-Grained REFLECT To maximize computational efﬁciency, previous approaches in T2T
coarsely retrieve the small number of promising references, X+ = {x+
1 ,...,x +
M}⊆ X, using efﬁcient
matching such as TF-IDF or inverted index, then rerank using more effective but expensive matching to
select best Kreferences, X∗⊆X+ where K <M ≪N.
Instead of text-based matching which is not available in our task, we propose to use attribute-
based parametric matching, which has O(N) time complexity and is fully parallelizable. More for-
mally, we match input attributes A with attributes of candidates AX = {Ax1 ,..., AxN }, where
1Though sharing the same rating can also be a criteria for reference candidates, we empirically found this would increase
the candidate size too much, while we can apply rating bias by including rating accuracy in relevance supervision instead.2288
Axj = {uxj ,pxj ,rxj }. We encode attribute features using embedding matrices followed by a fully
connect layer, and calculate relevance score using inner product between them. We denote concatenation
of the attribute vectors for the input identiﬁers, u/p/r, by a ∈R3×dc where dc is the number of fea-
tures in vectors. For simplicity, we use different superscripts for a to indicate that different embedding
matrices are used with the same identiﬁers, and use different subscripts to indicate different identiﬁers.
αxj = tanh(Hxaxj ) ∈Rdc (2)
α= tanh(Ha) ∈Rdc (3)
s(xj) =σ(α⊤
xj α) ∈(0,1), (4)
where axj and a are attribute vectors for Axj and A respectively, Hx and H are learnable matrices, and
σdenotes sigmoid function. For axj and a, we share the same embedding matrices for efﬁcient training.
Fine-Grained REFLECT Now that we narrowed down toM ≪Nreference candidates, we can afford
to use expensive textual input features. Fine-Grained R EFLECT accepts the references X+ as input and
outputs a score for each candidate that is used to select the ﬁnal Kreferences.
To get document-level encoding d+
j for each candidate x+
j , we use a bidirectional LSTM (Hochreiter
and Schmidhuber, 1997) with attention pooling (Bahdanau et al., 2015) using A as attention query.
α+ = tanh(Ma+) ∈Rdc (5)
h+
j = BiLSTM({w+
jk}
L+
j
k=1) ∈RL+
j ×df (6)
d+
j = softmaxk(v⊤
atanh(Ha[h+
jk; α+])h+
j ∈Rdf , (7)
where a+ is obtained from another embedding matrix using A as identiﬁers, M and Ha are learnable
matrices and va ∈Rdf is a learable vector. Note that, we use different parameters forαin Equation 3 and
α+ in Equation 5 since different features are required for Coarse-Grained R EFLECT and Fine-Grained
REFLECT : features for attribute-attribute matching and features for attribute-text matching respectively.
We also add a self-attention layer (Vaswani et al., 2017) to further contextualize reference{d+
j }M
j=1.
[q; k; v]j = [Wq; Wk; Wv]d+
j ∈Rdf (8)
˜d+
j = fFF
(
softmax
(
q⊤k√df
)
v
)
∈RM×df , (9)
where fFF is a residual feed-forward layer. Cross-reference contextualization further clariﬁes the mean-
ing of each reference by considering latent dependency over references (Liu and Lapata, 2019).
Then, we estimate relevance score using inner product between˜d+
j and α+, i.e., s(x+
j ) =σ(˜d+⊤
j α+).
5 RL-R EFLECT
SL has the advantage of efﬁcient selection of references by lexical and rating similarity, but this cannot
guarantee generated documents would preserve such similarity. In addition, “selection” of reference can
be generalized into “composing” multiple documents. We ﬁrst concretely describe the motivation for
introducing RL-REFLECT using the example presented in Figure 1.
Pseudo-labels evaluate the lexical and rating similarity of the entire reference document, and encour-
age to select ref#3 in Figure 1. However, in a hypothetical scenario without ref#3, ref#1 and ref#2 can
be “composed”, with the former contributing to aspects to be discussed ( e.g., “character” and “plot”)
and the latter to rating (e.g., “enjoyed”), while pseudo-labels may underestimate the importance of both.
Especially with respect to the goal of attaining rating semantics, pseudo-labels discourage R EFLECT
to retrieve ref#2, with nearly opposite rating to the ground truth, though other aspects can guide the
generation strongly.
Instead, to compute the aggregated effect of partial contributions, we employ Reinforcement Learning
(RL): We ﬁrst sample a set of references, generate a review using the references, and then estimate partial2289
relevance of those by how those are reﬂected in the generation. As illustrated with the above example,
RL is better than SL, especially in scenarios where multiple references collaboratively contribute to the
generation, while SL is comparable if some document is dominant in all aspects.
Speciﬁcally, training retrieval models using RL involves (1) sampling a set of references X∗from a
candidate pool; (2) generating a reviewY′using GEN with X∗; (3) calculating a reward forX∗based on
Y and Y′; and (4) adapting the sampling towards the direction to maximize the reward. We set reward
function by replacing xj with Y′in the previously deﬁned formula on relevance score (Equation 1). To
obtain rating accuracy of Y′, we ﬁrst train a standard sentiment classiﬁer comprising of a bidirectional
LSTM layer followed by an attention pooling layer, and then predict rating ofY′using the classiﬁer. For
efﬁcient RL training, we use ﬁltered references by Coarse-Grained R EFLECT , i.e., X+, as candidates.
A challenging part is adapting the sampling X∗(the 4th step described above) during training, since
sampling is a discrete operation which breaks continuity of a function and thus violates differentiability
assumption in standard backpropagation algorithm. Likelihood-ratio trick, proposed in (Williams, 1992),
enables the backpropagation of gradients regardless of discrete sampling.
∇J =Eτ[∇log p(X+)zY′] (10)
≈1
B
B∑
b=1
∇log p(X+
b )zb
Y′ (11)
where p(X+) is probability distribution of selecting{x+
j }M
j=1 which is obtained by normalizing relevance
scores, i.e., p(x+
j ) =s(x+
j )/∑M
m=1 s(x+
m), and Bis the number of sampling trials for approximation.
Stabilizing Training of RL Despite the above-mentioned strength of adapting to a heuristic scoring
function for generation, RL training is notoriously unstable and converges slowly (Ranzato et al., 2015).
Our key contribution is to make RL training practical. Note, we keep the weights of pretrained GEN ﬁxed
to keep RL training cost low. First, we use BLEU-1 as a proxy of look-ahead exploration. Compared
to random exploration for all references increasing variance too much, we prioritize exploration to those
with high lexical similarity ( e.g., BLEU-1 no less than 0.2). We empirically found this prioritization
reduces the variance without compromising the reference quality much. Second, as greedily maximizing
for Rinduces high variance, we maximizeR−˜Rinstead of R, where ˜Ris a sub-optimal reward obtained
by a baseline, speciﬁcally, p(X+) (Dong et al., 2018). Introducing a baseline performance is known to
decrease the variance (Weaver and Tao, 2001), and our experience was consistent.
6 G EN
GEN follows an encoder-decoder framework equipped with copying mechanism. First, we encode each
reference x∗
j with L∗
j words using a bidirectional LSTM with attention pooling.
h∗
j = BiLSTM(x∗
j) ∈RL∗
j ×de
g (12)
d∗
tj = softmaxk(v⊤
gw tanh(Hgw [h∗
jk; agw ; ot])h∗
j, (13)
where vgw ,h∗
jk,d∗
tj ∈Rde
g and ot ∈Rdh
g is a hidden state of decoder at t-th generation.
As in Fine-Grained REFLECT (Equation 8, 9), we further contextualize reference encodings,d∗
tj, using
a self-attention layer, yielding ˜d∗
tj. Then, we aggregate ˜d∗
tj using another attention layer.
oref
t = softmax(v⊤
gxtanh(Hgx[˜d∗
tj; agx; ot])˜d∗
t, (14)
where vgx,oref
t ∈Rde
g and ˜d∗
t ∈RK×de
g . Similar to encoding references, we attend over embedding
vectors of input attributes, aemb ∈R3×da
g , yielding oemb
t ∈Rda
g .
For decoder, we use a two-layer LSTM, and to obtain initial hidden state of LSTM, we use a multi-
layer perceptron following (Dong et al., 2017) taking attribute vectors as input.
h0 = tanh(Waemb) ∈Rdh
g (15)2290
For each generation step, decoder uses both reference vectors oref
t and attribute vectors oemb
t .
ot = 2-layer-LSTM(ot−1,yt) ∈Rdh
g (16)
st = tanh(Wo[oref
t ; oemb
t ; ht]) ∈Rdh
g (17)
gt = softmax(Hsst), (18)
where yt is generated word at step t.
As habitual words/phrases of users/products can be reused for generation, we allow G EN to copy
words from references.
zt = σ(Wz[yt; ht; oemb
t ; oref
t ]) ∈(0,1) (19)
pt = zt ×gt + (1−zt) ×attt, (20)
where att t is attention score distribution for words in references at Equation 13. For inference, we
greedily generate words by argmax-ing pt.
Training GEN We pretrain GEN using top-KBLEU-1 score references as inputs and cross-entropy on
gold reviews as the loss function. When we trained G EN using retrieved references by R EFLECT , per-
formance dropped signiﬁcantly. We suspect this is because even a small amount of irrelevant references
misguide training of GEN.
7 Experiments
We used the same dataset used in (Dong et al., 2017) (Amazon Book reviews) to evaluate models. Each
instance in the dataset consists of a review and aligned attributes (user ID, product ID, and a rating
ranging from 1 to 5). Statistics of the number of references for each attribute are as follows: (1) mini-
mum number of references is 6, 2, 13K for user, product, rating respectively, (2) maximum number of
references is 1265, 351, 405K, and (3) average number of references is 33.34, 8.17, 131K.
7.1 Training Details
For efﬁcient hyperparameter search on the vector dimension, we used 64 for attribute vectors (i.e.,dc,da
g)
following (Dong et al., 2017), and for others we choose the best value among 128, 256, and 512 using
validation set, where df,de
g,dh
g were 256, 128, and 512 respectively. Word embedding matrices for Fine-
Grained REFLECT and GEN were pre-trained using fastText (Bojanowski et al., 2017). For the number
of references, we set M and Kto be 50 and 10 respectively.
For training of G EN, we used the same setting with (Dong et al., 2017) including batch size, opti-
mizer, learning rate scheduling, initialization of parameters, dropout ratio, and gradient clipping. We
excluded references having BLEU-1 score to ground-truth review less than 0.2. If all reference can-
didates are excluded, we set oref
t as zero vector. For training of SL-R EFLECT , we set batch size to be
50,000 and 150 for Coarse- and Fine-Grained R EFLECT respectively, and use Adam (Kingma and Ba,
2015) optimizer with learning rate 0.001 for both models. For training of RL-R EFLECT , we set both
the number of samples B and batch size to be 50, and λto be 0.04. We searched the optimal λamong
[0.0,0.02,0.04,0.06,0.08,0.1]. We used Adam optimizer with learning rate 0.0001. Our experiments
were conducted on a GTX-2080Ti GPU.
7.2 Evaluation Results
Models As baselines, we report performance of Attr2Seq (Dong et al., 2017) and Cyclegen (Sharma
et al., 2018) which use embedding vectors to encode given attributes. For our models, we report per-
formance of G EN using retrieved references by Coarse- and Fine-Grained R EFLECT trained using SL
and RL, denoted by GEN-C-F (SL) and GEN-C-F (RL) respectively2. In addition, we also report perfor-
mances of RETRIEVE -C-F by evaluating the top-1 retrieved reference.
2In our preliminary evaluation on G EN-C-F (SL→RL) where we ﬁrst pretrained R EFLECT using SL and ﬁne-tuned using
RL and G EN-C-F (SL+RL) where we train R EFLECT by jointly optimizing both SL supervision and RL rewards, the models
performed poorly, so we do not consider such combination.2291
Model Lexical Sim Phrase Sim Rating
BLEU-1 BLEU-4 Accuracy
Baseline
Attr2Seq 30.48 5.03 -
Attr2Seq† 28.57 4.74 76.89
Cyclegen 30.63 5.46 -
Ours RETRIEVE -C-F 30.28 4.85 80.33
GEN-C-F (SL) 31.82 5.65 78.93
GEN-C-F (RL) 32.03 5.58 84.48
Table 1: BLEU and rating accuracy results on test dataset. We report performances of Attr2Seq model
reported in the original paper, and of our best-effort implementation for unreported measures (denoted
by Attr2Seq†). Bold denotes the best result, and underlined denotes results outperforming all baselines.
Model Output
Input Attributes
User : hooked, men, family, love, stories , Mackenzie, Hart , series, Lilianna
Product : characters , looking, next, story , believable, fast, author, mystery, moving
Rating : 1
Gold Hart is a good author but I found this series short on content and characters lacking.
Length and depth of story left you wanting more.
Attr2Seq † I am not sure what to say about this book. I am not sure what I was expecting but I
was disappointed.
RETRIEVE -C-F A chicks club with a difference, good fast moving story with believable
characters . I will read more of this author and looking forward to the next book to this series.
GEN -C-F (SL) The author has a great story line and the characters are great.
I will read more of her books. I will read more of her work.
GEN -C-F (RL) The beginning of the book was great, but the last “num”% of the book was boring
and the characters were boring. I didn’t like the story and the characters .
Table 2: Example generated outputs from four different systems. Bold-faced tokens are words that are
also found in the attribute-speciﬁc-frequent words. Words colored blue and red contain consistent and
inconsistent rating information to the given rating respectively.
Metrics We validate models based on two criteria and corresponding metrics as follows: (1) Con-
tent similarity between generated reviews and ground-truth reviews can be measured by widely adapted
metric, BLEU (Papineni et al., 2002). (2) We measure rating accuracy via classiﬁcation accuracy of
generated reviews using pretrained rating classiﬁer3.
Automatic Evaluation This section compares our model with existing baselines based on content
similarity and rating accuracy. Results are presented in Table 1.
RETRIEVE -C-F show worse performance compared to generation approaches including baselines, ex-
cept for rating accuracy. This is because aspects or opinions to be discussed can be ﬂexible depending
on input attributes, but retrieval approaches are limited to predicting existing reviews. On the other hand,
GEN-C-F (RL) and Gen-C-F (SL) utilizing existing reviews as references signiﬁcantly outperform all
baselines. This validates our hypothesis that inductive biases improve generation performance, and our
REFLECT models can retrieve helpful references.
Human Evaluation We also conducted human evaluations using Amazon Mechanical Turk system
to evaluate 150 randomly sampled texts. We compared a retrieval model, R ETRIEVE -C-F, and three
generation models including Attr2Seq†, GEN-C-F (SL), and GEN-C-F (RL).
For each attribute pair, participants were asked to blindly compare outputs of the four models and
annotate the best and worst. Speciﬁcally, three participants were shown each paired four outputs as well
as corresponding ground-truth text and were asked to decide which is the best and the worst according to
three criteria such as Informativeness (Does the review convey all information found in the gold standard
review?), Correctness (Is the information in the review factually accurate based on the information in the
3We use a BiLSTM with an attention layer for the rating classiﬁer, which is trained using Amazon review dataset and
optimized using cross entropy as objective function2292
gold standard review?), and Compactness (Is the review written in a complete yet concise manner?).
Model Best-Worst
Baseline Attr2Seq† -8.89%
Ours
RETRIEVE -C-F -2.22%
GEN-C-F (SL) -0.67%
GEN-C-F (RL) 11.78%
Table 3: Human evaluation result.
We can then observe whether any model is
signiﬁcantly better in terms of Best-Worst Scal-
ing (Louviere and Woodworth, 1991) where the
percentage of times it was selected as best sub-
tracted by the percentage of times it was selected
as worst. In Table 3, we can observe that G EN-
C-F (RL) signiﬁcantly outperforms Attr2Seq†as
well as RETRIEVE -C-F) at p< 0.05 using t-test.
Qualitative Example Table 2 shows examples of generated reviews from different models with given
attributes for the given attributes. We also present frequently used words of the user and the product
which are gathered by ranking words using TF-IDF.
As can be seen in the examples, Attr2Seq † tends to generate generic reviews, where the generated
words do not coincide with the attribute-speciﬁc-frequent words. R ETRIEVE -C-F is able to generate
attribute-speciﬁc-frequent words such as characters and story, however it also outputs inconsistent infor-
mation where rating semantic does not match to the given rating. Utilizing references as inputs, GEN-C-F
(SL) and -F (RL) are also able to generate attribute-speciﬁc terms. For rating accuracy, while G EN-C-F
(SL) generates words that are inconsistent with the given rating, such asgreat, GEN-C-F (RL), optimized
to increase rating accuracy of generated reviews, is able to preserve rating accuracy.
Analysis on Rating Consistency We analyze contribution of rating accuracy for relevance of refer-
ences in Equation 1 on overall quality and rating consistency of generated reviews, where BLEU-1/4 and
rating accuracy are adopted as evaluation metrics. We show results on Table 4.
Model BLEU-1 BLEU-4 RA
Attr2Seq† 28.57 4.74 76.89
GEN-C-F (SL) 31.82 5.65 78.93
- RA 31.43 5.60 75.49
GEN-C-F (RL) 32.03 5.58 84.48
- RA 31.95 5.53 74.75
Table 4: Ablation on relevance estimation. “RA”
denotes rating accuracy and “-RA” indicates λ = 0
in Equation 1. We bold the best performance on
each metric. Values colored red are performances
weaker than that of Attr2Seq†.
Introducing rating accuracy for relevance, in
addition to lexical similarity, increased rating ac-
curacy of generated reviews, and even BLEU-
1 and BLEU-4. Without rating accuracy, i.e.,
λ = 0 in Equation 1, both G EN-C-F (SL) and
GEN-C-F (RL) produced lower rating accuracy
of generated reviews than that of Attr2Seq †.
This shows that lexical similarity may fail to
capture rating semantics. Meanwhile, G EN-C-F
(RL) signiﬁcantly outperformed G EN-C-F (SL)
on rating accuracy with little sacriﬁce of BLEU-
4, which indicates that relevance of references
regarding rating semantics should be determined
based on generated reviews rather than on references.
8 Conclusion
In this work, we study the problem of review generation guided by reference documents. Attribute-
speciﬁc reference reviews provide useful inductive biases of given attributes, and such biases can be
explicitly delivered to generated reviews using copying mechanism. Inspired by promising results, as
future work, we will investigate alternative means of guidance, such as keywords or topic distribution of
attributes.
Acknowledgements
This work was supported by AI Graduate School Program (2020-0-01361) and by the MSIT (Ministry
of Science and ICT), Korea, under the ITRC (Information Technology Resarch Center) support program
(IITP-2020-2016-0-00464) supervised by the IITP (Institute for Information Communications Technol-
ogy Planning Evaluation).2293
References
Reinald Kim Amplayo, Kyungjae Lee, Jinyoung Yeo, and Seung-won Hwang. 2018. Translations as additional
contexts for sentence classiﬁcation. In IJCAI, pages 3955–3961.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to
align and translate. In ICLR.
Anja Belz, Mike White, Dominic Espinosa, Eric Kow, Deirdre Hogan, and Amanda Stent. 2011. The ﬁrst surface
realisation shared task: Overview and evaluation results. In ENLG, pages 217–226.
Jes´us Bobadilla, Fernando Ortega, Antonio Hernando, and Abraham Guti ´errez. 2013. Recommender systems
survey. Knowledge-based systems, 46:109–132.
Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching word vectors with
subword information. Transactions of the Association for Computational Linguistics, 5:135–146.
Deng Cai, Yan Wang, Victoria Bi, Zhaopeng Tu, Xiaojiang Liu, Wai Lam, and Shuming Shi. 2018. Skeleton-to-
response: Dialogue generation guided by retrieval memory. arXiv preprint arXiv:1809.05296.
Ziqiang Cao, Wenjie Li, Sujian Li, and Furu Wei. 2018. Retrieve, rerank and rewrite: Soft template based neural
summarization. In ACL, pages 152–161.
Li Dong, Shaohan Huang, Furu Wei, Mirella Lapata, Ming Zhou, and Ke Xu. 2017. Learning to generate product
reviews from attributes. In EACL, pages 623–632.
Yue Dong, Yikang Shen, Eric Crawford, Herke van Hoof, and Jackie Chi Kit Cheung. 2018. Banditsum: Extrac-
tive summarization as a contextual bandit. In EMNLP, pages 3739–3748.
Sebastian Gehrmann, Yuntian Deng, and Alexander M Rush. 2018. Bottom-up abstractive summarization. arXiv
preprint arXiv:1808.10792.
Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor OK Li. 2018. Search engine guided neural machine transla-
tion. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence.
Sepp Hochreiter and J ¨urgen Schmidhuber. 1997. Long short-term memory. Neural Computation, 9(8):1735–
1780.
Yoon Kim. 2014. Convolutional neural networks for sentence classiﬁcation. In EMNLP, pages 1746–1751.
Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In ICLR.
Karen Kukich. 1983. Design of a knowledge-based report generator. In ACL, pages 145–150.
Pan Li and Alexander Tuzhilin. 2019. Towards controllable and personalized review generation. In Proceed-
ings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International
Joint Conference on Natural Language Processing (EMNLP-IJCNLP) , pages 3237–3245, Hong Kong, China,
November. Association for Computational Linguistics.
Juncen Li, Robin Jia, He He, and Percy Liang. 2018. Delete, retrieve, generate: a simple approach to sentiment
and style transfer. In NAACL-HLT, pages 1865–1874.
Junyi Li, Wayne Xin Zhao, Ji-Rong Wen, and Yang Song. 2019. Generating long and informative reviews with
aspect-aware coarse-to-ﬁne decoding. In Proceedings of the 57th Annual Meeting of the Association for Com-
putational Linguistics, pages 1969–1979, Florence, Italy, July. Association for Computational Linguistics.
Yang Liu and Mirella Lapata. 2019. Hierarchical transformers for multi-document summarization. In ACL, pages
5070–5081.
Jordan J Louviere and George G Woodworth. 1991. Best-worst scaling: A model for the largest difference
judgments. University of Alberta: Working Paper.
Diego Marcheggiani and Laura Perez-Beltrachini. 2018. Deep graph convolutional encoders for structured data to
text generation. In INLG, pages 1–9.
Kathleen R. McKeown. 1992. Text generation - using discourse strategies and focus constraints to generate
natural language text. Studies in natural language processing. Cambridge University Press.2294
Hongyuan Mei, Mohit Bansal, and Matthew R. Walter. 2016. What to talk about and how? selective generation
using lstms with coarse-to-ﬁne alignment. In NAACL-HLT, pages 720–730.
Arjun Mukherjee and Bing Liu. 2012. Aspect extraction through semi-supervised modeling. In Proceedings
of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages
339–348, Jeju Island, Korea, July. Association for Computational Linguistics.
Jianmo Ni and Julian McAuley. 2018. Personalized review generation by expanding phrases and attending on
aspect-aware representations. In Proceedings of the 56th Annual Meeting of the Association for Computational
Linguistics (Volume 2: Short Papers) , pages 706–711, Melbourne, Australia, July. Association for Computa-
tional Linguistics.
Gaurav Pandey, Danish Contractor, Vineet Kumar, and Sachindra Joshi. 2018. Exemplar encoder-decoder for neu-
ral conversation generation. In Proceedings of the 56th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 1329–1338.
Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Found. Trends Inf. Retr., 2(1-2):1–135,
January.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation
of machine translation. In ACL, pages 311–318.
Hao Peng, Ankur P. Parikh, Manaal Faruqui, Bhuwan Dhingra, and Dipanjan Das. 2019. Text generation with
exemplar-based adaptive decoding. In NAACL-HLT, pages 2555–2565.
Laura Perez-Beltrachini, Rania Sayed, and Claire Gardent. 2016. Building RDF content for data-to-text genera-
tion. In COLING, pages 1493–1502.
Maria Pontiki, Dimitris Galanis, Haris Papageorgiou, Ion Androutsopoulos, Suresh Manandhar, Mohammad AL-
Smadi, Mahmoud Al-Ayyoub, Yanyan Zhao, Bing Qin, Orph´ee De Clercq, V´eronique Hoste, Marianna Apidi-
anaki, Xavier Tannier, Natalia Loukachevitch, Evgeniy Kotelnikov, Nuria Bel, Salud Mar´ıa Jim´enez-Zafra, and
G¨uls ¸en Eryi˘git. 2016. SemEval-2016 task 5: Aspect based sentiment analysis. In Proceedings of the 10th
International Workshop on Semantic Evaluation (SemEval-2016) , pages 19–30, San Diego, California, June.
Association for Computational Linguistics.
Ratish Puduppully, Li Dong, and Mirella Lapata. 2019a. Data-to-text generation with content selection and
planning. In AAAI, pages 6908–6915.
Ratish Puduppully, Li Dong, and Mirella Lapata. 2019b. Data-to-text generation with entity modeling. In ACL,
pages 2023–2035.
Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. 2015. Sequence level training with
recurrent neural networks. arXiv preprint arXiv:1511.06732.
Paul Resnick and Hal R Varian. 1997. Recommender systems. Communications of the ACM, 40(3):56–58.
Alexander M. Rush, Sumit Chopra, and Jason Weston. 2015. A neural attention model for abstractive sentence
summarization. In EMNLP, pages 379–389.
Abigail See, Peter J Liu, and Christopher D Manning. 2017. Get to the point: Summarization with pointer-
generator networks. arXiv preprint arXiv:1704.04368.
Vasu Sharma, Harsh Sharma, Ankita Bishnu, and Labhesh Patel. 2018. Cyclegen: Cyclic consistency based
product review generator from attributes. In INLG, pages 426–430.
Yiping Song, Cheng-Te Li, Jian-Yun Nie, Ming Zhang, Dongyan Zhao, and Rui Yan. 2018. An ensemble of
retrieval-based and generation-based human-computer conversation systems. In Proceedings of the 27th Inter-
national Joint Conference on Artiﬁcial Intelligence, pages 4382–4388.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and
Illia Polosukhin. 2017. Attention is all you need. In NIPS, pages 5998–6008.
Lex Weaver and Nigel Tao. 2001. The optimal reward baseline for gradient-based reinforcement learning. In
Proceedings of the Seventeenth conference on Uncertainty in artiﬁcial intelligence , pages 538–545. Morgan
Kaufmann Publishers Inc.2295
Jason Weston, Emily Dinan, and Alexander H. Miller. 2018. Retrieve and reﬁne: Improved sequence generation
models for dialogue. In Proceedings of the 2nd International Workshop on Search-Oriented Conversational AI,
SCAI@EMNLP 2018, Brussels, Belgium, October 31, 2018, pages 87–92.
Ronald J. Williams. 1992. Simple statistical gradient-following algorithms for connectionist reinforcement learn-
ing. Machine Learning, 8:229–256.
Sam Wiseman, Stuart M. Shieber, and Alexander M. Rush. 2017. Challenges in data-to-document generation. In
EMNLP, pages 2253–2263.
Hongyu Zang and Xiaojun Wan. 2017. Towards automatic generation of product reviews from aspect-sentiment
scores. In INLG, pages 168–177.
Rui Zhao and Kezhi Mao. 2017. Topic-aware deep compositional models for sentence classiﬁcation. IEEE/ACM
Trans. Audio, Speech & Language Processing, 25(2):248–260.
Qingfu Zhu, Lei Cui, Weinan Zhang, Furu Wei, and Ting Liu. 2019. Retrieval-enhanced adversarial training for
neural response generation. In ACL, pages 3763–3773.

In-Context Retrieval-Augmented Language Models
Ori Ram∗ Yoav Levine∗ Itay Dalmedigos Dor Muhlgay
Amnon Shashua Kevin Leyton-Brown Yoav Shoham
AI21 Labs, Israel
{orir,yoavl,itayd,dorm,amnons,kevinlb,yoavs}@ai21.com
Abstract
Retrieval-Augmented Language Modeling
(RALM) methods, which condition a language
model (LM) on relevant documents from a
grounding corpus during generation, were
shown to significantly improve language
modeling performance. In addition, they
can mitigate the problem of factually inac-
curate text generation and provide natural
source attribution mechanism. Existing
RALM approaches focus on modifying
the LM architecture in order to facilitate
the incorporation of external information,
significantly complicating deployment. This
paper considers a simple alternative, which
we dubIn-Context RALM: leaving the LM
architecture unchanged and prepending
grounding documents to the input, without
any further training of the LM. We show that
In-Context RALM that builds on off-the-shelf
general purpose retrievers provides surpris-
ingly large LM gains across model sizes and
diverse corpora. We also demonstrate that the
document retrieval and ranking mechanism
can be specialized to the RALM setting to
further boost performance. We conclude that
In-Context RALM has considerable potential
to increase the prevalence of LM grounding,
particularly in settings where a pretrained LM
must be used without modification or even
via API access.1
1 Introduction
Recent advances in language models (LMs)
have dramatically increased the usefulness of
machine-generated text across a wide range of
use-cases and domains (Brown et al., 2020). How-
ever, the mainstream paradigm of generating text
with LMs bears inherent limitations in access
to external knowledge. First, LMs are not cou-
pled with any source attribution, and must be
∗Equal contribution.
1Our code is available at https://github.com
/AI21Labs/in-context-ralm.
trained in order to incorporate up-to-date infor-
mation that was not seen during training. More
importantly, they tend to produce factual inac-
curacies and errors (Lin et al., 2022; Maynez
et al., 2020; Huang et al., 2020). This problem is
present in any LM generation scenario, and is ex-
acerbated when generation is made in uncommon
domains or private data. A promising approach
for addressing the above is Retrieval-Augmented
Language Modeling (RALM), grounding the LM
during generation by conditioning on relevant
documents retrieved from an external knowledge
source. RALM systems include two high level
components: (i) document selection, selecting the
set of documents upon which to condition; and
(ii) document reading, determining how to in-
corporate the selected documents into the LM
generation process.
Leading RALM systems introduced recently
tend to be focused on altering the language model
architecture (Khandelwal et al., 2020; Borgeaud
et al., 2022; Zhong et al., 2022; Levine et al.,
2022c; Li et al., 2022). Notably, Borgeaud et al.
(2022) introduced RETRO, featuring document
reading via nontrivial modifications that require
further training to the LM architecture, while us-
ing an off-the-shelf frozen BERT retriever for
document selection. Although the paper’s exper-
imental findings showed impressive performance
gains, the need for changes in architecture and ded-
icated retraining has hindered the wide adoption
of such models.
In this paper, we show that a very simple docu-
ment reading mechanism can have a large impact,
and that substantial gains can also be made by
adapting the document selection mechanism to the
task of language modeling. Thus, we show that
many of the benefits of RALM can be achieved
while working with off-the-shelf LMs, even via
API access. Specifically, we consider a simple but
powerful RALM framework, dubbed In-Context
1316
Transactions of the Association for Computational Linguistics, vol. 11, pp. 1316–1331, 2023. https://doi.org/10.1162/tacla 00605
Action Editor: Hinrich Sch¨utze. Submission batch: 3/2023; Revision batch: 5/2023; Published 11/2023.
c⃝ 2023 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license.
Downloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00605/2178834/tacl_a_00605.pdf by guest on 29 January 2025
Figure 1: An example ofIn-Context RALM: We simply prepend the retrieved document before the input prefix.
RALM (presented in Section 3), which employs
a zero-effort document reading mechanism: We
simply prepend the selected documents to the
LM’s input text (Figure 1).
Section 4 describes our experimental setup.
To show the wide applicability of our frame-
work, we performed LM experiments on a suite
of five diverse corpora: WikiText-103 (Merity
et al., 2016), RealNews (Zellers et al., 2019), and
three datasets from The Pile (Gao et al., 2021):
ArXiv, Stack Exchange, and FreeLaw. We use
open-source LMs ranging from 110M to 66B pa-
rameters (from the GPT-2, GPT-Neo, OPT, and
LLaMA model families).
In Section 5 we evaluate the application of
off-the-shelf retrievers to our framework. In this
minimal-effort setting, we found that In-Context
RALM led to LM performance gains equivalent
to increasing the LM’s number of parameters by
2–3× across all of the text corpora we exam-
ined. In Section 6 we investigate methods for
adapting document ranking to the LM task, a rela-
tively under-explored RALM degree of freedom.
Our adaptation methods range from using a small
LM to perform zero-shot ranking of the retrieved
documents, up to training a dedicated bidirec-
tional reranker by employing self-supervision
from the LM signal. These methods lead to fur-
ther gains in the LM task corresponding to an
additional size increase of 2× in the LM archi-
tecture. As a concrete example of the gains, a
345M parameter GPT-2 enhanced by In-Context
RALM outperforms a 762M parameter GPT-2
when employing an off-the-shelf BM25 retriever
(Robertson and Zaragoza, 2009), and outperforms
a 1.5B parameter GPT-2 when employing our
trained LM-oriented reranker (see Figure 2). For
large model sizes, our method is even more ef-
fective: In-Context RALM with an off-the-shelf
retriever improved the performance of a 6.7B
parameter OPT model to match that of a 66B
parameter parameter OPT model (see Figure 4).
Figure 2: Our framework, dubbed In-Context RALM,
provides large language modeling gains on the test set
of WikiText-103,without modifying the LM. Adapting
the use of a BM25 retriever (Robertson and Zaragoza,
2009) to the LM task (§5) yields significant gains, and
choosing the grounding documents via our new class
of Predictive Rerankers (§6) provides a further boost.
See Table 1 for the full results on five diverse corpora.
In Section 7 we demonstrate the applicability of
In-Context RALM to downstream open-domain
questions answering (ODQA) tasks.
In a concurrent work, Shi et al. (2023) also sug-
gest to augment off-the-shelf LMs with retrieved
texts by prepending them to the input. Their re-
sults are based on training a dedicated retriever for
language modeling. In contrast, we focus on the
gains achievable in using off-the-shelf retrievers
for this task. We show strong gains of this simpler
setting by investigating: (1) which off-the-shelf
retriever is best suited for language modeling, (2)
the frequency of retrieval operations, and (3) the
optimal query length. In addition, we boost the
off-the-shelf retrieval performance by introducing
two reranking methods that demonstrate further
gains in perplexity.
We believe that In-Context RALM can play
two important roles in making RALM systems
more powerful and more prevalent. First, given
its simple reading mechanism, In-Context RALM
can serve as a clean probe for developing docu-
ment retrieval methods that are specialized for the
LM task. These in turn can be used to improve
both In-Context RALM and other more elaborate
1317
Downloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00605/2178834/tacl_a_00605.pdf by guest on 29 January 2025
RALM methods that currently leverage general
purpose retrievers. Second, due to its compatibil-
ity with off-the-shelf LMs, In-Context RALM can
help drive wider deployment of RALM systems.
2 Related Work
RALM approaches can be roughly divided into
two families of models: (i)nearest-neighbor lan-
guage models (also called kNN-LM), and (ii)
retrieve and read models. Our work belongs to
the second family, but is distinct in that it involves
no further training of the LM.
Nearest Neighbor Language Models The
kNN-LM approach was first introduced in
Khandelwal et al. (2020). The authors suggest a
simple inference-time model that interpolates be-
tween two next-token distributions: one induced
by the LM itself, and one induced by thek neigh-
bors from the retrieval corpus that are closest to the
query token in the LM embedding space. Zhong
et al. (2022) suggest a framework for training
these models. While they showed significant gains
from kNN-LM, the approach requires storing the
representations for each token in the corpus,a n
expensive requirement even for a small corpus
like Wikipedia. Although numerous approaches
have been suggested for alleviating this issue (He
et al., 2021; Alon et al., 2022), scaling any of them
to large corpora remains an open challenge.
Retrieve and Read Models This family of
RALMs creates a clear division between docu-
ment selectionand document readingcomponents.
All prior work involves training the LM. We
begin by describing works that use this ap-
proach for tackling downstream tasks, and then
mention works oriented towards RALM. Lewis
et al. (2020) and Izacard and Grave (2021) fine
tuned encoder–decoder architectures for down-
stream knowledge-intensive tasks. Izacard et al.
(2022b) explored different ways of pretrain-
ing such models, while Levine et al. (2022c)
pretrained an autoregressive LM on clusters
of nearest neighbors in sentence embedding
space. Levine et al. (2022a) showed competitive
open domain question-answering performance by
prompt-tuning a frozen LM as a reader. Guu
et al. (2020) pretrained REALM, a retrieval aug-
mented bidirectional, maskedLM, later fine-tuned
for open-domain question answering. The work
closest to this paper—with a focus on the
language modeling task—is RETRO (Borgeaud
et al., 2022), which modifies an autoregressive
LM to attend to relevant documents via chunked
cross-attention, thus introducing new parameters
to the model. Our In-Context RALM differs from
prior work in this family of models in two key
aspects:
• We use off-the-shelf LMs for document
reading without any further training of the
LM.
• We focus on how to choose documents for
improved LM performance.
3 Our Framework
3.1 In-Context RALM
Language models define probability distributions
over sequences of tokens. Given such a sequence
x1,...,x n, the standard way to model its probabil-
ity is via next-token prediction:p(x1,...,x n)=∏ n
i=1 p(xi|x<i), where x<i := x1,...,x i−1 is the
sequence of tokens precedingxi, also referred to
as its prefix. This autoregressive model is usually
implemented via a learned transformer network
(Vaswani et al., 2017) parameterized by the set of
parameters θ:
p(x1,...,x n)=
n∏
i=1
pθ(xi|x<i), (1)
where the conditional probabilities are modeled by
employing a causal self-attention mask (Radford
et al., 2018). Notably, leading LMs such as GPT-2
(Radford et al., 2019), GPT-3 (Brown et al., 2020),
OPT (Zhang et al., 2022), or Jurassic-1 (Lieber
et al., 2021) follow this simple parameterization.
Retrieval augmented language models
(RALMs) add an operation that retrieves one or
more documents from an external corpusC,a n d
condition the above LM predictions on these docu-
ments. Specifically, for predictingxi, the retrieval
operation from C depends on its prefix:RC(x<i),
so the most general RALM decomposition is:
p(x1,...,x n)= ∏ n
i=1 p(xi|x<i,RC(x<i)).I n
order to condition the LM generation on the
retrieved document, previous RALM approaches
used specialized architectures or algorithms
(see §2). Inspired by the success of In-Context
Learning (Brown et al., 2020; Dong et al., 2023),
In-Context RALMrefers to the following specific,
simple method of concatenating the retrieved
1318
Downloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00605/2178834/tacl_a_00605.pdf by guest on 29 January 2025
documents2 within the Transformer’s input prior
to the prefix (see Figure 1), which does not
involve altering the LM weightsθ:
p(x1,...,x n)=
n∏
i=1
pθ (xi|[RC(x<i);x<i]), (2)
where [a;b]denotes the concatenation of stringsa
and b.
Since common Transformer-based LM imple-
mentations support limited length input sequences,
when the concatenation of the document and the
input sequence exceed this limit we remove to-
kens from the beginning ofxuntil the overall input
length equals that allowed by the model. Because
our retrieved documents are passages of limited
length, we always have enough context left from
x (see §4.3).
3.2 RALM Design Choices
We detail below two practical design choices often
made in RALM systems. In§5, we investigate the
effect of these in the setting of In-Context RALM.
Retrieval Stride While in the above formu-
lation a retrieval operation can occur at each
generation step, we might want to perform re-
trieval only once every s> 1 tokens due to the
cost of calling the retriever, and the need to replace
the documents in the LM prefix during generation.
We refer tosas theretrieval stride. This gives rise
to the following In-Context RALM formulation
(which reduces back to Eq. (2) fors =1 ):
p(x1,...,x n)=
ns−1∏
j=0
s∏
i=1
pθ
(
xs·j+i|
[
RC(x≤s·j);x<(s·j+i)
])
,
(3)
where ns = n/sis the number of retrieval strides.
Notably, in this framework the runtime costs
of each retrieval operation is composed of (a)
applying the retriever itself, and (b) recomputing
the embeddings of the prefix. In §5.2 we show
that using smaller retrieval strides,i.e., retrieving
as often as possible, is superior to using larger
ones (though In-Context RALM with larger strides
already provides large gains over vanilla LM).
Thus, choosing the retrieval stride is ultimately a
tradeoff between runtime and performance.
2We always use asingle document, but it is conceptually
simple to support multiple documents as well.
Retrieval Query Length While the retrieval
query above in principle depends on all prefix
tokens x≤s·j, the information at the very end
of the prefix is typically the most relevant to
the generated tokens. If the retrieval query is
too long then this information can be diluted.
To avoid this, we restrict the retrieval query at
stridej to the last ℓ tokens of the prefix, i.e.,
we use qs,ℓ
j := xs·j−ℓ+1,...,x s·j. We refer to
ℓ as the retrieval query length. Note that prior
RALM work couples the retrieval stridesand the
retrieval query length ℓ (Borgeaud et al., 2022).
In §5, we show that enforcings = ℓdegrades LM
performance. Integrating these hyper-parameters
into the In-Context RALM formulation gives
p(x1,...,x n)=
ns−1∏
j=0
s∏
i=1
pθ
(
xs·j+i|
[
RC(qs,ℓ
j );x<(s·j+i)
])
.
(4)
4 Experimental Details
We now describe our experimental setup, includ-
ing all models we use and their implementation
details.
4.1 Datasets
We evaluated the effectiveness of In-Context
RALM across five diverse language modeling
datasets and two common open-domain question
answering datasets.
Language Modeling The first LM dataset is
WikiText-103 (Merity et al., 2016), which has been
extensively used to evaluate RALMs (Khandelwal
et al., 2020; He et al., 2021; Borgeaud et al., 2022;
Alon et al., 2022; Zhong et al., 2022). Second,
we chose three datasets spanning diverse subjects
from The Pile (Gao et al., 2021): ArXiv, Stack
Exchange,a n dFreeLaw. Finally, we also investi-
gated RealNews (Zellers et al., 2019), since The
Pile lacks a corpus focused only on news (which
is by nature a knowledge-intensive domain).
Open-Domain Question Answering In order
to evaluate In-Context RALM on downstream
tasks as well, we use theNatural Questions(NQ;
Kwiatkowski et al. 2019) and TriviaQA (Joshi
et al., 2017) open-domain question answering
datasets.
1319
Downloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00605/2178834/tacl_a_00605.pdf by guest on 29 January 2025
4.2 Models
Language Models We performed our ex-
periments using the four models of GPT-2
(110M–1.5B; Radford et al., 2019), three models
of GPT-Neo and GPT-J (1.3B–6B; Black et al.,
2021; Wang and Komatsuzaki, 2021), eight mod-
els of OPT (125M–66B; Zhang et al. 2022), and
three models of LLaMA (7B–33B; Touvron et al.,
2023). All models are open source and publicly
available.3
We elected to study these particular models
for the following reasons. The first four (GPT-2)
models were trained on WebText (Radford et al.,
2019), with Wikipedia documents excluded from
their training datasets. We were thus able to evalu-
ate our method’s ‘‘zero-shot’’ performance when
retrieving from a novel corpus (for WikiText-103).
The rest of the models brought two further ben-
efits. First, they allowed us to investigate how
our methods scale to models larger than GPT-2.
Second, the fact that Wikipedia was part of their
training data allowed us to investigate the use-
fulness of In-Context RALM for corpora seen
during training. The helpfulness of such retrieval
has been demonstrated for previous RALM meth-
ods (Khandelwal et al., 2020) and has also been
justified theoretically by Levine et al. (2022c).
We ran all models with a maximum sequence
length of 1,024, even though GPT-Neo, OPT,
and LLaMA models support a sequence length of
2,048.4
Retrievers We experimented with both sparse
(word-based) and dense (neural) retrievers. We
used BM25 (Robertson and Zaragoza, 2009) as our
sparse model. For dense models, we experimented
with (i) a frozen BERT-base (Devlin et al., 2019)
followed by mean pooling, similar to Borgeaud
et al. (2022); and (ii) the Contriever (Izacard
et al., 2022a) and Spider (Ram et al., 2022) models,
which are dense retrievers that were trained in
unsupervised manners.
Reranking When training rerankers (Sec-
tion 6.2), we initialized from RoBERTa-base (Liu
et al., 2019).
3All models are available for use use via https://
huggingface.co/.
4In preliminary experiments, we observed similar im-
provements from In-Context RALM when using a sequence
length of 2,048. We used a sequence length of 1,024 in order
to facilitate a direct comparison between all models.
4.3 Implementation Details
We implemented our code base using the Trans-
formers library (Wolf et al., 2020). We based
our dense retrieval code on the DPR repository
(Karpukhin et al., 2020).
Retrieval Corpora For WikiText-103 and
ODQA datasets, we used the Wikipedia corpus
from Dec. 20, 2018, standardized by Karpukhin
et al. (2020) using the preprocessing from Chen
et al. (2017). To avoid contamination, we found
and removed all 120 articles of the development
and test set of WikiText-103 from the corpus.
For the remaining datasets, we used their training
data as the retrieval corpus. Similar to Karpukhin
et al. (2020), our retrieval corpora consist of
non-overlapping passages of 100 words (which
translate to less than 150 tokens for the vast
majority of passages). Thus, we truncate our
retrieved passages at 256 tokens when input to
the models, but they are usually much smaller.
Retrieval For sparse retrieval, we used the
Pyserini library (Lin et al., 2021). For dense
retrieval, we applied exact search using FAISS
(Johnson et al., 2021).
5 The Effectiveness of In-Context RALM
with Off-the-Shelf Retrievers
We now empirically show that despite its simple
document reading mechanism, In-Context RALM
leads to substantial LM gains across our diverse
evaluation suite. We begin in this section by
investigating the effectiveness of off-the-shelf re-
trievers for In-Context RALM; we go on in §6
to show that further LM gains can be made by
tailoring document ranking functions to the LM task.
The experiments in this section provided us
with a recommended configuration for applying
In-Context RALM: applying a sparse BM25 re-
triever that receives ℓ =3 2 query tokens and
is applied as frequently as possible. Practically,
we retrieve every s =4 tokens (ℓ and s are de-
fined in§3). Table 1 shows for the GPT-2 models
that across all the examined corpora, employing
In-Context RALM with an off-the-shelf retriever
improved LM perplexity to a sufficient extent that
it matched that of a 2–3× larger model. Figure 4
and Tables 2 and 5 show that this trend holds
across model sizes up to 66B parameters, for both
WikiText-103 and RealNews.
1320
Downloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00605/2178834/tacl_a_00605.pdf by guest on 29 January 2025
Model Retrieval Reranking WikiText-103 RealNews ArXiv Stack Exch. FreeLaw
word ppl token ppl token ppl token ppl token ppl
GPT-2 S
– – 37.5 21.3 12.0 12.8 13.0
BM25 §5 – 29.6 16.1 10.9 11.3 9.6
BM25 Zero-shot §6.1 28.6 15.5 10.1 10.6 8.8
BM25 Predictive §6.2 26.8 – – – –
GPT-2 M
– – 26.3 15.7 9.3 8.8 9.6
BM25 §5 – 21.5 12.4 8.6 8.1 7.4
BM25 Zero-shot §6.1 20.8 12.0 8.0 7.7 6.9
BM25 Predictive §6.2 19.7 – – – –
GPT-2 L
– – 22.0 13.6 8.4 8.5 8.7
BM25 §5 – 18.1 10.9 7.8 7.8 6.8
BM25 Zero-shot §6.1 17.6 10.6 7.3 7.4 6.4
BM25 Predictive §6.2 16.6 – – – –
GPT-2 XL
– – 20.0 12.4 7.8 8.0 8.0
BM25 §5 – 16.6 10.1 7.2 7.4 6.4
BM25 Zero-shot §6.1 16.1 9.8 6.8 7.1 6.0
BM25 Predictive §6.2 15.4 – – – –
Table 1: Perplexity on the test set of WikiText-103, RealNews and three datasets from the Pile. For
each LM, we report: (a) its performance without retrieval, (b) its performance when fed the top-scored
passage by BM25 (§5), and (c) its performance when applied on the top-scored passage of each of our
two suggested rerankers (§6). All models share the same vocabulary, thus token-level perplexity (token
ppl) numbers are comparable. For WikiText we follow prior work and report word-level perplexity
(word ppl).
Model Retrieval WikiText-103
word ppl
LLaMA-7B –9 . 9
BM25, §58 . 8
LLaMA-13B –8 . 5
BM25, §57 . 6
LLaMA-33B –6 . 3
BM25, §56 . 1
Table 2: The performance of models from the
LLaMA family, measured by word-level per-
plexity on the test set of WikiText-103.
5.1 BM25 Outperforms Off-the-Shelf Neural
Retrievers in Language Modeling
We experimented with different off-the-shelf
general purpose retrievers, and found that the
sparse (lexical) BM25 retriever (Robertson and
Zaragoza, 2009) outperformed three popular dense
(neural) retrievers: the self-supervised retrievers
Contriever (Izacard et al., 2022a) and Spider (Ram
et al., 2022), as well as a retriever based on the av-
erage pooling of BERT embeddings that was used
in the RETRO system (Borgeaud et al., 2022).
Figure 3: The performance of four off-the-shelf retriev-
ers used for In-Context RALM on the development set
of WikiText-103. All RALMs are run withs =4
(i.e., retrieval is applied every four tokens). For each
RALM, we report the result of the best query lengthℓ
(see Figures 6, 9, 10).
We conducted a minimal hyper-parameter search
on the query length ℓ for each of the retrievers,
and found that ℓ =3 2 was optimal for BM25
(Figure 6), and ℓ =6 4 worked best for dense
retrievers (Figures 9, 10).
Figure 3 compares the performance gains of In-
Context RALM with these four general-purpose
retrievers. The BM25 retriever clearly outper-
formed all dense retrievers. This outcome is
consistent with prior work showing that BM25
1321
Downloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00605/2178834/tacl_a_00605.pdf by guest on 29 January 2025
Figure 4: Results of OPT models (Zhang et al., 2022) on the test set of WikiText-103 (word-level perplexity) and
the development set of RealNews (token-level perplexity). In-Context RALM models use a BM25 retriever with
s =4 (i.e., the retriever is called every four tokens) andℓ=3 2(i.e., the retriever query is comprised of the last
32 tokens of the prefix). In-Context RALM with an off-the-shelf retriever improved the performance of a 6.7B
parameter OPT model to match that of a 66B parameter OPT model.
Figure 5: An analysis of perplexity as a function ofs,
the retrieval stride, i.e., the number of tokens between
consecutive retrieval operations, on the development
set of WikiText-103. Throughout the paper, we use
s =4 to balance perplexity and runtime.
outperforms neural retrievers across a wide
array of tasks, when applied in zero-shot set-
tings (Thakur et al., 2021). This result renders
In-Context RALM even more appealing since ap-
plying a BM25 retriever is significantly cheaper
than the neural alternatives.
5.2 Frequent Retrieval Improves
Language Modeling
We investigated the effect of varying the retrieval
stride s (i.e., the number of tokens between con-
secutive retrieval operations). Figure 5 shows that
LM performance improved as the retrieval op-
eration became more frequent. This supports the
intuition that retrieved documents become more
relevant the closer the retrieval query becomes
to the generated tokens. Of course, each retrieval
operation imposes a runtime cost. To balance
performance and runtime, we used s =4 in
our experiments. For comparison, RETRO em-
ployed a retrieval frequency ofs =6 4(Borgeaud
et al., 2022), which leads to large degradation in
perplexity. Intuitively, retrieving with high fre-
quency (low retrieval stride) allows to ground the
LM in higher resolution.
5.3 A Contextualization vs. Recency
Tradeoff in Query Length
We also investigated the effect of varyingℓ,t h e
length of the retrieval query for BM25. Figure 6
reveals an interesting tradeoff and a sweet spot
around a query length of 32 tokens. Similar exper-
iments for dense retrievers are given in Appendix A.
We conjecture that when the retriever query is
too short, it does not include enough of the in-
put context, decreasing the retrieved document’s
relevance. Conversely, excessively growing the
retriever query deemphasizes the tokens at the very
end of the prefix, diluting the query’s relevance to
the LM task.
1322
Downloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00605/2178834/tacl_a_00605.pdf by guest on 29 January 2025
Figure 6: An analysis of perplexity as a function of
the number of tokens in the queryℓfor BM25 on the
development set of WikiText-103. In the appendix,
we show similar trade-offs for dense retrievers within
WikiText-103. Throughout the paper, we use a query
length ofℓ=3 2tokens.
6 Improving In-Context RALM with
LM-Oriented Reranking
Since In-Context RALM uses a fixed document
reading component by definition, it is natural to
ask whether performance can be improved by
specializing its document retrieval mechanism to
the LM task. Indeed, there is considerable scope
for improvement: the previous section considered
conditioning the model only on the first document
retrieved by the BM25 retriever. This permits very
limited semantic understanding of the query, since
BM25 is based only on the bag of words signal.
Moreover, it offers no way to accord different
degrees of importance to different retrieval query
tokens, such as recognizing that later query tokens
are more relevant to the generated text.
In this section, we focus on choosing which
document to present to the model, by reranking the
top-kdocuments returned by the BM25 retriever.5
We use Figure 7 as motivation: It shows the
large potential for improvement among the top-16
documents returned by the BM25 retriever. We
act upon this motivation by using two rerankers.
Specifically, in §6.1 we show performance gains
across our evaluation suite obtained by using an
LM to perform zero-shot reranking of the top-k
BM25 retrieved documents (results in third row
for each of the models in Table 1). Then, in§6.2
we show that training a specialized bidirectional
reranker of the top-k BM25 retrieved documents
in a self-supervised manner via the LM signal can
5In both §6.1 and §6.2 we usek =1 6.
Figure 7: Potential for gains from reranking. Per-
plexity improvement (on the development set of
WikiText-103) from an oracle that takes the best of
the top-16 documents retrieved by BM25 rather than
the first.
provide further LM gains (results in forth row for
each of the models in Table 1).
6.1 LMs as Zero-Shot Rerankers
First, we used off-the-shelf language models as
document rerankers for the In-Context RALM
setting. Formally, for a queryq consisting of the
last ℓ tokens in the prefix of the LM inputx,l e t
{d1,...,d k} be the top-k documents returned by
BM25. For retrieval iteration j, let the text for
generation be y := xs·j+1,...,x s·j+s. Ideally, we
would like to find the documentdi∗ that maximizes
the probability of the text for generation,i.e.,
i∗ =a r gm a x
i∈[k]
pθ(y|[di;x≤s·j]). (5)
However, at test time we do not have access
to the tokens of y. Instead, we used the last
prefix tokens (which are available at test time),
denoted by y′, for reranking. Formally, let s′ be
a hyper-parameter that determines the number of
the prefix tokens by which to rerank. We define
y′ := xs·j−s′+1,...,x s·j (i.e., the stride of length
s′ that precedes y) and choose the document dˆi
such that
ˆi =a r gm a x
i∈[k]
pφ(y′|
[
di;x≤(s·j−s′)
]
). (6)
The main motivation is that since BM25 is a lex-
ical retriever, we want to incorporate a semantic
signal induced by the LM. Also, this reranking
shares conceptual similarities with the reranking
framework of Sachan et al. (2022) for open-
domain question answering, where y′ (i.e.,t h e
last prefix tokens) can be thought of as their
‘‘question’’.
1323
Downloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00605/2178834/tacl_a_00605.pdf by guest on 29 January 2025
Model Reranking Model WikiText-103 RealNews
word ppl token ppl
GPT-2 345M (M) GPT-2 110M (S) 20.8 12.1
GPT-2 345M (M) 20.8 12.0
GPT-2 762M (L) GPT-2 110M (S) 17.7 10.7
GPT-2 762M (L) 17.6 10.6
GPT-2 1.5B (XL) GPT-2 110M (S) 16.2 9.9
GPT-2 1.5B (XL) 16.1 9.8
Table 3: Perplexity for zero-shot reranking (§6.1) where the reranking models is smaller than the LM,
or the LM itself. Reranking is performed on the top 16 documents retrieved by BM25. Using a GPT-2
110M (S) instead of a larger language model as a reranker leads to only a minor degradation.
Note that our zero-shot reranking does not re-
quire that the LM used for reranking is the same
model as the LM used for generation (i.e.,t h eL M
in Eq. (6), parameterized byφ, does not need to
be the LM in Eq. (2), parameterized byθ). This
observation unlocks the possibility of reranking
with smaller (and thus faster) models, which is
important for two main reasons: (i) Rerankingk
documents requires k forward passes; and (ii) it
allows our methods to be used in cases where
the actual LM’s log probabilities are not available
(for example, when the LM is accessed through
an API).6
Results A minimal hyper-parameter search on
the development set of WikiText-103 revealed that
the optimal query length iss′=1 6,7 so we proceed
with this value going forward. Table 1 shows
the results of letting the LM perform zero-shot
reranking on the top-16 documents retrieved by
BM25 (third row for each of the models). It is
evident that reranking yielded consistently better
results than simply taking the first result returned
by the retriever.
Table 3 shows that a small LM (GPT-2 117M)
can be used to rerank the documents for all larger
GPT-2 models, with roughly the same perfor-
mance as having each LM perform reranking for
itself, supporting the applicability of this method
for LMs that are only accessible via an API.
6.2 Training LM-dedicated Rerankers
Next, we trained a reranker to choose one of the
top-k documents retrieved by BM25. We refer
to this approach as Predictive Reranking,s i n c e
6Note we do not require that the two models share the
same vocabulary.
7We experimented withs′∈{ 4, 8, 16, 32}.
the reranker learns to choose which document
will help in ‘‘predicting’’ the upcoming text. For
this process, we assume availability of training
data from the target corpus. Our reranker is a
classifier that gets a prefixx≤s·j and a document
di (for i ∈ [k]), and produces a scalarf(x≤s·j,di)
that should resemble the relevance of di for the
continuation of x≤s·j.
We then normalize these relevance scores:
prank(di|x≤s·j)= exp(f(x≤s·j,di))
∑ k
i′=1 exp(f(x≤s·j,di′))
, (7)
and choose the documentdˆi such that
ˆi =a r gm a x
i∈[k]
prank(di|x≤s·j). (8)
Collecting Training Examples To train our
predictive reranker, we collected training exam-
ples as follows. Let x≤s·j be a prefix we sample
from the training data, andy := xs·j+1,...,x s·j+s
be the text for generation upcoming in its next
stride. We run BM25 on the queryqs,ℓ
j de-
rived from x≤s·j (see §3.2) and get k documents
{d1,...,d k}. For each documentdi, we then run
the LM to compute pθ(y|[di;x≤s·j]) similar to
Eq. (4).
Training Our reranker was a fine-tuned
RoBERTa-base (Liu et al., 2019) that trained for
10,000 steps with a peak learning rate of10−5 and
a batch size of 32. Overall, we created 300,000
examples from the training set of WikiText-103
as explained above. The loss function we use to
train the reranker follows previous work (Guu
et al., 2020; Lewis et al., 2020):
−log
k∑
i=1
prank(di|x≤s·j)·pθ(y|[di;x≤s·j]). (9)
1324
Downloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00605/2178834/tacl_a_00605.pdf by guest on 29 January 2025
Figure 8: Zero-shot performance of In-Context RALM
on the development set of Natural Questions and
TriviaQA, when varying the number of documents
(retrieved by DPR) shown in-context.
Note that unlike those works, we train only the
reranker (prank), keeping the LM weightsθfrozen.
Results Table 1 shows the result of our
predictive reranker, trained on WikiText-103.
Specifically, we trained it with data produced
by GPT-2 110M (S), and tested its effectiveness
for all GPT-2 models. We observed significant
gains obtained from Predictive Reranking. For
example, the perplexity of GPT-2 110M (S) im-
proved from 29.6 to 26.8, and that of GPT-2
1.5B (XL) improved from 16.6 to 15.4. This trend
held for the other two models as well. Overall,
these results demonstrate that training a reranker
with domain-specific data was more effective than
zero-shot reranking (Section 6.1). Note that these
results—while impressive—still leave room for
further improvements, compared to the top-16
BM25 oracle results (see Figure 7). Moreover, the
oracle results themselves can be improved by re-
trieving k> 16 documents via a BM25 retriever,
or by training stronger retrievers dedicated to the
RALM task. We leave this direction for future
work.
7 In-Context RALM for Open-Domain
Question Answering
So far, we evaluated our framework on language
modeling benchmarks. To test its efficacy in ad-
ditional scenarios, and specifically downstream
tasks, we now turn to evaluate In-Context RALM
on open-domain question answering (ODQA;
Chen et al., 2017). This experiment is intended
to verify, in a controlled environment, that LMs
can leverage retrieved documents without fur-
Model Retrieval NQ TriviaQA
LLaMA-7B – 10.3 47.5
DPR 28.0 56.0
LLaMA-13B – 12.0 54.8
DPR 31.0 60.1
LLaMA-33B – 13.7 58.3
DPR 32.3 62.7
Table 4: Zero-shot results of In-Context RALM
on the test set of Natural Questions and TriviaQA
measured by exact match. In the open-book set-
ting, we include the top two documents returned
by DPR.
ther training and without any training examples.
Specifically, we use the LLaMA family (Touvron
et al., 2023) with and without In-Context RALM
(often referred to in ODQA literature as open-book
and closed-book settings, respectively). In con-
trast to most prior work on ODQA (e.g., Izacard
and Grave, 2021; Fajcik et al., 2021; Izacard
et al., 2022b; Levine et al., 2022b), our ‘‘reader’’
(i.e., the model that gets the question along with
its corresponding retrieved documents, and re-
turns the answer) is simply a frozen large LM:
not pretrained, fine-tuned, or prompted to be
retrieval-augmented. For the closed-book setting,
we utilize the prompt of Touvron et al. (2023). For
the open-book setting, we extend this prompt to in-
clude retrieved documents (see Appendix C). We
use DPR (Karpukhin et al., 2020) as our retriever.
Varying the Number of DocumentsTo inves-
tigate the the effect of the number of documents
shown to the model, we performed a minimal anal-
ysis on the development set of NQ and TriviaQA.
Figure 8 demonstrates that showing documents
in-context significantly improves the model’s per-
formance. In addition, most of the gain can be
obtained by using only two documents (or even a
single one in some cases).
Results Table 4 gives the results of In-Context
RALM on the test set of Natural Questions
and TriviaQA. Motivated by our previous find-
ings, we used two retrieved documents. It is
evident that showing the model relevant docu-
ments significantly boosted its performance. For
example, adding retrieved documents improved
LLaMA-13B in the zero-shot setting by more than
1325
Downloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00605/2178834/tacl_a_00605.pdf by guest on 29 January 2025
18 points on NQ (from 12.0% to 31.0%) and more
than 5 points on TriviaQA (from 54.8% to 60.1%).
8 Discussion
Retrieval from external sources has become a com-
mon practice in knowledge-intensive tasks (such
as factual question answering, fact checking, and
more; Petroni et al. 2021). In parallel, recent break-
throughs in LM generation capabilities has led to
LMs that can generate useful long texts. How-
ever, factual inaccuracies remain a common way
in which machine-generated text can fall short,
and lack of direct provenance makes it hard to
trust machine generated text. This makes language
modeling both a promising and an urgent new
application area for knowledge grounding, and
motivates promoting RALM approaches. Prior re-
search has already investigated RALM, of course,
but it is not yet widely deployed. One likely reason
is that existing approaches rely upon fine-tuning
the LM, which is typically difficult and costly,
and is even impossible for LMs accessible only
via an API.
This paper presented the framework of
In-Context RALM, enabling frozen, off-the-shelf
LMs to benefit from retrieval. We demonstrated
that substantial performance gains can be
achieved by using general purpose retrievers, and
showed that additional gains can be achieved
by tailoring the document selection to the LM
setting. A recent work by Muhlgay et al. (2023)
demonstrates that In-Context RALM is indeed
able to improve the factuality of large LMs.
Several directions for further improvement re-
main for future work. First, this paper considers
only the case of prepending a single external doc-
ument to the context; adding more documents
could drive further gains (for example, using the
framework of Ratner et al., 2022). Second, we re-
trieved documents every fixed interval ofstokens,
but see potential for large latency and cost gains
by retrieving more sparsely, such as only when a
specialized model predicts that retrieval is needed.
We release the code used in this work, for the
community to use and improve over. We hope it
will drive further research of RALM, which will
enable its wider adoption.
Acknowledgments
We would like to thank the reviewers and the
Action Editor for their valuable feedback.
References
Uri Alon, Frank Xu, Junxian He, Sudipta
Sengupta, Dan Roth, and Graham Neubig.
2022. Neuro-symbolic language modeling with
automaton-augmented retrieval. InICML.
Sid Black, Leo Gao, Phil Wang, Connor
Leahy, and Stella Biderman. 2021. GPT-Neo:
Large scale autoregressive language modeling
with mesh-tensorflow. https://doi.org
/10.5281/zenodo.5297715
Sebastian Borgeaud, Arthur Mensch, Jordan
Hoffmann, Trevor Cai, Eliza Rutherford, Katie
Millican, George Bm Van Den Driessche,
Jean-Baptiste Lespiau, Bogdan Damoc, Aidan
Clark, Diego De Las Casas, Aurelia Guy,
Jacob Menick, Roman Ring, Tom Hennigan,
Saffron Huang, Loren Maggiore, Chris Jones,
Albin Cassirer, Andy Brock, Michela Paganini,
Geoffrey Irving, Oriol Vinyals, Simon
Osindero, Karen Simonyan, Jack Rae, Erich
Elsen, and Laurent Sifre. 2022. Improving lan-
guage models by retrieving from trillions of
tokens. In ICML.
Tom B. Brown, Benjamin Mann, Nick Ryder,
Melanie Subbiah, Jared Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam,
Girish Sastry, Amanda Askell, Sandhini
Agarwal, Ariel Herbert-Voss, Gretchen
Krueger, Tom Henighan, Rewon Child, Aditya
Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens
Winter, Christopher Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin
Chess, Jack Clark, Christopher Berner, Sam
McCandlish, Alec Radford, Ilya Sutskever,
and Dario Amodei. 2020. Language models
are few-shot learners. In Advances in Neural
Information Processing Systems.
Danqi Chen, Adam Fisch, Jason Weston, and
Antoine Bordes. 2017. Reading Wikipedia
to answer open-domain questions. In Pro-
ceedings of the 55th Annual Meeting of
the Association for Computational Linguistics
(Volume 1: Long Papers), pages 1870–1879,
Vancouver, Canada. Association for Computa-
tional Linguistics. https://doi.org/10
.18653/v1/P17-1171
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training
1326
Downloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00605/2178834/tacl_a_00605.pdf by guest on 29 January 2025
of deep bidirectional transformers for language
understanding. In Proceedings of the 2019
Conference of the North American Chapter
of the Association for Computational Linguis-
tics: Human Language Technologies, Volume 1
(Long and Short Papers), pages 4171–4186,
Minneapolis, Minnesota. Association for Com-
putational Linguistics.https://doi.org
/10.18653/v1/N19-1423
Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng,
Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing
Xu, Lei Li, and Zhifang Sui. 2023. A survey on
in-context learning.https://doi.org/10
.48550/arXiv.2301.00234
Martin Fajcik, Martin Docekal, Karel Ondrej,
and Pavel Smrz. 2021. R2-D2: A modular
baseline for open-domain question answer-
ing. In Findings of the Association for
Computational Linguistics: EMNLP 2021 ,
pages 854–870, Punta Cana, Dominican Re-
public. Association for Computational Linguis-
tics. https://doi.org/10.18653/v1
/2021.findings-emnlp.73
Leo Gao, Stella Biderman, Sid Black, Laurence
Golding, Travis Hoppe, Charles Foster,
Jason Phang, Horace He, Anish Thite,
Noa Nabeshima, Shawn Presser, and Connor
Leahy. 2021. The pile: An 800gb dataset of
diverse text for language modeling.https://
doi.org/10.48550/arXiv.2101.00027
Kelvin Guu, Kenton Lee, Zora Tung, Panupong
Pasupat, and Ming-Wei Chang. 2020.
REALM: Retrieval-augmented language model
pre-training. In ICML.
Junxian He, Graham Neubig, and Taylor
Berg-Kirkpatrick. 2021. Efficient nearest
neighbor language models. In Proceed-
ings of the 2021 Conference on Empirical
Methods in Natural Language Processing ,
pages 5703–5714, Online and Punta Cana,
Dominican Republic. Association for Compu-
tational Linguistics.https://doi.org/10
.18653/v1/2021.emnlp-main.461
Minlie Huang, Xiaoyan Zhu, and Jianfeng
Gao. 2020. Challenges in building intelligent
open-domain dialog systems. ACM Transac-
tions on Information Systems, 38(3).https://
doi.org/10.1145/3383123
Gautier Izacard, Mathilde Caron, Lucas Hosseini,
Sebastian Riedel, Piotr Bojanowski, Armand
Joulin, and Edouard Grave. 2022a. Unsu-
pervised dense information retrieval with
contrastive learning. Transactions on Machine
Learning Research.
Gautier Izacard and Edouard Grave. 2021.
Leveraging passage retrieval with generative
models for open domain question answer-
ing. In Proceedings of the 16th Conference
of the European Chapter of the Association
for Computational Linguistics: Main Volume,
pages 874–880, Online. Association for Com-
putational Linguistics.https://doi.org
/10.18653/v1/2021.eacl-main.74
Gautier Izacard, Patrick Lewis, Maria Lomeli,
Lucas Hosseini, Fabio Petroni, Timo Schick,
Jane Dwivedi-Yu, Armand Joulin, Sebastian
Riedel, and Edouard Grave. 2022b. Atlas:
Few-shot learning with retrieval augmented
language models. https://doi.org/10
.48550/arXiv.2208.03299
Jeff Johnson, Matthijs Douze, and Herv ´e
J´egou. 2021. Billion-scale similarity search
with GPUs. IEEE Transactions on Big
Data, 7(3):535–547. https://doi.org
/10.1109/TBDATA.2019.2921572
Mandar Joshi, Eunsol Choi, Daniel Weld, and
Luke Zettlemoyer. 2017. TriviaQA: A large
scale distantly supervised challenge dataset
for reading comprehension. InProceedings
of the 55th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1:
Long Papers) , pages 1601–1611, Vancou-
ver, Canada. Association for Computa-
tional Linguistics. https://doi.org/10
.18653/v1/P17-1147
Vladimir Karpukhin, Barlas Oguz, Sewon Min,
Patrick Lewis, Ledell Wu, Sergey Edunov,
Danqi Chen, and Wen-tau Yih. 2020. Dense
passage retrieval for open-domain question an-
swering. InProceedings of the 2020 Conference
on Empirical Methods in Natural Language
Processing (EMNLP), pages 6769–6781, On-
line. Association for Computational Linguis-
tics. https://doi.org/10.18653/v1
/2020.emnlp-main.550
Urvashi Khandelwal, Omer Levy, Dan Jurafsky,
Luke Zettlemoyer, and Mike Lewis. 2020.
1327
Downloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00605/2178834/tacl_a_00605.pdf by guest on 29 January 2025
Generalization through memorization: Near-
est neighbor language models. InInternational
Conference on Learning Representations.
Tom Kwiatkowski, Jennimaria Palomaki, Olivia
Redfield, Michael Collins, Ankur Parikh, Chris
Alberti, Danielle Epstein, Illia Polosukhin,
Jacob Devlin, Kenton Lee, Kristina Toutanova,
Llion Jones, Matthew Kelcey, Ming-Wei
Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc
Le, and Slav Petrov. 2019. Natural questions:
A benchmark for question answering research.
Transactions of the Association for Computa-
tional Linguistics, 7:452–466.https://doi
.org/10.1162/tacl_a_00276
Yoav Levine, Itay Dalmedigos, Ori Ram,
Yoel Zeldes, Daniel Jannai, Dor Muhlgay,
Yoni Osin, Opher Lieber, Barak Lenz, Shai
Shalev-Shwartz, Amnon Shashua, Kevin
Leyton-Brown, and Yoav Shoham. 2022a.
Standing on the shoulders of giant frozen lan-
guage models. 10.48550/ARXIV.2204.10019.
https://doi.org/10.48550/arXiv.2204
.10019
Yoav Levine, Ori Ram, Daniel Jannai, Barak Lenz,
Shai Shalev-Shwartz, Amnon Shashua, Kevin
Leyton-Brown, and Yoav Shoham. 2022b.
Huge frozen language models as readers for
open-domain question answering. In ICML
2022 Workshop on Knowledge Retrieval and
Language Models.
Yoav Levine, Noam Wies, Daniel Jannai, Dan
Navon, Yedid Hoshen, and Amnon Shashua.
2022c. The inductive bias of in-context learn-
ing: Rethinking pretraining example design.
In International Conference on Learning
Representations.
Patrick Lewis, Ethan Perez, Aleksandra Piktus,
Fabio Petroni, Vladimir Karpukhin, Naman
Goyal, Heinrich K¨uttler, Mike Lewis, Wen-tau
Yih, Tim Rockt ¨aschel, Sebastian Riedel, and
Douwe Kiela. 2020. Retrieval-augmented gen-
eration for knowledge-intensive NLP tasks. In
Advances in Neural Information Processing
Systems, pages 9459–9474.
Zonglin Li, Ruiqi Guo, and Sanjiv Kumar. 2022.
Decoupled context processing for context aug-
mented language modeling. In Advances in
Neural Information Processing Systems.
Opher Lieber, Or Sharir, Barak Lenz, and Yoav
Shoham. 2021. Jurassic-1: Technical details and
evaluation.
Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin,
Jheng-Hong Yang, Ronak Pradeep, and
Rodrigo Nogueira. 2021. Pyserini: A Python
toolkit for reproducible information retrieval
research with sparse and dense representations.
In Proceedings of the 44th International ACM
SIGIR Conference on Research and Devel-
opment in Information Retrieval, SIGIR ’21,
pages 2356–2362, New York, NY, USA. Asso-
ciation for Computing Machinery. https://
doi.org/10.1145/3404835.3463238
Stephanie Lin, Jacob Hilton, and Owain Evans.
2022. TruthfulQA: Measuring how models
mimic human falsehoods. In Proceedings of
the 60th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long
Papers), pages 3214–3252, Dublin, Ireland.
Association for Computational Linguistics.
https://doi.org/10.18653/v1/2022
.acl-long.229
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei
Du, Mandar Joshi, Danqi Chen, Omer Levy,
Mike Lewis, Luke Zettlemoyer, and Veselin
Stoyanov. 2019. RoBERTa: A robustly opti-
mized bert pretraining approach. https://
doi.org/10.48550/arXiv.1907.11692
Joshua Maynez, Shashi Narayan, Bernd Bohnet,
and Ryan McDonald. 2020. On faithfulness
and factuality in abstractive summariza-
tion. In Proceedings of the 58th Annual
Meeting of the Association for Computa-
tional Linguistics, pages 1906–1919, Online.
Association for Computational Linguistics.
https://doi.org/10.18653/v1/2020
.acl-main.173
Stephen Merity, Caiming Xiong, James Bradbury,
and Richard Socher. 2016. Pointer sentinel
mixture models.https://doi.org/10.48550
/arXiv.1609.07843
Dor Muhlgay, Ori Ram, Inbal Magar, Yoav
Levine, Nir Ratner, Yonatan Belinkov, Omri
Abend, Kevin Leyton-Brown, Amnon Shashua,
and Yoav Shoham. 2023. Generating bench-
marks for factuality evaluation of language
models. https://doi.org/10.48550/arXiv
.2307.06908
1328
Downloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00605/2178834/tacl_a_00605.pdf by guest on 29 January 2025
Fabio Petroni, Aleksandra Piktus, Angela
Fan, Patrick Lewis, Majid Yazdani, Nicola
De Cao, James Thorne, Yacine Jernite,
Vladimir Karpukhin, Jean Maillard, Vassilis
Plachouras, Tim Rockt ¨aschel, and Sebastian
Riedel. 2021. KILT: A benchmark for
knowledge intensive language tasks. In Pro-
ceedings of the 2021 Conference of the
North American Chapter of the Associa-
tion for Computational Linguistics: Human
Language Technologies , pages 2523–2544,
Online. Association for Computational Lin-
guistics. https://doi.org/10.18653
/v1/2021.naacl-main.200
Alec Radford, Karthik Narasimhan, Tim Salimans,
and Ilya Sutskever. 2018. Improving language
understanding by generative pre-training.
Alec Radford, Jeff Wu, Rewon Child, David
Luan, Dario Amodei, and Ilya Sutskever. 2019.
Language models are unsupervised multitask
learners.
Ori Ram, Gal Shachaf, Omer Levy, Jonathan
Berant, and Amir Globerson. 2022. Learn-
ing to retrieve passages without supervision.
In Proceedings of the 2022 Conference of
the North American Chapter of the Associ-
ation for Computational Linguistics: Human
Language Technologies , pages 2687–2700,
Seattle, United States. Association for Compu-
tational Linguistics.https://doi.org/10
.18653/v1/2022.naacl-main.193
Nir Ratner, Yoav Levine, Yonatan Belinkov,
Ori Ram, Omri Abend, Ehud Karpas, Amnon
Shashua, Kevin Leyton-Brown, and Yoav
Shoham. 2022. Parallel context windows
improve in-context learning of large lan-
guage models. https://doi.org/10
.18653/v1/2023.acl-long.352
Stephen Robertson and Hugo Zaragoza. 2009.
The probabilistic relevance framework: BM25
and beyond. Foundations and Trends in In-
formation Retrieval, 3(4):333–389. https://
doi.org/10.1561/1500000019
Devendra Sachan, Mike Lewis, Mandar Joshi,
Armen Aghajanyan, Wen-tau Yih, Joelle
Pineau, and Luke Zettlemoyer. 2022. Improving
passage retrieval with zero-shot question gen-
eration. InProceedings of the 2022 Conference
on Empirical Methods in Natural Language
Processing, pages 3781–3797, Abu Dhabi,
United Arab Emirates. Association for Compu-
tational Linguistics.https://doi.org/10
.18653/v1/2022.emnlp-main.249
Weijia Shi, Sewon Min, Michihiro Yasunaga,
Minjoon Seo, Rich James, Mike Lewis,
Luke Zettlemoyer, and Wen tau Yih. 2023.
REPLUG: Retrieval-augmented black-box lan-
guage models. https://doi.org/10.48550
/arXiv.2301.12652
Nandan Thakur, Nils Reimers, Andreas R¨uckl´e,
Abhishek Srivastava, and Iryna Gurevych.
2021. BEIR: A heterogeneous benchmark for
zero-shot evaluation of information retrieval
models. In Proceedings of the Neural Informa-
tion Processing Systems Track on Datasets and
Benchmarks, volume 1.
Hugo Touvron, Thibaut Lavril, Gautier Izacard,
Xavier Martinet, Marie-Anne Lachaux,
Timoth´ee Lacroix, Baptiste Rozi `ere, Naman
Goyal, Eric Hambro, Faisal Azhar, Aurelien
Rodriguez, Armand Joulin, Edouard Grave,
and Guillaume Lample. 2023. LLaMA: Open
and efficient foundation language models.
https://doi.org/10.48550/arXiv.2302
.13971
Ashish Vaswani, Noam Shazeer, Niki Parmar,
Jakob Uszkoreit, Llion Jones, Aidan Gomez,
Łukasz Kaiser, and Illia Polosukhin. 2017.
Attention is all you need. In Advances in
Neural Information Processing Systems 30 ,
pages 5998–6008.
Ben Wang and Aran Komatsuzaki. 2021.
GPT-J-6B: A 6 billion parameter autoregressive
language model.
Thomas Wolf, Lysandre Debut, Victor Sanh,
Julien Chaumond, Clement Delangue, Anthony
Moi, Pierric Cistac, Tim Rault, Remi Louf,
Morgan Funtowicz, Joe Davison, Sam Shleifer,
Patrick von Platen, Clara Ma, Yacine Jernite,
Julien Plu, Canwen Xu, Teven Le Scao,
Sylvain Gugger, Mariama Drame, Quentin
Lhoest, and Alexander Rush. 2020. Trans-
formers: State-of-the-art natural language
processing. InProceedings of the 2020 Con-
ference on Empirical Methods in Natural
Language Processing: System Demonstrations,
pages 38–45, Online. Association for Compu-
tational Linguistics.https://doi.org/10
.18653/v1/2020.emnlp-demos.6
1329
Downloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00605/2178834/tacl_a_00605.pdf by guest on 29 January 2025
Rowan Zellers, Ari Holtzman, Hannah Rashkin,
Yonatan Bisk, Ali Farhadi, Franziska
Roesner, and Yejin Choi. 2019. Defending
against neural fake news. InAdvances in Neu-
ral Information Processing Systems, volume 32.
Curran Associates, Inc.
Susan Zhang, Stephen Roller, Naman Goyal,
Mikel Artetxe, Moya Chen, Shuohui Chen,
Christopher Dewan, Mona Diab, Xian Li,
Xi Victoria Lin, Todor Mihaylov, Myle Ott,
Sam Shleifer, Kurt Shuster, Daniel Simig,
Punit Singh Koura, Anjali Sridhar, Tianlu
Wang, and Luke Zettlemoyer. 2022. OPT:
Open pre-trained transformer language models.
https://doi.org/10.48550/arXiv.2205
.01068
Zexuan Zhong, Tao Lei, and Danqi Chen.
2022. Training language models with memory
augmentation. In Proceedings of the 2022
Conference on Empirical Methods in Natu-
ral Language Processing, pages 5657–5673,
Abu Dhabi, United Arab Emirates. Associa-
tion for Computational Linguistics.https://
doi.org/10.18653/v1/2022.emnlp-main
.382
A Query Length Ablations
Figure 9 and Figure 10 show ablations on the
optimal query lengthℓfor off-the-shelf dense re-
trievers (BERT and Contriever, respectively). We
omit the results of Spider as they are almost iden-
tical to those of Contriever. Consistently, using
ℓ =6 4(tokens) is optimal. This is in contrast to
similar experiments we conducted for BM25 (cf.
Figure 6), whereℓ=3 2is optimal.
Figure 9: An analysis of perplexity as a function ofthe
number of tokens in the queryfor an off-the-shelf BERT
retriever on the development set of WikiText-103.
Figure 10: An analysis of perplexity as a function of
the number of tokens in the queryfor Contriever on the
development set of WikiText-103.
Model Retrieval Wiki-103 RealNews
word ppl token ppl
GPT-Neo 1.3B – 17.5 12.3
BM25, §5 14.6 9.9
GPT-Neo 2.7B – 15.1 11.0
BM25, §5 12.8 9.0
GPT-J 6B – 11.6 9.2
BM25, §5 10.0 7.7
Table 5: The performance of models from the
GPT-Neo family, measured by word-level per-
plexity on the test set of WikiText-103 and
token-level perplexity on the development set
of RealNews.
B GPT-Neo Results
Table 5 gives the results of applying In-Context
RALM to the models from the GPT-Neo model
family on WikiText-103 and RealNews.
C Open-Domain Question Answering
Experiments: Further Details
Closed-Book Setting For the closed-book set-
ting, we adopt the prompt of Touvron et al.
(2023):
Answer these questions:
Q: Who got the first nobel
prize in physics?
A:
Open-Book Setting For the open-book setting,
we extend the above prompt as follows:
Nobel Prize
A group including 42
Swedish writers, artists,
1330
Downloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00605/2178834/tacl_a_00605.pdf by guest on 29 January 2025
and literary critics
protested against this
decision, having expected
Leo Tolstoy to be awarded.
Some, including Burton
Feldman, have criticised
this prize because they...
Nobel Prize in Physiology
or Medicine
In the last half century
there has been an
increasing tendency
for scientists to work
as teams, resulting in
controversial exclusions.
Alfred Nobel was born
on 21 October 1833 in
Stockholm, Sweden, into
a family of engineers...
Based on these texts,
answer these questions:
Q: Who got the first nobel
prize in physics?
A:
1331
Downloaded from http://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00605/2178834/tacl_a_00605.pdf by guest on 29 January 2025


A Survey on Retrieval-Augmented Text Generation
Huayang Li♥,∗ Yixuan Su♠,∗ Deng Cai♦,∗ Yan Wang♣,∗ Lemao Liu♣,∗
♥Nara Institute of Science and Technology ♠University of Cambridge
♦The Chinese University of Hong Kong ♣Tencent AI Lab
li.huayang.lh6@is.naist.jp, ys484@cam.ac.uk
thisisjcykcd@gmail.com, brandenwang@tencent.com
lemaoliu@gmail.com
Abstract
Recently, retrieval-augmented text generation
attracted increasing attention of the compu-
tational linguistics community. Compared
with conventional generation models, retrieval-
augmented text generation has remarkable ad-
vantages and particularly has achieved state-of-
the-art performance in many NLP tasks. This
paper aims to conduct a survey about retrieval-
augmented text generation. It ﬁrstly highlights
the generic paradigm of retrieval-augmented
generation, and then it reviews notable ap-
proaches according to different tasks including
dialogue response generation, machine trans-
lation, and other generation tasks. Finally, it
points out some promising directions on top of
recent methods to facilitate future research.
1 Introduction
Retrieval-augmented text generation, as a new
text generation paradigm that fuses emerging deep
learning technology and traditional retrieval tech-
nology, has achieved state-of-the-art (SOTA) per-
formance in many NLP tasks and attracted the at-
tention of the computational linguistics community
(Weston et al., 2018; Dinan et al., 2018; Cai et al.,
2021). Compared with generation-based counter-
part, this new paradigm has some remarkable ad-
vantages: 1) The knowledge is not necessary to be
implicitly stored in model parameters, but is explic-
itly acquired in a plug-and-play manner, leading
to great scalibility; 2) Instead of generating from
scratch, the paradigm generating text from some re-
trieved human-written reference, which potentially
alleviates the difﬁculty of text generation.
This paper aims to review many representative
approaches for retrieval-augmented text generation
tasks including dialogue response generation (We-
ston et al., 2018), machine translation (Gu et al.,
2018) and others (Hashimoto et al., 2018). We
∗All authors contributed equally.
ﬁrstly present the generic paradigm of retrieval-
augmented generation as well as three key com-
ponents under this paradigm, which are retrieval
sources, retrieval metrics and generation models.
Then, we introduce notable methods about
retrieval-augmented generation, which are orga-
nized with respect to different tasks. Speciﬁcally,
on the dialogue response generation task, exem-
plar/template retrieval as an intermediate step has
been shown beneﬁcial to informative response gen-
eration (Weston et al., 2018; Wu et al., 2019; Cai
et al., 2019a,b). In addition, there has been growing
interest in knowledge-grounded generation explor-
ing different forms of knowledge such as knowl-
edge bases and external documents (Dinan et al.,
2018; Zhou et al., 2018; Lian et al., 2019; Li et al.,
2019; Qin et al., 2019; Wu et al., 2021; Zhang et al.,
2021). On the machine translation task, we summa-
rize the early work on how the retrieved sentences
(called translation memory) are used to improve
statistical machine translation (SMT) (Koehn et al.,
2003) models (Simard and Isabelle, 2009; Koehn
and Senellart, 2010) and in particular, we inten-
sively highlight several popular methods to inte-
grating translation memory to NMT models (Gu
et al., 2018; Zhang et al., 2018; Xu et al., 2020;
He et al., 2021). We also review the applications
of retrieval-augmented generation in other genera-
tion tasks such as abstractive summarization (Peng
et al., 2019), code generation (Hashimoto et al.,
2018), paraphrase (Kazemnejad et al., 2020; Su
et al., 2021b), and knowledge-intensive generation
(Lewis et al., 2020b). Finally, we also point out
some promising directions on retrieval-augmented
generation to push forward the future research.
2 Retrieval-Augmented Paradigm
In this section, we ﬁrst give a general formulation
of retrieval-augmented text generation. Then, we
discuss three major components of the retrieval-
augmented generation paradigm, including the re-
arXiv:2202.01110v2  [cs.CL]  13 Feb 2022Input
Sources (Sec. 2.2):Training CorpusExternal DataUnsupervised DataMetrics(Sec. 2.3):Sparse-vector RetrievalDense-vector RetrievalTask-specific Retrieval
Retrieval MemoryGeneration Model
Sec. 4: Machine TranslationSec. 5: Other TasksData AugmentationAttention MechanismSkeleton & Templates
Information RetrievalTasks:Sec. 3: Dialogue GenerationModels (Sec 2.4):
Output
Figure 1: The overview of this survey.
trieval source, retrieval metric and integration meth-
ods.
2.1 Formulation
Most text generation tasks can be formulated as a
mapping from input sequence x to output sequence
y : y = f(x). For instance, x and y could be the
dialogue history and the corresponding response
for dialogue response generation, the text in the
source language and the translation in the target
language for machine translation, and so on.
Recently, some researchers suggest to endow
models the capability to access external memory
via some information retrieval techniques, so that
they can acquire more information in the generation
process (Gu et al., 2018; Weston et al., 2018; Cai
et al., 2019b). The retrieval-augmented generation
can be further formulated as:
y = f(x, z) (1)
where z = {⟨xr, yr⟩}is a set of relevant instances
retrieved from the original training set or external
datasets. The main idea of this paradigm is that yr
may beneﬁt the response generation, if xr (or yr)
is similar (or relevant) to the input x. It is worth
noting that xr = ∅when unsupervised retrieval
sources are used. In general, the retrieval mem-
ory can be retrieved from three kinds of sources:
the training corpus, external datasets in the same
format with the training corpus, and large-scale
unsupervised corpus (§2.2). Metrics that evaluate
the relevance between text are varied as well, in
§2.3 we divided them into three categories: sparse-
vector retrieval, dense-vector retrieval, and training-
based retrieval. Finally, how to integrate the re-
trieval memory to the generation model is also sig-
niﬁcant, we also introduce some popular integra-
tion approaches in §2.4.
2.2 Retrieval Sources
Training Corpus Most previous studies search
the external memory from itstraining corpus (Song
et al., 2016; Gu et al., 2018; Weston et al., 2018).
In the inference time, retrieved examples with high
relevant scores could be regarded as extra refer-
ences and reduce model’s uncertainty in generation.
The main motivation of those works is to to store
knowledge not only in the model parameters but
also in an explicit and accessible form, making the
model be able to re-access it during inference.
External Data Some researchers also propose to
retrieval relevant samples from external datasets
(Su et al., 2021c; Xiao et al., 2021). In these stud-
ies, the retrieval pool is different with the training
corpus, which can further provide additional infor-
mation that are not contained in the training corpus.
This is especially beneﬁcial for applications such
as domain adaptation and knowledge update. For
example, Khandelwal et al. (2020a); Zheng et al.
(2021a) employ the in-domain dataset as the exter-
nal memory to achieve fast domain adaptation for
machine translation.
Unsupervised Data One limitation for previous
two sources is that the datasets have to be super-
vised datasets consisting of aligned input-output
pairs. For machine translation, Cai et al. (2021) pro-
pose a cross-lingual retriever to directly retrieve tar-
get sentence from unsupervised corpus (i.e., mono-
lingual corpus in the target language). The main
idea is aligning source-side sentences and the corre-
sponding target-side translations in a dense vector
space, i.e., aligning x and yr when xr is absent.
As a result, the retriever directly connects the dots
between the source-side input and target-side trans-
lations, enabling monolingual data in the targetlanguage to be used alone as memories.
2.3 Retrieval Metrics
Sparse-vector Retrieval Given an input se-
quence x and a retrieval corpus, retrieval model
aims to retrieve a set of relevant examples z =
{⟨xr, yr⟩}from the corpus. When a supervised
corpus is used, {⟨xr, yr⟩}is retrieved by measur-
ing the similarity between x and xr. For simi-
larity measurement, sparse-vector retrieval meth-
ods such as TF-IDF and BM25 (Robertson and
Zaragoza, 2009) are widely used. They match key-
words efﬁciently with an inverted index.
Dense-vector Retrieval However, these meth-
ods prefer examples with similar surfaces, and may
fail to retrieve examples that are only semantically
relevant. To alleviate above problem, some stud-
ies (Cao and Xiong, 2018) attempt to retrieve in
dense-vector space instead of the lexical overlap.
Recent work (Lee et al., 2019) makes use of pre-
trained language models, which encodes the text to
low-dimensional dense vectors via BERT-based en-
coders. The retrieval score are computed via inner
products between vectors.
Task-speciﬁc Retrieval Similarity-based re-
trieval is based on a simple heuristic. That is, the
more xr resembles with x, the more likely xr
and yr will help the generation. However, the
most similar one by universal textual similarity
does not necessarily serve the best for downstream
models. Ideally, the retrieval metric would be
learned from the data in a task-dependent way: we
wish to consider a memory only if it can indeed
boost the quality of ﬁnal generation. To this end,
Cai et al. (2021) propose to unify the memory
retriever and its downstream generation model
into a learnable whole. Such memory retrieval is
end-to-end optimized for task-speciﬁc objectives.
2.4 Integration
Data Augmentation There are several ways to
integrate the retrieved external memory in gener-
ation. One straightforward way is data augmen-
tation, which constructs some augmented inputs
by concatenating spans from {⟨xr, yr⟩}with the
original input x. By training on the augmented
inputs, a generation model implicitly leans how
to integrate the retrieved information. Despite the
simplicity, this kind of methods works efﬁciently
in lots of tasks (Song et al., 2016; Weston et al.,
2018; Bulte and Tezcan, 2019).
Attention Mechanisms Another integration
method is based on attention mechanisms
(Bahdanau et al., 2014). The main idea of this
fashion is adopting additional encoders (in various
architectures) to encode retrieved target sentences,
and integrate them through attention (Cao and
Xiong, 2018; Gu et al., 2018; Bapna and Firat,
2019). Since the attention mechanism is becoming
(Bahdanau et al., 2014; Vaswani et al., 2017) a
key module in lots of NLP models, integrating
retrieved memory through attention becomes a
very nature and efﬁcient way.
Skeleton Extraction In the previous two meth-
ods, the downstream generation model learns how
to ﬁlter out irrelevant or even harmful informa-
tion from the retrieved examples implicitly. There
also exist some works that try to explicitly extract
useful information, i.e., skeleton extraction, from
the retrieved memory (Cai et al., 2019a; Wu et al.,
2019; Cai et al., 2019b). For example, one skeleton
should be a part of a whole utterance with irrelevant
content masked, and the generation model only in-
tegrate this skeleton in the generation process.
3 Dialogue Response Generation
Background Dialogue systems can be grouped
into two categories: chit-chat systems and task-
oriented systems. While task-oriented dialogue
systems are designed to accomplish speciﬁc user
tasks such as air tickets booking, chit-chat dialogue
systems aim at giving a meaningful and ﬂuent re-
sponse for any dialogue history in the open domain.
Dialogue response generation in chit-chat dialogue
system is challenging partly due to the diversity
of possible responses to a single dialogue history
(i.e., the one-to-many problem). The dialogue his-
tory alone cannot decide a meaningful and speciﬁc
response. Also, external knowledge that is not
present in the dialogue history are often necessary
for avoiding safe but boring responses. We focus
on recent efforts tackling the challenges to develop
chit-chat dialogue systems.
Most modern chit-chat dialogue systems can
be categorized into two classes, namely, retrieval-
based models and generation-based models. The
retrieval-based models (Ji et al., 2014; Hu et al.,
2014) directly copy an existing response from cu-
rated dialogue corpora (i.e., the retrieval pool)
when receiving a response request. The retrieved
responses are often informative and grammatical
as they are collected from real-world conversa-tions and possibly post-edited by a human. How-
ever, such systems perform poorly when a given
dialogue history is substantially different from
those in the retrieval pool. On the other hand,
the generation-based models (Shang et al., 2015;
Vinyals and Le, 2015; Li et al., 2016a) generate
a new utterance from scratch. Those generation-
based models have better generalization capacity
when handling unseen dialogue contexts. Never-
theless, the generated utterances are inclined to be
dull and non-informative (e.g., “I don’t know”, “I
think so”, “Me too” etc.) (Li et al., 2016a).
Shallow Integration As discussed, retrieval-
based models may give informative but inappro-
priate responses while generation-based models
often do the opposite. It is desirable to combine the
best of both worlds. Early work (Qiu et al., 2017)
attempts to re-rank the output from both models.
For a deep integration, Song et al. (2016) and Yang
et al. (2019) extend the standardSEQ2SEQ encoder-
decoder model (Bahdanau et al., 2014) with an ex-
tra encoder for encoding the retrieval result. The
output of the extra encoder, along with the output
from the original encoder for dialogue history, is
used to feed the decoder. Weston et al. (2018) use
a single encoder that takes the concatenation of
the original dialogue history and the retrieved as
input. Wu et al. (2019) note that the retrieved infor-
mation should be used in awareness of the context
difference, and further proposed to construct an
edit vector by explicitly encoding the lexical differ-
ences between the input dialogue history and the
retrieved dialogue history. Pandey et al. (2018) fur-
ther propose to weight different training instances
by context similarity.
Deep Integration To prevent the inﬂow of er-
roneous information, Cai et al. (2019a) propose
a general framework that ﬁrst extracts a skeleton
from the retrieved response and then generates the
response based on the extracted skeleton. This
framework is also adopted for stylistic response
generation (Su et al., 2021c). Gupta et al. (2021)
suggest to use the semantic structure of an exem-
plar response, instead of the tokens of the exem-
plar response, to guide generation. Despite their
differences, a common issue is that the genera-
tion model easily learns to ignore the retrieved re-
sponse entirely and collapses to a vanilla seq2seq
model. This happens with improper training in-
stances. Due to the one-to-many nature, it hap-
pens frequently that a retrieved response (extracted
skeleton) is suitable for responding to the query,
but inconsistent with the current target response.
Earlier studies (Weston et al., 2018; Wu et al.,
2019; Cai et al., 2019a) alleviate the above prob-
lems by putting hard constraints on the data (e.g.,
discarding data with low similarity of the retrieved
response and the target response), which, however,
greatly reduces the amount of usable data. Cai
et al. (2019b) employ a random mechanism for
generating the skeletons used for training, which
extract skeletons from the corresponding responses
with some deliberate disturbance. Paranjape et al.
(2021) propose to model the retriever after the pos-
terior distribution of retrieval given the input and
the target output and train it jointly with the stan-
dard retriever and the generator by maximizing the
evidence lower bound (ELBo) in expectation over
retrieval.
Knowledge-Enhanced Generation The afore-
mentioned work demonstrates that retrieval-based
dialogue systems can be used for building bet-
ter generation-based models. In general, this is
done by conditioning the generation on some re-
trieved responses. More traditionally, to infuse
the response with external knowledge, the retrieval
pool is not necessarily a dialogue corpus. In fact,
knowledge-grounded dialogue response generation
exploring different forms of knowledge such as
knowledge bases and external documents (Dinan
et al., 2018; Zhou et al., 2018; Lian et al., 2019;
Li et al., 2019; Qin et al., 2019; Wu et al., 2021;
Zhang et al., 2021; Komeili et al., 2021) has been
actively explored.
Limitations We note that there are three major
limitations in existing work for dialogue response
generation. First, current methods only use one
retrieved response for generation. It can be more
beneﬁcial to combine multiple retrieval responses.
However, this can be difﬁcult due to the one-to-
many nature of dialogue response generation. Sec-
ond, current methods use universal relevance score
for retrieval. It can be more effective if we can
use more customized retrieval metric especially
for controlled dialogue response generation (e.g.,
persona, emotion, etc). Third, the retrieval pool
of existing methods is limited to dialogue corpora
(context-response pairs) or documents. It might
be useful to enlarge the retrieval pool by including
more corpora in other domains or in other modali-ties. As discussed, there leaves plenty of possible
directions to explore in the future.
4 Machine Translation
Retrieval augmented translation originates from hu-
man translation scenarios (Somers, 2003). When
translating ˆy from an input source sentencex, a hu-
man translator typically involves a search engine to
retrieve similar sentences {⟨xr, yr⟩}from a bilin-
gual database. Such a technique called translation
memory is helpful to improve the translation qual-
ity and efﬁciency for human translators (Dillon
and Fraser, 2006). As the development of ma-
chine translation techniques, there is a surge of
interests in improving machine translation models
with translation memory. In the rest of this section,
we will review translation memory for both statisti-
cal machine translation (SMT) and neural machine
translation (NMT).
4.1 Translation Memory in SMT
Generally, SMT includes three key components in
a pipeline manner such as phrase table extraction,
parameter tuning and decoding (Koehn et al., 2003;
Chiang, 2007). As a result, many efforts have been
made to make use of translation memory (TM) on
top of each component.
Constrained Decoding with TM Constrained
decoding is the most straightforward way to in-
tegrating TM into SMT (Smith and Clark, 2009;
Koehn and Senellart, 2010; Zhechev and Van Gen-
abith, 2010; Ma et al., 2011). Its basic idea is
to reuse the useful segments in yr while trans-
late other segments by SMT. Speciﬁcally, the ap-
proach consists of three steps: 1) identify the un-
matched segments in both xr and x through the
edit-distance algorithm; 2) identify the unmatched
segments in yr, each of which is aligned to one
unmatched segment in xr by a word alignment
algorithm; 3) decode each unmatched segment in
x by SMT and then use the result to replace its
corresponding unmatched segment in yr. Li et al.
(2016b) further extend this approach from sentence
level to phrase level. The advantage in constrained
decoding is that it does not require to change the
translation model (including phrase table and pa-
rameters) and can be applied in a plug-and-play
way. This approach is successful when x is highly
similar to xr; otherwise its performance is de-
graded largely, because it explicitly isolates TM
matching and SMT decoding and reuses the results
in xr or not in a deterministic way.
Phrase Table Aggregation with TM There are
also notable efforts to augment the phrase table
for SMT by extracting translation rules from the
retrieved bilingual sentences {⟨xr, yr⟩}. Then
they re-tune the parameters for the SMT model
which makes use of translation knowledge from
{⟨xr, yr⟩}in a implicit way when translating x.
For example, Biçici and Dymetman (2008); Simard
and Isabelle (2009) directly combine the extracted
translation rules into the phrase table in a shallow
combination way. They introduce an additional fea-
ture to indicate that whether translation rule is from
{⟨xr, yr⟩}or not and then train all feature weights
with MERT (Och, 2003). One characteristic of
these work is that a translation rule extracted from
{⟨xr, yr⟩}which can not exactly match any seg-
ments in x is useless even if it may contain some
useful words in its target side. To remedy this ob-
servation, Wang et al. (2013, 2014) resort to a deep
combination way to using the extracted translation
rules. For each rule in the phrase table, it designs
a generative model to reward the rules which are
similar to those extracted from {⟨xr, yr⟩}. Then
this generative model is used as a feature in the log-
linear based SMT model whose weight is tuned
together with other features by MERT. In addition,
Li et al. (2014) employ a similar way to reward
the rules but it relies on a discriminative model
which is easy to integrate potential features from
{⟨xr, yr⟩}.
Parameter Tuning with TM Unlike the above
two research lines, Liu et al. (2012, 2014) make use
of translation memory only in tuning parameters.
To be speciﬁc, when translating an input sentence
x, they ﬁrstly retrieve many similar bilingual sen-
tences {⟨xr, yr⟩}, and then tune the parameters on
top of the retrieved sentences as well as a given de-
velopment dataset in a sentence-wise manner, i.e.,
it performs an independent tuning for each input
sentence. To improve the efﬁciency of each tuning
step, it propose a local update on top of {⟨xr, yr⟩}
from a baseline model.
Despite the successes of translation memory in
SMT, there are still some limitations for the above
three kinds of methods. Firstly, all these methods
employ fuzzy score for retrieval which is highly de-
pendent on word matching and thus can not recall
such examples which are similar in word seman-tics but different in surface form. Secondly, these
methods integrate the retrieved examples into a
module of SMT in the ways which can not make
full use of the knowledge in retrieved examples.
For example, the integration ways in the ﬁrst two
kinds (constrained decoding and phrase table ag-
gregation) are heuristic and not optimized towards
translation quality; the parameter tuning method
ﬁne-tunes few parameters for log-linear based SMT
which are not enough to preserve sufﬁcient knowl-
edge from retrieved examples. Thirdly, since SMT
performs in a pipeline manner, it is intractable to
jointly optimize retrieval metrics as well as SMT
models. Consequently, all these methods adopt an
off-the-shelf metric for retrieval, leading to sub-
optimal performance.
4.2 Translation Memory in NMT
Translation memory has been widely explored in
Neural Machine Translation (NMT). Depending
on when retrieval is involved, we can categorize
previous works into two classes: 1) an NMT model
leans how to cooperate with the retrieval model in
the training phase; 2) an NMT model is only aware
of the retrieved data in the inference phase.
Inference Phase The key point of literature in
this line is to reward some target words based on
words in yr in the inference process. Thus, a de-
cision can be made based on both the distribution
of generation model and the additional reward of
retrieval model. Some previous works propose to
reward target words based on the sentence-level
similarity between x and xr, and the word align-
ment between xr and yr. Given the input sentence
x, Zhang et al. (2018) try to assign target words
in ˆy with higher rewards, when they appear in yr
and the aligned source words are in both xr and
x. He et al. (2019) follow a similar framework
and consider the position information of those tar-
get words when rewarding. Those works reward
the target words in an explicit way, however, the
one-sentence-one-model approach (Li et al., 2016c;
Turchi et al., 2017) propose to reward target word
implicitly. For each testing input x, their approach
will ﬁrst ﬁnetune the translation model on retrieved
memory {⟨xr, yr⟩}and then translate x.
Others try to reward target words based on token-
level similarity score. Most works in this line are
based on the dense retriever (Khandelwal et al.,
2020a), e.g., faiss. Khandelwal et al. (2020a) build
a key-value datastore, where key h(xr, yr
<t) is the
hidden state at each time step when translating yr
from xr, and value is its golden-truth target word
yr
t . Therefore, in the inference time, they can use
the h(x, ˆy<t) as query and reward target words
with similar hidden representations in the datas-
tore. Although this method achieves signiﬁcant
performance gain, one drawback of it is the high la-
tency. To address this issue, Meng et al. (2021) use
some heuristics, e.g., pre-ﬁltering, to avoid search-
ing on the entire datastore. The reward score of
previous works is got from some non-parametric
approaches, however, Zheng et al. (2021a) propose
a light-weight network to learn the reward score.
Since dense retrieval has the potential of cross-
lingual retrieval, Zheng et al. (2021b) use a similar
approach to achieve unsupervised domain adapta-
tion, where a main change is to create the datastore
based on synthetic sources sentence and the real
target sentences.
Training Phase Different from those model-
agnostic approaches, previous works in this line
aim to train the generation model to learn how
to cooperate with the retrieval model. It is also
worth noting that most works in this line adopt
the sentence-level retrieval, when integrating the
retrieval information in the training process. To
achieve its goal, Bulte and Tezcan (2019) and
Hossain et al. (2020) propose a data augmenta-
tion method to integrate the retrieved information,
where x is concatenated with yr before feeding
into the model . Following the data augmentation
approach, Xu et al. (2020) propose more matching
methods to determine including which retrieved
example in the source is better.
There also exist some works that propose new
architectures to integrate the retrieval information.
Under the RNN-based framework, Cao and Xiong
(2018) and Gu et al. (2018) use the gating and at-
tention mechanism to incorporate the retrieved tar-
get sentences. When Transformer (Vaswani et al.,
2017) becomes the backbone of NMT, some works
also use additional transformer encoders to en-
code retrieved target sentences, and integrate them
through attention mechanism (Bapna and Firat,
2019; Cao et al., 2019). Xia et al. (2019) repre-
sent the retrieved target sentences in a different
data structure, i.e., a graph structure, and integrate
it through attention mechanism. He et al. (2021)
propose a light-weight method to encode the re-
trieved target sentences and leverage the alignment
information to ﬁlter out irrelevant information. Dif-ferent from previous works that rely on bilingual
memories, Cai et al. (2021) propose a framework
that can retrieve the most similar target sentence in
a monolingual dataset, using a source sentence as
query.
Limitations In the section of SMT, we have
showed some limitations of the retrieval augmented
approaches. There also exist some limitations in
the line of NMT. First, the information used for
deriving reward scores is limited. The similarity
between an input and retrieved examples is the
primary feature to derive reward scores. How-
ever, some information, e.g., frequencies of words
and context, may also be beneﬁcial for integrating
the translation memory. Second, it remains to be
an open question that when should we use the re-
trieved information and when not. In the inference
phase, approaches tend to integrate the translation
memory excessively, e.g., at each time step, which
not only reduces the translation efﬁciency but may
also dampen the ﬂuency of generated results.
5 Other Tasks
In addition to dialogue system and machine trans-
lation, retrieval-augmented generation techniques
have shown to be beneﬁcial in many other tasks. In
the following, we highlight several key tasks that
apply retrieval-augmented generation approaches.1
Language Modelling It has been shown that
properly leveraging information from retrieval
memory could improve the performance of large
pre-trained language model. To build a more accu-
rate language model, Khandelwal et al. (2020b) pro-
pose to incorporate a soft memory module into the
system. Speciﬁcally, an index is built by caching
the hidden states of the training corpus. Then, the
language model accesses the index via k-NN search
and displays a greatly improved performance. As
another example, Guu et al. (2020) propose a new
paradigm that applies retrieval-augmented tech-
nique into the pre-training of generative language
model. During learning, they train a neural se-
lector that dynamically samples a relevant text to
guide the reconstruction of a corrupted input se-
quence. In this way, the pre-trained model deliv-
ers better results by explicitly grounding on the
retrieval memory. Lewis et al. (2020a) combine
language model pre-training with a paraphrasing
1Here, we focus on tasks other than question answering.
We refer readers interested in QA to Chen and Yih (2020).
approach. During learning, an input sequence to
the model is ﬁrst corrupted. In the meantime, a set
of multi-lingual texts are retrieved based on which
the model learns to reconstruct the original input
sequence. Recently, Borgeaud et al. (2021) pro-
pose RETRO , a large pre-trained language model
enhanced with retrieved documents, and obtained
comparable performances with GPT-3 using 25×
fewer parameters.
Summarization Text summarization is another
research area that beneﬁts from retrieval-
augmented text generation. Peng et al. (2019)
propose an adaptive decoding framework which
ﬁrst retrieves an exemplar document given the
source document. Then, the summarization of the
source document is derived through an adaptive
generation process based on the retrieved template.
Different from Peng et al. (2019), Cao et al.
(2018) and Hossain et al. (2020) introduce an
intermediate re-ranking stage into the generation
pipeline. Speciﬁcally, before generating the
document summary, the retrieval documents are
ﬁrst re-ranked based on their similarity scores
with respect to the source document. Then, the
document summarization is produced by re-writing
the selected templates.
Paraphrase Generation To address the lack of
quality as well as diversity in the generation of para-
phrases, Kazemnejad et al. (2020) propose a gen-
eration framework which ﬁrst retrieves a sentence
that is similar to input sentence. Then, based on
the retrieved sentence, a neural editor produces the
resulting paraphrased sentence. Chen et al. (2019)
investigate a different aspect of paraphrasing, i.e.
how to control the linguistic syntax displayed in
the generated text. To achieve this goal, Chen et al.
(2019) propose to ﬁrst extract a sentential exem-
plar that serves as the syntax template. A neural
model then generates the paraphrase with desired
linguistic syntax following the retrieved exemplar.
Text Style Transfer To improve the quality of
generated text, Li et al. (2018) propose a retrieval-
augmented framework which ﬁrst retrieves texts
that are similar to the input based on lexical-level
similarity. Then, the retrieved tokens that are irrel-
evant to the source are deleted, and the output is
derived from the edited template. Xiao et al. (2021)
also adopte this framework by incorporating re-
trieval information from two sources (i.e. sparse
and dense memories) and obtained an improvedmodel performance.
Data-to-Text Generation Recently, retrieval-
augmented generation has been adapted to the task
of data-to-text generation. To bridge the gap be-
tween the structured data and natural language
text, Su et al. (2021a) propose a novel retrieval-
augmented framework. Speciﬁcally, given the
source data, a set of candidate texts are ﬁrst re-
trieved from a large unlabelled corpus. Then, a
neural selector is applied to measure the similari-
ties between the source data and candidate texts,
and extract a set of more ﬁne-grained prototypes
from the candidates. Lastly, a generation model
takes the prototypes as input to produce the text
that describes the given structured data.
While retrieval-augmented generation has been
widely explored in the NLP community, we sug-
gest that future research could extend this approach
to tasks that involve data from multiple modali-
ties. For instance, with recent advancements in
image-text retrieval (Jia et al., 2021; Radford et al.,
2021), the structural gap between images and texts
is largely bridged. Some early studies (Zhang et al.,
2020) have shown that information retrieved from
images could improve the performance of neural
machine translation model. Naturally, such meth-
ods could be extended to other multi-modal tasks,
such as image captioning (Karpathy and Li, 2015).
A similar idea could also be applied to tasks be-
yond images, such as speech-to-text transcription
(Gales and Young, 2007).
6 Future Directions
Despite the current success of retrieval augmented
text generation, there is still a long way to go as
discussed in previous sections. We highlight some
directions to facilitate the future research as fol-
lows:
Retrieval Sensitivity The performance of re-
trieval augmented text generation is very sensitive
to the retrieval quality, i.e., the similarity between
the query and the retrieved examples. Currently, re-
trieval augmented text generation models perform
well when the retrieved examples are very simi-
lar to the query. However, they are even worse
than the generation models without retrieval when
the retrieval examples are less similar. Therefore,
it would be important to exploit new methods to
address such an issue on similarity.
Retrieval Efﬁciency Generally, if one enlarges
the retrieval memory to some extent, it would be
possible to retrieve an example which is very simi-
lar to the query.Unfortunately, the downside is that
the overall inference for the retrieval augmented
generation models is less efﬁcient due the consid-
erable retrieval overhead. In this sense, it is urgent
to consider some methods to trade off the retrieval
memory size and retrieval efﬁciency, for example,
data compression for the retrieval memory.
Local vs. Global Optimization Theoretically, it
seems promising to jointly learn retrieval metrics
and generation models. However, in practice, there
is an essential gap about the retrieval metric be-
tween the training and inference phrases. In the
training phase, the loss is locally back-propagated
to only a few retrieved examples while in the infer-
ence phase the metric is globally conducted among
all examples in the memory. It would be interesting
to narrow such a gap when learning a better metric
for generation tasks.
Multi-Modalities With recent advancement in
image-text retrieval, directly associating images
with relevant text becomes possible. This urges
researchers to investigate the possibility of retrieval-
based text generation in tasks that involve data from
different modalities. One typical task is image
captioning. Beyond images, other tasks like speech-
to-text transcription could potentially beneﬁt from
retrieval-based generation methods as well.
Diverse & Controllable Retrieval Most of the
existing approaches adopt a universal metric for
retrieval, such as lexical similarities of sentences.
Future work should explore how to use customized
metrics for retrieval. This can be beneﬁcial for
more controlled text generation. For example, in-
stances with emotions and styles may be more de-
sirable in the personalized dialogue generation, par-
allel data that contains speciﬁc terminologies is
more helpful in machine translation, and so on. On
the other hand, using a universal metric for retrieval
may lead to the lack of diversity of the retrieval re-
sults. Collecting a diverse set of retrieval results
can improve the coverage of useful information.
Thus, considering multiple different metrics for re-
trieval may lead to generation with higher quality
in the future.7 Conclusion
In this paper, we surveyed recent approaches for
retrieval-augmented text generation. We reviewed
and summarized the development of different com-
ponents of retrieval-augmented text generation in-
cluding retrieval metrics, retrieval sources, and in-
tegration paradigms. We gave in-depth discussions
when retrieval-augmented text generation comes to
different applications including dialogue response
generation, machine translation, and other genera-
tion tasks. We also pointed out some future direc-
tions for retrieval-augmented text generation.
References
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2014. Neural machine translation by jointly
learning to align and translate. arXiv preprint
arXiv:1409.0473.
Ankur Bapna and Orhan Firat. 2019. Non-parametric
adaptation for neural machine translation. In Pro-
ceedings of the 2019 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, Vol-
ume 1 (Long and Short Papers), pages 1921–1931.
Ergun Biçici and Marc Dymetman. 2008. Dynamic
translation memory: Using statistical machine trans-
lation to improve translation memory fuzzy matches.
In International Conference on Intelligent Text Pro-
cessing and Computational Linguistics , pages 454–
465. Springer.
Sebastian Borgeaud, Arthur Mensch, Jordan Hoff-
mann, Trevor Cai, Eliza Rutherford, Katie Millican,
George van den Driessche, Jean-Baptiste Lespiau,
Bogdan Damoc, Aidan Clark, Diego de Las Casas,
Aurelia Guy, Jacob Menick, Roman Ring, Tom Hen-
nigan, Saffron Huang, Loren Maggiore, Chris Jones,
Albin Cassirer, Andy Brock, Michela Paganini, Ge-
offrey Irving, Oriol Vinyals, Simon Osindero, Karen
Simonyan, Jack W. Rae, Erich Elsen, and Laurent
Sifre. 2021. Improving language models by retriev-
ing from trillions of tokens. CoRR, abs/2112.04426.
Bram Bulte and Arda Tezcan. 2019. Neural fuzzy re-
pair: Integrating fuzzy matches into neural machine
translation. In Proceedings of the 57th Annual Meet-
ing of the Association for Computational Linguistics,
pages 1800–1809.
Deng Cai, Yan Wang, Wei Bi, Zhaopeng Tu, Xi-
aojiang Liu, Wai Lam, and Shuming Shi. 2019a.
Skeleton-to-response: Dialogue generation guided
by retrieval memory. In Proceedings of the 2019
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long and Short
Papers), pages 1219–1228.
Deng Cai, Yan Wang, Wei Bi, Zhaopeng Tu, Xiao-
jiang Liu, and Shuming Shi. 2019b. Retrieval-
guided dialogue response generation via a matching-
to-generation framework. In Proceedings of the
2019 Conference on Empirical Methods in Natu-
ral Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP), pages 1866–1875.
Deng Cai, Yan Wang, Huayang Li, Wai Lam, and
Lemao Liu. 2021. Neural machine translation with
monolingual translation memory. In Proceedings of
the 59th Annual Meeting of the Association for Com-
putational Linguistics and the 11th International
Joint Conference on Natural Language Processing
(Volume 1: Long Papers), pages 7307–7318, Online.
Association for Computational Linguistics.
Qian Cao, Shaohui Kuang, and Deyi Xiong. 2019.
Learning to reuse translations: Guiding neural ma-
chine translation with examples. arXiv preprint
arXiv:1911.10732.
Qian Cao and Deyi Xiong. 2018. Encoding gated
translation memory into neural machine translation.
In Proceedings of the 2018 Conference on Empiri-
cal Methods in Natural Language Processing, pages
3042–3047.
Ziqiang Cao, Wenjie Li, Sujian Li, and Furu Wei.
2018. Retrieve, rerank and rewrite: Soft template
based neural summarization. In Proceedings of the
56th Annual Meeting of the Association for Com-
putational Linguistics, ACL 2018, Melbourne, Aus-
tralia, July 15-20, 2018, Volume 1: Long Papers ,
pages 152–161. Association for Computational Lin-
guistics.
Danqi Chen and Wen-tau Yih. 2020. Open-domain
question answering. In Proceedings of the 58th An-
nual Meeting of the Association for Computational
Linguistics: Tutorial Abstracts , pages 34–37, On-
line. Association for Computational Linguistics.
Mingda Chen, Qingming Tang, Sam Wiseman, and
Kevin Gimpel. 2019. Controllable paraphrase gen-
eration with a syntactic exemplar. In Proceedings of
the 57th Conference of the Association for Compu-
tational Linguistics, ACL 2019, Florence, Italy, July
28- August 2, 2019, Volume 1: Long Papers , pages
5972–5984. Association for Computational Linguis-
tics.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. computational linguistics, 33(2):201–228.
Sarah Dillon and Janet Fraser. 2006. Translators and
tm: An investigation of translators’ perceptions of
translation memory adoption. Machine Translation,
20(2):67–79.
Emily Dinan, Stephen Roller, Kurt Shuster, Angela
Fan, Michael Auli, and Jason Weston. 2018. Wizard
of wikipedia: Knowledge-powered conversational
agents. arXiv preprint arXiv:1811.01241.Mark J. F. Gales and Steve J. Young. 2007. The applica-
tion of hidden markov models in speech recognition.
Found. Trends Signal Process., 1(3):195–304.
Jiatao Gu, Yong Wang, Kyunghyun Cho, and Vic-
tor OK Li. 2018. Search engine guided neural ma-
chine translation. In Proceedings of the AAAI Con-
ference on Artiﬁcial Intelligence, volume 32.
Prakhar Gupta, Jeffrey Bigham, Yulia Tsvetkov, and
Amy Pavel. 2021. Controlling dialogue generation
with semantic exemplars. In Proceedings of the
2021 Conference of the North American Chapter of
the Association for Computational Linguistics: Hu-
man Language Technologies, pages 3018–3029, On-
line. Association for Computational Linguistics.
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-
pat, and Ming-Wei Chang. 2020. REALM: retrieval-
augmented language model pre-training. CoRR,
abs/2002.08909.
Tatsunori B Hashimoto, Kelvin Guu, Yonatan Oren,
and Percy S Liang. 2018. A retrieve-and-edit frame-
work for predicting structured outputs. In Advances
in Neural Information Processing Systems , pages
10052–10062.
Qiuxiang He, Guoping Huang, Qu Cui, Li Li, and
Lemao Liu. 2021. Fast and accurate neural machine
translation with translation memory. In Proceed-
ings of the 59th Annual Meeting of the Association
for Computational Linguistics and the 11th Interna-
tional Joint Conference on Natural Language Pro-
cessing (Volume 1: Long Papers), pages 3170–3180.
Qiuxiang He, Guoping Huang, Lemao Liu, and Li Li.
2019. Word position aware translation memory for
neural machine translation. In CCF International
Conference on Natural Language Processing and
Chinese Computing, pages 367–379. Springer.
Nabil Hossain, Marjan Ghazvininejad, and Luke Zettle-
moyer. 2020. Simple and effective retrieve-edit-
rerank text generation. In Proceedings of the 58th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 2532–2538.
Baotian Hu, Zhengdong Lu, Hang Li, and Qingcai
Chen. 2014. Convolutional neural network architec-
tures for matching natural language sentences. In
NIPS, pages 2042–2050.
Zongcheng Ji, Zhengdong Lu, and Hang Li. 2014. An
information retrieval approach to short text conver-
sation. arXiv preprint arXiv:1408.6988.
Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana
Parekh, Hieu Pham, Quoc V . Le, Yun-Hsuan Sung,
Zhen Li, and Tom Duerig. 2021. Scaling up visual
and vision-language representation learning with
noisy text supervision. In Proceedings of the 38th In-
ternational Conference on Machine Learning, ICML
2021, 18-24 July 2021, Virtual Event, volume 139 of
Proceedings of Machine Learning Research , pages
4904–4916. PMLR.
Andrej Karpathy and Fei-Fei Li. 2015. Deep visual-
semantic alignments for generating image descrip-
tions. In IEEE Conference on Computer Vision and
Pattern Recognition, CVPR 2015, Boston, MA, USA,
June 7-12, 2015, pages 3128–3137. IEEE Computer
Society.
Amirhossein Kazemnejad, Mohammadreza Salehi, and
Mahdieh Soleymani Baghshah. 2020. Paraphrase
generation by learning how to edit from samples. In
Proceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics , pages 6010–
6021, Online. Association for Computational Lin-
guistics.
Urvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke
Zettlemoyer, and Mike Lewis. 2020a. Near-
est neighbor machine translation. arXiv preprint
arXiv:2010.00710.
Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke
Zettlemoyer, and Mike Lewis. 2020b. Generaliza-
tion through memorization: Nearest neighbor lan-
guage models. In 8th International Conference on
Learning Representations, ICLR 2020, Addis Ababa,
Ethiopia, April 26-30, 2020. OpenReview.net.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings
of the 2003 Human Language Technology Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 127–133.
Philipp Koehn and Jean Senellart. 2010. Convergence
of translation memory and statistical machine trans-
lation. In Proceedings of AMTA Workshop on MT
Research and the Translation Industry, pages 21–31.
Mojtaba Komeili, Kurt Shuster, and Jason Weston.
2021. Internet-augmented dialogue generation.
arXiv preprint arXiv:2107.07566.
Kenton Lee, Ming-Wei Chang, and Kristina Toutanova.
2019. Latent retrieval for weakly supervised
open domain question answering. arXiv preprint
arXiv:1906.00300.
Mike Lewis, Marjan Ghazvininejad, Gargi Ghosh, Ar-
men Aghajanyan, Sida Wang, and Luke Zettlemoyer.
2020a. Pre-training via paraphrasing. In Advances
in Neural Information Processing Systems 33: An-
nual Conference on Neural Information Processing
Systems 2020, NeurIPS 2020, December 6-12, 2020,
virtual.
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio
Petroni, Vladimir Karpukhin, Naman Goyal, Hein-
rich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-
täschel, et al. 2020b. Retrieval-augmented gen-
eration for knowledge-intensive nlp tasks. arXiv
preprint arXiv:2005.11401.
Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,
and Bill Dolan. 2016a. A diversity-promoting ob-
jective function for neural conversation models. In
NAACL, pages 110–119.Juncen Li, Robin Jia, He He, and Percy Liang. 2018.
Delete, retrieve, generate: a simple approach to sen-
timent and style transfer. In Proceedings of the 2018
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, NAACL-HLT 2018, New
Orleans, Louisiana, USA, June 1-6, 2018, Volume
1 (Long Papers), pages 1865–1874. Association for
Computational Linguistics.
Liangyou Li, Andy Way, and Qun Liu. 2014. A
discriminative framework of integrating translation
memory features into smt. In Proceedings of the
11th Conference of the Association for Machine
Translation in the Americas , volume 1, pages 249–
260.
Liangyou Li, Andy Way, and Qun Liu. 2016b. Phrase-
level combination of smt and tm using constrained
word lattice. Association for Computational Lin-
guistics (ACL).
Xiaoqing Li, Jiajun Zhang, and Chengqing Zong.
2016c. One sentence one model for neural machine
translation. arXiv preprint arXiv:1609.06490.
Zekang Li, Cheng Niu, Fandong Meng, Yang Feng,
Qian Li, and Jie Zhou. 2019. Incremental trans-
former with deliberation decoder for document
grounded conversations. In Proceedings of the 57th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 12–21.
Rongzhong Lian, Min Xie, Fan Wang, Jinhua Peng,
and Hua Wu. 2019. Learning to select knowledge
for response generation in dialog systems. arXiv
preprint arXiv:1902.04911.
Lemao Liu, Hailong Cao, Taro Watanabe, Tiejun Zhao,
Mo Yu, and Conghui Zhu. 2012. Locally training
the log-linear model for smt. In Proceedings of the
2012 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning, pages 402–411.
Lemao Liu, Tiejun Zhao, Taro Watanabe, Hailong Cao,
and Conghui Zhu. 2014. Discriminative training for
log-linear based smt: Global or local methods. ACM
Transactions on Asian Language Information Pro-
cessing (TALIP), 13(4):1–25.
Yanjun Ma, Yifan He, Andy Way, and Josef van Gen-
abith. 2011. Consistent translation using discrim-
inative learning-a translation memory-inspired ap-
proach. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 1239–1248.
Yuxian Meng, Xiaoya Li, Xiayu Zheng, Fei Wu, Xi-
aofei Sun, Tianwei Zhang, and Jiwei Li. 2021.
Fast nearest neighbor machine translation. arXiv
preprint arXiv:2105.14528.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting of the Association for Compu-
tational Linguistics, pages 160–167, Sapporo, Japan.
Association for Computational Linguistics.
Gaurav Pandey, Danish Contractor, Vineet Kumar, and
Sachindra Joshi. 2018. Exemplar encoder-decoder
for neural conversation generation. In ACL, pages
1329–1338.
Ashwin Paranjape, Omar Khattab, Christopher Potts,
Matei Zaharia, and Christopher D Manning. 2021.
Hindsight: Posterior-guided training of retrievers for
improved open-ended generation. arXiv preprint
arXiv:2110.07752.
Hao Peng, Ankur P. Parikh, Manaal Faruqui, Bhuwan
Dhingra, and Das Dipanjan. 2019. Text generation
with exemplar-based adaptive decoding. In Proceed-
ings of the Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies.
Lianhui Qin, Michel Galley, Chris Brockett, Xiaodong
Liu, Xiang Gao, William B Dolan, Yejin Choi, and
Jianfeng Gao. 2019. Conversing by reading: Con-
tentful neural conversation with on-demand machine
reading. In Proceedings of the 57th Annual Meet-
ing of the Association for Computational Linguistics,
pages 5427–5436.
Minghui Qiu, Feng-Lin Li, Siyu Wang, Xing Gao, Yan
Chen, Weipeng Zhao, Haiqing Chen, Jun Huang,
and Wei Chu. 2017. Alime chat: A sequence to se-
quence and rerank based chatbot engine. In ACL,
pages 498–503.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish
Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
Gretchen Krueger, and Ilya Sutskever. 2021. Learn-
ing transferable visual models from natural lan-
guage supervision. In Proceedings of the 38th In-
ternational Conference on Machine Learning, ICML
2021, 18-24 July 2021, Virtual Event, volume 139 of
Proceedings of Machine Learning Research , pages
8748–8763. PMLR.
Stephen Robertson and Hugo Zaragoza. 2009. The
probabilistic relevance framework: BM25 and be-
yond. Now Publishers Inc.
Lifeng Shang, Zhengdong Lu, and Hang Li. 2015. Neu-
ral responding machine for short-text conversation.
In ACL, pages 1577–1586.
Michel Simard and Pierre Isabelle. 2009. Phrase-based
machine translation in a computer-assisted transla-
tion environment. Proceedings of the Twelfth Ma-
chine Translation Summit (MT Summit XII) , pages
120–127.
James Smith and Stephen Clark. 2009. Ebmt for smt:
a new ebmt-smt hybrid. In Proceedings of the 3rd
International Workshop on Example-Based Machine
Translation, pages 3–10. Citeseer.Harold Somers. 2003. Translation memory systems.
Benjamins Translation Library, 35:31–48.
Yiping Song, Rui Yan, Xiang Li, Dongyan Zhao, and
Ming Zhang. 2016. Two are better than one: An en-
semble of retrieval-and generation-based dialog sys-
tems. arXiv preprint arXiv:1610.07149.
Yixuan Su, Zaiqiao Meng, Simon Baker, and Nigel Col-
lier. 2021a. Few-shot table-to-text generation with
prototype memory. In Findings of the Association
for Computational Linguistics: EMNLP 2021, Vir-
tual Event / Punta Cana, Dominican Republic, 16-
20 November, 2021, pages 910–917. Association for
Computational Linguistics.
Yixuan Su, David Vandyke, Simon Baker, Yan Wang,
and Nigel Collier. 2021b. Keep the primary, rewrite
the secondary: A two-stage approach for paraphrase
generation. In Findings of the Association for Com-
putational Linguistics: ACL-IJCNLP 2021 , pages
560–569, Online. Association for Computational
Linguistics.
Yixuan Su, Yan Wang, Deng Cai, Simon Baker, Anna
Korhonen, and Nigel Collier. 2021c. PROTOTYPE-
TO-STYLE: dialogue generation with style-aware
editing on retrieval memory. IEEE ACM Trans. Au-
dio Speech Lang. Process., 29:2152–2161.
Marco Turchi, Matteo Negri, M Farajian, and Marcello
Federico. 2017. Continuous learning from human
post-edits for neural machine translation.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in neural information pro-
cessing systems, pages 5998–6008.
Oriol Vinyals and Quoc Le. 2015. A neural conversa-
tional model. In ICML (Deep Learning Workshop).
Kun Wang, Chengqing Zong, and Keh-Yih Su. 2013.
Integrating translation memory into phrase-based
machine translation during decoding. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 11–21.
Kun Wang, Chengqing Zong, and Keh-Yih Su. 2014.
Dynamically integrating cross-domain translation
memory into phrase-based machine translation dur-
ing decoding. In Proceedings of COLING 2014,
the 25th International Conference on Computational
Linguistics: Technical Papers, pages 398–408.
Jason Weston, Emily Dinan, and Alexander Miller.
2018. Retrieve and reﬁne: Improved sequence gen-
eration models for dialogue. In Proceedings of the
2018 EMNLP Workshop SCAI: The 2nd Interna-
tional Workshop on Search-Oriented Conversational
AI, pages 87–92.
Yu Wu, Furu Wei, Shaohan Huang, Yunli Wang, Zhou-
jun Li, and Ming Zhou. 2019. Response generation
by context-aware prototype editing. In Proceedings
of the AAAI Conference on Artiﬁcial Intelligence ,
volume 33, pages 7281–7288.
Zeqiu Wu, Michel Galley, Chris Brockett, Yizhe Zhang,
Xiang Gao, Chris Quirk, Rik Koncel-Kedziorski,
Jianfeng Gao, Hannaneh Hajishirzi, Mari Ostendorf,
et al. 2021. A controllable model of grounded re-
sponse generation. In Proceedings of the AAAI Con-
ference on Artiﬁcial Intelligence , volume 35, pages
14085–14093.
Mengzhou Xia, Guoping Huang, Lemao Liu, and
Shuming Shi. 2019. Graph based translation mem-
ory for neural machine translation. In Proceedings
of the AAAI Conference on Artiﬁcial Intelligence ,
volume 33, pages 7297–7304.
Fei Xiao, Liang Pang, Yanyan Lan, Yan Wang, Huawei
Shen, and Xueqi Cheng. 2021. Transductive learn-
ing for unsupervised text style transfer. In Proceed-
ings of the 2021 Conference on Empirical Methods
in Natural Language Processing, EMNLP 2021, Vir-
tual Event / Punta Cana, Dominican Republic, 7-11
November, 2021, pages 2510–2521. Association for
Computational Linguistics.
Jitao Xu, Josep M Crego, and Jean Senellart. 2020.
Boosting neural machine translation with similar
translations. In Proceedings of the 58th Annual
Meeting of the Association for Computational Lin-
guistics, pages 1580–1590.
Liu Yang, Junjie Hu, Minghui Qiu, Chen Qu, Jian-
feng Gao, W Bruce Croft, Xiaodong Liu, Yelong
Shen, and Jingjing Liu. 2019. A hybrid retrieval-
generation neural conversation model. In Proceed-
ings of the 28th ACM international conference on in-
formation and knowledge management, pages 1341–
1350.
Jingyi Zhang, Masao Utiyama, Eiichiro Sumita, Gra-
ham Neubig, and Satoshi Nakamura. 2018. Guiding
neural machine translation with retrieved translation
pieces. In Proceedings of the 2018 Conference of the
North American Chapter of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, Volume 1 (Long Papers), pages 1325–1335.
Yizhe Zhang, Siqi Sun, Xiang Gao, Yuwei Fang, Chris
Brockett, Michel Galley, Jianfeng Gao, and Bill
Dolan. 2021. Joint retrieval and generation train-
ing for grounded text generation. arXiv preprint
arXiv:2105.06597.
Zhuosheng Zhang, Kehai Chen, Rui Wang, Masao
Utiyama, Eiichiro Sumita, Zuchao Li, and Hai Zhao.
2020. Neural machine translation with universal
visual representation. In 8th International Confer-
ence on Learning Representations, ICLR 2020, Ad-
dis Ababa, Ethiopia, April 26-30, 2020 . OpenRe-
view.net.Ventsislav Zhechev and Josef Van Genabith. 2010.
Seeding statistical machine translation with trans-
lation memory output through tree-based structural
alignment. In Proceedings of the 4th Workshop
on Syntax and Structure in Statistical Translation ,
pages 43–51.
Xin Zheng, Zhirui Zhang, Junliang Guo, Shujian
Huang, Boxing Chen, Weihua Luo, and Jiajun Chen.
2021a. Adaptive nearest neighbor machine transla-
tion. arXiv preprint arXiv:2105.13022.
Xin Zheng, Zhirui Zhang, Shujian Huang, Boxing
Chen, Jun Xie, Weihua Luo, and Jiajun Chen. 2021b.
Non-parametric unsupervised domain adaptation for
neural machine translation. In Findings of the As-
sociation for Computational Linguistics: EMNLP
2021, pages 4234–4241.
Kangyan Zhou, Shrimai Prabhumoye, and Alan W
Black. 2018. A dataset for document grounded con-
versations. arXiv preprint arXiv:1809.07358.

Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics
Volume 1: Long Papers, pages 10014–10037
July 9-14, 2023 ©2023 Association for Computational Linguistics
Interleaving Retrieval with Chain-of-Thought Reasoning
for Knowledge-Intensive Multi-Step Questions
Harsh Trivedi† Niranjan Balasubramanian†
†Stony Brook University
Stony Brook, U.S.A.
{hjtrivedi,niranjan}@cs.stonybrook.edu
Tushar Khot‡ Ashish Sabharwal‡
‡Allen Institute for AI
Seattle, U.S.A.
{tushark,ashishs}@allenai.org
Abstract
Prompting-based large language models
(LLMs) are surprisingly powerful at gener-
ating natural language reasoning steps or
Chains-of-Thoughts (CoT) for multi-step
question answering (QA). They struggle,
however, when the necessary knowledge is
either unavailable to the LLM or not up-to-date
within its parameters. While using the question
to retrieve relevant text from an external
knowledge source helps LLMs, we observe
that this one-step retrieve-and-read approach
is insufficient for multi-step QA. Here, what
to retrieve depends on what has already
been derived, which in turn may depend on
what was previously retrieved . To address
this, we propose IRCoT, a new approach
for multi-step QA that interleaves retrieval
with steps (sentences) in a CoT, guiding the
retrieval with CoT and in turn using retrieved
results to improve CoT. Using IRCoT with
GPT3 substantially improves retrieval (up to
21 points) as well as downstream QA (up
to 15 points) on four datasets: HotpotQA,
2WikiMultihopQA, MuSiQue, and IIRC. We
observe similar substantial gains in out-of-
distribution (OOD) settings as well as with
much smaller models such as Flan-T5-large
without additional training. IRCoT reduces
model hallucination, resulting in factually
more accurate CoT reasoning.1.
1 Introduction
Large language models are capable of answer-
ing complex questions by generating step-by-
step natural language reasoning steps—so called
chains of thoughts (CoT)—when prompted appro-
priately (Wei et al., 2022). This approach has been
successful when all information needed to answer
the question is either provided as context (e.g., al-
gebra questions) or assumed to be present in the
model’s parameters (e.g., commonsense reasoning).
1Code, data, and prompts are available at https://
github.com/stonybrooknlp/ircot
In what country was  
Lost Gravity manufactured?
The Lost Gravity was  
manufactured by Mack Rides.
Mack Rides is a company  
from Germany .
The answer is Germany .
cumulate docs
cumulate docs
cumulate docs
Figure 1: IRCoT interleaves chain-of-thought (CoT)
generation and knowledge retrieval steps in order to
guide the retrieval by CoT and vice-versa. This inter-
leaving allows retrieving more relevant information for
later reasoning steps, compared to standard retrieval us-
ing solely the question as the query.
However, for many open-domain questions, all re-
quired knowledge is not always available or up-to-
date in models’ parameters and it’s beneficial to
retrieve knowledge from external sources (Lazari-
dou et al., 2022; Kasai et al., 2022).
How can we augment chain-of-thought prompt-
ing for open-domain, knowledge-intensive tasks
that require complex, multi-step reasoning?
While a one-shot retrieval from a knowledge
source based solely on the question can success-
fully augment LMs with relevant knowledge for
many factoid-based tasks (Lewis et al., 2020; Guu
et al., 2020; Borgeaud et al., 2022; Izacard et al.,
2022), this strategy has clear limitations for more
complex multi-step reasoning questions. For such
questions, one often must retrieve partial knowl-
edge, perform partial reasoning, retrieve additional
information based on the outcome of the partial
10014reasoning done so far, and iterate. As an example,
consider the question illustrated in Fig. 1, “In what
country was Lost Gravity manufactured?” . The
Wikipedia document retrieved using the question
(in particular, the roller coaster Lost Gravity) as the
query does not mention where Lost Gravity was
manufactured. Instead, one must first infer that
it was manufactured by a company called Mack
Rides, and then perform further retrieval, guided
by the inferred company name, to obtain evidence
pointing to the manufacturing country.
Thus, the retrieval and reasoning steps must in-
form each other. Without retrieval, a model is likely
to generate an incorrect reasoning step due to hallu-
cination. Additionally, without generating the first
reasoning step, the text supporting the second step
can’t be identified easily given the lack of lexical or
even semantic overlap with the question. In other
words, we need retrieved facts in order to generate
factually correct reasoning steps and the reasoning
steps to retrieve relevant facts.
Based on this intuition, we propose an interleav-
ing approach to this problem, where the idea is to
use retrieval to guide the chain-of-thought (CoT)
reasoning steps and use CoT reasoning to guide the
retrieval. Fig. 1 shows an overview of our retrieval
method, which we call IRCoT.2 We begin by re-
trieving a base set of paragraphs using the question
as a query. Subsequently, we alternate between the
following two steps: (i) extend CoT: use the ques-
tion, the paragraphs collected thus far, and the CoT
sentences generated thus far to generate the next
CoT sentence; (ii) expand retrieved information:
use the last CoT sentence as a query to retrieve
additional paragraphs to add to the collected set.
We repeat these steps till the CoT reports an an-
swer or we reach the maximum allowed number
of reasoning steps. Upon termination, all collected
paragraphs are returned as the retrieval outcome.
Finally, we use these as the context for answering
the question via direct QA prompting (Brown et al.,
2020) or CoT prompting (Wei et al., 2022).
We evaluate the efficacy of our system
on 4 multi-step reasoning datasets under an
open-domain setting: HotpotQA (Yang et al.,
2018), 2WikiMultihopQA (Ho et al., 2020),
MuSiQue (Trivedi et al., 2022), and IIRC (Fer-
guson et al., 2020). Our experiments using OpenAI
GPT3 (code-davinci-002) (Brown et al., 2020;
Ouyang et al., 2022; Chen et al., 2021) demon-
2Interleaved Retrieval guided by Chain-of-Thought.
strate that retrieval using IRCoT is substantially
more effective than the baseline, one-step, question-
based retrieval by 11-21 recall points under a fixed-
budget optimal recall setup.3 When IRCoT is used
in conjunction with a prompting-based reader, it
also leads to substantial improvement (up to 15 F1
points) in downstream few-shot QA performance
and reduces factual errors in generated CoT by
up to 50%. Our approach also works on much
smaller Flan-T5 models (11B, 3B, and 0.7B) show-
ing similar trends. In particular, we find QA using
Flan-T5-XL (3B) with IRCoT even outperforms
the 58X larger GPT3 with a one-step question-
based retrieval. Furthermore, these improvements
also hold up in an out-of-distribution (OOD) setting
where the demonstrations from one dataset are used
when testing on another dataset. Lastly, we note
that our QA scores exceed those reported by recent
works on few-shot prompting for open-domain QA
(ODQA) (Khot et al., 2023; Press et al., 2022; Yao
et al., 2022), although a fair apples-to-apples com-
parison with them isn’t possible (cf. Appendix C).
In summary, our maincontribution is a novel re-
trieval method, IRCoT, that leverages LMs’ chain-
of-thought generation capabilities to guide retrieval
and uses retrieval in turn to improve CoT reasoning.
We demonstrate that IRCoT:
1. improves both retrieval and few-shot QA per-
formance on several multi-step open-domain
QA datasets, in both IID and OOD settings;
2. reduces factual errors in generated CoTs; and
3. improves performance with both large-scale
(175B models) as well as smaller-scale mod-
els (Flan-T5-*, ≤11B) without any training.
2 Related Work
Prompting for Open-Domain QA.LLMs can
learn various tasks by simply using a few exam-
ples as prompts (Brown et al., 2020). They’ve
also been shown to answer complex questions
by producing step-by-step reasoning (chain-of-
thoughts, or CoT) when prompted with a few or
zero demonstrations (Wei et al., 2022; Kojima et al.,
2022). Prompting has been applied to open-domain
QA (Lazaridou et al., 2022; Sun et al., 2022; Yu
et al., 2023) but its value in improving retrieval and
QA for multi-step open-domain questions remains
relatively underexplored.
3We explain later (in the Metric section and Footnote 7)
the appropriateness of this metric in our setting as opposed to
more mainstream information recall metrics.
10015Recently three approaches have been proposed
for multi-step open-domain QA. SelfAsk (Press
et al., 2022) prompts LLMs to decompose a ques-
tion into subquestions and answers subquestions by
a call to Google Search API. DecomP (Khot et al.,
2023) is a general framework that decomposes a
task and delegates sub-tasks to appropriate sub-
models. They also decompose questions but dele-
gate retrieval to a BM25-based retriever. Both of
these approaches are not developed for CoT reason-
ing, do not focus on the retrieval problem, and re-
quire a single-hop QA model to answer the decom-
posed questions. Recently proposed ReAct (Yao
et al., 2022) system frames the problem as generat-
ing a sequence of reasoning and action steps. These
steps are much more complex, rely on much larger
models (PaLM-540B), and require fine-tuning to
outperform CoT for multi-step ODQA. Further-
more, none of these works have been shown to be
effective for smaller models without any training.
While a direct comparison with these approaches is
not straightforward (difference in knowledge cor-
pus, LLMs, examples), we find that our ODQA
performance is much higher than all their reported
numbers where available (§5).
Supervised Multi-Step Open-Domain QA.
Prior work has explored iterative retrieval for
open-domain QA in a fully supervised setting. Das
et al. (2019) proposes an iterative retrieval model
that retrieves using a neural query representation
and then updates it based on a reading compre-
hension model’s output. Feldman and El-Yaniv
(2019) apply similar neural query reformulation
idea for multihop open-domain QA. Xiong et al.
(2021) extends the widely-used Dense Passage
Retrieval (DPR) (Karpukhin et al., 2020) to
multihop setting, which has since been improved
by Khattab et al. (2021). Asai et al. (2020)
leverages the graph structure induced by the entity
links present in Wikipedia paragraphs to perform
iterative multi-step retrieval. GoldEn (Gold Entity)
retriever (Qi et al., 2019) iteratively generates
text queries based on paragraphs retrieved from
an off-the-shelf retriever but requires training
data for this next query generator. Nakano et al.
(2021) used GPT3 to answer long-form questions
by interacting with the browser but relied on
human annotations of these interactions. All of
these methods rely on supervised training on a
large-scale dataset and can not be easily extended
to a few-shot setting.
3 Chain-of-Thought-Guided Retrieval
and Open-Domain QA
Our goal is to answer a knowledge-intensive multi-
step reasoning question Q in a few-shot setting
by using a knowledge source containing a large
number of documents. To do this we follow a
retrieve-and-read paradigm (Zhu et al., 2021),
where the retriever first retrieves documents from
the knowledge source and the QA model reads the
retrieved documents and the question to generate
the final answer. Our contribution is mainly in the
retrieve step (§3.1), and we use standard prompt-
ing strategies for the read step (§3.2).
As noted earlier, for multi-step reasoning, re-
trieval can help guide the next reasoning step,
which in turn can inform what to retrieve next. This
motivates our interleaving strategy, discussed next.
3.1 Interleaving Retrieval with
Chain-of-Thought Reasoning
Our proposed retriever method, IRCoT, can be
instantiated from the following three ingredients:
(i) a base retriever that can take a query and re-
turn a given number of paragraphs from a corpus
or knowledge source; (ii) a language model with
zero/few-shot Chain-of-Thought (CoT) generation
capabilities; and (iii) a small number of annotated
questions with reasoning steps explaining how to
arrive at the answer in natural language (chain of
thoughts) and a set of paragraphs from the knowl-
edge source that collectively support the reasoning
chain and the answer.
The overview of IRCoT is given in Fig. 2. We
first gather a base set of paragraphs by retrievingK
paragraphs using the questionQ as the query. Then,
we interleave two steps ( reason and retrieve)
iteratively until the termination criterion is met.
The retrieval-guided reasoning step (“Rea-
son”) generates the next CoT sentence using the
question, the paragraphs collected thus far, and
the CoT sentences generated thus far. The prompt
template for the task looks as follows:
Wikipedia Title: <Page Title>
<Paragraph Text>
...
Wikipedia Title: <Page Title>
<Paragraph Text>
Q: <Question>
A: <CoT-Sent-1> ... <CoT-Sent-n>
For in-context demonstrations, we use the com-
plete CoT in the above format. For a test instance,
10016Who wrote the 1970 international hit song that Murray Head is most recognized for?
The 1970 international hit song that
Murray Head is most recognized for
is "Super Star"
"Super Star" was written by
Andrew Lloyd W ebber and T im Rice.
So the answer is:
Andrew Lloyd W ebber and T im Rice.
Retrieve (Q) →
Retrieve (T1)  → 
Retrieve (T2)  → 
T1 ← Reason (Q,      ,       )
T2 ← Reason (Q,      +     , T1)
T3 ← Reason (Q,      +      +     , T1+T2)
T1
Q
T2
T3
Stop
+ +
IRCoT 
Interleaved Retrieval guided
by Chain-of-Thought Reasoning
Retrieve(         )
Wikipedia T itle: Mack Rides
Mack Rides GmbH & Co KG, also ...
Q: In what country was
Lost Gravity manufactured?
A: The Lost Gravity was manufactured by Mack
Rides. Mack Rides is a company from
Germany . The answer is Germany .
...
Wikipedia T itle: Murray Head
Murray Seafield St George Head ..
...
Wikipedia T itle: Most Beautifullest Hits
The Most Beautifullest Hits is ...
Q: Who wrote the 1970 international hit ..
A: The 1970 international hit song that
Murray Head is most recognized for
is "Super Star". "Super Star" was written
by . Andrew Lloyd W ebber and T im Rice.
q
Reason(        ,        ,         ) Q
 T1
Q
T1
q
Figure 2: IRCoT interleaves chain-of-thought (CoT) generation and retrieval steps to guide the retrieval by CoT and
vice-versa. We start by retrieving K documents using the question as they query and repeat two steps alternatingly
until termination. (i) reason-step generates next CoT sentence based on the question, so far retrieved paragraphs,
and CoT sentences. (ii) retrieve-step retrieves K more paragraphs based on the last CoT sentence. The process
terminates when the generated CoT has “answer is” or the number of steps exceeds a threshold. The collection of
all paragraphs is returned as the retrieval result on the termination.
we show the model only the CoT sentences gen-
erated thus far and let it complete the rest. Even
though the model may output multiple sentences,
for each reason-step, we only take the first gen-
erated sentence and discard the rest.
For the paragraphs in the in-context demonstra-
tions, we use ground-truth supporting paragraphs
and M randomly sampled paragraphs shuffled and
concatenated together in the above format. For a
test instance, we show all the paragraphs collected
thus far across all the previous retrieve-steps.
If the generated CoT sentence has the “answer
is:” string or the maximum number of steps 4 has
been reached, we terminate the process and return
all collected paragraphs as the retrieval result.
The CoT-guided retrieval step (“Retrieve”)
uses the last generated CoT sentence as a query
to retrieve more paragraphs and adds them to the
collected paragraphs. We cap the total number of
collected paragraphs5 so as to fit in at least a few
demonstrations in the model’s context limit.
3.2 Question Answering Reader
The QA reader answers the question using retrieved
paragraphs taken from the retriever. We consider
4set to 8 in our experiments.
5set to 15 in our experiments.
two versions of the QA reader implemented via two
prompting strategies: CoT Prompting as proposed
by Wei et al. (2022), Direct Prompting as proposed
by Brown et al. (2020). For CoT prompting, we use
the same template as shown in §3.2, but at test time
we ask the model to generate the full CoT from
scratch. The final sentence of CoT is expected to
be of the form “answer is: ...”, so that the answer
can be extracted programmatically. If it’s not in
that form, the full generation is returned as the
answer. For Direct Prompting, we use the same
template as CoT Prompting but the answer field
(“A: ”) contains only the final answer instead of
CoT. See App. G for details.
4 Experimental Setup
We evaluate our method on 4 multi-step
QA datasets in the open-domain setting:
HotpotQA (Yang et al., 2018), 2WikiMul-
tihopQA (Ho et al., 2020), answerable subset of
MuSiQue (Trivedi et al., 2022), and answerable
subset of IIRC (Ferguson et al., 2020). For
HotpotQA, we use the Wikipedia corpus that
comes with it for the open-domain setting. For
each of the other three datasets, which originally
come in a reading comprehension or mixed setting,
we used the associated contexts to construct a
10017corpus for our open-domain setting (see App. A
for details). For each dataset, we use 100 randomly
sampled questions from the original development
set for tuning hyperparameters, and 500 other
randomly sampled questions as our test set.
4.1 Models
Retriever. We use BM25 (Robertson et al., 2009)
implemented in Elasticsearch6 as our base retriever.
We compare two retriever systems:
(i) One-step Retriever (OneR)uses the ques-
tion as a query to retrieve K paragraphs. We select
K ∈{5,7,9,11,13,15}that’s best on the dev set.
(ii) IRCoT Retriever is our method de-
scribed in §3. We use BM25 as its underly-
ing retriever and experiment with OpenAI GPT3
(code-davinci-002) (Brown et al., 2020; Ouyang
et al., 2022; Chen et al., 2021) and Flan-T5 (Chung
et al., 2022) of different sizes as its CoT generator.
For demonstrating in-context examples to these
LMs, we wrote CoTs for 20 questions for all the
datasets (see App. §G). We then create 3 demon-
stration (“training”) sets by sampling 15 questions
each for each dataset. For each experiment, we
search for the best hyperparameters for the dev set
using the first demonstration set and evaluate each
demonstration set on the test set using the selected
hyperparameters. We report the mean and standard
deviation of these 3 results for each experiment.
At test time, we pack as many demonstrations
as possible within the model’s context length limit.
The context limit for GPT3 (code-davinci-002)
is 8K word pieces. Flan-T5-* doesn’t have any
hard limit as it uses relative position embeddings.
But we limit Flan-T5’s context to 6K word pieces,
which is the maximum we could fit in the memory
of our 80G A100 GPUs.
IRCoT Retriever has one key hyperparameter:
K ∈{2,4,6,8}, the number of paragraphs to re-
trieve at each step. Additionally, when creating
“training” demonstrations for IRCoT’s Reasoner
module, we use gold paragraphs and a smaller num-
ber M ∈{1,2,3}of distractor paragraphs (§3.1).
Retrieval Metric:We allow a maximum of 15
paragraphs for all retriever systems and measure
the recall of the gold paragraphs among the re-
trieved set of paragraphs. We search for the hyper-
parameter K (and M for IRCoT) that maximizes
the recall on the dev set and use it on the test set.
6https://www.elastic.co/
The reported metric can thus be viewed as thefixed-
budget optimal recall for each system considered.7
QA Reader. To implement the reader, we use
the same LMs as used in the reason-step of
IRCoT Retriever. We found that QA readers im-
plemented with Flan-T5-* perform better with the
Direct Prompting strategy and GPT3 performs bet-
ter with CoT Prompting strategy (see App. E).
Hence we use Direct prompting strategy for QA
with Flan-T5-* and CoT with GPT3 for the experi-
ments.8
The QA reader has one hyperparameter M: the
number of distractor paragraphs in the in-context
demonstrations. We search for M in {1,2,3}.
When used in conjunction with IRCoT retriever
M is tied for the CoT generator and the reader.
Open-Domain QA (ODQA) Models.Putting re-
trievers and readers together, we experiment with
ODQA models constructed from the various lan-
guage models denoted as OneR QAand IRCoT
QA. For IRCoT QA, the choice of LM for the CoT
generator and the reader is kept the same. We also
experiment with retriever-less QA readersNoR QA
to assess how well LMs can answer the question
from their parametric knowledge alone. To select
the best hyperparameters for the ODQA model,
we search for the hyperparameters K and M that
maximize the answer F1 on the development set.
IIRC is structured slightly differently from the
other datasets, in that its questions are grounded
in a main passage and other supporting paragraphs
come from the Wikipedia pages of entities men-
tioned in this passage. We slightly modify the re-
trievers and readers to account for this (see App. B).
5 Results
IRCoT retrieval is better than one-step.Fig. 3
compares OneR with IRCoT retrievers made from
7Note that our retrieved documents are not ranked, mak-
ing standard information retrieval metrics such as MAP and
DCG inapplicable. Further, we can only limit the number of
retrieved paragraphs per step to K. Since the total number
of reasoning steps varies for questions, and in some cases,
we don’t even obtain all K paragraphs in a given step, the
total number of retrieved paragraphs also varies (even though
capped at 15). This makes Recall@k, Precision@k, etc., also
not applicable as metrics for any given k.
8IRCoT, by construction, produces a CoT as a part of its
retrieval process. Thus, instead of having a separate post-hoc
reader, one can also just extract the answer from the CoT
generated during retrieval. However, we found this to be a
suboptimal choice, so we always use a separate reader (see
App. F).
10018Figure 3: Retrieval recall for one-step retriever (OneR) and IRCoT instantiated from Flan-T5-XXL (left) and GPT3
(right) models. IRCoT outperforms OneR for both models and all datasets.
Figure 4: Answer F1 for ODQA model made using (i) no retriever (NoR QA) (ii) one-step retriever (OneR QA) and
(iii) IRCoT QA instantiated from Flan-T5-XXL (left) and GPT3 (right) models. IRCoT QA outperforms OneR QA
and NoR QA for both models on all datasets, except for GPT3 on IIRC.
Flan-T5-XXL and GPT3 LMs. For both models,
IRCoT significantly outperforms one-step retrieval
across all datasets. For Flan-T5-XXL, IRCoT im-
proves our recall metric relative to one-step re-
trieval, on HotpotQA by 7.9, on 2WikiMultihopQA
by 14.3, on MuSiQue by 3.5, and on IIRC by 10.2
points. For GPT3, this improvement is by 11.3, 22.6,
12.5, and 21.2 points, respectively.
IRCoT QA outperforms NoR and OneR QA.
Fig. 4 compares ODQA performance using
NoR, OneR and IRCoT retriever made from
Flan-T5-XXL and GPT3 LMs. For Flan-T5-XXL,
IRCoT QA outperforms OneR QA on HotpotQA
by 9.4, on 2WikiMultihopQA by 15.3, on MuSiQue
by 5.0 and IIRC by 2.5 F1 points. For GPT3, the
corresponding numbers (except for IIRC) are 7.1,
13.2, and 7.1 F1 points. For GPT3, IRCoT doesn’t
improve the QA score on IIRC, despite signifi-
cantly improved retrieval (21 points as shown in
Fig. 3). This is likely because IIRC relevant knowl-
edge may already be present in GPT3, as also ev-
idenced by its NoR QA score being similar. For
other datasets and model combinations, NoR QA is
much worse than IRCoT QA, indicating the limits
of the models’ parametric knowledge.
IRCoT is effective in OOD setting.Since CoT
may not always be easy to write for new datasets,
we evaluate NoR, OneR, and IRCoT on generaliza-
tion to new datasets, i.e. OOD setting. To do so,
we use prompt demonstrations from one dataset to
evaluate on another dataset.9 For all pairs of the
datasets10 and for both Flan-T5-XXL and GPT3, we
find the same trend as in the IID setting: IRCoT re-
trieval outperforms OneR (Fig. 5), and IRCoT QA
outperforms both OneR QA and NoR QA (Fig. 6).
IRCoT generates CoT with fewer factual errors.
To assess whether our approach also improves the
factuality of generated CoTs, we manually anno-
tated CoTs generated by NoR QA, OneR QA, and
IRCoT QA using GPT3 for 40 randomly sampled
questions from each of the four datasets. We con-
sidered CoT to have a factual error if at least one
9We use the evaluation dataset’s corpus for retrieval.
10We skip IIRC in this exploration as the task is structured
a bit differently and requires special handling (see App. B).
10019Figure 5: Retrieval recall for OneR and IRCoT using Flan-T5-XXL (Left) and GPT3 (Right) in out-of-distribution
(OOD) setting. HQ (HotpotQA), 2W (2WikiMultihopQA), MQ (MuSiQue). The result X →Y indicates prompt
demonstrations are from dataset X and evaluation is on dataset Y .IRCoT outperforms OneR in such an OOD setting.
Figure 6: Answer F1 for NoR QA, OneR QA and IRCoT QA using Flan-T5-XXL (Left) and GPT3 (Right) in
out-of-distribution (OOD) setting. HQ (HotpotQA), 2W (2WikiMultihopQA), MQ (MuSiQue). The result X→Y
indicates prompt demonstrations are from dataset X and evaluation is on dataset Y . IRCoTQA outperforms OneR
QA and NoR QA in such OOD setting.
Figure 7: Number of questions, out of 40, where CoT
generated by GPT3 using different methods has at least
1 factual error. Factual errors: IRCoT < OneR < NoR.
of the facts11 is not true.12 As Fig. 7 shows, NoR
makes the most factual errors, OneR makes fewer,
11all sentences before the final “answer is:” sentence.
12Note that factual error doesn’t necessarily mean the pre-
dicted answer is incorrect and vice-versa. This is because the
model can generate a wrong answer despite all correct facts,
and vice-versa. We also account for the possibility of answer
annotation errors in the original datasets.
and IRCoT the least. In particular, IRCoT reduces
the factual errors over OneR by 50% on HotpotQA
and 40% on 2WikiMultihopQA.
Table 2 illustrates how the CoT predictions for
different methods vary qualitatively. Since NoR
relies completely on parametric knowledge, it often
makes a factual error in the first sentence, which
derails the full CoT. OneR can retrieve relevant
information closest to the question and is less likely
to make such errors early on, but it still makes
errors later in the CoT. IRCoT, on the other hand,
is often able to prevent such errors in each step.
IRCoT is also effective for smaller models.To
see how effective IRCoT is at different LM sizes,
we show the scaling plots in Fig. 8. 13 We com-
pare the recall for OneR andIRCoT using Flan-T5
{base (0.2B), large (0.7B), XL (3B), XXL (11B)},
and GPT3 code-davinci-002 (175B). IRCoT
with even the smallest model (0.2B) is better than
13We skip IIRC here as the smaller models are not good at
identifying Wikipedia titles from a paragraph and a question
which is necessary for IIRC (see App. B).
10020Figure 8: Retrieval recall for OneR (bottom) and IRCoT (top) for LMs of increasing sizes: Flan-T5 {base (0.2B),
large (0.7B), XL (3B), XXL (11B)} and GPT3 (175B) on HotpotQA, 2WikiMultihopQA, MuSiQue. IRCoT
outperforms OneR for all model sizes, including the 0.3B model, and the difference roughly grows with model size.
Note: OneR doesn’t use LM in its retrieval and so has a fixed score.
Figure 9: Answer F1 for ODQA models made using OneR (bottom) and IRCoT (top) for LMs of increasing sizes:
Flan-T5 {base (0.2B), large (0.7B), XL (3B), XXL (11B)} and GPT3 (175B) on HotpotQA, 2WikiMultihopQA and
MuSiQue. IRCoT QA outperforms OneR QA for all model sizes except for the smallest, 0.3B. IRCoT with 3B
model even outperforms OneR with 58X larger GPT3 model showing the value of improved retrieval.
OneR, and the performance roughly improves with
the model size. This shows the CoT generation
capabilities of even small models can be leveraged
for improving retrieval. Furthermore, we show the
effect of model size on the QA score in Fig. 9. For
all sizes except the smallest (0.2B), we see IRCoT
QA is better than OneR QA. Moreover, IRCoT
with a 3B model even outperforms OneR and NoR
with a 58X larger 175B GPT3 model in all datasets.
IRCoT is SOTA for few-shot multistep ODQA.14
We compare IRCoT QA with five recent ap-
proaches to using LLMs for ODQA: Internet-
Augmented QA (Lazaridou et al., 2022), RE-
CITE (Sun et al., 2022) ReAct (Yao et al., 2022),
SelfAsk (Press et al., 2022), and DecomP (Khot
et al., 2022). Although these are not head-to-head
comparisons as different methods use different
APIs, knowledge sources, and even LLMs (see
App. C for details), it is still informative to ex-
plore, in a leaderboard-style fashion, how IRCoT
performs relative to the best numbers published for
these recent systems.
14App. §C reports updated SOTA numbers, including con-
temporaneous and newer works.
Model HpQA Br HpQA 2WikiMQA MQ 2H
InterAug −| − 30.3 | − − | − − | −
RECITE −| − 37.1 | 48.4 −| − − | −
ReAct −| − 35.1 | − − | − − | −
SelfAsk −| − − | − 40.1 | − 15.2 | −
DecomP −| 50.0 −| − − | 59.3 −| −
IRCoT QA 45.8 | 58.5 49.3 | 60.7 57.7 | 68.0 34.2 | 43.8
Table 1: Comparison with other LLM-based ODQA
systems on EM and F1 scores. ‘ −’: score is unavail-
able. HpQABr: Bridge questions subset of HotpotQA.
MQ2H: MuSiQue 2-hop questions. IRCoT QA with
GPT3 (ours) outperforms other systems by a large mar-
gin. Note: Comparisons aren’t head-to-head as dis-
cussed in the text. App. §C reports updated SOTA num-
bers, including contemporaneous and newer works.
As shown in Table 1, IRCoT QA significantly
outperforms all of these recent systems by a large
margin, setting a new state of the art in terms of
what’s achievable via retrieval-augmented LLMs
(without supervised training).
6 Conclusions
Chain-of-thought prompting has significantly im-
proved LLMs’ ability to perform multi-step reason-
10021Question Gold Facts NoR QA OneR QA IRCoT QA
HotpotQA
The tap dancing
soloist in My
Feet Keep
Dancing also
starred in what
1935 MGM
musical?
⇒ The tap dancing
soloist in My Feet
Keep Dancing is Fayard
Nicholas.
⇒ Fayard Nicholas
starred in the 1935
MGM musical An
All-Colored Vaudeville
Show.
The tap dancing soloist
in My Feet Keep Danc-
ing is Eleanor Pow-
ell. Eleanor Powell
also starred in the 1935
MGM musical Broad-
way Melody of 1936.
So the answer is: Broad-
way Melody of 1936.
The tap dancing soloist
in My Feet Keep Danc-
ing is Fayard Nicholas.
Fayard Nicholas also
starred in the 1935
MGM musical Top Hat.
So the answer is: Top
Hat.
The tap dancing soloist
in My Feet Keep Danc-
ing is Fayard Nicholas.
Fayard Nicholas starred
in the 1935 MGM mu-
sical An All-Colored
Vaudeville Show. So
the answer is: An
All-Colored Vaudeville
Show.
2WikiMQA
Who is the
father-in-law
of Queen Hyo-
jeong?
⇒ Queen Hyojeong
is the wife of King
Heonjong of Joseon.
⇒ King Heonjong
of Joseon is the son
of Crown Prince Hy-
omyeong.
Queen Hyojeong has a
husband named King
Gojong. King Gojong
has a father named
Heungseon Daewongun.
Thus, Queen Hyojeong
has a father-in-law
named Heungseon
Daewongun. So the
answer is: Heungseon
Daewongun.
Queen Hyojeong is the
wife of King Heonjong
of Joseon. King Heon-
jong of Joseon is the
son of King Sejo of
Joseon. Thus, King Sejo
of Joseon is the father-
in-law of Queen Hyo-
jeong. So the answer is:
King Sejo of Joseon.
Queen Hyojeong is the
wife of King Heonjong
of Joseon. King Heon-
jong of Joseon is the son
of Crown Prince Hy-
omyeong. Thus, Crown
Prince Hyomyeong is
the father-in-law of
Queen Hyojeong. So
the answer is: Crown
Prince Hyomyeong.
MuSiQue
What is the
name of the
castle in the
city where the
performer of
A Collection
1984–1989 was
born?
⇒ A Collection
1984–1989 was per-
formed by Jane Siberry.
⇒ Jane Siberry was
born in Toronto.
⇒The castle in Toronto
is the Casa Loma.
The performer of A Col-
lection 1984–1989 is
The The. The The was
born in London. The
name of the castle in
London is the Tower of
London. So the answer
is: the Tower of Lon-
don.
A Collection
1984–1989 was
performed by Jane
Siberry. Jane Siberry
was born in Toronto.
The castle in Toronto
is Peqin Castle. So the
answer is: Peqin Castle.
A Collection
1984–1989 was
performed by Jane
Siberry. Jane Siberry
was born in Toronto.
The castle in Toronto is
the Casa Loma. So the
answer is: Casa Loma.
Table 2: Example CoTs generated by GPT3 with different methods. Since NoR relies on parametric knowledge, it
often makes a factual error in the first sentence derailing the full CoT. OneR can retrieve relevant information closest
to the question and is less likely to make such errors early on, but it still makes errors later in the CoT. AsIRCoT
performs retrieval after each step, it is often able to prevent such errors in each step. More examples are in App. D.
ing. We leveraged this ability to improve retrieval,
and in turn, improve QA performance for com-
plex knowledge-intensive open-domain tasks in a
few-shot setting. We argued that one-step question-
based retrieval is insufficient for such tasks, and
introduced IRCoT, which uses interleaved CoT rea-
soning and retrieval steps that guide each other
step-by-step. On four datasets, IRCoT significantly
improves both retrieval and QA performance when
compared to one-step retrieval, for both large and
relatively smaller-scale LMs. Additionally, CoTs
generated by IRCoT contain fewer factual errors.
Limitations
IRCoT relies on the base LM to have a zero or
few-shot CoT-generation ability. While this is com-
monly available in large LMs (over 100B), it’s not
as common for small LMs (under 20B), which to
some extent limits IRCoT adoptability. Given the
recent surge of interest (Tay et al., 2023; Magis-
ter et al., 2022; Ho et al., 2022), however, smaller
LMs will likely increasingly acquire such ability,
making IRCoT compatible with many more LMs.
IRCoT also relies on the base LM to support
long inputs as multiple retrieved paragraphs need
to fit in the LM’s input, in addition to at least
a few demonstrations of QA or CoT with para-
graphs. This was supported by the models we used
as code-davinci-002 (GPT3) allows 8K tokens
and Flan-T5-* uses relative position embeddings
making it as extensible as the GPU memory con-
straints allow. Future work can explore strategies to
rerank and select the retrieved paragraphs instead
of passing all of them to the LM to alleviate the
need for the LM to support long input.
The performance gain of IRCoT retriever and
QA (over OneR and ZeroR baselines) come with
an additional computational cost. This is because
IRCoT makes a separate call to an (L)LM for each
sentence of CoT. Future work can focus on, for
instance, dynamically deciding when to retrieve
more information and when to perform additional
reasoning with the current information.
10022Lastly, a portion of our experiments was carried
out using a commercial LLM API from OpenAI
(code-davinci-002). This model was deprecated
by OpenAI after our submission making the repro-
duction of these experiments challenging despite
our best efforts, just like any other work using such
APIs. The trends discussed in the paper ( IRCoT
> OneR > NoR), we believe, would still hold.
Additionally, all our experiments using Flan-T5-*,
which exhibit similar trends as that of GPT3, will
remain reproducible, thanks to its publicly avail-
able model weights.
Ethical Considerations
Language models are known to hallucinate incor-
rect and potentially biased information. This is
especially problematic when the questions asked
to it are of a sensitive nature. While retrieval-
augmented approaches such as ours are expected
to alleviate this issue to some extent by grounding
generation in external text, this by no means solves
the problem of generating biased or offensive state-
ments. Appropriate care should thus be taken if
deploying such systems in user-facing applications.
All the datasets and models used in this work
are publicly available with permissible licenses.
HotpotQA has CC BY-SA 4.0 license 15, 2Wiki-
MultihopQA has Apache-2.0 license16, MuSiQUe
and IIRC have CC BY 4.0 license17, and Flan-T5-*
models have Apache-2.0 license.
Acknowledgments
We thank the reviewers for their valuable feedback
and suggestions. We also thank OpenAI for provid-
ing access to thecode-davinci-002 API. This ma-
terial is based on research supported in part by the
Air Force Research Laboratory (AFRL), DARPA,
for the KAIROS program under agreement number
FA8750-19-2-1003, in part by the National Science
Foundation under the award IIS #2007290, and in
part by an award from the Stony Brook Trustees
Faculty Awards Program.
References
Akari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi,
Richard Socher, and Caiming Xiong. 2020. Learning
to retrieve reasoning paths over wikipedia graph for
15https://creativecommons.org/licenses/by-sa/4.
0/
16https://www.apache.org/licenses/LICENSE-2.0
17https://creativecommons.org/licenses/by/4.0
question answering. In International Conference on
Learning Representations.
Sebastian Borgeaud, Arthur Mensch, Jordan Hoff-
mann, Trevor Cai, Eliza Rutherford, Katie Milli-
can, George Bm Van Den Driessche, Jean-Baptiste
Lespiau, Bogdan Damoc, Aidan Clark, Diego
De Las Casas, Aurelia Guy, Jacob Menick, Roman
Ring, Tom Hennigan, Saffron Huang, Loren Mag-
giore, Chris Jones, Albin Cassirer, Andy Brock,
Michela Paganini, Geoffrey Irving, Oriol Vinyals,
Simon Osindero, Karen Simonyan, Jack Rae, Erich
Elsen, and Laurent Sifre. 2022. Improving language
models by retrieving from trillions of tokens. In
Proceedings of the 39th International Conference
on Machine Learning, volume 162 of Proceedings
of Machine Learning Research , pages 2206–2240.
PMLR.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. Advances in neural information processing
systems, 33:1877–1901.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming
Yuan, Henrique Ponde de Oliveira Pinto, Jared Ka-
plan, Harri Edwards, Yuri Burda, Nicholas Joseph,
Greg Brockman, et al. 2021. Evaluating large
language models trained on code. arXiv preprint
arXiv:2107.03374.
Hyung Won Chung, Le Hou, Shayne Longpre, Bar-
ret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
2022. Scaling instruction-finetuned language models.
arXiv preprint arXiv:2210.11416.
Rajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer,
and Andrew McCallum. 2019. Multi-step retriever-
reader interaction for scalable open-domain question
answering. In International Conference on Learning
Representations.
Yair Feldman and Ran El-Yaniv. 2019. Multi-hop para-
graph retrieval for open-domain question answering.
In Proceedings of the 57th Annual Meeting of the As-
sociation for Computational Linguistics, pages 2296–
2309, Florence, Italy. Association for Computational
Linguistics.
James Ferguson, Matt Gardner, Hannaneh Hajishirzi,
Tushar Khot, and Pradeep Dasigi. 2020. IIRC: A
dataset of incomplete information reading compre-
hension questions. In EMNLP.
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat,
and Mingwei Chang. 2020. Retrieval augmented
language model pre-training. In Proceedings of the
37th International Conference on Machine Learning,
volume 119 of Proceedings of Machine Learning
Research, pages 3929–3938. PMLR.
10023Namgyu Ho, Laura Schmid, and Se-Young Yun. 2022.
Large language models are reasoning teachers. arXiv
preprint arXiv:2212.10071.
Xanh Ho, A. Nguyen, Saku Sugawara, and Akiko
Aizawa. 2020. Constructing a multi-hop qa dataset
for comprehensive evaluation of reasoning steps. In
COLING.
Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas
Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-
Yu, Armand Joulin, Sebastian Riedel, and Edouard
Grave. 2022. Atlas: Few-shot learning with re-
trieval augmented language models. arXiv preprint
arXiv:2208.03299.
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick
Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and
Wen-tau Yih. 2020. Dense passage retrieval for open-
domain question answering. In Proceedings of the
2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 6769–6781,
Online. Association for Computational Linguistics.
Jungo Kasai, Keisuke Sakaguchi, Yoichi Takahashi,
Ronan Le Bras, Akari Asai, Xinyan Yu, Dragomir
Radev, Noah A Smith, Yejin Choi, and Kentaro Inui.
2022. RealTime QA: What’s the answer right now?
arXiv preprint arXiv:2207.13332.
Omar Khattab, Christopher Potts, and Matei Zaharia.
2021. Baleen: Robust multi-hop reasoning at scale
via condensed retrieval. In Advances in Neural Infor-
mation Processing Systems.
Omar Khattab, Keshav Santhanam, Xiang Lisa Li,
David Hall, Percy Liang, Christopher Potts, and
Matei Zaharia. 2023. Demonstrate-search-predict:
Composing retrieval and language models for
knowledge-intensive NLP.
Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu,
Kyle Richardson, Peter Clark, and Ashish Sabharwal.
2022. Decomposed prompting: A modular approach
for solving complex tasks.
Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao
Fu, Kyle Richardson, Peter Clark, and Ashish Sab-
harwal. 2023. Decomposed prompting: A modular
approach for solving complex tasks. In The Eleventh
International Conference on Learning Representa-
tions.
Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-
taka Matsuo, and Yusuke Iwasawa. 2022. Large lan-
guage models are zero-shot reasoners. In ICML 2022
Workshop on Knowledge Retrieval and Language
Models.
Angeliki Lazaridou, Elena Gribovskaya, Wojciech
Stokowiec, and Nikolai Grigorev. 2022. Internet-
augmented language models through few-shot
prompting for open-domain question answering.
arXiv preprint arXiv:2203.05115.
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio
Petroni, Vladimir Karpukhin, Naman Goyal, Hein-
rich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-
täschel, Sebastian Riedel, and Douwe Kiela. 2020.
Retrieval-augmented generation for knowledge-
intensive nlp tasks. In Advances in Neural Infor-
mation Processing Systems, volume 33, pages 9459–
9474. Curran Associates, Inc.
Lucie Charlotte Magister, Jonathan Mallinson, Jakub
Adamek, Eric Malmi, and Aliaksei Severyn. 2022.
Teaching small language models to reason. arXiv
preprint arXiv:2212.08410.
Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,
Long Ouyang, Christina Kim, Christopher Hesse,
Shantanu Jain, Vineet Kosaraju, William Saunders,
et al. 2021. WebGPT: Browser-assisted question-
answering with human feedback. arXiv preprint
arXiv:2112.09332.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,
Carroll Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Gray, John
Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,
Maddie Simens, Amanda Askell, Peter Welinder,
Paul Christiano, Jan Leike, and Ryan Lowe. 2022.
Training language models to follow instructions with
human feedback. In Advances in Neural Information
Processing Systems.
Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,
Noah A Smith, and Mike Lewis. 2022. Measuring
and narrowing the compositionality gap in language
models. arXiv preprint arXiv:2210.03350.
Peng Qi, Xiaowen Lin, Leo Mehr, Zijian Wang, and
Christopher D. Manning. 2019. Answering complex
open-domain questions through iterative query gen-
eration. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natu-
ral Language Processing (EMNLP-IJCNLP), pages
2590–2602, Hong Kong, China. Association for Com-
putational Linguistics.
Stephen Robertson, Hugo Zaragoza, et al. 2009. The
probabilistic relevance framework: Bm25 and be-
yond. Foundations and Trends® in Information Re-
trieval, 3(4):333–389.
Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and
Denny Zhou. 2022. Recitation-augmented language
models. arXiv preprint arXiv:2210.01296.
Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia,
Jason Wei, Xuezhi Wang, Hyung Won Chung, Dara
Bahri, Tal Schuster, Steven Zheng, Denny Zhou, Neil
Houlsby, and Donald Metzler. 2023. UL2: Unifying
language learning paradigms. In The Eleventh Inter-
national Conference on Learning Representations.
Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot,
and Ashish Sabharwal. 2022. MuSiQue: Multi-
hop questions via single-hop question composition.
TACL, 10:539–554.
10024Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,
and Denny Zhou. 2022. Chain of thought prompt-
ing elicits reasoning in large language models. In
Advances in Neural Information Processing Systems.
Wenhan Xiong, Xiang Li, Srini Iyer, Jingfei Du, Patrick
Lewis, William Yang Wang, Yashar Mehdad, Scott
Yih, Sebastian Riedel, Douwe Kiela, and Barlas
Oguz. 2021. Answering complex open-domain ques-
tions with multi-hop dense retrieval. In International
Conference on Learning Representations.
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-
gio, William W. Cohen, Ruslan Salakhutdinov, and
Christopher D. Manning. 2018. HotpotQA: A dataset
for diverse, explainable multi-hop question answer-
ing. In EMNLP.
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak
Shafran, Karthik Narasimhan, and Yuan Cao. 2022.
ReAct: Synergizing reasoning and acting in language
models. arXiv preprint arXiv:2210.03629.
Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu,
Mingxuan Ju, Soumya Sanyal, Chenguang Zhu,
Michael Zeng, and Meng Jiang. 2023. Generate
rather than retrieve: Large language models are
strong context generators. In The Eleventh Inter-
national Conference on Learning Representations.
Fengbin Zhu, Wenqiang Lei, Chao Wang, Jianming
Zheng, Soujanya Poria, and Tat-Seng Chua. 2021.
Retrieving and reading: A comprehensive survey on
open-domain question answering. arXiv preprint
arXiv:2101.00774.
10025A Constructing Retrieval Corpora
HotpotQA already comes with the associated
Wikipedia corpus for the open-domain setting,
so we use it directly. 2WikiMultihopQA and
MuSiQue, however, are originally reading com-
prehension datasets. Questions in 2WikiMulti-
hopQA and MuSiQue are associated with 10 and 20
paragraphs respectively, 2-4 of which are support-
ing and others are non-supporting. To turn these
datasets into an open-domain setting, we make two
corpora, one for each dataset, by combining all
supporting and non-supporting paragraphs for all
its questions in the train, development, and test
sets. IIRC is originally a mix between reading
comprehension and an open-domain setting. Each
question is grounded in one main paragraph, which
contains links to multiple Wikipedia pages with
several paragraphs each. We create a corpus out
of all the paragraphs from all the Wikipedia pages
present in the dataset.18 We do assume the avail-
ability of the main passage which doesn’t need
to be retrieved and is always present. We don’t
assume the availability of Wikipedia links in the
main passage, however, to keep the retrieval prob-
lem challenging.19
B Special Handling of Models for IIRC
IIRC is slightly different from the other datasets,
in that the question is grounded in the main pas-
sage and other supporting paragraphs come from
the Wikipedia pages of entities mentioned in this
passage. We modify the retrievers and readers to
account for this difference: (i) We always keep the
main passage as part of the input to the model re-
gardless of the retrieval strategy used. (ii) For all
the retrieval methods, we first prompt the model to
generate a list of Wikipedia page titles using the
main passage and the question. We map these gen-
erated titles to the nearest Wikipedia page titles in
the corpus (found using BM25), and then the rest
of the paragraph retrieval queries are scoped within
only those Wikipedia pages.
To prompt the model to generate Wikipedia page
titles using the main passage and the question for
18Following are the corpus sizes for the datasets: Hot-
potQA (5,233,329), 2WikiMultihopQA (430,225), MuSiQue
has (139,416), and IIRC (1,882,415)
19IIRC corpus has a positional bias, i.e., the majority of sup-
porting paragraphs are always within the first few positions of
the Wikipedia page. To keep the retrieval problem challenging
enough we shuffle the paragraphs before indexing the corpus,
i.e., we don’t use positional information in any way.
IIRC, we use the following template.
Wikipedia Title: <Main Page Title>
<Main Paragraph Text>
Q: The question is: '<Question>'. Generate titles
of <N> Wikipedia pages that have relevant
information to answer this question.
A: ["<Title-1>", "<Title-2>", ...]
For “training”, i.e., for demonstrations, N (≤3)
is the number of supporting Wikipedia page titles
for the question. At test time, since the number
of supporting page titles is unknown, we use a
fixed value of 3. We found this trick of prompting
the model to generate more titles at the test time
improves its recall over letting the model decide by
itself how many titles to generate.
C Comparison with Previous Systems for
ODQA with LLMs
We showed a leaderboard-style comparison with
previous approaches to using large language mod-
els for open-domain QA in § 5. We noted though
that the comparison is not head-to-head given vari-
ous differences. We briefly describe each method
and the differences in API, LLM, retrieval corpus,
and other choices here.
Internet-Augmented QA (Lazaridou et al., 2022)
does (one-step) Google Search retrieval, performs
additional LLM-based filtering on it, and then
prompts an LLM to answer the question using
the resulting context. It uses the Gopher 280B
language model. RECITE (Sun et al., 2022) by-
passes the retrieval and instead prompts an LLM
to first generate (recite) one or several relevant pas-
sages from its own memory, and generate the an-
swer conditioned on this generation. They exper-
iment with many LLMs, the highest performing
of which is code-davinci-002 which we report
here. ReAct (Yao et al., 2022) prompts LLMs to
produce reasoning and action traces where actions
are calls to a Wikipedia API to return the sum-
mary for a given Wikipedia page title. It uses
the PALM 540B model. SelfAsk (Press et al.,
2022) prompts LLMs to decompose a question
into subquestions and answers these subquestions
by issuing separate calls to the Google Search
API. It uses the GPT3 (text-davinci-002) model.
Finally, DecomP (Khot et al., 2023) is a gen-
eral framework that decomposes a task and del-
egates sub-tasks to appropriate sub-models. Sim-
ilar to our system, it uses BM25 Search and the
GPT3 ( code-davinci-002) model. And lastly,
10026Model HpQA Br HpQA 2WikiMQA MQ 2H MQ
InterAug (Lazaridou et al., 2022) −| − 30.3 | − − | − − | − − | −
RECITE (Sun et al., 2022) −| − 37.1 | 48.4 −| − − | − − | −
ReAct (Yao et al., 2022) −| − 35.1 | − − | − − | − − | −
SelfAsk (Press et al., 2022) −| − − | − 40.1 | − 15.2 | − − | −
DecomP (Khot et al., 2022) −| 50.0 −| − − | 59.3 −| − − | −
DecomP (Khot et al., 2023) * −| − − | 53.5 −| 70.8 −| − − | 30.9
DSP (Khattab et al., 2023) * −| − 51.4 | 62.9 −| − − | − − | −
IRCoT QA (ours) 45.8 | 58.5 49.3 | 60.7 57.7 | 68.0 34.2 | 43.8 26.5 | 36.5
Table 3: Extended comparison with published LLM-based ODQA systems (as of May 25, 2023) on EM and
F1 scores (with new numbers marked with *). ‘ −’: score is unavailable. HpQA Br: Bridge questions subset of
HotpotQA. MQ2H: MuSiQue 2-hop questions. IRCoT remains SOTA for MuSiQue and is close to SOTA for
HotpotQA and 2WikiMultihopQA. Note the comparisons here are not head-to-head as discussed in the text.
Flan-T5-XXL GPT3
Model HotpotQA 2WikiMQA MuSiQue IIRC HotpotQA 2WikiMQA MuSiQue IIRC
ZeroR QA Direct 25.3± 0.3 32.7± 0.3 13.7± 0.3 28.9± 0.3 41.0± 1.1 38.5± 1.1 19.0± 1.2 40.9± 0.7
CoT 22.9 ± 0.1 31.7± 1.5 10.3± 0.5 24.4± 0.1 47.5± 0.4 41.2± 1.0 25.2± 1.2 52.1± 0.1
OneR QA Direct 49.7± 0.5 51.2± 0.3 25.8± 0.6 40.0± 1.3 50.7± 0.1 46.4± 2.9 20.4± 0.3 40.1± 0.9
CoT 43.1 ± 0.7 47.8± 0.9 17.6± 0.2 34.5± 1.5 53.6± 0.7 54.8± 2.1 29.4± 0.8 49.8± 2.3
IRCoT QA Direct 59.1± 0.9 66.5± 1.4 30.8± 0.2 42.5± 2.1 60.6± 1.0 63.5± 2.7 36.0± 0.5 47.9± 2.3
CoT 52.0 ± 0.6 55.1± 1.0 24.9± 1.0 36.5± 1.3 60.7± 1.1 68.0± 1.5 36.5± 1.2 49.9± 1.1
Table 4: Answer F1 for different ODQA models made from NoR, One and IRCoT retrievals, and Direct and
CoT prompting readers. For Flan-T5-XXL, Direct prompting is a better choice for the reader, and for GPT3, CoT
prompting is a better choice for the reader. Hence, we make different reader choices for Flan-T5 and GPT3 for the
experiments in the main paper. Note that IRCoT QA > OneR QA > ZeroR QA holds up regardless of this choice.
DSP (Khattab et al., 2023) provides a way to pro-
grammatically define interactions between LLM
and retrieval for ODQA (e.g., via question decom-
position), bootstrap demonstrations for such a pro-
gram, and use them to make the answer prediction.
It uses GPT3.5 LLM with ColBERT-based retrieval.
Since most of these methods use different knowl-
edge sources or APIs and are built using different
LLMs and retrieval models, it’s difficult to make a
fair scientific comparison across these systems. Ad-
ditionally, the evaluations in the respective papers
are on different random subsets (from the same
distribution) of test instances.
Despite these differences, it is still informative to
explore, in a leaderboard-style fashion, howIRCoT
performs relative to the best numbers published
for these recent systems. Table 3 shows results
from different systems, including contemporane-
ous and newer numbers. The two new systems in
this table (relative to Table 1) are DecomP (newer
version) and DSP. While IRCoT remains SOTA on
MuSiQue, DSP outperforms it on HotpotQA by 2.0
points and the newer version of Decomp outper-
forms IRCoT on 2WikiMultihopQA by 2.8 points.
We speculate DecomP performs well on 2WikiMul-
tihopQA because it has only a few easy-to-predict
decomposition patterns, which DecomP’s question
decomposition can leverage. The lack of such pat-
terns in HotpotQA and MuSiQue causes it to un-
derperform compared to IRCoT. Lastly, it will be
useful to assess whether DSP, which is hardcoded
for 2-hop questions like that of HotpotQA, will
work well for a dataset with a varied number of
hops like that of MuSiQue. We leave this further
investigation to future work.
D Additional CoT Generation Examples
Table 5 provides illustrations, in addition to the
ones provided in Table 2, for how the CoT gen-
erations for NoR QA, OneR QA, and IRCoT QA
methods vary. This gives an insight into how IR-
CoT improves QA performance. Since NoR re-
lies completely on parametric knowledge, it often
makes a factual error in the first sentence, which de-
rails the full reasoning chain. Some of this factual
information can be fixed by OneR, especially infor-
mation closest to the question (i.e., can be retrieved
using the question). This is insufficient for fixing
10027Question Gold Facts NoR QA OneR QA IRCoT QA
HotpotQA
What was the
island, on
which Marinelli
Glacier is lo-
cated, formerly
known as?
⇒Marinelli Glacier is
located on the island of
Tierra del Fuego.
⇒The island of Tierra
del Fuego was formerly
known as Isla de Xativa.
Marinelli Glacier is
located on the island of
Graham Land. Graham
Land was formerly
known as Graham’s
Land. So the answer is:
Graham’s Land.
Marinelli Glacier is lo-
cated on Isla Grande
de Tierra del Fuego.
Isla Grande de Tierra
del Fuego was formerly
known as Tierra del
Fuego. So the answer
is: Tierra del Fuego.
Marinelli Glacier is lo-
cated on the island of
Tierra del Fuego. The is-
land of Tierra del Fuego
was formerly known as
Isla de Xativa. So the
answer is: Isla de Xa-
tiva.
2WikiMQA
Who is the
child of the
director of film
Mukhyamantri
(1996 Film)?
⇒ The director of
Mukhyamantri (1996
film) is Anjan Choud-
hury.
⇒ Anjan Choudhury
has a child named
Sandip Choudhury.
The film Mukhyamantri
was directed by S. V . Ra-
jendra Singh Babu. S.
V . Rajendra Singh Babu
has a child named S.
V . Rajendra Singh Babu.
So the answer is: S. V .
Rajendra Singh Babu.
Mukhyamantri (1996
film) was directed
by Anjan Choudhury.
Anjan Choudhury has a
child named Aniruddha
Choudhury. So the
answer is: Aniruddha
Choudhury.
The director of Mukhya-
mantri (1996 film)
is Anjan Choudhury.
Anjan Choudhury has
a child named Sandip
Choudhury. So the
answer is: Sandip
Choudhury.
MuSiQue
When did the
Governorship
end of the
city where the
author of Sac-
erdotii Nostri
Primordia died?
⇒Sacerdotii Nostri Pri-
mordia was written by
Pope John XXIII.
⇒ Pope John XXIII
died in Vatican City.
⇒The Governorship of
Vatican City ended in
1952.
The author of Sacer-
dotii Nostri Primordia is
Pope Pius IX. Pope Pius
IX died in the city of
Rome. The Governor-
ship of Rome ended in
1870. So the answer is:
1870.
Sacerdotii Nostri Pri-
mordia was written by
Pope John XXIII. Pope
John XXIII died in the
city of Rome. The
Governorship of Rome
ended in 1870. So the
answer is: 1870.
Sacerdotii Nostri Pri-
mordia was written by
Pope John XXIII. Pope
John XXIII died in Vat-
ican City. The Gover-
norship of Vatican City
ended in 1952. So the
answer is: 1952.
Table 5: Additional CoTs generated by GPT3 with different methods. ZeroR is most prone to factual errors. OneR
often fixes some of the factual information which is closest to the question but doesn’t always fix it all the way.
Since IRCoT retrieves after each step, it can also fix the errors at each step. More examples are in Table 2.
all the mistakes. Since IRCoT involves retrieval
after each step, it can fix errors at each step.
E Direct vs CoT Prompting Readers
Table 4 compares reader choice (Direct vs CoT
Prompting) for Flan-T5-XXL and GPT3. We find
that Flan-T5-XXL works better with Direct Prompt-
ing as a reader and GPT3 works better with CoT
Prompting as a reader. Therefore, for the experi-
ments in the main paper, we go with this choice.
Note though that the trends discussed in § 5 (IRCoT
QA > OneR QA > ZeroR QA) hold regardless of
the choice of the reader.
F Separate Reader in IRCoTQA
IRCoT, by construction, produces a CoT as a part
of its retrieval process. So, instead of having a sep-
arate post-hoc reader, one can also just extract the
answer from the CoT generated during retrieval.
As Table 6 shows the effect of such an ablation.
For Flan-T5-XXL having a separate reader is sig-
nificantly better. For GPT3, this is not always true,
but at least a model with a separate reader is al-
ways better or close to the one without. So overall
we go with the choice of using the reader for the
experiments in this paper.
Model HotpotQA 2WikiMQA MuSiQue IIRC
Flan
IRCoT QA 59.1± 0.9 66.5± 1.4 30.8± 0.2 42.5± 2.1
w/o reader 52.6 ± 0.3 60.9± 0.6 24.9± 0.2 40.3± 0.2
GPT3
IRCoT QA 60.7 ± 1.1 68.0± 1.5 36.5± 1.2 49.9± 1.1
w/o reader 61.0± 0.7 70.4± 1.5 31.5± 0.6 48.4± 1.0
Table 6: Answer F1 of IRCoT QA with and without
a separate reader for Flan-T5-XXL (top two rows) and
GPT3 (bottom two rows). When the reader is not used,
the answer is extracted from the CoT generated by
IRCoT while doing the retrieval. Ablating the reader
usually hurts the performance.
G Prompts
Our manually written chain-of-thought annotations
for HotpotQA, 2WikiMultihopQA, MuSiQue, and
IIRC are given in Listing 1, 2, 3 and 4 respec-
tively. Our prompts for GPT3 CoT Prompting are
the same as these, except they have Wikipipedia
paragraphs on the top of the questions as shown
in § 3.120. Our prompts for GPT3 Direct Prompt-
ing are the same as that of CoT prompting, except
have the answer after "A:" directly. Our prompts
for Flan-T5-* are slightly different from that of
GPT3. For CoT Prompting, we prefix the question
20We are not showing the paragraphs in the paper for brevity
but they can be obtained from the released code.
10028line: "Q: Answer the following question by reason-
ing step-by-step. <actual-question>". For Direct
Prompting, we prefix the question line: "Q: Answer
the following question. <actual-question>". We
did this to follow Flan-T5-*’s training format and
found it to help its CoT generation.
10029Listing 1: Chain-of-Thought annotations for HotpotQA.
Q: Jeremy Theobald and Christopher Nolan share what profession?
A: Jeremy Theobald is an actor and producer. Christopher Nolan is a director, producer, and screenwriter. Therefore, they
both share the profession of being a producer. So the answer is: producer.
Q: What film directed by Brian Patrick Butler was inspired by a film directed by F.W. Murnau?
A: Brian Patrick Butler directed the film The Phantom Hour. The Phantom Hour was inspired by the films such as Nosferatu
and The Cabinet of Dr. Caligari. Of these Nosferatu was directed by F.W. Murnau. So the answer is: The Phantom Hour.
Q: How many episodes were in the South Korean television series in which Ryu Hye−young played Bo−ra?
A: The South Korean television series in which Ryu Hye−young played Bo−ra is Reply 1988. The number of episodes Reply
1988 has is 20. So the answer is: 20.
Q: Vertical Limit stars which actor who also played astronaut Alan Shepard in "The Right Stuff"?
A: The actor who played astronaut Alan Shepard in "The Right Stuff" is Scott Glenn. The movie Vertical Limit also starred
Scott Glenn. So the answer is: Scott Glenn.
Q: What was the 2014 population of the city where Lake Wales Medical Center is located?
A: Lake Wales Medical Center is located in the city of Polk County, Florida. The population of Polk County in 2014 was
15,140. So the answer is: 15,140.
Q: Who was born first? Jan de Bont or Raoul Walsh?
A: Jan de Bont was born on 22 October 1943. Raoul Walsh was born on March 11, 1887. Thus, Raoul Walsh was born the
first. So the answer is: Raoul Walsh.
Q: In what country was Lost Gravity manufactured?
A: The Lost Gravity (roller coaster) was manufactured by Mack Rides. Mack Rides is a German company. So the answer is:
Germany.
Q: Which of the following had a debut album entitled "We Have an Emergency": Hot Hot Heat or The Operation M.D.?
A: The debut album of the band "Hot Hot Heat" was "Make Up the Breakdown". The debut album of the band "The Operation
M.D." was "We Have an Emergency". So the answer is: The Operation M.D..
Q: In which country did this Australian who was detained in Guantanamo Bay detention camp and published "Guantanamo:
My Journey" receive para−military training?
A: The Australian who was detained in Guantanamo Bay detention camp and published "Guantanamo: My Journey" is David
Hicks. David Hicks received his para−military training in Afghanistan. So the answer is: Afghanistan.
Q: Does The Border Surrender or Unsane have more members?
A: The Border Surrender band has following members: Keith Austin, Simon Shields, Johnny Manning and Mark Austin. That
is, it has 4 members. Unsane is a trio of 3 members. Thus, The Border Surrender has more members. So the answer is:
The Border Surrender.
Q: Which band formed first, Sponge Cola or Hurricane No. 1?
A: Sponge Cola band was formed in 1998. Hurricane No. 1 was formed in 1996. Thus, Hurricane No. 1 band formed the first.
So the answer is: Hurricane No. 1.
Q: James Paris Lee is best known for investing the Lee−Metford rifle and another rifle often referred to by what acronymn?
A: James Paris Lee is best known for investing the Lee−Metford rifle and Leeâ˘A¸ SEnfield series of rifles. Leeâ˘A¸ SEnfield is
often referred to by the acronym of SMLE. So the answer is: SMLE.
Q: Who was born first, James D Grant, who uses the pen name of Lee Child, or Bernhard Schlink?
A: James D Grant, who uses the pen name of Lee Child, was born in 1954. Bernhard Schlink was born in 1944. Thus,
Bernhard Schlink was born first. So the answer is: Bernhard Schlink.
Q: Which American neo−noir science fiction has Pierce Gagnon starred?
A: Pierce Gagnon has starred in One Tree Hill, Looper, Wish I Was Here and Extant. Of these, Looper is an American neo−
noir science fiction. So the answer is: Looper.
Q: What year did Edburga of Minster−in−Thanet's father die?
A: The father of Edburga of Minster−in−Thanet is King Centwine. Centwine died after 685. So the answer is: after 685.
Q: Were Lonny and Allure both founded in the 1990s?
A: Lonny (magazine) was founded in 2009. Allure (magazine) was founded in 1991. Thus, of the two, only Allure was
founded in 1990s. So the answer is: no.
Q: The actor that stars as Joe Proctor on the series "Power" also played a character on "Entourage" that has what last name?
10030A: The actor that stars as Joe Proctor on the series "Power" is Jerry Ferrara. Jerry Ferrara also played a character on Entourage
named Turtle Assante. Thus, Turtle Assante's last name is Assante. So the answer is: Assante.
Q: Nobody Loves You was written by John Lennon and released on what album that was issued by Apple Records, and was
written, recorded, and released during his 18 month separation from Yoko Ono?
A: The album issued by Apple Records, and written, recorded, and released during John Lennon's 18 month separation from
Yoko Ono is Walls and Bridges. Nobody Loves You was written by John Lennon on Walls and Bridges album. So the
answer is: Walls and Bridges.
Q: How many awards did the "A Girl Like Me" singer win at the American Music Awards of 2012?
A: The singer of "A Girl Like Me" singer is Rihanna. In the American Music Awards of 2012, Rihana won one award. So the
answer is: one.
Q: who is older Jeremy Horn or Renato Sobral ?
A: Jeremy Horn was born on August 25, 1975. Renato Sobral was born on September 7, 1975. Thus, Jeremy Horn is older. So
the answer is: Jeremy Horn.
Listing 2: Chain-of-Thought annotations for 2WikiMultihopQA.
Q: When did the director of film Hypocrite (Film) die?
A: The film Hypocrite was directed by Miguel Morayta. Miguel Morayta died on 19 June 2013. So the answer is: 19 June
2013.
Q: Do director of film Coolie No. 1 (1995 Film) and director of film The Sensational Trial have the same nationality?
A: Coolie No. 1 (1995 film) was directed by David Dhawan. The Sensational Trial was directed by Karl Freund. David
Dhawan's nationality is India. Karl Freund's nationality is Germany. Thus, they do not have the same nationality. So the
answer is: no.
Q: Are both Kurram Garhi and Trojkrsti located in the same country?
A: Kurram Garhi is located in the country of Pakistan. Trojkrsti is located in the country of Republic of Macedonia. Thus,
they are not in the same country. So the answer is: no.
Q: Who was born first out of Martin Hodge and Ivania Martinich?
A: Martin Hodge was born on 4 February 1959. Ivania Martinich was born on 25 July 1995. Thus, Martin Hodge was born
first. So the answer is: Martin Hodge.
Q: Which film came out first, The Night Of Tricks or The Genealogy?
A: The Night of Tricks was published in the year 1939. The Genealogy was published in the year 1979. Thus, The Night of
Tricks came out first. So the answer is: The Night Of Tricks.
Q: When did the director of film Laughter In Hell die?
A: The film Laughter In Hell was directed by Edward L. Cahn. Edward L. Cahn died on August 25, 1963. So the answer is:
August 25, 1963.
Q: Which film has the director died later, The Gal Who Took the West or Twenty Plus Two?
A: The film Twenty Plus Two was directed by Joseph M. Newman. The Gal Who Took the West was directed by Frederick de
Cordova. Joseph M. Newman died on January 23, 2006. Fred de Cordova died on September 15, 2001. Thus, the person
to die later from the two is Twenty Plus Two. So the answer is: Twenty Plus Two.
Q: Who is Boraqchin (Wife Of Ã˝Ugedei)'s father−in−law?
A: Boraqchin is married to Ã ˝Ugedei Khan. Ã ˝Ugedei Khan's father is Genghis Khan. Thus, Boraqchin's father−in−law is
Genghis Khan. So the answer is: Genghis Khan.
Q: What is the cause of death of Grand Duke Alexei Alexandrovich Of Russia's mother?
A: The mother of Grand Duke Alexei Alexandrovich of Russia is Maria Alexandrovna. Maria Alexandrovna died from
tuberculosis. So the answer is: tuberculosis.
Q: Which film has the director died earlier, When The Mad Aunts Arrive or The Miracle Worker (1962 Film)?
A: When The Mad Aunts Arrive was directed by Franz Josef Gottlieb. The Miracle Worker (1962 film) was directed by
Arthur Penn. Franz Josef Gottlieb died on 23 July 2006. Arthur Penn died on September 28, 2010. Thus, of the two, the
director to die earlier is Franz Josef Gottlieb, who directed When The Mad Aunts Arrive. So the answer is: When The
Mad Aunts Arrive.
Q: Which album was released earlier, What'S Inside or Cassandra'S Dream (Album)?
A: What's Inside was released in the year 1995. Cassandra's Dream (album) was released in the year 2008. Thus, of the two,
the album to release earlier is What's Inside. So the answer is: What's Inside.
Q: Are both mountains, Serre Mourene and Monte Galbiga, located in the same country?
A: Serre Mourene is located in Spain. Monte Galbiga is located in Italy. Thus, the two countries are not located in the same
10031country. So the answer is: no.
Q: What is the date of birth of the director of film Best Friends (1982 Film)?
A: The film Best Friends was directed by Norman Jewison. Norman Jewison was born on July 21, 1926. So the answer is:
July 21, 1926.
Q: Which film has the director born first, Two Weeks With Pay or Chhailla Babu?
A: Two Weeks with Pay was directed by Maurice Campbell. Chhailla Babu was directed by Joy Mukherjee. Maurice
Campbell was born on November 28, 1919. Joy Mukherjee was born on 24 February 1939. Thus, from the two directors,
Chhailla Babu was born first, who directed Two Weeks With Pay. So the answer is: Two Weeks With Pay.
Q: Who is the grandchild of Krishna Shah (Nepalese Royal)?
A: Krishna Shah has a child named Rudra Shah. Rudra Shah has a child named Prithvipati Shah. Thus, Krishna Shah has a
grandchild named Prithvipati Shah. So the answer is: Prithvipati Shah.
Q: When was the director of film P.S. Jerusalem born?
A: P.S. Jerusalem was directed by Danae Elon. Danae Elon was born on December 23, 1970. So the answer is: December 23,
1970.
Q: Which album was released more recently, If I Have to Stand Alone or Answering Machine Music?
A: If I Have to Stand Alone was published in the year 1991. Answering Machine Music was released in the year 1999. Thus,
of the two, the album to release more recently is Answering Machine Music. So the answer is: Answering Machine
Music.
Q: Where did the director of film Maddalena (1954 Film) die?
A: The film Maddalena is directed by Augusto Genina. Augusto Genina died in Rome. So the answer is: Rome.
Q: When did the director of film The Boy And The Fog die?
A: The director of The Boy and the Fog is Roberto GavaldÃ¸ sn. Roberto GavaldÃ¸ sn died on September 4, 1986. So the answer
is: September 4, 1986.
Q: Are the directors of films The Sun of the Sleepless and Nevada (1927 film) both from the same country?
A: The director of Sun of the Sleepless is Temur Babluani. The director of Nevada (1927 film) is John Waters. John Waters is
from the country of America. Temur Babluani is from the country of Georgia. Thus, John Walters and Temur Babluani
are not from the same country. So the answer is: no.
Listing 3: Chain-of-Thought annotations for MuSiQue.
Q: When did the first large winter carnival take place in the city where CIMI−FM is licensed to broadcast?
A: CIMI−FM is licensed to broadcast in Quebec City. The first large winter carnival in Quebec City took place in 1894. So
the answer is: 1894.
Q: When was Neville A. Stanton's employer founded?
A: The employer of Neville A. Stanton is University of Southampton. The University of Southampton was founded in 1862.
So the answer is: 1862.
Q: What county is Hebron located in, in the same province the Heritage Places Protection Act applies to?
A: Heritage Places Protection Act applies to the jurisdiction of Prince Edward Island. Hebron, Prince Edward Island is located
in the Prince County. So the answer is: Prince County.
Q: What weekly publication in the Connecticut city with the most Zagat rated restaurants is issued by university of America−
Lite: How Imperial Academia Dismantled Our Culture's author?
A: The author of America−Lite: How Imperial Academia Dismantled Our Culture is David Gelernter. David Gelernter was
educated at the Yale University. The city in Connecticut that has the highest number of Zagat−rated restaurants is New
Haven. The weekly publication in New Haven that is issued by Yale University is Yale Herald. So the answer is: Yale
Herald.
Q: What is the headquarters for the organization who sets the standards for ISO 21500?
A: The standards for ISO 21500 were set by International Organization for Standardization. The International Organization
for Standardization has headquarters in Geneva. So the answer is: Geneva.
Q: What did the publisher of Banjo−Tooie rely primarily on for its support?
A: The publisher of Banjo−Tooie is Nintendo. Nintendo relied primarily for its support on first−party games. So the answer is:
first−party games.
Q: In which county was the birthplace of the Smoke in tha City performer?
A: The performer of Smoke in tha City is MC Eiht. MC Eiht's birthplace is Compton. Compton is located in the county of Los
Angeles County. So the answer is: Los Angeles County.
Q: What region of the state where Guy Shepherdson was born, contains SMA Negeri 68?
10032A: Guy Shepherdson was born in Jakarta. SMA Negeri 68 Jakarta is located in Central Jakarta. So the answer is: Central
Jakarta.
Q: When did Britain withdraw from the country containing Hoora?
A: Hoora is in the country of Bahrain. Britain withdrew from Bahrain in 1971. So the answer is: 1971.
Q: Where does the Snake River start, in the state where Lima Mountain is located?
A: Lima Mountain is located in the state of Minnesota. The snake river in Minnesota starts in southern Aitkin County. So the
answer is: southern Aitkin County.
Q: What shares a border with RiviÃ´lre−Verte in the province WRSU−FM broadcasts in?
A: WRSU−FM was licensed to broadcast to New Brunswick. RiviÃ´lre−Verte, New Brunswick shares border with
Edmundston. So the answer is: Edmundston.
Q: When was the state of emergency declared in the country where the Senate is located?
A: The Senate is in the country of Kenya. The state of emergency was declared in Kenya on 20 October 1952. So the answer
is: 20 October 1952.
Q: How long is the US border with the country that borders the state where Finding Dory takes place?
A: Finding Dory is supposed to take place in California. The country that shares a border with California is Mexico. The
length of the us border with Mexico is 1,989 mi. So the answer is: 1,989 mi.
Q: What genre is the record label of the performer of So Long, See You Tomorrow associated with?
A: The performer of So Long, See You Tomorrow is Bombay Bicycle Club. The record label of Bombay Bicycle Club is
Island Records. The genre of Island Records is jazz. So the answer is: jazz.
Q: When did the first large winter carnival happen in Olivier Robitaille's place of birth?
A: Olivier Robitaille was born in Quebec City. The first large winter carnival in Quebec City happened in the 1894. So the
answer is: 1894.
Q: What is the genre of the record label of the band that performed on the Crush Tour?
A: The Crush Tour is performed by the band Bon Jovi. The record label of Bon Jovi is Island Records. The genre of Island
Records is jazz. So the answer is: jazz.
Q: When was the first railway line constructed between Kotri and the city where Marie Adelaide Leprosy Centre is located?
A: Marie Adelaide Leprosy Centre is located in Karachi. The first railway line between Kotri and Karachi was constructed in
April 1858. So the answer is: April 1858.
Q: Where is the crying stone found in the country in which Raphael Tuju holds citizenship?
A: Raphael Tuju is a citizen of Kenya. The crying stone in Kenya is found along the highway towards Kisumu. So the answer
is: along the highway towards Kisumu.
Q: When did Britain withdraw from the country where the village of Wadyan is found?
A: Wadyan is in the country of Bahrain. Britain withdraw from Bahrain in 1971. So the answer is: 1971.
Q: How many countries in Pacific National University's continent are recognized by the organization that mediated the truce
ending the Iran−Iraq war?
A: Pacific National University is located in Khabarovsk, Russia Khabarovsk, Russian is in the continent of Asia. The entity
that mediated the truce which ended the Iran−Iraq War is the UN. The number of member states that UN recognises in
Asia is 53. So the answer is: 53.
Listing 4: Chain-of-Thought annotations for IIRC.
Q: What is the age difference between the kicker and the quarterback for the Chargers?
A: The kicker for the Chargers is Nate Kaeding. The quarterback (QB) for the Chargers is Philip Rivers. Nate Kaeding was
born in the year 1982. Philip Rivers was born in the year 1981. Thus, the age difference between them is of 1 year. So
the answer is: 1.
Q: How many years was the ship that took the battalion from New South Wales to Ceylon in service?
A: The ship that took the battalion from New South Wales to Ceylon is General Hewitt. General Hewitt was launched in
Calcutta in 1811. General Hewitt was sold for a hulk or to be broken up in 1864. So she served for a total of 1864 −
1811 = 53 years. So the answer is: 53.
Q: What year was the theatre that held the 2016 NFL Draft built?
A: The theatre that held the 2016 NFL Draft is Auditorium Theatre. The Auditorium Theatre was built in 1889. So the answer
is: 1889.
Q: How long had Milan been established by the year that Nava returned there as a reserve in the first team's defense?
A: Nava returned to Milan as a reserve in the first team's defense in the year 1990. Milan had been established in the year
100331899. Thus, Milan had been established for 1990 − 1899 = 91 years when Milan returned to Milan as a reserve in the
first team's defense. So the answer is: 91.
Q: When was the town Scott was born in founded?
A: Scott was born in the town of Cooksville, Illinois. Cooksville was founded in the year 1882. So the answer is: 1882.
Q: In what country did Wright leave the French privateers?
A: Wright left the French privateers in Bluefield's river. Bluefields is the capital of the South Caribbean Autonomous Region (
RAAS) in the country of Nicaragua. So the answer is: Nicaragua.
Q: Who plays the A−Team character that Dr. Hibbert fashioned his hair after?
A: Dr. Hibbert fashioned his hair after Mr. T from The A−Team. Mr T.'s birthname is Lawrence Tureaud. So the answer is:
Lawrence Tureaud.
Q: How many people attended the conference held near Berlin in January 1942?
A: The conference held near Berlin in January 1942 is Wannsee Conference. Wannsee Conference was attended by 15 people.
So the answer is: 15.
Q: When did the country Ottwalt went into exile in founded?
A: Ottwalt went into exile in the country of Denmark. Denmark has been inhabited since around 12,500 BC. So the answer is:
12,500 BC.
Q: When was the J2 club Uki played for in 2001 founded?
A: The J2 club that Uki played for is Montedio Yamagata. Montedio Yamagata was founded in 1984. So the answer is: 1984.
Q: When was the person who produced A Little Ain't Enough born?
A: A Little Ain't Enough was produced by Bob Rock. Bob Rock was born on April 19, 1954. So the answer is: April 19, 1954.
Q: Which of the schools Fiser is affiliated with was founded first?
A: The schools that Fiser is affiliated with (1) Academy of Music, University of Zagreb (2) Mozarteum University of Salzburg
(3) Croatian Music Institute orchestra. Academy of Music, University of Zagreb was founded in the year 1829.
Mozarteum University of Salzburg was founded in the year 1841. Croatian Music Institute was founded in the year 1827.
Thus, the school founded earliest of these is Croatian Music Institute. So the answer is: Croatian Music Institute.
Q: How many casualties were there at the battle that Dearing fought at under Jubal Early?
A: Under Jubal Early, Dearing fought the First Battle of Bull Run. First Battle of Bull Run has 460 union casualties and 387
confederate casualties. Thus, in total the First Battle of Bull Run had 460 + 387 = 847 casualties. So the answer is: 847.
Q: Which of the two congregations which provided leadership to the Pilgrims was founded first?
A: The congregations which provided leadership to the Pilgrims are Brownists and Separatist Puritans. Brownist was founded
in 1581. The Separatist Puritans was founded in 1640. Thus, Brownist was founded first. So the answer is: Brownist.
Q: How long had the Rock and Roll Hall of Fame been open when the band was inducted into it?
A: The band was inducted into Rock and Roll Hall of Fame in the year 2017. Rock and Roll Hall of Fame was established in
the year of 1983. Thus, Rock and Roll Hall of Fame been open for 2018 − 1983 = 34 years when the band was inducted
into it. So the answer is: 34.
Q: Did the Lord Sewer who was appointed at the 1509 coronation live longer than his king?
A: Lord Sewer who was appointed at the 1509 coronation was Robert Radcliffe, 1st Earl of Sussex. Lord Sever's king in 1509
was Henry VIII of England. Robert Radcliffe, 1st Earl of Sussex was born in the year 1483, and died in the year 1542.
So Robert lived for 1542 − 1483 = 59 years. Henry VIII of England was born in the year 1491 and died in the year 1547.
So Henry VIII lived for 1547 − 1491 = 56 years. Thus, Robert Radcliffe lived longer than Henry VIII. So the answer is:
yes.
Q: When was the place near where Manuchar was defeated by Qvarqvare established?
A: Manuchar was defeated by Qvarqvare near Erzurum. Erzurum was founded during the Urartian period. So the answer is:
Urartian period.
Q: What year was the man who implemented the 46 calendar reform born?
A: The man who implemented the 46 calendar reform is Julius Caesar. Julius Caesar was born in the year 100 BC. So the
answer is: 100 BC.
Q: How many years after the first recorded Tommy John surgery did Scott Baker undergo his?
A: The first recorded Tommy John surgery happened when it was invented in the year 1974. Scott Baker underwent Tommy
John surgery in the year 2012. Thus, Scott Baker underwent Tommy John surgery 2012 − 1974 = 38 years after it was
first recorded. So the answer is: 38.
Q: Which was the older of the two players who found the net in the Double−Headed Eagle of the North in the sixth final for
PAOK?
10034A: The two players who found the net in the Double−Headed Eagle of the North in the sixth final for PAOK are Koudas and
Matzourakis. Koudas was born on 23 November 1946. Matzourakis was born on 6 June 1949. Thus, the older person
among the two is Koudas. So the answer is: Koudas.
10035ACL 2023 Responsible NLP Checklist
A For every submission:
□ A1. Did you describe the limitations of your work?
7
□ A2. Did you discuss any potential risks of your work?
8
□ A3. Do the abstract and introduction summarize the paper’s main claims?
1
□ A4. Have you used AI writing assistants when working on this paper?
Left blank.
B □ Did you use or create scientiﬁc artifacts?
4
□ B1. Did you cite the creators of artifacts you used?
4
□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?
8
□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided
that it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is
compatible with the original access conditions (in particular, derivatives of data accessed for research
purposes should not be used outside of research contexts)?
8
□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any
information that names or uniquely identiﬁes individual people or offensive content, and the steps
taken to protect / anonymize it?
Not applicable. Left blank.
□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and
linguistic phenomena, demographic groups represented, etc.?
Not applicable. Left blank.
□ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,
etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the
number of examples in train / validation / test splits, as these provide necessary context for a reader
to understand experimental results. For example, small differences in accuracy on large test sets may
be signiﬁcant, while on small test sets they may not be.
4
C □ Did you run computational experiments?
4
□ C1. Did you report the number of parameters in the models used, the total computational budget
(e.g., GPU hours), and computing infrastructure used?
4,5
The Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing
assistance.
10036□ C2. Did you discuss the experimental setup, including hyperparameter search and best-found
hyperparameter values?
4
□ C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary
statistics from sets of experiments), and is it transparent whether you are reporting the max, mean,
etc. or just a single run?
5
□ C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,
etc.)?
Not applicable. Left blank.
D □ Did you use human annotators (e.g., crowdworkers) or research with human participants?
Left blank.
□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,
disclaimers of any risks to participants or annotators, etc.?
Not applicable. Left blank.
□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)
and paid participants, and discuss if such payment is adequate given the participants’ demographic
(e.g., country of residence)?
Not applicable. Left blank.
□ D3. Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? For example, if you collected data via crowdsourcing, did your instructions to
crowdworkers explain how the data would be used?
Not applicable. Left blank.
□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?
Not applicable. Left blank.
□ D5. Did you report the basic demographic and geographic characteristics of the annotator population
that is the source of the data?
Not applicable. Left blank.
10037

89
Dense Text Retrieval Based on Pretrained Language Models:
A Survey
WAYNE XIN ZHAO,Renmin University of China, China
JING LIU,Baidu Inc., China
RUIYANG RENand JI-RONG WEN,Renmin University of China, China
Text retrieval is a long-standing research topic on information seeking, where a system is required to return
relevantinformationresourcestouser’squeriesinnaturallanguage.Fromheuristic-basedretrievalmethods
tolearning-basedrankingfunctions,theunderlyingretrievalmodelshavebeencontinuallyevolvedwiththe
ever-lasting technical innovation. To design effective retrieval models, a key point lies in how to learn text
representations and model the relevance matching. The recent success of pretrained language models (PLM)
sheds light on developing more capable text-retrieval approaches by leveraging the excellent modeling ca-
pacity of PLMs. With powerful PLMs, we can effectively learn the semantic representations of queries and
texts in the latent representation space, and further construct the semantic matching function between the
dense vectors for relevance modeling. Such a retrieval approach is calleddense retrieval, since it employs
dense vectors to represent the texts. Considering the rapid progress on dense retrieval, this survey systemat-
ically reviews the recent progress on PLM-based dense retrieval. Different from previous surveys on dense
retrieval, we take a new perspective to organize the related studies by four major aspects, including archi-
tecture, training, indexing and integration, and thoroughly summarize the mainstream techniques for each
aspect. We extensively collect the recent advances on this topic, and include 300+ reference papers. To sup-
port our survey, we create a website for providing useful resources, and release a code repository for dense
retrieval.Thissurveyaimstoprovideacomprehensive,practicalreferencefocusedonthemajorprogressfor
dense text retrieval.
CCS Concepts: •Information systems→Information retrieval;
Additional Key Words and Phrases: Text retrieval, dense retrieval, pretrained language models
ACM Reference format:
Wayne Xin Zhao, Jing Liu, Ruiyang Ren, and Ji-Rong Wen. 2024. Dense Text Retrieval Based on Pretrained
Language Models: A Survey.ACM Trans. Inf. Syst.42, 4, Article 89 (February 2024), 60 pages.
https://doi.org/10.1145/3637870
This work was partially supported by National Natural Science Foundation of China under Grants No. 62222215 and
No. U2001212, and Beijing Natural Science Foundation under Grant No. 4222027.
Authors’ addresses: W. Xin Zhao and R. Ren, Gaoling School of Artificial Intelligence, Renmin University of China, No. 59
Zhongguancun Street, Haidian District, Beijing, China, 100872; e-mails: batmanfly@gmail.com, reyon. ren@ruc.edu.cn; J.
Liu (Corresponding author), Baidu Inc., Beijing, China, 100193; e-mail: liujing46@baidu.com; J.-R. Wen, Gaoling School of
Artificial Intelligence, School of Information, Renmin University of China, No. 59 Zhongguancun Street, Haidian District,
Beijing, China, 100872; e-mail: jrwen@ruc.edu.cn.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee
provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and
the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be
honored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,
requires prior specific permission and/or a fee. Request permissions frompermissions@acm.org.
© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.
1046-8188/2024/02-ART89 $15.00
https://doi.org/10.1145/3637870
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.
89:2 W. Xin et al.
1 INTRODUCTION
Text retrieval aims at finding relevant information resources (e.g., documents or passages) in re-
sponsetouser’squeries.Itreferstoaspecificinformation-seekingscenariowherethequeriesand
resourcesarepresentintheformofnaturallanguagetext.Asoneofthemostessentialtechniques
to overcome information overload, text-retrieval systems have been widely employed to support
many downstream applications, including question answering [111, 270], dialog system [30, 107],
entity linking [73,286], and Web search [183].
Theideaofdevelopingtext-retrievalsystemshasalonghistoryintheresearchliterature.Early
inthe1950s,pioneeringresearchershavestartedtostudyhowtoindexthetextsbyselectingrepre-
sentativetermsforinformationretrieval[ 110].Amongtheearlyefforts,asignificantachievement
is thevector space model[231, 233] based on the “bag-of-words” assumption, representing both
documents and queries as sparse term-based vectors. To construct sparse vector representations,
various term weighting methods have been designed and implemented, including the classictf-
idf method [1, 225, 232]. Based on this scheme, the relevance can be estimated according to the
lexical similarity between sparse query and text vectors. Such a representation scheme is further
supported by the well-known data structure ofinverted index[336,337], which organizes the text
content as term-oriented posting lists, for efficient text retrieval. To better understand the under-
lying retrieval mechanism, probabilistic relevance frameworks have been proposed for relevance
modeling,exemplifiedbytheclassicBM25model[ 226,227].Furthermore,statisticallanguagemod-
eling approaches [304] have been widely explored for text ranking. These early contributions lay
thefoundationofmoderninformation-retrievalsystems,whiletheproposedretrievalmethodsare
usually based on heuristic strategies or simplified probabilistic principles.
With the development of machine learning discipline,learning to rank[125, 149] introduces
supervised learningfor text ranking. The basic idea is to design feature-based ranking functions
takingasinputhand-craftedfeatures(notonlylimitedtolexicalfeatures)andthentraintherank-
ingfunctionwithrelevancejudgements(binaryorgradedrelevanceannotationsoverdocuments).
Despitetheflexibility,learningtorankmethodsstillrelyonhumaneffortsforfeatureengineering.
Further,there-surge of neuralnetworksshedslights on developing more capabletext-retrieval
systems, which no longer require hand-crafted text features. As an important progress in infor-
mation retrieval, deep learning approaches [96] can learn query and document representations
from labeled data in an automatic way, where both queries and documents are mapped into low-
dimensional vectors (calleddense vectorsorembeddings) in the latent semantic space. In this man-
ner,therelevancecanbemeasuredaccordingtothesemanticsimilaritybetweenthedensevectors.
In contrast to sparse vectors in the classic vector space model, embeddings do not correspond to
explicit term dimensions, but instead aim at capturing latent semantic characteristics for match-
ing. Such a retrieval paradigm is calledNeural Information Retrieval (Neural IR) [75,77,182],
which can be considered as initial explorations for dense retrieval techniques. Following the con-
vention in References [57, 137], we refer to these neural IR methods at this stage aspre-BERT
models.
Recently,basedonthepowerfulTransformerarchitecture[ 268],pretrainedlanguagemodels
(PLM) [53] have significantly pushed forward the progress of language intelligence. Pretrained
on large-scale general text data, PLMs can encode large amounts of semantic knowledge, thus
having an improved capacity to understand and represent the semantics of text content. Follow-
ing the “pretrain then fine-tune” paradigm, PLMs can be further fine-tuned according to specific
downstream tasks. Inspired by the success of PLMs in natural language processing, since 2019,
researchers have started to develop text-retrieval models based on PLMs, leading to the new gen-
eration of text-retrieval approaches, i.e.,PLM-based dense retrieval models.
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.Dense Text Retrieval Based on Pretrained Language Models: A Survey 89:3
In the past four years, a large number of studies on PLM-based dense retrieval have been pro-
posed[22,57,137],whichhavelargelyraisedtheperformancebaronexistingbenchmarkdatasets.
IthasbeenreportedthatPLM-basedapproacheshavedominatedthetopresultsforbothdocument
and passage retrieval tasks at the 2021 Deep Learning Track [46]. The reason for the success of
PLM-based dense retrieval models can be given in two major aspects. First, the excellent text rep-
resentation capability of PLMs enables the text-retrieval systems to answer difficult queries that
cannot be solved via simple lexical match. Second, the availability of large-scale labeled retrieval
datasets (e.g., MS MARCO [185] and Natural Questions [116] datasets) makes it feasible to train
(or fine-tune) capable PLMs for text retrieval. For example, TREC 2019 and 2020 Deep Learning
Track [44, 47] released a training set of 0.367 million queries (derived from MS MARCO [185])
for document retrieval task, which is significantly larger than those in previous retrieval tasks in
TREC.
Considering the important progress on dense retrieval, this survey aims to provide a system-
atic review of existing text-retrieval approaches. In particular, we focus on the PLM-based dense
retrieval approaches (short asdense retrieval modelsthroughout this survey) instead of previous
neuralIRmodels(i.e.,thepre-BERTmethods[ 75,96]).Thissurveytakes first-stageretrieval asthe
core,andextensivelydiscussesfourrelatedaspectstobuildadenseretrievalsystem,including ar-
chitecture(howtodesignthenetworkarchitecturesfordenseretrievers), training(howtooptimize
the dense retriever with special training strategies),indexing (how to design efficient data struc-
ture for indexing and retrieving dense vectors), andintegration (how to integrate and optimize a
complete retrieval pipeline). Our survey extensively discusses various useful topics or techniques
for building a dense retrieval system, which aims to provide acomprehensive, practical reference
to this research direction for researchers and engineers.
Weareawareofseveralcloselyrelatedsurveysorbooksondenseretrieval[ 22,57,77,137,263].
For example, Guo et al. [77] review early contributions in neural IR, Cai et al. [22] summarize the
major progress for the first-stage retrieval in three different paradigms, Lin et al. [137]p r e s e n t
a review of text ranking methods mainly based on pretrained Transformers, Fan et al. [57]r e -
view the major progress for information retrieval based on pretraining methods, and Nicola
Tonellotto [263] present a lecture note of recent progress on neural information retrieval. Dif-
ferent from these literature surveys, we highlight three new features of this survey as follows:
(i) We concentrate on the research ofPLM-based dense retrieval, and organize the related stud-
ies by a new categorization in four major aspects, i.e., architecture, training, indexing and inte-
gration; (ii) We take a special focus onpractical techniquesfor dense retrieval, extensively dis-
cussing the approaches to train the retrieval models, to build the dense index, and optimize the
retrievalpipeline;(iii)Wealsodiscussseveralemergingresearchtopics(e.g.,modelbasedretrieval),
which will complement the content of dense retrieval in alternative approaches or improvement
techniques.
To support this survey, we create a referencing website for including related resources (e.g.,
papers,dataset,andlibrary)ondenseretrievalresearch,atthelink: https://github.com/RUCAIBox/
DenseRetrieval. Furthermore, we implement and release a code repertory for a number of dense
retrieval models, and provide flexible interfaces to use or retrain dense retrievers.
Theremainderofthesurveyisorganizedasfollows:Section 2providestheoverallintroduction
totheterminology,notationsandtasksettingsfordenseretrieval.Asthecorecontent,Sections 3,4,
5,and 6reviewtheprogressonthemainstreamarchitecture,trainingapproach,indexmechanism
andretrievalpipelinefordenseretrieval.Then,wecontinuethediscussionontheadvancedtopics
(Section 7) and the applications (Section8). Finally, we conclude this survey by summarizing the
major findings and remaining issues in Section9.
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.89:4 W. Xin et al.
Fig. 1. Illustration for the overall pipeline of an information-retrieval system.
2O V E R V I E W
This section first introduces the background about dense text retrieval and then discusses the key
aspects for designing the dense retrieval models.
2.1 Task Setting, Terminology and Evaluation
Inthissurvey,wefocusonthetaskoffindingrelevanttextsfromalargetextcollectioninresponse
to a natural language query issued by a user. Specially, both query and text (e.g., a document) are
presentedintheformofasequenceofwordtokensfromavocabulary.Inparticular, textscanbein
differentsemanticgranularities(e.g.,document,passageorsentence),leadingtodifferenttypesof
retrievaltaskssuchasdocumentretrievalandpassageretrieval.Sinceweaimtointroducegeneral
methodologies for various retrieval tasks or settings, we refer to these tasks astext retrievalin a
unifiedway.Typically,acompleteinformation-retrievalsystemconsistsofmultipleproceduresor
stages arranged in a processing pipeline [57, 137]. In this pipeline, the first-stage retrieval aims
to reduce the candidate space by retrieving relevant candidates, and we refer to the component
that implements the first-stage retrieval asretriever [22, 137]. Correspondingly, the subsequent
stages mainly focus onreranking (or ranking) the candidate texts, calledreranking stages,w h i c h
are supported by thererankers [137]. Based on the retrieval results from the first-stage retriever,
a retrieval system usually sets up one or multiple reranking stages to refine the initial results
and derive the final search results. For other fine-grained retrieval tasks, e.g., question answering,
another component calledreader [29] may be further integrated to analyze the returned texts by
the retriever (or reranker) and locate the answers for the query.
Comparedwithtraditionalretrievalmodels,denseretrievalmodelsaremoredatahungry,requir-
ing large-scale labeled datasets to learn the parameters of the PLMs. Among the recently released
datasets,MSMARCO dataset[185]containsalargeamountofquerieswithannotatedrelevantpas-
sagesinWebdocuments,and Natural Questions (NQ) [116]datasetcontainssearchqueriesfrom
Google and documents with paragraphs and answer spans from the top-ranked Wikipedia pages.
Arecentstudy[ 48]hassummarizedthepromotioneffectofMSMARCOontheprogressofdense
retrieval: “The MS MARCO datasets have enabled large-data exploration of neural models.” Besides,
basedonMSMARCO,severalvariantshavebeencreatedtoenrichtheevaluationcharacteristicson
somespecificaspect,e.g.,themultilingualversionmMARCO[ 18]andtheMSMARCOChameleons
dataset [3] (consisting of obstinate queries that are difficult to answer by neural retrievers while
havingsimilarquerylengthanddistributionofrelevancejudgementswitheasyqueries).Also,Sci-
avolino et al. [236] create EntityQuestionsdataset,as a challenging test set for the models trained
on NQ, which contains simple factoid questions about entities from Wikipedia. Further, several
comprehensive benchmark datasets are released to evaluate the overall retrieval capability of the
retrieval models by aggregating representative datasets and conducting diverse evaluation tasks,
suchas BEIR[260]and KILT [198].Althoughexistingdatasetslargelyimprovethetrainingofdense
retrievers, in these datasets, a query typically corresponds to very few relevance judgements. Al-
thoughexistingdatasetslargelyimprovethetrainingofdenseretrievers,inthesedatasets,aquery
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.Dense Text Retrieval Based on Pretrained Language Models: A Survey 89:5
typicallycorrespondstoveryfewrelevancejudgements.Forexample,Nogueiraetal.observethat
most of queries in MS MARCO dataset contains only one labeled positive[4], which is likely to
be smaller than the actual number of relevant ones in the collection. Incomplete relevance an-
notations will lead to several training issues such as false negative, which potentially affects the
retrieval performance. Corresponding to these public datasets, a number of open-sourced dense
retrieval libraries have been also released, including Tevatron [69], Pyserini [136], SentenceTrans-
formers [215], Asyncval [334], and OpenMatch [155].
Toevaluatetheretrievalcapacityofaninformation-retrievalsystem,anumberoffactorsneedto
be considered [81, 81, 267]: effectiveness, efficiency, diversity, novelty, and so on. Existing studies
mainlyfocusontheeffectivenessfortheretrievalsystem,andtheywidelyadoptTop- K Accuracy,
Recall, Precision,Mean Average Precision (MAP), Mean Reciprocal Rank (MRR),a n dNor-
malized Discounted Cumulative Gain (NDCG)as evaluation metrics. Note that these metrics
can be used in both first-stage retrieval and reranking. However, we only describe the evaluation
metricsfromageneralrankingperspectivewithoutconsideringspecifictaskscenarios.Inparticu-
lar,top-rankedtextsaremoreimportantforretrievaltasks,andthereforecut-offmetricsareoften
adopted to examine the quality of texts at top positions.
Duetothespacelimit,weomitthedetaileddescriptionsofdatasets,metricsandtools.And,we
provide a comprehensive description of the three parts in the Appendix of this survey.1
2.2 Formulation for Dense Retrieval
Formally, letq denote a natural language query anddi denote a text from a large text collection
D= {di}m
i=1 consistingof mdocuments.Givenaquery,textretrievalaimstoreturnarankedlistof
nmostrelevanttexts L= [d1,d2,··· dn]accordingtotherelevancescoresbyaretrievalmodel.As
thetechnicalapproach,eithersparseretrievalmodels(relyingonexactlexicalmatching)ordense
retrieval models (relying on semantic matching) can be used to implement the retriever.
Thekeyofdenseretrievalliesinthefactthatqueriesandtextsarerepresentedbydensevectors,
suchthattherelevancescorecanbecomputedaccordingtosomesimilarityfunctionbetweenthese
dense vectors [22,57, 137], denoted by
Rel(q,d) = fsim
(
ϕ(q),ψ(d)
)
, (1)
whereϕ(·) ∈Rl andψ(·) ∈Rl arefunctionsmappingqueriesandtextsinto l-dimensionalvectors,
respectively. For dense retrieval,ϕ(·) and ψ(·) are developed based on neural network encoder,
andsomemeasurementfunction(e.g.,innerproduct)canbeusedtoimplement fsim(·).Atthepre-
BERTtime,2 theencodersareusuallyimplementedbymulti-layeredneuralnetworks,whileinthis
survey we focus on the text encoder based on PLMs, calleddense retrievers.
To learn the dense retrieval models, we also assume that a set of relevance judgements are
provided for training or fine-tuning the PLMs. Specially, we only consider the setting with binary
(positiveonly)relevancejudgements:foraquery q,alistofpositivetexts {d+
i }aregivenastraining
data. Usually, negative texts (callednegatives) are not directly available, and we need to obtain
negatives through sampling or other strategies (detailed in Section4.2). Note that in some cases,
we can also obtain graded relevance judgements [104, 246], where a label indicates the relevance
degree of a text (e.g., relevant, partially relevant and non-relevant). However, such fine-grained
relevance labels are difficult to obtain in real-world retrieval systems. Thus, we mainly consider
the binary relevance judgements in this survey.
1https://github.com/RUCAIBox/DenseRetrieval/blob/main/Appendix.pdf
2FollowingReference[ 189],werefertotheneuralinformation-retrievalmethodsas pre-BERTapproaches beforetheuseof
PLMs.
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.89:6 W. Xin et al.
2.3 Key Aspects
This survey takes thefirst-stage dense retrievalas the core, and extensively discusses four key
aspects for building a capable retrieval system, which are detailed as follows:
—Architecture(Section3).Itisimportanttodesignsuitablenetworkarchitecturesforbuilding
the relevance model. Currently, a number of general-purpose PLMs have been developed,
and we need to adapt existing PLMs to text retrieval according to the task requirement.
Training (Section 4). Different from traditional retrieval models, it is more difficult to train
PLM-basedretrievers,consistingofahugenumberofparameters.Weneedtodevelopeffec-
tive training approaches to achieving desirable retrieval performance.
—Indexing(Section5).Traditionally,invertedindexcanefficientlysupportthesparseretrieval
models. However, dense vectors do not explicitly correspond to lexical terms. We need to
design suitable indexing mechanism for dense retrieval.
—Integration (Section 6). As mentioned before, retrieval systems are usually implemented via
a pipeline consisting of retrievers and rerankers. It is necessary to study how to integrate
and optimize the dense retriever in a whole retrieval pipeline.
3 ARCHITECTURE
This section describes the major network architectures for dense text retrieval. We start with the
backgroundofTransformerandPLMs,thenintroducetwotypicalneuralarchitecturesforbuilding
dense retrieval models, and finally compare sparse and dense retrieval models.
3.1 Background
3.1.1 Transformer and Pretrained Language Models.Transformer [268] has become the main-
stream backbone for language model pretraining, which was originally proposed for efficiently
modeling sequence data. Different from traditional sequence neural networks (i.e., RNN and its
variants[40,88]),Transformernolongerprocessesthedatainorder,butinsteadintroducesanew
self-attentionmechanism:a token attends to all thepositionsfrom the input.Suchan architecture
isparticularlysuitedtobeparallelizedwiththehardwaresupport,makingitfeasibletotrainvery
large neural networks. Based on Transformer and its variants, PLM are proposed [21, 53, 152]i n
thefieldofnaturallanguageprocessing.PLMsarepretrainedoveralarge-scalegeneraltextcorpus
with specially designed self-supervised loss functions, and can be further fine-tuned according to
downstreamtasks,called pretrainingthenfine-tuningparadigm [53,148],whichisoneofthemost
striking achievements on language intelligence in recent years. As one of the most representative
PLMs, BERT [53] pretrains deep bidirectional architectures for learning general text representa-
tions(withthetrickofwordmasking),whichhaslargelyraisedtheperformancebarinavarietyof
naturallanguageprocessingtasks.Withexcellentrepresentationcapacities,PLMsalsoshedlights
on developing more effective retrieval models.
3.1.2 PLMsforSparseRetrieval. BeforeintroducingtheuseofPLMsfordenseretrieval,thispart
brieflyreviewshowtoutilizePLMsforimprovingsparseretrievalmodelsthatrelyonlexicalmatch.
Basically speaking, this line of research work can be roughly divided into two major categories,
namely, term weighting and term expansion.
The first category of work aims to improve term weighting based on per-token contextualized
representations. As a representative study, DeepCT [51] utilizes the learned BERT token repre-
sentations for estimating context-specific importance of an occurring term in each passage. The
basic idea is to regress the token representations into real-valued term weights. HDCT [50]e x -
tendsittolongdocumentsbysplittingthedocumentsintopassages,andaggregatestheestimated
term weights in each passage. Specifically, it leverages three kinds of data resources (i.e., titles,
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.Dense Text Retrieval Based on Pretrained Language Models: A Survey 89:7
Table 1. Detailed ListofDifferent Dense RetrievalMethodsinthe LiteraturewithDetailed Configurations
Years Methods PLMs Arch. Training Other TricksLoss Negative Data Aug. Pretrain
2020 Poly-encoders [ 98]B E R T base MR CE TAP Context-candidate attention
2019 ICT [ 120]B E R T base SR IOC TAP Joint training
2020 ICT+BFS+WLP [ 28]B E R T base SR IOC TAP
2020 REALM [ 79]B E R T base SR IOC RAP Joint training + Async. index
2020 DPR [ 111]B E R T base SR NLL IB+SHN ALD
2020 ColBERT [ 113]B E R T base MR CE Multi-core pre-processing
2020 ME-BERT [ 161]B E R T large MR CE IB+DyHN Sparse-dense hybrid
2020 RepBERT [ 309]B E R T base SR IOC IB
2020 ANCE [ 289]R o B E R T a base SR NLL DyHN Async. index
2020 EZRQG [ 135]B E R T base SR IOC Query generation
2020 AugmentedBERT [ 297]B E R T base SR IOC+KL SHN KD Kernel density estimation
2020 In-batch KD [ 89]B E R T base SR MSE IB KD Cross-architecture KD
2020 RocketQA [ 207] ERNIE base SR NLL CB+DeHN KD Iter. training
2020 AugmentSBERT [ 259]B E R T base SR NLL SHN+DeHN KD Kernel density estimation
2020 TCT-ColBERT [ 141]B E R T base SR KL IB+SHN KD Sparse-dense hybrid
2020 Multi-stage [ 157]B E R T base SR NLL IB+SHN KD TAP Embedding fusion
2020 DKRR[ 102]B E R T base SR KL SHN KD Iter. training
2020 DensePhrases [ 118] SpanBERT base PR IOC+KL IB KD Pre-batch negatives
2020 UnifiedQA [ 192]B E R T base SR NLL IB ALD
2021 Individual Top-k [ 229]B E R T large SR IOC+NLL IB+SHN TAP Joint training
2021 Grad. Cache [ 70]B E R T base SR NLL CB Gradient caching
2021 TAS-Balanced [ 92]B E R T base SR MSE IB+SHN KD Balanced topic aware sampling
2021 ADORE+STAR [ 308]R o B E R T abase SR NLL SHN+DyHN Async. index
2021 Condenser [ 65] Condenser SR CE IB REP
2021 DRPQ [ 256]B E R T base MR NLL IB+DyHN Multiple pseudo query embeddings
2021 RANCE [ 202]R o B E R T a base SR NLL DeHN
2021 DANCE [ 132]R o B E R T a base SR IOC SHN Contrastive dual learning
2021 DPR-PAQ [ 193]R o B E R T a large SR NLL IB+SHN ALD GAP
2021 JPQ [ 306]B E R T base SR IOC DyHN Quantization
2021 coCodenser [ 66] Condenser SR NLL SHN REP Corpus-aware pretraining
2021 PAIR [ 218] ERNIE base SR IOC+NLL IB+SHN KD REP Passage-centric constrain
2021 ST5 [ 186]T 5 11B SR NLL IB
2021 ANCE-PRF [ 301]R o B E R T a base SR NLL DyHN Pseudo relevance feedback
2021 Trans-Encoder [ 144]R o B E R T abase SR MSE DyHN KD Iter. training + Mutual distillation
2021 AR2-G [ 311] ERNIE base SR IOC DyHN Adversarial training
2021 RepCONC [ 307]B E R T base SR IOC DyHN Quantization
2021 RocketQAv2 [ 219] ERNIE base SR KL SHN+DeHN KD Joint training + Mutual distillation
2021 SPAR [ 37]B E R T base SR NLL SHN KD
2021 SEED [ 159]B E R T base SR NLL DyHN TAP Autoencoder pretraining
2021 ColBERTv2 [ 235]B E R T base SR NLL DeHN KD Residual compression + Quantization
2021 GPL [ 277] DistilBERT base SR MSE IB+SHN KD TAP Query generation
2021 Spider [ 213]B E R T base SR NLL IB+SHN TAP Sparse-dense hybrid
2021 DrBoost [ 122]B E R T base SR NLL SHN Boosting
2021 GTR [ 187]T 5 11B SR NLL IB+DeHN TAP
2021 Contriever [ 101]B E R T base SR NLL IB REP Contrastive learning
2022 Uni-Retriever [ 313]B E R T base SR IOC+NLL CB + SHN
2022 LaPraDoR [ 290] DistilBERT base SR IOC+NLL IB REP Sparse-dense hybrid
2022 HLP [ 322]B E R T base SR NLL IB TAP
2022 DAR [ 106]R o B E R T a large SR NLL IB+SHN Interpolation and perturbation
2022 MVR [ 316]B E R T base MR IOC+NLL SHN Multi-view representation
2022 ColBERTer [ 91] DistilBERT base MR IOC+MSE SHN
2022 CharacterBERT [ 335] CharacterBERT SR NLL+KL IB+SHN Self-teaching
2022 GNN-encoder [ 146] ERNIE base SR IOC+NLL IB+DeHN KD Query-passage interaction
2022 COSTA [ 164]B E R T base SR NLL SHN TAP Contrastive span prediction
2022 CL-DRD [ 303] DistilBERT base SR KL KD Curriculum Learning
2022 ERNIE-Search [ 160] ERNIE large SR NLL+KL IB KD Joint training + Mutual distillation
2022 RetroMAE [ 154]B E R T base SR IOC+NLL DyHN KD REP MAE pretraining
2022 DCSR [ 93]B E R T base MR NLL IB+SHN
2022 ART [ 230]B E R T large SR KL Query reconstruction
2022 SimLM [ 279]B E R T base SR NLL IB+DeHN KD REP Iter. training
2022 RoDR [ 38]B E R T base SR NLL+KL IB+SHN Passage-centric constraint
2022 Aggretriever [ 139] Condenser SR NLL IB+SHN Text aggregation
2022 CoT-MAE [ 287]B E R T base SR NLL SHN GAP
2022 CPDAE [ 167]B E R T base SR IOC+NLL SHN TAP Autoencoder pretraining
2022 DPTDR [ 257]R o B E R T a large SR NLL IB+DeHN KD TAP Deep prompt tuning
2022 LED [ 314] Condenser SR IOC+NLL IB+SHN KD Rank-consistent regularization
2022 LexMAE [ 240]B E R T base SR IOC+NLL+KL SHN KD REP MAE pretraining
2022 Promptagator [ 52] FLAN SR NLL IB+SHN Prompt-based query generation
2022 PROD [ 142] ERNIE base SR NLL+KL IB+SHN KD Curriculum learning
2022 TASER [ 39] Condenser SR NLL IB+SHN Mixture-of-experts
Abbreviations are defined in Table2.
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.89:8 W. Xin et al.
Table 2. Abbreviations for Different Techniques or Strategies
Type Abbr
Architecture
Single-representation (Section3.2.2) SR
Multi-representation (Section3.2.2) MR
Phrase-level representation (Section3.2.2) PR
Training
Loss
Negative log-likelihood loss (Section4.1.1) NLL
Cross-entropy loss (Section4.1.1) CE
Triplet ranking loss (Section4.1.1) TR
Incorporating optimization constraints (Section4.1.2) IOC
MSE loss (Section4.4) MSE
KL-divergence loss (Section4.4) KL
Negative selection
In-batch negatives (Section4.2.1) IB
Cross-batch negatives (Section4.2.2) CB
Hard negatives
Static hard negatives (Section4.2.3) SHN
Dynamic hard negatives (Section4.2.3) DyHN
Denoised hard negatives (Section4.2.3) DeHN
Data augmentation Auxiliary and synthetic data enrichment (Section4.3) ALD
Knowledge distillation (Section4.4) KD
Generation-augmented pretraining (Section4.3.2) GAP
Pretraining
Task adaptive pretraining (Section4.5.1) TAP
Retrieval-augmented pretraining (Section4.5.2) RAP
Representation enhanced pretraining (Section4.5.3) REP
Web inlinks, and pseudo-relevance feedback) to generate weak supervision signals for learning
term weights. It can better learn the importance of a term by modeling its text context. Further,
COIL[67]utilizesthecontextualizedtokenrepresentationsof exactmatchingterms forestimating
the relevance. It computes the dot product between the token representations from the query en-
coderandtextencoder(onlyconsideringtheoverlappingterms),andthensumstheterm-specific
similarity scores as the relevance score (also including the semantic matching based on “[CLS]”
embeddings). To better understand the above models, Lin et al. [138] propose a conceptual frame-
worktounifytheseapproaches:DeepCTaimstoassign scalarweights toqueryterms,whileCOIL
aims to assignvector weightsto query terms. Furthermore, uniCOIL [138]i sa l s op r o p o s e db yr e -
ducingtheweightvectorinCOILtoonedimension,andtheexperimentsinReference[ 138]show
that uniCOIL can retain the effectiveness while increasing the efficiency.
The second category of work expands queries or documents by using PLMs to mitigate the
vocabulary mismatching problem. For example, docTTTTTquery [190] predicts a set of queries
that a document will be relevant to, such that the predicted queries can be used to enrich the
document content. In addition, SPLADE [63] and SPLADEv2 [62] project each term in queries
andtextstoavocabulary-sizedweightvector,inwhicheachdimensionrepresentstheweightofa
terminthevocabularyofPLMs.Theweightsareestimatedbyusingthelogitsofmaskedlanguage
models.Then,thefinalrepresentationofawholetext(orquery)isobtainedbycombining(e.g.,sum
ormaxpooling)theweightvectorsestimatedbyallthetokensofthetext.Sucharepresentationcan
beviewedasanexpansionofaquery(oratext),sinceitcontainsthetermsthatdonotoccurinthe
query(orthetext).Asparsityregularizationisfurtherappliedtoobtainthesparserepresentation,
to efficiently use the inverted index.
Despite the use of PLMs, these approaches are still based on lexical matching. They can reuse
the traditional index structure by incorporating additional payloads (e.g., contextualized embed-
dings[67]).ForamorethoroughdiscussionofPLM-enhancedsparseretrievalmodels,thereaders
can refer to References [22,57, 137].
3.2 Neural Architecture for Dense Retrieval
Theessenceofdenseretrievalistomodelthe semanticinteraction betweenqueriesandtextsbased
on the representations learned in latent semantic space. Based on different interaction modeling
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.Dense Text Retrieval Based on Pretrained Language Models: A Survey 89:9
Fig. 2. Illustration of dual-encoder and cross-encoder architectures.
ways, there are two mainstream architectures for dense retrieval, namely, the cross-encoder and
bi-encoder. We next introduce the two kinds of major architectures.
3.2.1 Cross-encoder Architecture. As a direct application of PLMs, cross-encoder considers a
query-text pair as an entire “sentence.”To be specific,the input of cross-encoder is the concatena-
tion of a query and a text, separated by a special symbol “[SEP].” To obtain the overall represen-
tation, another symbol “[CLS]” is inserted at the beginning of the concatenated sentence. Then,
the query-text sequence is fed into a PLM for modeling the semantic interaction between any
two tokens of the input sequence. After the representations for each token in the sequence have
been learned, we can obtain the match representation for this query-text pair. A commonly used
way is to take the “[CLS]” representation as the semantic matching representation [203]. Other
variants can be also used, e.g., averaging token embeddings [215]. Such an architecture is similar
to interaction-based architecturein the pre-BERT studies [75, 288] ,s i n c ei ta l l o w st h et o k e n st o
interact across queries and texts.
3.2.2 Bi-encoderArchitecture. Thebi-encoder(a.k.a.,dual-encoder)adoptsthetwo-towerarchi-
tecture,whichissimilartorepresentation-basedapproaches(e.g.,DSSM[ 96])inpre-BERTstudies.
Themajordifferenceisthatitreplacesthepreviouslyusedmulti-layeredperceptions(orotherneu-
ral networks) with PLM-based encoders. Next, we will discuss the major progress of bi-encoder
architecture for dense retrieval.
Single-representation bi-encoder.As the basic form of bi-encoder architecture, it first learns
latent semantic representations for both query and text with two separate encoders, calledquery
embedding andtext embedding, respectively. Then, the relevance score can be computed via some
similarity function (e.g., cosine similarity and inner product) between the query embedding and
text embedding. As mentioned before, we can directly encode the query and text by placing a
specialsymbol (e.g., “[CLS]” in BERT) at the beginning of a text sequence,so that the learned rep-
resentationofthespecialsymbolcanbeusedtorepresentthesemanticsofatext(orquery).Most
of the single-representation dense retrievers [111, 207, 289] learn the query and text representa-
tions with encoder-only PLMs (e.g., BERT [53], RoBERTa [53], and ERNIE [253]). More recently,
text-to-textTransformer(T5) [210],whichisanencoder-decoderbasedPLM,hasbeenexplored
to learn text representations for dense retrieval [186, 187]. It has been shown that a T5-based
sentence embedding model outperforms Sentence-BERT [216] on SentEval [42] and SentGLUE
[210]t a s k s .
Multi-representationbi-encoder. Amajorlimitationofsingle-representationbi-encoderisthat
it cannot well model fine-grained semantic interaction between query and text. To address this
issue, several researchers propose to explore multiple-representation bi-encoder for enhancing
the text representation and semantic interaction. The poly-encoder model [98] learnsm different
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.89:10 W. Xin et al.
representations for modeling the semantics of a text in multiple views, calledcontext codes in
the original paper. During query time, by attending to the query vector, thesem representations
are then combined into a single vector. Finally, the inner product between the query vector and
aggregated vector is computed as the relevance score. As another related study, ME-BERT [161]
also generates m representations for a candidate text, by directly taking the contextualized
representations of the firstm tokens. The text relevance is computed using the maximum inner
product between the query vector and them contextual representations. Furthermore, ColBERT
[113] designs an extreme multi-representation semantic matching model, where per-token
contextualized representations are kept for query-text interaction. Different from the repre-
sentation scheme of cross-encoder (Section3.2.1), these contextualized representations do not
directly interact across queries and texts during encoding. Instead, they introduce aquery-time
mechanismforper-tokenrepresentationinteraction,called lateinteraction [113].Astheextension
of ColBERT, ColBERTer [91] designs an approach by combining single-representation (the
“[CLS]” embedding) and multi-representation (per-token embeddings) mechanisms for matching,
achieving performance improvement over ColBERT.
Thesemulti-representationbi-encoderssharetheideaofusingmultiplecontextualembeddings
forrepresentingqueriesortexts,suchthatthesimilaritycanbemeasuredfromdifferentsemantic
views. These contextualized representations are learned and stored during training and indexing;
while at query time, we can model fine-grained semantic interaction between query embeddings
and pre-computed text embeddings. Such an approach is effective to improve the retrieval perfor-
mance, but causes a significantly high cost to maintain the multi-representation implementation
(e.g.,increasedindexsize),whichisnotpracticalinreal-worldapplications.Consideringthisissue,
somespecificstrategiesproposedinColBERTer[ 91]canbeusedtoreducethemulti-representation
costs,suchasembeddingdimensionreduction,bagofuniquewhole-wordrepresentationsandcon-
textualizedstopwordremoval.TheexperimentsinReference[ 91]demonstratethatColBERTercan
significantly reduce the space cost while retaining the effectiveness.
Phrase-level representation.Generally, dense retrieval models focus on solving document- or
paragraph-level text-retrieval tasks. It is natural to extend existing retrieval models by consid-
ering more fine-grained retrieval units (e.g., phrases or contiguous text segments) for specific
tasks, such as open-domain question answering and slot filling [118]. Recently, several studies
propose to learn phrase-level representations for directly retrieving phrases as the answers to
queries [118, 119, 237, 238]. The basic idea is to preprocess all the documents in a query-agnostic
wayandgeneratephrase-levelrepresentations(called phraseembeddings )forcontiguoustextseg-
ments in the documents. Then, the answer finding task is cast into the problem of retrieving
the nearest phrase embeddings to the query embedding. PIQA (phrase-indexed question answer-
ing) [237] presents the first approach that leverages dense phrase index for question answering.
DenSPI [238] further combines dense and sparse vectors of phrases to capture both semantic and
lexical information, to improve the performance of phrase-indexed question answering. While,
such an approach heavily relies on sparse vectors for achieving good performance. Considering
thisissue,Seoetal.[ 118]proposeDensePhrases,anapproachforimprovingthelearningofdense
phraseencoderwithoutusingsparserepresentations.DensePhraseslearnsdensephraserepresen-
tations by data augmentation and knowledge distillation, and further employs enhancednegative
training (considering both in-batch and pre-batch negatives) and query-side fune-tuning. An in-
terestingobservationfoundinReference[ 119]isthatadensephraseretrievalsystemcanachieve
a better passage retrieval accuracy than DPR on NQ and TriviaQA, without any re-training. It
shows that phrase-level information is useful for relevance matching by capturing fine-grained
semantic characteristics. They further propose that dense phrase retriever can be considered as a
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.Dense Text Retrieval Based on Pretrained Language Models: A Survey 89:11
dynamic version of multi-representation retriever, since it dynamically generates a set of phrase
embeddings for each text.
Other improvement variants. Besides, several studies also attempt to improve these basic
approaches, including composite architectures that combine different dense retrieval compo-
nents [122, 129, 150], lightweight architectures that prune or compress the retrieval models
[235, 264, 306], feedback enhanced models that leverage (pseudo)relevance feedback signals
[126, 127, 280, 281, 301, 331], and parameter-efficient tuning that efficiently adapts PLMs to the
retrieval task [254,257].
3.2.3 Comparison between Cross- and Bi-encoder. Ever since the pre-BERT age,interaction-
based or representation-based approaches have been proposed as the two major architectures for
neural IR [75], according to whether fined-grained tokens can interact across queries and texts.
Followingthisconvention,cross-encoderandbi-encodercanbeunderstoodasPLM-basedinstan-
tiations for the two kinds of architectures, respectively. To have a deeper understanding, we next
compare cross-encoder and bi-encoder from the perspective of dense text retrieval.
First, cross-encoder is more capable of learning fine-grained semantic interaction information
for the query-text pair. It is widely recognized that cross-encoder is more effective in relevance
matching [188, 207, 297]. In contrast, bi-encoder (in an original implementation) cannot cap-
ture the fine-grained interaction between query and text representations. As discussed above, a
number of studies aim to improve the representation capacity of bi-encoder by using multiple
context embeddings [98, 113]. According to the reported results on MS MARCO [219, 235], the
multi-representation bi-encoder performs better than the single-representation bi-encoder, but
still worse than the cross-encoder, since it attempts to mimic the cross-encoder in modeling fine-
grained semantic interaction. Second, bi-encoder is more flexible and efficient in architecture. For
flexibility,itcanemploydifferentencodingnetworksforencodingqueriesandtexts,whichallows
more flexible architecture design for dense retrieval, e.g., phrase index [238]. For efficiency, it can
be accelerated via approximate nearest-neighbor search for large-scale vector recall [108], which
is particularly important for practical use of dense retrieval approaches.
Considering the pros and cons, they are often jointly used in retrieval systems for a trade-off
between effectiveness and efficiency. Typically, bi-encoder is used for large-scale candidate re-
call (i.e., first-stage retrieval), and cross-encoder is adopted to implement the reranker or reader
(Section 6.1.2). Besides, cross-encoder is often utilized to improve the bi-encoder, e.g., knowledge
distillation [219,259] (Section4.4) and pseudo labeled data generation [207,218] (Section4.3.2).
3.3 Connections between Dense Retrieval and Sparse Retrieval
In this part, we first discuss the difference/connection between sparse and dense retrieval, and
then introduce how to combine both kinds of approaches for retrieval.
3.3.1 Discussions. To understand how dense retrieval models behave and their connections
withsparseretrievalmodels,wepresentsomediscussionsbyfocusingonthefollowingquestions.
—[A]Whataretherespectivestrengthsandweaknessesofdenseandsparseretrievalmodels?
In the early literature of dense retrieval [111, 207, 289], a number of experimental studies have
reported that dense retrieval models often outperform classic sparse models (e.g., BM25), espe-
ciallyonthebenchmarks(e.g.,MSMARCO)thatwereoriginallydesignedforquestionanswering.
Sincethesebenchmarksfocusoncomplexquerysolving,deepsemanticmatchingcapabilitiesare
required to resolve queries containing few overlapping terms with the answers. Take an example
query that cannot be resolved by a classic sparse retriever from the DPR paper [111]: Given the
question“Whoisthebadguyinlordoftherings? ”adenseretrievercanretrievearelevanttextspan
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.89:12 W. Xin et al.
“SalaBakerisbestknownforportrayingthevillainSauronintheLordoftheRingstrilogy ”bycaptur-
ingthesemanticrelatednessbetween“ badguy ”and“ villain”[111].Toresolvethetermmismatch-
ing issue in traditional retrievers, several improved approaches (exemplified by SPLADE [62, 63])
learnmoreeffectivesparserepresentationsbyallowingtermexpansion,andtheseimprovedsparse
retrieval models can also achieve impressive performance on retrieval benchmarks.3 In contrast
with dense retrieval models, sparse retrieval models usually consume less memory [240], and can
also leverage classic inverted index structure for efficient retrieval.
A major limitation of dense retrievers is that they rely on labeled data for learning the model
parameters. It has been reported that dense retrieval models might perform worse than sparse
retrieval models under thezero-shot setting (without relevance judgement data from the target
domain)onthetestdatasetsfromtheBEIRbenchmark[ 260].TheconductedexperimentsonBEIR
show that the zero-shot retrieval capacity of dense retrieval models is highly limited compared to
classicretrievalmodelssuchasBM25[ 260].Hence,anumberofstudiesaredevelopedtoimprove
the zero-shot retrieval capabilities of dense retrievers (see Section7.1 for a detailed discussion).
Besides, sparse models are more capable in solving the queries that require exact match (e.g., key-
word or entity retrieval), while dense retrievers seem to perform worse in these cases, especially
when the entities or compositions are rare or do not appear in the training set [147, 236]. For ex-
ample, the DPR paper [111] shows that it fails to capture the entity phrase “Thoros of Myr”f o r
the query “Who plays Thoros of Myr in Game of Thrones?” while BM25 succeeds in this case. Such
a finding is indeed consistent with the underlying mechanism of the two kinds of technical ap-
proaches: sparse retrievers are based on lexical matching, while dense retrievers employ latent
embeddingsfor semanticmatching,whichmay resultinthelossofsalientinformationdue tothe
compressed semantic representations. Section3.3.2 will discuss how to enhance the exact match
capacity of dense retrieval models by combining sparse and dense retrievers.
—[B] Is there any theoretical analysis on the capacity of dense representations on relevance
matching, especially how they mimic and relate to sparse retrieval? Is it possible to apply
the traditional IR axioms to dense retrieval?
Sparse retrieval builds the lexical matching model based on term based representations, which
has a good interpretability in retrieval behaviors. In contrast, dense retrieval employs latent se-
mantic representations for relevance matching, and thus performs better on complex queries but
worse on exact match queries. It is important to understand the retrieval behavior of dense re-
trievalmodelsandtheirconnectionwithsparseretrievalmodels(e.g.,howdenseretrievalmodels
perform exact match). For this purpose, Luan et al. [161] investigate the effect of the embedding
size on the ability to mimic sparse retrieval (e.g., bag-of-words models). They demonstrate that
theembeddingsizeofdenseretrieversshouldbeincreasedtoachievecomparableretrievalperfor-
mance of bag-of-words models, when the document length increases. In addition, another related
study shows that corpus scale has a larger effect on dense retrieval [217]: “the performance for
dense representations can decrease quicker for increasing index sizes than for sparse representa-
tions.” They explain this finding based on a proof that the probability for false positives becomes
larger when the index size increases, especially with a decreasing dimensionality.
Anotherpossibleapproachto understandingthebehaviorofdense retrieversis axiomaticanal-
ysis from classic IR literature [58–60]. Specifically, IR axioms are formalized as a set of relevance
constraints that reasonable IR models are desired to satisfy or at least partially satisfy [59, 228],
e.g.,adocumentcontainingmoreoccurrencesofaquerytermshouldreceiveahigherscore,which
canprovidebothinterpretableevidenceandlearningguidancefortraditionalIRmodels.However,
3https://europe.naverlabs.com/research/machine-learning-for-robotics/splade-models/
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.Dense Text Retrieval Based on Pretrained Language Models: A Survey 89:13
it has been found that existing IR axioms may only partially explain [269] or even not be suitable
toanalyze[ 24]thebehaviorofPLM-basedIRmodels.Somefurtheranalysis[ 169]alsoshowsthat
neural ranking models have different characteristics compared with sparse ranking models. For
example, dense models are easier to be affected by adding non-relevant contents, and different
ranking architectures based on the same language model may lead to varied retrieval behaviors.
Although BERT-based ranking models are not well aligned with traditional IR axioms, recent re-
search [64] also shows that some dense retrieval model (e.g., ColBERT [113]) is aware of term
importanceintextrepresentations,andcanalsoimplicitlymimicexactmatchforimportantterms.
Overall, theoretical exploration of dense retrieval still remains to be an open research direc-
tion. More efforts are required to understand the relevance matching behaviors of dense retrieval
models.
3.3.2 Combining Sparse and Dense Retrieval models. In practice, the two kinds of retrieval
models can benefit each other by leveraging complementary relevance information. For exam-
ple, sparse retrieval models can be utilized to provide initial search results for training dense re-
trieval models [111, 156], while PLMs can be also used to learn term weights to improve sparse
retrieval[50,51](asdiscussedinSection 3.1.2).Inparticular,italsoshowsthatsparseretrievalmod-
els can be utilized to enhance the zero-shot retrieval capacity of dense retrieval models (detailed
in Section7.1).
Considering different relevance characteristics captured by both approaches, there are increas-
ingstudiesthatdevelopahybridofsparseanddenseretrievalmodels.Astraightforwardapproach
istoaggregatetheretrievalscoresofthetwoapproaches[ 111,161].Further,aclassifierisemployed
toselectamongsparse,dense,orhybridretrievalstrategiesspeciallyforeachindividualquery[ 5],
which aims to balance the cost and utility of retrievers.
However, these hybrid approaches have to maintain two different indexing systems (inverted
indexanddensevectorindex),whichistoocomplicatedtobedeployedinrealpractice.Toaddress
this issue, Lin et al. [139, 140] propose to learn low-dimensional dense lexical representations
by densifying high-dimensional sparse lexical representations, making it feasible to end-to-end
learn lexical-semantic combined retrieval systems. The basic idea of densification is to first di-
videthehigh-dimensionallexicalrepresentationsintoslicesandreduceeachslicedrepresentation
by specifically designed pooling approaches. Then, these representations are concatenated as the
dense lexical representation. For text ranking, the dense lexical representations and the dense se-
mantic representations(i.e., the representationof “[CLS]”) are combined for computing similarity
scores.TheexperimentresultsinReferences[ 139,140]demonstratethatthisapproachcanlargely
improve the retrieval effectiveness by combining the benefits of both kinds of representations,
showingasuperiorperformanceonzero-shotevaluation.Furthermore,Chenetal.[ 37]proposeto
learn a dense lexical retriever from the weakly supervised data constructed by BM25, so that the
learneddenselexicalretrievercanimitatetheretrievalbehaviorofthesparseretrievers.Inthisap-
proach,therepresentationsfromthedenselexicalretrieveranddensesemanticretriever(e.g.,DPR)
arecombinedforrelevancecomputation.IthasbeenshownthattheknowledgelearnedfromBM25
can help the dense retriever on queries containing rare entities, to improve the robustness [37].
4 TRAINING
After introducing the network architecture, we next discuss how to effectively train PLM-based
dense retrievers, which is the key to achieve good retrieval performance. Focused on the train-
ing of bi-encoder for first-stage retrieval, we will first formulate the loss functions and introduce
threemajorissuesfortraining(i.e.,large-scalecandidatespace,limitedrelevancejudgementsand
pretraining discrepancy), and then discuss how to address the three issues, respectively.
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.89:14 W. Xin et al.
4.1 Formulation and Training Issues
This section presents the formulation and issues for training the PLM-based dense retrievers.
4.1.1 Loss Function. In this part, we first formulate the retrieval task as a learning task via the
negative log-likelihood loss, and then discuss several variants to implement the loss functions.
Negativelog-likelihoodloss. FollowingthenotationsintroducedinSection 2.2,weassumethat
a set of binary positive relevance judgements (e.g., like or click) are given as supervision signals,
denoted byR= {⟨qi,d+
i ⟩},w h e r ed+
i denotes a relevant document for queryqi. To optimize the
dense retrievers, the core idea is to maximize the probabilities of the relevant texts w.r.t. queries.
For simplicity, we introduce the negative log-likelihood loss for a query-positive pair as follows:
L(⟨qi,d+
i
⟩)= −log ef (ϕ(qi ),ψ(d+
i ))
ef (ϕ(qi ),ψ(d+
i )) +∑
d′∈D−ef (ϕ(qi ),ψ(d′)) , (2)
where f (·) is a similarity function (often set to the dot product) that measures the similarity be-
tweenqueryembedding ϕ(qi) andtextembedding ψ(d+
i ),and ϕ(·) andψ(·) arethequeryencoder
and text encoder (Equation (1)), respectively. Here, we enumerate all the text candidates in the
collection Dexcept the positives for computing the exact likelihood. Futher, a negative sampling
trick is usually adopted to reduce the computational cost [111,207]:
L(⟨qi,d+
i
⟩)= −log ef (ϕ(qi ),ψ(d+
i ))
ef (ϕ(qi ),ψ(d+
i )) +∑
d′∈Nqi
ef (ϕ(qi ),ψ(d′)) , (3)
wherewesampleorselectasmallsetofnegativesamplesforquery qi,denotedby Nqi .Theabove
training objective advocates to increase the likelihood of positive texts and decrease the likeli-
hoodofsamplednegativeones.Actually,suchanoptimizationobjectiveissimilartotheInfoNCE
loss [266] from contrastive learning, where the loss is constructed by contrasting a positive pair
of examples against a set of random example pairs.
Other loss functions.Instead of optimizing over a set of negatives, triplet ranking loss [55,111]
optimizes the difference between a positive and a negative given a query, which is defined as
L(⟨qi,d+
i ,d−
i
⟩)= max
(
0,1−(f (ϕ(qi),ψ(d+
i )) −f (ϕ(qi),ψ(d−
i )))
)
.
It has been reported that negative log-likelihood loss is better than the triplet ranking loss [111]
based on the retrieval accuracy on NQ dataset. Different from the above loss functions,binary
cross-entropy(BCE) lossismorecommonlyusedtooptimizethererankermodel[ 188](detailed
inSection 6).Itfirstlyutilizesthedenserepresentationsofqueryandtexttoderiveamatchvector,
and then predicts the relevance probability of a query-text pair based on the match vector:
Pr(rel = 1|qi,di) = σ
(
д(ϕ(qi) ⊙ψ(di))
)
, (4)
whereσ(·) isthesigmoidfunction, д(·) isalinearfunctionand ⊙isvectorcombinationoperation
(e.g., concatenation). Then, the BCE loss can be constructed as follows:
L(⟨qi,di⟩) = −yqi,di ·Prqi,di −(1−yqi,di ) ·(1−Prqi,di ),
wheredi can be either positive or negative, and Prqi,di = Pr(rel = 1|qi,di) defined in Equation (4).
Similaritymeasurement. Toinstantiatethefunction f (·),variousvectorsimilarity(ordistance)
measurements can be used to compute the query-text similarity, including inner product, cosine
similarity, and Euclidean distance. Several studies have examined the effect of different similarity
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.Dense Text Retrieval Based on Pretrained Language Models: A Survey 89:15
functionsontheretrievalperformance.Thakuretal.[ 260]conductananalysisexperimentbytrain-
ing two BERT-based models (an identical parameter configuration on MS MARCO dataset) with
cosine similarity and inner product, respectively. They observe that the variants with cosine simi-
larity and inner product prefer retrieving shorter and longer documents, respectively. Karpukhin
et al. [111]also examine the effectof Euclideandistance as thedistance function,however,no sig-
nificantdifferenceswereobservedinretrievalperformance.Sofar,intheliterature,innerproduct
has been widely adopted as the similarity measurement.
4.1.2 Incorporating Optimization Constraints. Besides the above formulation, there are several
variants that aim to improve the optimization objective by incorporating more constraints.
Text-orientedoptimization. Intheabove,thenegativelog-likelihoodloss(Equation( 3))ismod-
eled in a query-oriented way, which optimizes the likelihood of texts conditioned on the query
and the text set. Similarly, we can model the relevance in a text-oriented way as follows:
LT (⟨qi,d+
i ⟩) = −log ef (ψ(d+
i ),ϕ(qi ))
ef (ψ(d+
i ),ϕ(qi )) +∑
q−∈Q−ef (ψ(d+
i ),ϕ(q−)) , (5)
where a set of sampled negative queries denoted byQ−is incorporated to compute the negative
query likelihood. Given both query-oriented and text-oriented loss functions, it has become an
important optimization trick to train the dense retrievers by jointly optimizing Equations (3)a n d
(5). Similar symmetric optimization tricks have been used in a number of follow-up studies [132,
134,290,295], showing an improved retrieval performance.
Text-text similarity constraints. Besides directly optimizing query-text similarities, Ren
et al. [218] find that it is useful to incorporatetext-text similarity constraints in the optimization
objective.Theyarguethatpreviousoptimizationgoalsaremainlyquery-centric,whichisdifficult
to discriminate between positive texts and semantically similar negative texts. Therefore, a text
similarityconstrainthasbeenproposed,assumingthatthesimilaritybetweenpositivepassage d+
andquery qshouldbelargerthanthesimilaritybetweenpositivepassage d+ andnegativepassage
d−, i.e.,fsim(d+,q) > fsim(d+,d−). Such a loss is formulated as follows:
LTT (⟨qi,d+
i ⟩) = −log ef (ψ(d+
i ),ϕ(qi ))
ef (ψ(d+
i ),ϕ(qi )) +∑
d′∈Nqi
ef (ψ(d+
i ),ψ(d′)) . (6)
The major difference between Equations (3) and (6) lies in the underlined part, where it incorpo-
rates text-text similarity as the normalization term. Not only optimizing the similarity between
queryandpositivetext,thisapproachtriestoincreasethedistancebetweenpositiveandsemanti-
cally similar negative texts.
4.1.3 Challenges for Training Dual-encoders.Despite the conceptual simplicity of bi-encoders,
itisnontrivialtotraincapablebi-encodersfordenseretrieval(Equation( 3)).Therearethreemajor
training issues for the effective training of bi-encoders.
•Issue 1:Large-scale candidate space. In retrieval tasks, there is typically a huge number of
candidates in a large text collection, while only a few texts among them are actually relevant to
a query. Consequently, it is challenging to train capable dense retrievers that perform well on
large-scale text collections. Specifically, due to the computational limits, we can sample only a
small number of negative samples for computing the loss function (Equation (3)) during training,
resultinginashiftinthecandidatespacebetweentraining(asmallpopulationoftexts)andtesting
(the entire collection). It has been widely found the sampled negatives have a significant effect on
the retrieval performance [111, 207,289].
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.89:16 W. Xin et al.
•Issue2: Limitedrelevancejudgements .Inpractice,itisdifficulttoconstructlarge-scalelabeled
data with complete relevance judgements for dense retrieval. Even though several benchmark
datasets with large sizes have been released (e.g., MS MARCO), it is still limited to training very
largePLM-baseddenseretrievers.Additionally,therelevanceannotationsinthesedatasetsarefar
fromcomplete.Therefore,“ falsenegatives”(actuallyrelevantbutnotannotated)arelikelytooccur
in negative sampling, which has become a major challenge for training dense retrievers [207].
•Issue3: Pretrainingdiscrepancy.PLMsaretypicallytrainedusingpre-designedself-supervised
loss functions, such as maskedword predictionand next sentence prediction[53]. These pretrain-
ing tasks are not specially optimized for the retrieval tasks [28, 165, 213], thus likely leading to
suboptimal retrieval performance. Besides, to represent a text, existing work usually adopts the
“[CLS]”embeddingofPLMsasthetextrepresentation,whilethe“[CLS]” embeddingisnotexplic-
itly designed to capture the semantics of the whole text [65]. Considering this issue, it needs to
design specific pretraining tasks that are more suited to dense retrieval for the underlying PLMs.
Theabovethreeissueshavebecomethemajortechnicalbottlenecksfordenseretrieval,attract-
ing much research attention. Next, we will review the recent progress on addressing the above
issues, and discuss three major techniques for dense retrieval, detailed in the following three sub-
sections: Section4.2 (negative selection for tackling Issue 1), Section4.3 (auxiliary and synthetic
data enrichment for tacklingIssue 2), Section4.4(knowledge distillation for tackling Issue2), and
Section 4.5 (pretraining for tackling Issues 2 and 3).
4.2 Negative Selection
To optimize the dense retriever (Equation (3)), a certain number of sampled negatives are needed
for computing the negative log-likelihood loss. Thus, how to select high-quality negatives has
become an important issue for improving the retrieval performance. Next, we will review three
major negative selection methods, and then present the theoretical discussions.
4.2.1 In-batch Negatives. A straightforward approach for negative selection is random sam-
pling, i.e., each positive text is paired with several random negatives. However, most PLMs are
optimized in a batch mode on GPU with limited memory, which makes it infeasible to use a large
number of negatives during training. Considering this problem, in-batch negatives are used for
optimizing the dense retriever: given a query, the positive texts paired with the rest queries from
thesamebatchareconsideredasnegatives.Assumethatthereare b queries(b > 1)inabatchand
each query is associated with one relevant text. With in-batch negative technique, we can obtain
b −1 negatives for each query in the same batch, which largely increases the number of available
negatives per query under the memory limit. The in-batch technique was firstly proposed in Ref-
erence [85] for the response selection task ofSmart Reply, while it has been explicitly utilized for
denseretrievalbyDPR[ 111].In-batchnegativesareshowntobeeffectivetoimprovethelearning
of bi-encoder by increasing the number of negatives [111,207].
4.2.2 Cross-batch Negatives. By reusing the examples from a batch, in-batch negative training
can increase the number of negatives for each query in a memory-efficient way. To further opti-
mize the training process with more negatives, another improvement strategy calledcross-batch
negatives [70,207] are proposed under the multi-GPU setting. The basic idea is to reuse examples
acrossdifferentGPUs.Assumethereare a GPUsfortrainingthedenseretriever.Wefirstcompute
the text embeddings at each single GPU, and then communicate them across all the GPUs. In this
way, the text embeddings from other GPUs can be also used as negatives. For a given query, we
obtaina ×b −1 negatives froma GPUs, which is approximatelya times as in-batch negatives. In
thisway,morenegativescanbeusedduringtrainingforimprovingtheretrievalperformance.The
idea of cross-batch negatives can be also extended to a single-GPU setting with the technique of
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.Dense Text Retrieval Based on Pretrained Language Models: A Survey 89:17
gradientcaching[ 70],whereonecanaccumulatemultiplemini-batchesforincreasingthenumber
of negatives (while taking a larger computational cost).
4.2.3 HardNegatives. Althoughin-batchandcross-batchnegativescanincreasethenumberof
availablenegatives,theycannotguaranteetogenerate hardnegatives,whichrefertotheirrelevant
texts but having a high semantic similarity with the query. It is particularly important to incor-
porate hard negatives to improve the capacity in discriminating between relevant and irrelevant
texts[111,207,289].Anumberofstudieshavedesigneddifferenthardnegativeselectionstrategies
for improving the retrieval performance. According to whether the negative selector (i.e., a sam-
pler over the text collection based on some relevance model) is fixed or updated, hard negatives
can be roughly divided intostatic hard negativesand dynamic hard negatives[289, 308]. Besides,
theselectedhardnegativesmightcontainnoisydata(e.g.,falsenegatives),andthus denoisedhard
negatives are also used to train dense retrievers [207]. Next, we present the detailed discussions
for each kind of hard negatives.
Statichardnegatives. Forstatichardnegatives,thenegativeselectorisfixedduringthetraining
of the dense retriever. The goal is to select negatives that are difficult to be discriminated by the
dense retriever. To achieve this, a straightforward way is to sample negatives from top retrieval
resultsfromsomeotherretriever,eithersparseordense.SinceBM25[ 178]usuallygivesverycom-
petitive retrieval performance, several studies select hard negatives based on BM25, i.e., sampling
lexically similar texts (but without containing the answers) returned by BM25 [111]. In Reference
[156],multiplekindsofnegativesarealsomixedfortraining,includingretrievednegatives(based
on BM25, coarse and fine semantic similarity) and heuristics-based context negatives. This study
shows that an ensemble approach combining models trained with mixed hard negatives is able to
improve the retrieval performance. As a follow-up study, the authors [157] further design three
fusionstrategiestocombinedifferentkindsofnegatives,namely,mixingfusion,embeddingfusion
andrankfusion.Besides,Hofstatteretal.[ 92]proposeatopic-awaresamplingmethodtocompose
training batches for dense retrieval, which first clusters the queries before training and then sam-
ples queries out of one cluster per batch. In this way, the examples in a batch are highly similar,
which implicitly derives hard negatives with the in-batch sampling technique.
Dynamic (or periodically updated) hard negatives.As discussed above, static hard negatives
areobtainedfroma fixed negativeselector.Sincethetrainingofdenseretrieverisaniterativepro-
cess,itisbettertouseadaptivelyupdatednegatives,called dynamichardnegatives ,formodeltrain-
ing. The ANCE approach [289] proposes to sample from the top retrieved texts by the optimized
retriever itself as negatives, which they callglobal hard negatives. It has been shown in Reference
[289]thatgloballyselectednegativescanleadtofasterlearningconvergence.Forretrievingglobal
negatives, it needs to refresh the indexed text embeddings after updating the model parameters,
whichisverytime-consuming.Toreducethetimecost,ANCEusesanasynchronousindexrefresh
strategy during the training, i.e., it performs a periodic update for eachm batches. Furthermore,
Zhan et al. [308] propose a new approachADORE for selecting dynamic hard negatives, which
samples negatives from dynamic retrieval results according to the being-updated query encoder.
Unlike ANCE [289], ADORE fixes the text encoder and the text embedding index, and utilizes an
adaptivequeryencodertoretrievetop-rankedtextsashardnegatives.Ateachiteration,sincethe
query encoder for negative selection is optimized during training, it can generate adaptive nega-
tives for the same queries. A note is that before training with dynamic hard negatives, the model
should be warmed up (i.e., BM25 and the STAR training approach in Reference [289]). To better
understandtheeffectofstaticanddynamicnegatives,wecanroughlytakeanadversarialperspec-
tive to illustrate their difference: for a given query, static approaches use fixed negatives during
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.89:18 W. Xin et al.
training (i.e., fixed generator), while dynamic approaches generate adaptive negatives according
to the being-optimized retriever (i.e., adaptive generator). Intuitively, dynamic hard negatives are
more informative to train the dense retrievers (i.e., the discriminator). Besides, considering the
large-scale text corpus, dynamic negatives can potentially alleviate the training-test discrepancy
in the candidate space, since it can “see” more informative negatives during training.
Denoised hard negatives.Negatives play a key role in the training of dense retriever, and it has
beenfoundthatdenseretrieversareactuallysensitivetothequalityofnegatives[ 202,207].When
thesampledtextscontainnoisynegatives,theytendtoaffecttheretrievalperformance.Thisissue
becomes more severe for hard negatives, which are more likely to be false negatives [4, 23, 324]
(i.e., unlabeled positives), because they are top-ranking retrieval results of high relevance scores
toqueries.Toresolvethisissue,RocketQA[ 207]proposesadenoisednegativeselectionapproach,
and it utilizes a well-trained cross-encoder to filter top-ranked texts that are likely to be false
negatives. Since the cross-encoder architecture is more powerful in capturing rich semantic in-
teractions, it can be utilized to refine the selected hard negatives from the bi-encoder. Given the
top-ranked texts retrieved by a bi-encoder, only the predicted negatives with confident scores by
thecross-encoderareretainedasfinalhardnegatives.Inthisway,theoriginallyselectednegatives
are denoised, which are more reliable to be used for training. More recently, SimANS [323]p r o -
posetosample ambiguousnegatives thatarerankednearthepositives,withamoderatesimilarity
(neither too hard nor too easy) to the query. They empirically show that such negatives are more
informative, and less likely to be false negatives.
4.2.4 Discussions on the Effect of Negative Sampling.As discussed in previous subsections, a
number of negative sampling approaches have been developed to enhance the retrieval perfor-
mance.Here,wepresentsomediscussionsabouttheeffectofnegativesamplingfordenseretrieval.
It has been shown that in-batch sampling (Section4.2.1) cannot generate sufficiently informa-
tivenegativesfordenseretrieval[ 156,157,289].Inparticular,Luetal.[ 157]analyzewhyin-batch
negatives may not include informative negatives. They cast the negative log-likelihood objective
with in-batch negatives as a special case of the ranking-basedNoise Contrastive Estimation
(NCE). Instead of sampling a negative from the whole collection, in-batch sampling considers a
rathersmallcollection(reflectingadifferentdistribution)forsampling,i.e.,theannotatedrelevant
texts to the queries in the query set. A similar discussion about the informativeness of in-batch
negatives is also presented in Reference [289]: Since the batch size and the number of informa-
tive negatives are significantly smaller than the collection size, the probability of sampling in-
formative negatives from a random batch (or mini-batch) tends to be close to zero. Besides, the
study in Reference [289] shows that the informativeness of negatives is key to the training of
dense retrieval models, from the perspective of convergence rate w.r.t. gradient norms. Further-
more, Zhan et al. [308] show that random negative sampling and hard negative sampling indeed
optimizedifferentretrievalobjectives:randomnegativesamplingmainlyminimizesthetotalpair-
wise errors, which might over-emphasize the optimization of difficult queries; while in contrast,
hard negative sampling minimizes top pairwise errors, which is more suited for optimizing top
rankings.
4.3 Auxiliary and Synthetic Data Enrichment
TotrainPLM-baseddenseretrievers,theamountofavailablerelevancejudgementsisusuallylim-
ited w.r.t. the huge number of model parameters. Therefore, it is important to increase the avail-
ability of (pseudo)relevance judgement data. Next, we discuss two approaches for enriching the
labeled data, namely, collecting auxiliary labeled data and generating synthetic labeled data.
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.Dense Text Retrieval Based on Pretrained Language Models: A Survey 89:19
4.3.1 Auxiliary Labeled Data Collection. Several studies propose to incorporate auxiliary la-
beled datasets for enriching the relevance judgements. Karpukhin et al. [111] collect five datasets
ofNQ[ 116],TriviaQA[ 109],WebQuestions[ 13],TREC[ 11],andSQuAD[ 211],thentrainamulti-
dataset encoder by leveraging all the training data (excluding the SQuAD dataset), and test the
unified encoder on each of the five datasets. As shown in Reference [111], the performance on
mostofthedatasetsbenefitsfrommoretrainingexamples,especiallythesmallestdatasetinthese
five datasets. They further conduct an analysis experiment by training DPR on NQ dataset only
and testing it on the WebQuestions and CuratedTREC datasets. The results show that DPR trans-
fers well across different datasets (focusing on the QA task), with some slight performance loss.
Furthermore, Maillard et al. [173] propose to train a universal dense retriever across multiple re-
trievaltasks.Specifically,theydevisetwosimplevariants(Variant1:separatequeryencodersand
sharedpassageencodersformultipletasks;Variant2:sharedqueryandpassageencodersformul-
tipletasks)ofDPR,andexaminetheuniversalretrievalcapacitywithmulti-datasettraining.They
show that such a multi-task trained model can yield comparable performance with task-specific
models and achieves a better performance in a few-shot setting. Besides leveraging multiple QA
datasets, one can also utilize different types of data sources. Oguz et al. [192] propose a unified
open-domainquestionansweringapproachbyutilizingmultipleresourcesoftext,tables,lists,and
knowledge bases. The basic idea is to flatten the structured data into plain texts, so that we can
process these data in a unified text form. As shown in Reference [192], overall, it is useful to com-
binemultiplesourcesintheexperimentsonfiveQAdatasets,inbothper-datasetandmulti-dataset
settings. When there is no training data for the target task, it becomes thezero-shot retrieval.I n
this setting, though we can leverage auxiliary datasets for allievating the data scarcity, it should
be noted that the final performance is highly affected by these auxiliary datasets [220, 241, 310]
(detailed in Section7.1).
4.3.2 Synthetic Labeled Data Generation. In addition to existing datasets, several studies pro-
pose to directly generate pseudo question-text pairs for retrieval tasks. Specially, these data gen-
eration approaches can be divided into two major categories, either a pipeline or end-to-end way.
For the first category, Alberti et al. [2] propose a pipeline approach to constructing question-
answer pairs from large text corpora. It consists of three major stages, including answer extrac-
tion, question generation and roundtrip filtering. Experimental results show that pretraining on
the synthetic data significantly improves the performance of question answering on SQuAD 2.0
andNQdatasets.Inasimilarway,Lewisetal.[ 123]furtherintroducepassageselectionandglobal
filtering to construct a dataset calledPAQfor question answering, which contains 65 million syn-
thetically generated question-answer pairs from Wikipedia. Based on the PAQ dataset, a related
study [193] pretrains the bi-encoder retrievers, leading to consistent performance improvement
over the variants pretrained with the tasks of ICT and BFS (Section4.5.1).
Furthermore,severalstudies[ 135,163,214,277]alsoexplorethegeneration-augmentedtraining
approach in thezero-shot setting, where there are no training datasets in the target domain. Their
resultsshowthatsyntheticdatagenerationisusefultoimprovethezero-shotretrievalcapacityof
dense retrievers. We will discuss more on zero-shot retrieval in Section7.1.
4.4 Knowledge Distillation
Considering that relevance judgement is limited, knowledge distillation becomes an important
approachtoimprovingthecapacityofthebi-encoder.Inmachinelearning,knowledgedistillation
referstotheprocessoftransferringknowledgefromamorecapablemodel(called teacher)toaless
capablemodel(called student)[87].Followingthisconvention,ourgoalistoimprovethestandard
bi-encoder (the student) with a more powerful model (the teacher) on a given labeled dataset.
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.89:20 W. Xin et al.
Trainingtheteachernetwork. Toimplementtheteachernetwork,wecanadoptawell-trained
cross-encoder for knowledge distillation, since it is more capable in modeling fined-grained se-
mantic interaction across queries and texts. Typically, the training of the teacher network is in-
dependent from the training of the student network. As an improved approach, RocketQA [207]
introducesanimprovedstrategybyincorporatingtheinformationinteractionbetweentheteacher
and student when training the teacher network. The basic idea is to randomly sample the top-
rankedtextsfromthestudentnetworkasnegativesfortrainingtheteacher.Thisstrategyenables
thecross-encodertoadjusttotheretrievalresultsbythebi-encoder.Besides,dualteachers,respec-
tively trained with pairwise and in-batch negatives, are adopted to improve the retrieval perfor-
mance of the student network [92]. Further, an empirical study [89] is conducted to analyze the
effectivenessofknowledgedistillationwithasingleteacherandateacherensemble,andithasbeen
foundthattheperformanceimproveswithincreasingeffectivenessofasingleteacherortheensem-
bleofteachers.While,itshouldbenotedthattheteachernetworkisnotnecessarytobethecross-
encoder. In principle, any retriever that is more capable than the basic student network can serve
astheteachernetwork.Forexample,Linetal.[ 141]explorethepossibilityofusinganenhancedbi-
encoder(i.e.,ColBERT[ 113]thatuseslateinteraction)astheteachernetwork.Asanotherinterest-
ingwork,Yuetal.[ 302]utilizeawell-trainedadhocdenseretrievertoimprovethequeryencoder
in a conversational dense retriever, to mimic the corresponding ad hoc query representation.
Distillation for the student network.After training the teacher network, we utilize it to im-
prove the student network. The basic idea is to run the well-trained teacher network on the unla-
beled (or even labeled) data to produce pseudo relevance labels for training the student network.
Accordingtothetypesofthederivedlabels,wecancategorizethedistillationapproachesintotwo
major categories, namely, hard-label and soft-label distillation.
Hard-label distillation. Given the unlabeled texts, the first approach directly sets the binary rel-
evance labels for unlabeled texts according to the relevance scores of the teacher network. Since
the predictions of the teacher network might contain noise or errors, thresholding methods are
oftenadoptedtoremovetextswithlowconfidencescores.Forexample,RocketQA[ 207]generates
pseudorelevancelabelsfortop-rankedpassagesaccordingtotheirrankingscores:positive(higher
than 0.9) and negative (lower than 0.1), and discards the rest with unconfident predictions. They
also manually examine a small number of denoised texts, and find that this method is generally
effective to remove false negatives or positives.
Soft-label distillation. Instead of using hard labels, we can also approximate the outputs of the
teacher network by tuning the student network. Formally, letr(t)
q,d and r(s)
q,d denote the relevance
scoresoftext d w.r.t.toquery q assignedbytheteachernetworkandthestudentnetwork,respec-
tively. Next, we introduce several distillation functions.
—MSE loss. This function directly minimizes the difference between the relevance scores be-
tween the teacher and student using mean-squared error loss:
LKD
MSE = 1
2
∑
q∈Q
∑
d∈D
(
r(t)
q,d −r(s)
q,d
)2
, (7)
where Qand Ddenote the sets of queries and texts, respectively.
—KL-divergence loss. This function first normalizes the relevance scores of candidate docu-
ments into probability distributions by queries, denoted by˜r(t)
q,d and ˜r(s)
q,d, respectively, and
then reduces their KL-divergence loss:
LKD
KL = −
∑
q∈Q,d∈D
˜r(s)
q,d ·
(
log ˜r(s)
q,d −log ˜r(t)
q,d
)
. (8)
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.Dense Text Retrieval Based on Pretrained Language Models: A Survey 89:21
—Max-margin loss. This function adopts a max-margin loss for penalizing the inversions in
the ranking generated by the retriever:
LKD
MM =
∑
q,d1,d2
max
(
0,γ −sign
(
Δ(t)
q,d1,d2
×Δ(s)
q,d1,d2
))
, (9)
whereq ∈Q,d1,d2 ∈D, Δ(t)
q,d1,d2
= r(t)
q,d1
−r(t)
q,d2
, Δ(s)
q,d1,d2
= r(s)
q,d1
−r(s)
q,d2
,γ is the margin and
sign(·) is the sign function indicating whether the value is positive, negative or zero.
—Margin-MSE loss. This function reduces the margin difference for a positive-negative pair
between the teacher and student, via the MSE loss:
LKD
M−MSE = MSE
(
r(t)
q,d+ −r(t)
q,d−,r(s)
q,d+ −r(s)
q,d−
)
. (10)
To examine the effectiveness of different distillation functions, Izacard et al. [102] perform an
empirical comparison of the above loss functions, and they find that the KL-divergence loss leads
to a better distillation performance than MSE loss and max-margin loss for the task of question
answering. Further, the researchers empirically find that the Margin-MSE loss is more effective
than the other options [89,92], e.g., pointwise MSE loss.
Advanced distillation methods.Recent studies [223, 321] show that knowledge distillation
mightbecomelesseffectivewhenthereexistsalargecapacitygapbetweentheteacherandstudent
models.Thus,insteadofusingdirectdistillation,aprogressivedistillationapproach[ 142,160,303]
should be adopted when using a strong teacher model, which can be implemented in two ways.
Asthefirstway,weusegraduallyenhancedteachermodelsatdifferentstagesofdistillation,tofit
the learning of the student model. PROD [142] proposes to use a progressive chain of the teacher
model with improved model architectures and increased network layers: 12-layer bi-encoder→
12-layer cross-encoder →24-layer cross-encoder (given the 6-layer bi-encoder as the student
model). ERNIE-Search [160] introduces two distillation mechanisms for reducing the large capac-
itygapbetweentheteacherandstudentmodels,namely,(i) late-interactionmodels (e.g.,ColBERT)
to bi-encoder, and (ii)cross-encoder to late-interaction modelsand then tobi-encoder.A st h es e c -
ond way, we fix the strong teacher model, and gradually increase the difficulty of the distilled
knowledge.CL-DRD[ 303]proposesacurriculumlearningapproach,andschedulesthedistillation
process with gradually increased difficulty levels: more difficult samples (with a larger query-text
similarity)arearrangedatalaterstage.Intheabove,weassumethattheteachermodelisfixeddur-
ingadistillationprocess.RocketQA-v2[ 219]extendsthedistillationwaybyintroducinga dynamic
listwisedistillation mechanism:boththeretriever(student)andthereranker(teacher)aremutually
updated and improved. Based on the KL-divergence loss (Equation (8)), the teacher model is also
able to be adjusted according to the student model, yielding a better distillation performance.
4.5 Pretraining for Dense Retrieval Models
The original purpose of PLMs is to learn universal semantic representations that can generalize
across different tasks. However, such representations often lead to suboptimal performance on
downstreamapplicationsduetothelackoftask-specificoptimization[ 28,120,165,213].Consider-
ingthisissue,recentstudiesemploytask-relatedpretrainingstrategiesfordenseretrieval.Besides
reducing the pretraining discrepancy (Issue 2), these approaches can also alleviate the scarcity of
labeled relevance data (Issue 3). Next, we describe the pretraining approaches for dense retrieval.
4.5.1 TaskAdaptivePretraining. Thislineofpretrainingtasksessentiallyfollowwhathavebeen
usedinBERT[ 53],buttrytomimictheretrievaltaskinaself-supervisedway.Below,welistseveral
representative pretraining tasks for dense retrieval.
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.89:22 W. Xin et al.
•Inverse Cloze Task (ICT) [120] randomly selects a sentence of a given text as the query, and
the rest sentences are considered as gold matching text. This task aims to capture the semantic
context of a sentence and enhance the matching capacity between query and relevant contexts.
•Body First Selection (BFS) [28] utilizes the sentences from the first section of a Wikipedia
articleasanchorstotherestsectionsoftexts.Itconsidersarandomsentencefromthefirstsection
as the query and a randomly sampled passage in the following sections as a matched text.
•Wiki Link Prediction (WLP) [28] utilizes the hyperlink to associate thequery with text,
where a sampled sentence of the first section from a Wikipedia page is considered as the query
and a passage from another article containing the hyperlink link to the query page is considered
text.
•Recurring Span Retrieval (RSR) [213] proposes to userecurring spans(i.e., ngrams with
multiple occurrences in the corpus) to associate related passages and conduct unsupervised pre-
trainingfordenseretrieval.Givenarecurringspan,itfirstcollectsasetofpassagesthatcontainthe
recurring span, and then transforms one of the passages into a query and treats the rest passages
as positives.
•Representative wOrds Prediction (ROP) [165] utilizes a document language model to sam-
ple a pair of word sets for a given text. Then, a word set with a higher likelihood is considered to
bemore“representative”forthedocument,andthePLMispretrainedtopredictthepairwisepref-
erencebetweenthetwosetsofwords.FollowingROP,anothervariantcalled B-ROP [166]replaces
the unigram language model by a BERT model, and samples the representative words from acon-
trastive term distributionconstructed based on document-specific and random term distributions.
Instead of using a pipeline way, Shakeri et al. [239] propose an end-to-end approach to gener-
ating question-answer pairs based on machine reading comprehension by using a pretrained LM
(e.g., BART [121]). It aims to train a sequence-to-sequence network for generating a pair of ques-
tion and answer conditioned on the input text. Reddy et al. [214] further extend this approach by
incorporating data selection for enhancing the generated question-text pairs for retrieval tasks.
4.5.2 Retrieval-augmentedPretraining. ToenhancethemodelingcapacityofPLMs,anotherline
ofpretrainingtasksistoincorporateanexternalretrieverbyenrichingtherelevantcontexts.The
basic idea is to enhance the training of themasked language modeling (MLM)task with more
referring contexts from a knowledge retriever.
Asarepresentativework,REALM[ 79]utilizesaknowledgeretrieverforretrievingrelevanttexts
fromalargecorpus,andtheretrievedtextsarefurtherencodedandattendedtoaugmentlanguage
model pretraining. In REALM, the basic idea is to reward or discourage the retriever according to
whether the retrieved context is useful to improve the prediction of the masksed words. Without
usingexplicithumanannotation,thecontextretrieverisfurthertrainedvialanguagemodelingpre-
training,inwhichtheretrievedtextsaremodeledbyalatentvariablethroughmarginallikelihood.
By fine-tuning the model, REALM performs well on the task of open-domain question answer-
ing. It further proposes to use an asynchronous optimization approach based on the maximum
inner product search. As an extension work, Balachandran et al. [10] perform a thorough tuning
of REALM on a variety of QA tasks. They conduct extensive experiments with multiple training
tricks,includingusingexactvectorsimilaritysearch,trainingwithalargerbatch,retrievingmore
documents for the reader, and incorporating human annotations for evidence passages. Their ex-
periments show that REALM was not sufficiently trained for fine-tuning, and it is important to
design suitable training and supervision strategies for improving open-domain QA systems.
4.5.3 Representation Enhanced Pretraining. For bi-encoder based dense retrievers, a typical ap-
proach is to utilize the inserted “[CLS]” token to obtain the representation of a query or text.
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.Dense Text Retrieval Based on Pretrained Language Models: A Survey 89:23
However, the original “[CLS]” embedding is not explicitly designed to represent the meaning of a
whole text. Hence, this may lead to suboptimal retrieval performance, and it needs more effective
approaches to enhance the “[CLS]” embedding for dense retrieval.
Autoencoder enhanced pretraining.Inspired by the success of autoencoders in data represen-
tation learning [49, 128,247, 248], several studies explore autoencoders to enhance the text repre-
sentation for dense retrieval. The basic idea is to compress the text information into the “[CLS]”
embeddingbyusinganencodernetwork,andthenuseapaireddecodertoreconstructtheoriginal
textbasedonthe“[CLS]”embedding.Inthisway,thelearned“[CLS]”embeddingisenforcedtocap-
turemoresufficienttextsemanticsthantheoriginalattentionmechanismsinPLMs.Gaoetal.[ 65]
proposetheCondenserarchitectureconsistingofthreeorderlyparts:earlyencoderbackbonelay-
ers, late encoder backbone layers and Condenser head layers (only used during pretraining). The
keypointisthatCondenserremovesfullyconnectedattentionacrosslateandCondenserheadlay-
ers,whilekeepingashortcircuitfromearlylayerstoCondenserheadlayers.SinceCondenserhead
layerscanonlyreceiveinformationoflatebackbonelayersviathelate“[CLS],”thelate“[CLS]”em-
beddingisthereforeenforcedtoaggregatetheinformationofthewholetextaspossible,forrecov-
ering the masked tokens. Similar attempts have been made inTransformer-based Sequential
Denoising Auto-encoder (TSDAE)[276], which is a Transformer-based sequential denoising
autoencoder for enhancing the text representation. Furthermore, co-Condenser [66] extends the
Condenser architecture by incorporating a query-agnostic contrastive loss based on the retrieval
corpus,whichpullsclosethetextsegmentsfromthesamedocumentwhilepushingawayotherun-
related segments. Following the information bottleneck principle [262],4 recent studies [240,279]
refertothekeyrepresentations(e.g.,the“[CLS]”embedding)thataimstocapturealltheimportant
semantics in the above autoencoder approaches asrepresentation bottlenecks.
Unbalanced autoencoder based pretraining.Recently, Lu et al. [159] have reported an inter-
esting finding that a stronger decoder may lead to worse sequence representations for the au-
toencoder. It is explained as thebypass effect: when the decoder is strong, it may not refer to the
information representation from the encoder, but instead perform the sequence generation condi-
tionedonprevioustokens.Theyprovidetheoreticalanalysisonthisfinding,andimplementaweak
decoder based on a shallow Transformer with restricted access to previous context. In this way, it
explicitlyenhancesthedependencyofthedecoderontheencoder.Thisworkhasinspiredseveral
studies that use the “strong encoder, simple decoder” architecture for unsupervised text represen-
tation pretraining. Based on such an unbalanced architecture, SimLM [279] pretrains the encoder
and decoder with replaced language modeling, where it aims to recover the original tokens after
replacement. To optimize the dense retriever, it further employs a multi-stage training procedure,
which uses hard negative training and cross-encoder distillation. Furthermore, RetroMAE [154]
proposes to use a large masking ratio for the decoder (50∼90%) while a common masking ratio
for the encoder (15%). It also introduces an enhanced decoding mechanism with two-stream at-
tention and position-specific attention mask. Based on RetroMAE, Liu et al. [143] present a rigor-
oustwo-stagepretrainingapproach(i.e.,general-corpuspretraininganddomain-specificcontinual
pretraining), which shows strong performance on a variety of benchmark datasets. Besides, Lex-
MAE[240]appliessucharepresentationenhancedpre-trainingstrategytolearnedsparseretrieval
(e.g.,SPLADE[ 63])basedonPLMs,whereitincorporateslexicon-basedrepresentationbottleneck
(i.e., continuous bag-of-words representations with learned term weights) for pretraining.
4InReference[ 262],theauthorsdefinethegoalofinformationbottleneckas“findingamaximallycompressedmappingof
the input variable that preserves as much as possible the information on the output variable.”
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.89:24 W. Xin et al.
Contrastive learning-enhanced pretraining.Another promising direction is to apply con-
trastivelearning(eitherunsupervisedorsupervised)toenhancethetextrepresentationsfordense
retrieval. Contrastive learning is originated from the field of computer vision (pioneered by the
worksofSimCLR[ 35]andMoCo[ 84]),wherethekeyideaistolearntherepresentationofimages
by making similar ones close and vice versa. Typically, contrastive learning is conducted in two
majorsteps.First,theaugmentedviewsaregeneratedforeachimageusingvarioustransformation
methods (e.g.,croppingand rotation).The augmented views are usuallyconsideredaspositives to
the original images, while randomly sampled instances are considered asnegatives to the original
images. Such a data augmentation process is particularly important to the final performance
of contrastive learning. Second, a discrimination task is designed assuming that the positives
should be closer to the original one than the negatives. Following the same idea, we can generate
large-scale positive and negative text pairs for unsupervised pretraining of text embeddings.
As a representative study, SimCSE [71] produces positives of one sample text by applying
different dropout masks, and uses in-batch negatives. ConSERT [294] uses four ways to generate
different views (i.e., positives) for texts, including adversarial attacks, token shuffling, cutoff and
dropout.Contriever[ 101]proposesgeneratingpositivesbyusingICTandcropping(i.e.,sampling
two independent spans from a text to form a positive pair) and generating negatives by using
in-batch and cross-batch texts. Experimental results demonstrate that unsupervised contrastive
pretraining leads to good performance in both zero-shot and few-shot settings [101]. In a similar
manner, LaPraDoR [290] generates positives by using ICT and dropout, and proposes an iterative
contrastivelearningapproachthattrainsthe queryand documentencodersin an alternative way.
In contrast to autoencoder enhanced pretraining, Ma et al. [164] further propose to pretrain an
encoder-only network with a new contrastive span prediction task, which aims to to fully reduce
thebypasseffectofthedecoder.Itdesignsagroup-wisecontrastiveloss,consideringtherepresen-
tations of a text and the spans that it contains as positive pairs and cross-text representations as
negative pairs. Besides, it has shown that unsupervised contrastive pretraining [184] (pretraining
with text pairs and text-code pairs) can also improve both text and code retrieval performance.
Discussion. Compared with the pretraining approaches in previous parts (Sections4.5.1, 4.5.2,
and 4.5.3), representation enhanced pretraining does not explicitly use retrieval (or retrieval-like)
tasks as optimization goals. Instead, it aims to enhance the informativeness of the text represen-
tation, i.e., the “[CLS]” embedding. For dense retrieval, the text representations should capture
thesemanticmeaningofthewholetext.Bydesigningeffectivepretraningstrategies(autoencoder
or constrastive learning), these approaches can produce more informative text representations,
thus improving the retrieval performance [66, 159, 276, 279, 290]. Besides, it has been shown
that these approaches can improve the retrieval performance even in zero-shot or low resource
settings [65,101,290], which also alleviates the scarcity issue of relevance judgements.
4.6 Empirical Performance Analysis with RocketQA
Previoussectionshavediscussedvariousoptimizationtechniquestoimprovethetrainingofdense
retrievers.Inthissection,wetakeRocketQA[ 207]asthestudiedmodelandexaminehowdifferent
optimization techniques improve its retrieval performance on benchmark datasets.
4.6.1 Experimental Setup. To prepare our experiments, we adopt the widely used MS MARCO
passageretrievaldataset[ 185]forevaluation,andthedetailedstatisticsofthisdatasetarereported
in Table4. For comparison, we consider two variants of RocketQA, namely, RocketQA [207]a n d
RocketQAv2[219],andarelatedextensioncalledRocketQA PAIR (originallycalled PAIR).ForRock-
etQA, it uses three major training tricks, namely, cross-batch negatives (Section4.2.2, increas-
ing the number of negative examples optimized for each query by reusing data across batches),
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.Dense Text Retrieval Based on Pretrained Language Models: A Survey 89:25
Table 3. Empirical Comparison of Different RocketQA Variants on MS MARCO Dataset for the Passage
Retrieval Task (in Percentage), Wherebs Denotes the Batch Size
Model Technique MRR@10
BM25 (0) BM25 18.7
DPRERNIE (1) in-batch ( bs = 4,096) 32.4
RocketQA
(2a) cross-batch ( bs = 4,096) 33.3
(2b) cross-batch (bs = 2,048) 32.9
(2c) cross-batch ( bs = 1,024) 31.4
(2d) cross-batch (bs = 512) 29.8
(2e) cross-batch ( bs = 256) 29.5
(2f) cross-batch ( bs = 128) 28.6
(2g) cross-batch (bs = 64) 26.9
(2h) cross-batch + hard neg w/o denoising (bs = 4,096) 26.0
(2i) cross-batch + hard neg w/ denoising (bs = 4,096) 36.4
(2j) cross-batch + denoised hard negatives + data augmentation (208K) (bs = 4,096) 36.5
(2k) cross-batch + denoised hard negatives + data augmentation (416K) (bs = 4,096) 36.9
(2l) cross-batch + denoised hard negatives + data augmentation (832K) (bs = 4,096) 37.0
RocketQAPAIR
(3a) in-batch + hard-label distillation (bs = 4,096) 37.2
(3b) in-batch + hard-label distillation + p-centric pretraining (bs = 4,096) 37.9
RocketQAv2
(4a) soft-label distillation (bs = 96) 36.5
(4b) soft-label distillation + joint retriever-reranker training (7 hard negs) (bs = 96) 36.5
(4c) soft-label distillation + joint retriever-reranker training (31 hard negs) (bs = 96) 37.3
(4d) soft-label distillation + joint retriever-reranker training (63 hard negs) (bs = 96) 38.1
(4e) soft-label distillation + joint retriever-reranker training (127 hard negs) (bs = 96) 38.3
denoised hard negatives (Section4.2.3, removing passages from negative passages that are likely
positive) and distillation-based data augmentation (Section4.4, using cross-encoder for distilla-
tion to construct more training data). For RocketQAv2, it incorporates a new soft-label distilla-
tionmechanism,calleddynamiclistwisedistillation(Section 4.4,jointlytrainingdual-encoderand
cross-encoder). For RocketQAPAIR , it is built on RocketQA and incorporates the passage-centric
pretraining technique (Section4.1.1, modeling the relation between query-passage similarity and
passage-passagesimilarity). Consideringthetunableconfiguration(e.g.,thenumberofnegatives
and batch size) of these models, we include multiple variants with different settings. Although
RocketQA variants do not include all the optimization techniques introduced in Section4,t h e y
provideaunifiedbasemodelbyexaminingtheeffectsofdifferentoptimizationtechniquesinarel-
atively fair way. As a comparison, we incorporate the classic BM25 [227] and DPR [111] methods
as baselines. To reproduce the experiments in this part, we implement a code repertory for dense
retrieval at the linkhttps://github.com/PaddlePaddle/RocketQA and release the script or code to
reproduce the results in Table3. This code repertory is implemented based on the library Pad-
dlePaddele, consisting of RocketQA, RocketQAPAIR and RocketQAv2. For a fair comparison, we
re-implement the DPR model with PaddlePaddele and use ERNIE as the base PLM [253].
4.6.2 Results and Analysis. Table3 presents the performance comparsion of different methods
or variants on the MS MARCO dataset for passage retrieval. We have the following observations:
•First, cross-batch technique is able to improve the retrieval performance. Compared with
DPRERNIE , the RocketQA variants 2a ∼2д have the same configuration and implementation, ex-
ceptthecross-batchtechnique.Suchatechniquecanleadtosignificantimprovement,i.e.,0.9per-
centage point in absoluteMRR@10 performance (variant 1 versus 2a). Besides, it is key to use as
more negatives as possible during the cross-batch training, which means that a large batch size
should be used.
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.89:26 W. Xin et al.
•Second,itiseffectivetousedenoisedhardnegatives,whichleadstoasignificantimprovement
of3.1percentagepointsinabsolute MRR@10performance(variant2 aversus2i).Asacomparison,
whenusingnon-denoisedhardnegatives,theperformancedecreasesquickly.Apossiblereasonis
that hard negatives are more likely to be false negatives that will harm the final performance.
•Third,dataaugmentationcanfurtherimprovetheretrievalperformancewithbothcross-batch
technique and denoised hard negatives, which leads to a substantial improvement of 0.6 percent-
age point in absoluteMRR@10 performance (variant 2i versus 2l). Besides, as we can see, the
performance improves with the increasing amount of pseudo labeled data.
•Fourth, passage-centric pretraining can boost the performance, leading to a substantial im-
provement of 0.7 percentage point in absoluteMRR@10 performance (variant 3a versus 3b). It
characterizes a more comprehensive semantic relation among query, positives, and negatives.
•Finally, the variant with dynamic listwise distillation (variant 4e) achieves the best perfor-
mance among all the RocketQA variants. Dynamic listwise distillation provides a joint training
approach for retriever and reranker based on soft-label distillation (detailed in Section6.2.3).
To conclude the discussion of this part, we can see that effective optimization techniques are
key to achieve good retrieval performance for dense retrievers.
5 INDEXING FOR DENSE RETRIEVAL
Previously, we have extensively discussed how to design and train a dense retrieval model from
an algorithmic perspective. To implement a dense retrieval system, it is important to develop an
efficientindexstructurethatcansupportrelevanceretrievalin densevectorspace.Inthissection,
we discuss the data structures and algorithms for efficient dense retrieval.
5.1 Traditional Inverted Index for Sparse Retrieval
To start with, we review the key data structure to support traditional sparse retrieval, i.e., term-
basedinvertedindex.Inessence,invertedindexmaintainsanefficientmappingfromtermstotheir
locationsindocuments[ 179].Thebasicideaistoconstructterm-basedpostinglistsbyaggregating
the occurrences of a term. To be specific, each term in the vocabulary is associated with a unique
posting list, and the posting list stores the identifiers of the documents (possibly with more de-
tailedpositionalinformation)inwhichthetermoccurs.Thekeypointofusingtheinvertedindex
for relevance evaluation is to examine the documents that at least contain a query term. For re-
trieval, given a query, it first fetches the postings lists associated with query terms and traversing
the postings to compute the result set. As major lexical retrieval toolkits, Apache Lucene and its
extensions including Elastic Search and Apache Solr have becomede factosoftware for building
invertedindex-basedretrievalsystems.Besides,Anserini[ 296],alsobuiltontopofLucene,allows
researchers to easily reproduce sparse retrieval models on standard IR test collections.
5.2 Approximate Nearest-neighbor Search for Dense Retrieval
Sincedenseretrievaldoesnotrelyonlexicalmatch,term-basedinvertedindexisnolongersuited
for embedding-based retrieval. Dense retrieval represents both queries and texts as dense vectors,
which can be cast into the problem ofnearest-neighbor search: finding the most close vector(s)
from a collection of candidate vectors (i.e., the texts in the collection) w.r.t. a query vector based
on some similarity or distance measurements. In what follows, we first formulate the nearest-
neighborsearchunderthesettingofdenseretrieval,thendiscusstwomajordirectionstoimprove
search efficiency, and finally introduce the implementation and software.
5.2.1 Formulation and Overview. In this part, we formulate the dense retrieval task as the
nearest-neighbor search problem in the vector space of embeddings. Given a collection of
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.Dense Text Retrieval Based on Pretrained Language Models: A Survey 89:27
candidate embeddingsP(P⊂ Rl and |P| = m) and a query embeddingq ∈ Rl, the goal of
nearest neighbor search [15, 242] is to efficiently search forp ∈P that are most close toq, i.e.,
p∗ = argmaxp∈P fsim(q,p),w h e r efsim(·) can be implemented by various similarity or distance
functions, e.g., inner product or cosine similarity. The most straightforward approach for nearest-
neighbor search is to enumerate all the candidate embeddings, i.e., computing the similarity
between a query embedding and each candidate embedding in the collection. However, it will
be very time-consuming when the number of candidate embeddings is large. To reduce the high
computational costs of brute-force enumeration, a number ofApproximate Nearest-neighbor
Search (ANNS) algorithms [15, 80, 131, 242] are developed to retrieve the approximates of the
exact nearest neighbors, possibly with some loss in search quality.
Generally speaking, the design of ANNS algorithms needs to make a trade-off between search
efficiency and quality. To evaluate different ANNS aglorithms, ANN-Benchmarks [8]h a sb e e nr e -
leasedwithamillion-scalebenchmark,andanenlargedversioncalledBig-ANN-Benchmarks[ 244]
furtherextendsthedatasizetoabillionscale.Thesetwobenchmarksmaintainpublicperformance
rankings of different ANNS algorithms under different settings, which provides a guidance to se-
lect suitable algorithms according to dataset size and efficiency/accuracy requirement. In general,
therearetwofeasibledirectionstoimprovethesearchefficiencyofANNS:(1)reducingtheamount
of similarity computes based on various index structures, and (2) reducing the overhead of each
similarity compute by product quantization. In what follows, we will discuss index structures for
ANNS(Section 5.2.2)andproductquantizationalgorithmsforANNS(Section 5.2.3),andintroduce
public ANNS software of for building efficient dense retrieval systems (Section5.3).
5.2.2 Improving Search Efficiency by Index Structures.As a major direction to improve search
efficiency,wecandesignspecialindexstructurestoreducetheamountofsimilaritycomputes,and
there are several ways to develop efficient index structure for dense retrieval, including hashing-
based approaches, clustering-based inverted index approaches, and graph-based approaches.
•Hashing-based approaches[99, 242, 275] assign vectors into different buckets according to a
pre-designedhashingfunction.Theideaistoallocatehighlysimilarvectors(i.e.,similartexts)into
the same buckets. Given a query vector, we employ a hash function to map it into some specific
bucket,andthensearchthevectorsonlyinthesebuckets.Inthisway,thesearchefficiencywillbe
improved. Such an approach is more suitable when the dimension of vectors is small.
•Clustering-basedindexapproaches [245]partitionthesearchspacebyclustering.Givenaquery
vector, we first compare it with the cluster centroids for finding the most similar clusters. Then,
we can locate a limited number of clusters that contain the target vectors, and further perform
in-cluster search for finding the target vectors. This approach is flexible to use, with high search
quality and reasonable efficiency. Further, this approach can be implemented in a memory-disk
hybrid way for indexing large-scale data, where cluster centroids are stored in memory while
posting lists to centroids are stored in disk [34].
•Graph-based approaches[174,274] (e.g., SPTAG and HNSW) work by navigating a graph that
is built by associating the vertices with their nearest neighbors. Given a query, it will search
on the graph by greedily selecting the similar neighbors. Such approaches are efficient due to
the small world phenomenon[114]: the average path length of between two vertices (that might
be not directly connected) is short. Graph-based approaches usually perform well on large high-
dimensional datasets, while taking a high memory cost.
5.2.3 ImprovingSearchEfficiencybyProductQuantization. Besidesreducingtheamountofsim-
ilarity computes, another possible way is to reduce the overhead of each similarity compute. Dif-
ferentfromsparserepresentations(mostlycomposedofintegeridentifiers),denserepresentations
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.89:28 W. Xin et al.
Fig. 3. Illustration of text representation and similarity search based on product quantization (PQ). Here,
we assume that there areb1 centroid sets, each withb2 centroid embeddings. In this representation, an
original text vector will be mapped tob1 PQ indices, where each PQ index maps to a centroid embedding
from the corresponding centroid set. The quantization-based representation of a text is a vector ofb1 PQ
indices (corresponding to theb1 nearest centroid embeddings). When evaluating the similarity of a text, we
can simply sum the entries from the similarity table with itsb1 PQ indices.
(i.e., text embeddings) are composed of real-value numbers, and they take significantly more time
for similarity computation, leading to an increase in memory cost.
In this part, we describe aproduct quantization (PQ) approach [72, 105] to compressing
the text embeddings and accelerating similarity compute, reducing both time and memory costs.
In practice, product quantization can be also jointly used with efficient index structures, e.g.,
clustering-basedindexandgraph-basedindex.Inthefollowing,wewillfirstdescribehowtocom-
press the text embeddings as quantization-based representations, and then introduce how such
representations reduce the cost of similarity computation when searching for a query. To intro-
duce product quantization, we focus on discussing the case of a single embedding vectorvi ∈Rl,
w h i l ei tc a nb ee a s i l ye x t e n d e dt oas e to ft e x te m b e d d i n g s .
Quantization-basedrepresentations. AsshowninFigure 3(a),tocompressthetextembeddings,
given a corpus ofm texts, product quantization [72, 105] firstly constructsb1 centroid sets of em-
beddings,witheachconsistingof b2 centroidembeddings.Torepresentan l-dimensionalvector vi,
we first split it intob1 equal-length subvectors{v(j)
i }b1
i=1. Then, thejth subvectorv(j)
i ∈Rl/b1 from
thevector vi ismappedtothenearestcentroid(recordedasa PQindex rangingfrom1to b2)inthe
corresponding jth centroid set. Since the number of centroid embeddings in a centroid set is usu-
allysmallerthan256(i.e., b2 < 256),wecanuseonebytetorepresentaPQindex,whichisableto
largelyreducethespacecost.Finally,theoriginalvectoriscompressedasavectorof b1 PQindices
(withb1 bytes), calledquantization-based representation. The quantization-based representations
of all text embeddings in a corpus can be pre-computed and stored before search.
Accelerated similarity computation.Figure3(b) illustrates how similarity computation can be
implemented in an efficient way by using quantization-based representations. First, a query em-
beddingissplitinto b1 subvectorsinthesamewayasthattextvectorsaresplit.Then,wecompute
the similarity between each query subvector and the centroid embeddings in the corresponding
centroidset.Asaresult,asimilaritytableisconstructedwith b1 ·b2 entries(eachentryrepresents
the similarity score between one query subvector and one centroid embedding), and the table en-
tries can be accessed by the PQ indices. To evaluate the similarity of a text w.r.t. the query, we
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.Dense Text Retrieval Based on Pretrained Language Models: A Survey 89:29
employ the quantization-based representation of a text (i.e., a vector ofb1 PQ indices) to look up
the similarity table, and then sum the correspondingb1 entries as the similarity score of the text.
Note that the similarity table will be shared for all the texts, i.e., the entire corpus only requires
a total ofb1 ·b2 embedding similarity computes, independent of the large corpus size. With this
similarity table, the similarity evaluation of a text can be efficiently implemented by table lookup.
Optimizingquantization-basedindexforretrieval. Theoriginalpurposeofproductquantiza-
tionistosolvegeneralvectorcompressionproblems,anditcannotbedirectlyend-to-endlearned
in neural network models, due to the involved non-differentiable operations. Besides, dense re-
trieval requires specific optimization strategies (e.g., negative sampling), which are not supported
byproductquantizationtechniques.Toaddresstheaboveissues,Zhanetal.[ 306]proposeanend-
to-end learning approach based on product quantization for dense retrieval, where three major
optimization strategies are proposed, including ranking-oriented loss, centroid optimization, and
end-to-endnegativesampling.BasedonReference[ 306],theauthorsfurtherincorporatethetech-
niqueofconstrainedclusteringtoachieveabettercentriodassignmentforagivendocument[ 307].
Besides, Zhang et al. [312] also propose several improved training strategies to jointly optimize
deepretrievalmodelswithproductquantization-basedembeddingindex.Yamadaetal.[ 292]incor-
poratealearnedhashfunctiontorepresentbothqueriesandtextsasbinarycodesinanend-to-end
way. The proposed model achieves comparable performance to DPR, and further significantly re-
duces the computational and memory costs.
Besides efficiency improvement, it has been found that discrete representations derived from
dense embeddings are useful to interpret the retrieval results by analyzing different aspects of
input that a dense retriever focuses on Reference [305].
5.3 Implementation for ANNS Algorithms
In dense retrieval systems, efficient similarity search is used at multiple stages. The primary use
is to recall the most relevant text embeddings given the query vector. Besides, it can be also ap-
plied to select the (static or dynamic) hard negatives or retrieve supporting context. For example,
REALM [79] implements a textual knowledge retriever that retrieves related context information
for a given query, and ANCE [289] adopts it to identify global negatives based on vector search.
As aforementioned, most of existing dense retrieval studies adopt Faiss [108] for nearest-
neighbor search, which is a publicly released library for efficiently searching and clustering large-
scale dense vectors. Basically, Faiss provides the exact nearest-neighbor search function, which
is widely used by previous studies [207, 289]. Moreover, it supports the implementations of sev-
eral ANNS approaches discussed in Section5.2.3 and Section5.2.2 (including clustering-based ap-
proaches, graph-based approaches, and quantization approaches) in both CPU implementations
andGPUimplementations,sinceefficiencyisasignificantfactorinpractice.Italsosupportsdiffer-
entsimilarityfunctionsforvectorsearch,includingdotproductandcosinesimilarity.Tomeetthe
requirementsofeffectivenessandefficiency,userscansettheappropriateconfigurationwithFaiss
according to different hardware profile and data sizes. Besides, HNSWlib [175] and SPTAG [33]
arelibrariesfocusingontheefficientimplementationforgraph-basedapproach.ScaNN[ 78]isde-
velopedbasedonquantization-basedapproaches,whichachievesexcellentperformanceonANN-
Benchmarks.Distributed-Faiss[ 200]providesthedistributedsolutionsforANNS,whentheindex
istoolargetofitintothememoryofasinglemachine.Thissurveymainlylimitsthediscussionto
in-memory solutions. When the data size is extremely large (e.g., 100 billion scale), it is generally
not feasible to use completely in-memory ANNS index for dense retrieval, where we need to de-
velop multi-level index using a hybrid of memory index and disk index, e.g., DiskANN [250]a n d
SPANN [34].
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.89:30 W. Xin et al.
6 INTEGRATION WITH RERANKING
Inacompleteretrievalsystem,itusuallyconsistsoffirst-stageretrieversandsubsequentrerankers.
Inthissection,wefirstbrieflyintroducetheretrievalpipeline,andthenpresentseveralapproaches
to optimizing the retrieval pipeline.
6.1 Retrieval Pipeline
This part introduces the retrieval pipeline and PLM-based rerankers.
6.1.1 General Retrieval Pipeline. To start our discussion, we consider a simplified retrieval
pipeline,consistingoftwomajorstages,namely,thefirst-stageretrievalandthereranking.Toob-
tain the final ranked list, a certain number of possibly relevant documents (e.g., several hundreds
to thousands) to a given query are retrieved from a corpus by a retriever, such as dense retriever
or BM25. Then, the candidate texts are scored and reranked by a more capable relevance model
(called reranker). Finally, top-ranked texts (e.g., several or tens) will be returned as the search re-
sults to downstream tasks, such as question answering and dialog systems. Generally, first-stage
retrievalandrerankingstageshavedifferentfocusesinaretrievalsystem[ 9,137,188].First-stage
retrieval aims to efficiently recall relevant candidates from a large text corpus. As a result, it is
practically infeasible to employ time-consuming models in first-stage retrieval. In contrast, the
goal of reranking is to reorder the candidate results from the proceeding stages, where the num-
ber of candidate texts is generally smaller (e.g., hundreds or thousands) and more complicated
models can be used to implement the rerankers. Therefore, bi-encoder is often used for imple-
menting the retriever, and cross-encoder is often used as the architecture of the reranker. Note
that a retrieval pipeline usually contains multiple reranking stages by successively refining a re-
ducedcandidatelistforproducingthefinalresults.Besides,theremayalsoexistmultiplefirst-stage
retrieversinapracticalretrievalsystem,wheretheresultsfrommultipleretrieversareaggregated
as the input of the rerankers. Interested readers can refer to References [9, 57, 137] for detailed
discussions.
6.1.2 PLM-based Rerankers and Multi-stage Ranking.In this part, we first discuss the typical
rerankermodelsbasedonPLMs,andthenintroducemulti-stagerankingmechanismthatinvolves
PLM-based rerankers.
Reranker models. To implement the reranker, a typical approach is to employ the cross-
encoder as the ranking model, showing substantial improvements over the traditional meth-
ods [188, 204, 284, 293]. Specifically, BERT is widely used to implement the cross-encoder for
estimating the relevance score [188, 204], e.g., monoBERT [191]. As the input, a query and a can-
didate text are concatenated as a sequence proceeded by the “[CLS]” token. After encoding the
query-text sequence, the “[CLS]” embedding is adopted as the match representation between the
queryandtext,whichwillbesubsequentlyfedintoaneuralnetworkforcomputingtherelevance
scoreofthetextbeingrelevant(Equation( 4)).Therelevancescoreiscomputedforeachtextinde-
pendently,andthefinalrankingoftextsisobtainedbysortingthemaccordingtorelevancescores.
For training the rerankers, it usually formulates the ranking problem as a binary classification
task [188] using the BCE loss as shown in Equation (5). To optimize the BCE loss, it also needs to
generate negatives for learning, which can be randomly selected or sampled from the top results
of the retriever [68, 325]. Furthermore, duoBERT [191] implements the BERT-based text ranking
inapairwiseway,whereittakesasinputaqueryandapairoftextstocompare(withaconcatena-
tion pattern “[CLS]query [SEP]text1 [SEP]text2”). The training objective is to reserve the partial
order of semantic relevance for a given text pair, such that it can predict the relevance preference
for ranking the texts. Besides encoder-only models, encoder-decoder-based PLMs (e.g., T5) have
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.Dense Text Retrieval Based on Pretrained Language Models: A Survey 89:31
been also utilized to implement the reranker, which takes as input a query-text pair and outputs
the relevance label [189] or ranking score [330].
Multi-stage ranking.To construct a complete system, a commonly used strategy is to arrange
theretrieverandthereranker(s)inaprocessingpipeline.Nogueiraetal.[ 191]presentamulti-stage
rankingarchitecturetoconstructaneffectiveretrievalpipeline.Inthisarchitecture,thefirst-stage
ranker is implemented by the BM25 algorithm, while the second-stage and third-stage rankers
are implemented by the monoBERT and duoBERT, respectively. In this approach, different stages
score each candidate separately. Recently, a new retriever-reranker integration architecture has
been proposed [318] for multi-stage ranking optimization (a similar approach SetRank [194]
has been proposed). As the major novelty, the learned relevance information from retriever and
reranker are transformed into input embeddings and positional embeddings, respectively, for
another Transformer encoder, which produces the final ranking scores. Instead of scoring each
candidate individually, the Transformer encoder derives the ranking scores by taking a listwise
contrastive loss. Thus, the relevance information from the retrieval and reranking stages can be
effectively utilized and optimized in a listwise way. In practice, multi-stage ranking is widely
adopted in production systems (e.g., Facebook Search [95] and Walmart Product Search [171]),
where more comprehensive factors are considered and more complicated strategies are designed.
Besides, there is a large body of studies that utilize PLMs to enhance pre-BERT neural rank-
ing models, e.g., CEDR [170]. In these studies, PLMs are used to generate contextual features to
enhance the semantic representation or similarity, which do not serve as the ranking backbone.
Thus, we omit the discussion of this topic, and interested readers can find a detailed discussion
in References [22,28,168].
6.2 Retrieval Pipeline Training
The retrieval pipeline forms a cascaded structure by stacking the retriever and (one or multiple)
reranker(s), and it needs to design suitable optimization algorithms for the entire pipeline. Next,
we introduce three major optimization approaches for the retrieval pipeline. For simplicity, we
onlydiscussthescenariowhereonlyaretrieverandarerankerareinvolvedinaretrievalpipeline.
6.2.1 SeparateTraining. Astraightforwardapproachistoseparatelyoptimizetheretrieverand
reranker, considering them as two independent components. In this approach, the training of the
retrieverandrerankeraretransparenttoeachotherwithoutinformationinteraction(exceptthere-
trievalresults).Typically,wecanfirstlyoptimizetheretrieverandthenlearnthereranker.Suchan
optimizationapproachcannotsufficientlycapturethecascadinginformationcorrelationsbetween
differentstagesintheretrievalpipeline,thuspossiblyleadingtoasuboptimalrankingperformance.
Thereasonsaretwofold.First,whentheretrievedcandidatelistimproves,thecontainednegatives
alsobecomeincreasinglydifficulttobediscriminatedbythereranker,whicharemorelikelytobe
falsenegatives[ 68,207].Second,withoutconsideringthefirst-stageretrievalresults,theoptimiza-
tion of reranker cannot be well adjusted to the result distribution of the retriever, which is not
speciallyoptimizedaccordingtotheentirepipeline[ 219].Therefore,itisnecessarytoincorporate
information interaction between the retriever and reranker during training.
6.2.2 Adaptive Training. As an improvement approach, adaptive training enables the informa-
tion interaction between the retriever and reranker. The basic idea is to let the two components
adapt to the retrieval results (or intermediate data representations) of each other to achieve a bet-
ter overall retrieval performance. For example, Qu et al. [207] propose to train a cross-encoder
by sampling from top results of the retriever (a dual-encoder) as negatives, which lets the cross-
encoder adjust to the distribution of the retrieval results by the dual-encoder. Another commonly
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.89:32 W. Xin et al.
adopted way is to alternatively update the retriever and reranker during the training process. At
each iteration, the fixed component can provide necessary relevance information or guidance sig-
nals to another being-optimized component. Trans-encoder [144] designs an iterative joint train-
ing framework by alternating between the cross-encoder and bi-encoder. Specifically, during the
learningprocess,onecomponentwillproducepseudorelevancelabelsforupdatingtheotherone.
Furthermore, an adversarial learning approach has been proposed in Reference [311]: the roles of
the retriever and ranker are to retrieve negative documents and identify ground-truth documents
from a mixed list including both positive and negative ones, respectively. The entire optimization
process of the retriever and ranker is formulated as a minimax game.
6.2.3 Joint Training. In implementation, retriever and reranker are often optimized in differ-
ent ways, which makes it difficult for joint optimization. Specially, the retriever is usually trained
by maximizing the probabilities of positive passages against a list of sampled negatives. It is op-
timized according to the overall ranking for the candidate list of positive and negatives, called
listwise approach.5 As a comparison, the training of reranker is modeled aspointwise or pairwise
optimization, where the model is learned based on a single text or a pair of texts. Due to the dif-
ferent optimization ways, it is difficult to jointly train the two components. To address the above
issue, RocketQAv2 [219] proposes a unified listwise learning approach to jointly optimizing a bi-
encoder retriever and a cross-encoder reranker. For both retriever and reranker, their relevance
predictions are modeled as listwise distributions. To unify the learning process, RocketQAv2 de-
signsa dynamiclistwisedistillation mechanismbydistillingthererankertotheretriever.Different
from previous distillation methods, it adaptively updates the retriever and reranker by enforcing
the interaction of relevance information between the two modules. For optimizing the listwise
distillation, RocketQAv2 also employs data augmentation to construct high-quality training data.
Extension. In the above subsection, we have discussed how to optimize a retrieval pipeline that
integrates a dense retriever and reranker(s). Besides retrieval systems, dense retrievers have been
widely used in a variety of application systems that require external knowledge resources, such
as open-domain question answering and entity linking. For example, REALM [79] constructs a
retrieval-augmented question answering system that can be jointly trained based on asynchro-
nous embedding index update. Other related studies [103, 124, 229] also discuss how to optimize
retrieval-augmentedsystemsforopen-domainquestionanswering.Ingeneral,withdenseretrieval
techniques,itiseasiertodevelopretrieval-basedapproachesfordownstreamtasks.Wewilldiscuss
the use of dense retrieval techniques for specific applications in Section8.
7 ADVANCED TOPICS
In this section, we discuss several important advanced topics for dense retrieval.
7.1 Zero-shot Dense Retrieval
The success of dense retrievers heavily relies on large-scale relevance judgement data. This poses
a challenge for a wide range of task scenarios, as it is difficult and expensive to acquire sufficient
training corpus when a new domain or task is introduced. In addition, due to large discrepancies
across various domains, it becomes challenging for a model trained on a specific dataset to adapt
to other domains. Thus, it is important to examine the zero-shot capabilities of dense retrievers,
as well as their out-of-distribution performance.
Zero-shot evaluation datasets.To our knowledge, BEIR [260] is the first heterogeneous bench-
mark for examining the zero-shot capabilities of dense retrieval methods. BEIR includes nine
5Differentfromlearningtorank[ 26],here, listwise denotesthattheoptimizationisbasedonanentirecandidatelist[ 219].
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.Dense Text Retrieval Based on Pretrained Language Models: A Survey 89:33
different retrieval tasks spanning 18 diverse datasets. Based on BEIR, it has been found that a
number of bi-encoder dense retrievers that outperform BM25 on in-domain evaluation perform
poorly on out-of-distribution evaluation, while cross-attentional reranking models and late inter-
action models have better zero-shot performance on BEIR. Furthermore, Sciavolino et al. [236]
create a dataset containing simple entity-centric questions and find that BM25 significantly out-
performsdenseretrieversonthisdataset.Theiranalysisshowsthatdenseretrieversperformbetter
on common entities than rare entities, and can also generalize to unseen entities to some extent
when question patterns exist in the training data. Liu et al. [147] further measure three kinds of
generalization capacities in different settings based on NQ dataset, including training set over-
lap,compositionalgeneralization,andnovel-entitygeneralization.Theyfindthatdenseretrievers
perform worse in the latter two settings than the first setting.
Key conclusions of influence factors on zero-shot dense retrieval.Besides the findings
drawnfromtheoverallperformancecomparison,itisessentialtounderstandhowdifferentfactors
affect the performance of dense retrievers in a zero-shot setting. For this purpose, Ren et al. [220]
have conducted a thorough study on the effect of underlying influencing factors on zero-shot re-
trievalperformancebasedonatotalof12commonlyusedretrievaldatasets.Thisstudyframesthe
zero-shotretrievalsettingbyintroducingsourcedomaindata(availablefortraining)andtargetdo-
main data (only available for testing). They mainly focus on examining the influence factors from
sourcetrainingcorpus(i.e.,queryset,documentset,anddatascale),andalsodiscussotherfactors
suchasquerytypedistributionandvocabularyoverlap.Theyempiricallyfindthatsourcetraining
datasethassignificantinfluenceonthezero-shotretrievalperformanceofthedenseretrievalmod-
els,sinceonlysourcedomaindatacanbeutilizedfortraining.Suchaneffectcanbeattributedtoa
number of specific factors, including vocabulary overlap (larger is better), query type distribution
(morecomprehensiveisbetter ),anddatascale( morequeriesarebetterbutnotfordocuments ).Besides,
they also find that the dataset bias of the test set potentially affects the performance comparison:
Since some datasets are constructed based on lexical matching models [249, 265], they tend to be
more favorable for sparse retrieval methods due to a larger term overlap.
Existing solutions for zero-shot retrieval.To improve the performance of dense retrievers
in the zero-shot setting, there are generally three important approaches explored in the litera-
ture [220], including training data augmentation for target domain, term-matching capacity en-
hancement, and model size scaling. Next, we introduce each approach in detail.
•Augmenting the target training data. Considering the lack of labeled data from the target do-
main,amajorsolutionistogeneratelarge-scalesyntheticdataforimprovingthetrainingofdense
retrievers. After being trained on large-scale synthetic training data, the zero-shot capabilities of
denseretrieverscanbeimprovedtosomeextent.Typically,adatagenerationmodelisemployedfor
generating large-scale query-text pairs in the target domain [135,163,214,277]. Recently, Promp-
tagator[52]leveragesalargelanguagemodelconsistingof137billionparameters,i.e.,FLAN[ 285],
to generate large-scale query-text pairs by using only a small number of labeled examples. As an
alternativeapproach,knowledgedistillationiscommonlyadoptedfortacklingthescarcityoftrain-
ingdata,whichutilizesamorecapableteachermodeltoimprovethestudentmodel[ 37,259,277]in
zero-shotretrievalscenarios.Besides,severalstudies[ 101,290]conductunsupervisedpretraining
by leveraging large-scale positive and negative pairs with different data augmentation methods,
e.g., ICT [120] and SimCSE [71].
•Enhancingtheterm-matchingcapability .Unlikesparseretrievers(e.g.,BM25),denseretrievers
no longer rely on exact term matching for text retrieval, but instead learn semantic matching
for capturing the text relevance. While, empirical studies show that the capability of exact term
matchingisusefultoimprovezero-shotretrievalperformance[ 36,260],sincetermoverlappingisa
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.89:34 W. Xin et al.
strongindicatorfortextrelevance.Thus,sparseretrieversareeasiertoadapttozero-shotsettings
without training data. Inspired by this, several studies propose to enhance the lexical matching
capacity of dense retrievers by leveraging sparse retrievers, such as the fusion of rankings [36]o r
relevancescores[ 290]forbothsparseanddenseretrievers.Besides,wecanalsoemployknowledge
distillation for improving dense retrievers [37], taking sparse retrievers as the teacher model.
•Scaling up the model capacity.Recently, scaling law for language models has been widely ex-
plored for raising the performance bar of various tasks. It has been shown useful to improve the
zero-shot performance by increasing the model size of dense retrievers. As shown in Reference
[187], based on a T5-based dense retriever trained on large-scale question-answer pairs, scaling
up the model size with multi-stage training can significantly improve the zero-shot retrieval per-
formance on the BEIR benchmark. Similarly, performance improvement has been observed when
the model size is increased from 0.1 billion to 2.4 billion in ERNIE-Search [160].
Besides,Zhanetal.[ 310]examinehowdenseretrieversgeneralizetoout-of-domaintestqueries,
calledextrapolationcapacity intheirpaper.Theyfindthatcross-encodercanextrapolatewell,but
notbi-encoderandsparseretriever. Thispartcanbeextendedtoamoregeneraltopic low-resourced
dense retrieval, and readers can find a comprehensive discussion in a recent survey [241].
7.2 Improving the Robustness to Query Variations
Besidestheout-of-distributionissueinzero-shotretrieval,denseretrievalmodelsareshownmore
sensitive toquery variationsthan traditional lexical matching methods [38, 196, 333]. Generally,
queryvariationsexistwidelyinrealsearchbehaviors,e.g.,unexpectedquerytyposduetospelling
errors and diverse query formulations for the same information need. We next discuss the effect
of query variations on retrieval performance and introduce several enhanced training methods.
Effect of query variations on retrieval.There are increasing concerns on the robustness of
dense retrieval models to query variations. These studies typically create different types of query
variations and examine the performance of dense retrieval models under different query varia-
tions. Zhuang et al. [333] present a study on the impact of query typos based on character op-
erations (insertion, deletion, substitution, and swap), showing a significant performance drop for
BERT-basedretrieverandreranker.Penhaetal.[ 196]aimtoexaminehowdifferenttypesofquery
variations negatively affect the robustness of the retrieval pipeline. To generate query variations,
theyconsiderfoursyntax-changingtypes(misspelling,naturality,ordering,andparaphrasing)and
two semantics-changing types (gen./specialization, aspect change) for query transformations. Ex-
perimental results show that retrieval pipelines are sensitive to query variations (especially the
misspelling), which lead to an average 20% drop on performance compared with that evaluated
on original queries. Similar findings are reported about the performance drop due to query typos
inReference[ 38],wheretheyconsidereighttypesofquerytypos,includingnewtransformations
such as adding extra punctuation, stopword removal, and verb-tense shift.
Enhancedtrainingmethods. Toimprovetherobustness,existingstudiesproposeaseriesofen-
hanced training methods that take the query variations into consideration. Basically, these meth-
odsutilizedataaugmentationstrategiestoincorporateaugmentedqueriesintrainingdata,sothat
dense retrieval models can be aware of query variations during training. Zhuang et al. [333]p r o -
pose a typos-aware training approach, which changes the queries (with 50% chance) in training
set according to different typo types. In this way, the retrieval model is trained with a training
set consisting of both original queries and augmented queries with different typo types. Given
such an augmentation dataset, Sidiropoulos and Kanoulas [243] propose to construct a query-
side contrastive loss: modified queries and randomly sampled queries are considered as positives
and negatives, respectively. Based on the work in Reference [333], the same authors [335] further
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.Dense Text Retrieval Based on Pretrained Language Models: A Survey 89:35
proposeaself-teachingapproachbyincorporatingadistillationloss,anditrequirestheunderlying
retrieval model to produce similar rankings under original queries and corresponding variations.
TheycharacterizesuchanideabyreducingtheKL-divergencelossbetweenthedistributionsover
the same candidate list for original and augmented queries. Similarly, Chen et al. [38]p r o p o s ea
local ranking alignment method that enforces the original queries and the corresponding varia-
tions to produce similar rankings: the probability distributions of the in-batch passages given an
original query and its variation should be similar, and the probability distributions over in-batch
queries or their variations given a passage should be similar.
Indeed, query variations are not always harmful for retrieval systems. For example, we can uti-
lize query extensionor query rewriting[27] to derive better search results by reformulating the is-
suedquery.Furthermore,thereareseveralstudiesthatfocusonthemodelvulnerabilities[ 145,283],
showingthatdenseretrievalmodelsarebrittletodeliberateattacks.Thus,adversarialtrainingap-
proachesareoftenusedforenhancingthemodelrobustness[ 145,283].Welimitourdiscussionto
query variations, more likely to occur in real search scenarios than deliberate attack [196].
7.3 Model-based Retrieval
Recently,model-based retrieval[180, 222, 258] has been proposed as an alternative paradigm for
information retrieval. It essentially learns a parametric model to capture or encode the semantic
information of the whole text collection, and further fulfills the retrieval task by predicting the
identifiersofrelevantdocuments(i.e.,docids)basedonthelearnedmodel.Thisparadigmprovides
a generative viewpoint to understand information retrieval. Since existing model-based retrieval
approaches [133, 327] are also developed by pretraining and fine-tuning the Transformer archi-
tecture, we include this topic to complement feasible retrieval approaches based on pretrained
models.
The generative scheme.In essence, model-based retrieval adopts a generative scheme for pre-
dictingrelevanttexts.Indeed,theideaofgenerativeretrievalhasbeenfirstexploredinthetaskof
entity linking [25], where entity identifiers (e.g., unique entity names) are generated conditioned
on the text context in an autoregressive manner, and then GRLS [117] extends this idea to long
sequence retrieval by introducing multi-step generative retrieval. Specially, the perspective pa-
per [180] envisions themodel-based paradigmthat simplifies the classicindex-retrieve-then-rank
paradigmbyaunifiedrelevancemodel.Further,arepresentativework[ 258]called DSI developsa
generativeretrievalmodelbasedonaseq2seqencoder-decoderarchitecture(i.e.,T5),wherequery
and docids correspond to input and output, respectively. DSI frames the approach by introducing
two key procedures:indexing (associating the content of a document with its corresponding do-
cid) andretrieval (autoregressively generating docids given the input query). They are essentially
relatedtothetwokeyissues:(i)howtorepresentthetextswithmeaningfuldocidsand(ii)howto
map a query to relevant docids. Next, we discuss the two issues in detail.
Representing docids.Since, in model-based retrieval, we do not directly use the text content
for relevance evaluation, we need to design effective docid representations to reflect the underly-
ingsemanticsofatext.DSI[ 258]introducesthreemajordocidrepresentationmethods,including
unstructuredatomicidentifiers(auniqueintegeridentifier),simplestringidentifiers(tokenaizable
strings)andsemanticallystructuredidentifiers(clustering-basedprefixrepresentation).Intuitively,
thesecondapproachismoreflexibletouse,andinprincipleonecanusevariouskindsofsequence
datatorepresentadocument,e.g.,titlesandURLs;whilethethirdapproachseemstobemoremean-
ingfulatthecostofadditionalpre-processing(e.g.,clusteringbasedonBERTembeddings),which
can be further effectively leveraged by a prefix-based decoding way. DSI shows that the semanti-
cally structured identifiers leads to the best performance in its implementation. Later extensions
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.89:36 W. Xin et al.
almost follow the three major methods for representing docids: DynamicRetriever [327] (atomic
docid: original docid), SEAL [14] (simple string docid: full ngram signatures with the efficient
support of Ferragina Manzini index), NCI [282] (semantic string docid: hierarchical clustering),
DSI-QG [332] (simple string docid: generated query), TOME [222] (simple string docid: URL),
Ultron [326] (simple string docids: URL and title, semantic string docid: product quantization),
GERE [31] (semantic string docids: title and evidence), and CorpusBrain [32] (semantic string do-
cid: title).
Pretraining for semantic mapping.The essence of model-based retrieval is to establish the se-
manticmappingfromqueriestodocids,withoutanexplicitreferencetothetextcontent.Thus,in
principle, it is more difficult to train generative retrieval models than previous bi-encoder mod-
els. For learning query-to-docid mapping, DSI employs two major training strategies, namely,
memorization-basedpretraining(contentmapping)andretrieval-orientedfine-tuning:theformer
learns to map the content of a document to its corresponding docid and the latter learns to map
queries to relevant docids. For content mapping, various pretraining tasks are further proposed
to enhance the association between the text content and the docids [32, 258, 326], e.g., mapping
a sampled content (e.g., passage, word sets, and ngrams [326,327]) or associated content (e.g., an-
chor text [32]) from a text to its docid. Such a strategy can be considered aslearning from pseudo
queries, where various kinds of sampled contents are considered as pseudo queries. Besides, since
the amount of available relevance judgement data is usually limited, query generation is useful
in these generative retrieval models for enhancing the learning from queries to docids [326, 332],
where the original content of a document can be employed to generate potential queries.
Comparison with traditional models.Different from traditional retrieval approaches, model-
based retrieval simplifies the classic index-retrieve-then-rank paradigm witha generation-based
paradigm,whereamoreunifiedsolutiontoinformation-retrievaltaskscouldbepotentiallydevel-
oped for various Web resources, such as figures and videos (as envisioned in References [180]).
Since such an approach is built on a fully parametric model, it is more easier to be optimized in
an end-to-end way. We could potentially get rid of the tedious pipeline training (e.g., jointly opti-
mizing the retriever and reranker) in existing retrieval systems. Further, without using elaborate
inverted-index or large-sized embedding index, we do not need to manage a complicated index
structure for retrieval, which is easier to use and serve in real-world systems. However, most
of model-based retrieval studies only demonstrate the effectiveness of their approaches on MS
MARCO subsets or task-specific datasets, while evaluation at a larger scale (e.g., full MS MARCO
dataset) is still challenging for model-based retrieval. This emerging retrieval paradigm is still
under exploration.
7.4 Retrieval-augmented Language Model
In recent years, neurallanguage models (LM), especially PLMs, have proven to be powerful [12,
181, 197, 209] in a variety of tasks. It is shown that PLMs can encode a large amount of semantic
knowledge [21,199] into large-scale model parameters. To enhance the representation capacity, a
number of studies explore the scaling law of PLMs [21, 210] by increasing the model size, which
makes the training and use of PLMs prohibitively expensive in a resource-limited setting.
To alleviate this issue, retrieval-augmented LMs have been proposed by allowing the model to
explicitly access external data [19, 79, 83, 112, 300]. Retrieval-augmented models tend to be more
parameterefficient,sincetheyretrievetheknowledgeratherthanfullystoringitinmodelparam-
eters.Khandelwaletal.[ 112]present kNN-LM,anapproachthatlinearlyinterpolatesbetweenthe
LM’s next word distribution and the next word distribution ofk-nearest-neighbor model. The ex-
perimentsdemonstratethat kNN-LMworksparticularlywellwhenpredictingrarepatterns,since
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.Dense Text Retrieval Based on Pretrained Language Models: A Survey 89:37
itallowsexplicitretrievalofrarepatterns.Furthermore,Yogatamaetal.[ 300]incorporateagating
mechanism to replace the fixed interpolation parameter inkNN-LM, which adaptively combines
short-termmemorywithlong-termmemory(i.e.,knowledgeretrievedfromexternalmemory)for
prediction conditioned on the context.
Since external knowledge resource is very important to retrieval-augmented LMs, Borgeaud
etal.[ 19]furtherexplorescalinguptheretrievalcorpustotrillionsoftokensforlargeLMsranging
from150Mto7Bparameters,whichleadstoanimprovedcapacityofLMs.Inthesestudies,LMsare
frozen pretrained retrievers. As a comparison, Guu et al. [79] propose REALM to jointly optimize
the knowledge retriever and the knowledge-augmented encoder (detailed in Section4.5.2).
8 APPLICATIONS
In this section, we review the applications of dense retrieval technique in three aspects, namely,
the applications to different retrieval settings and industry practice.
8.1 Specific Retrieval Scenarios
Inthispart,wedescribeseveralspecificretrievalsettingsandcorrespondingretrievalapproaches.
8.1.1 Temporal Retrieval. Most of existing works conduct the study of dense retrieval on syn-
chronicdocumentcollections(e.g.,Wikipedia).However,therearealargeproportionoftemporal-
dependent questions in real application scenarios. To advance the research on temporal retrieval,
Zhang et al. [315] create an open-retrieval question answering dataset calledSituatedQA,w h e r e
the systems are required to answer questions given the temporal or geographical contexts. Their
experimental results show that existing dense retrievers cannot produce satisfying answers that
arefrequentlyupdated.Besides,Wangetal.[ 273]createanotherlargequestionansweringdataset
called ArchivalQA, consisting of 1,067,056 question-answer pairs, for temporal news question an-
swering. These datasets can be used to develop time-sensitive dense retrievers.
8.1.2 StructuredDataRetrieval. Previoussectionsmainlyreviewtheprogressofdenseretrieval
onunstructuredtext.Recently,denseretrievalhasalsobeenappliedtostructureddata,e.g.,tables
andknowledgebases.Herzigetal.[ 86]applyabi-encoder-baseddenseretrieverontableretrieval,
which is a Transformer-based language model pretrained on millions of tables with the aware-
ness of tabular structure. As shown in Reference [86], the proposed approach outperforms BM25
by more than 40 percentage points on the NQ-TABLES dataset. Oguz et al. [192] further study
the problem of unifying open-domain question answering with both structured and unstructured
data. They first linearize tables and knowledge bases into texts, and then train a bi-encoder-based
dense retriever on multiple datasets consisting of texts, lists, tables, and knowledge bases. The
experimental results show that the proposed approach performs well on both entity-oriented and
text-oriented benchmarks. Kostic et al. [115] also explore the unified retrieval by proposing a tri-
encoder,withoneuniqueencodereachforquestion,textandtable,respectively,showingthatthe
tri-encoder performs better than the bi-encoder with one encoder for the question, the other one
for both text and tables.
8.1.3 MultilingualRetrieval. Intheliterature,monolingualretrieval(mainlyinEnglish)ismost
commonly studied, since available large-scale labeled datasets are mainly in English (e.g., MS
MARCO and NQ). To facilitate the research on other languages rather than English, Bonifacio
etal.[ 18]createamultilingualversionoftheMSMARCOpassagerankingdatasetusingmachine
translation, calledmMARCO, consisting of 13 languages. The experimental results show that the
models fine-tuned on multilingual datasets perform better than that fine-tuned only on the origi-
nalEnglishdataset.Zhangetal.[ 317]presentthemulti-lingualbenchmarkdatasetcalled Mr.TYDI,
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.89:38 W. Xin et al.
consisting of monolingual retrieval corpus in 11 languages. They show that BM25 performs bet-
ter than a multi-lingual adaptation of DPR on this benchmark dataset. The above multilingual
retrieval settings aim to find the answer from the corpus in the same language as the question.
However, many languages face the scarcity of text resources, where there are few reference texts
written in a specific language. Hence, Asai et al. [6] propose a cross-lingual task, calledCross-
lingualOpenRetrievalQuestionAnswering(XORQA) ,thatrequiresansweringquestionsin
one language based on a text in another language. They create a large-scale dataset consisting of
40Kquestionsacrosssevennon-Englishlanguagesforthisproposedtask.Theproposedapproach
relies on machine translation modules for question translation and answer translation. Since the
multi-step machine translations (e.g., query translation and answer translation) in previous ap-
proachesmayleadtoerrorpropagation,Asaietal.[ 7]furtherproposeamultilingualdensepassage
retriever that extends DPR to retrieve candidate passages from multilingual document collections
in a cross-lingual way. It adopts an iterative training approach to fine-tuning a multilingual PLM
(e.g., mBERT [201]), and achieves performance improvement in a number of languages.
8.1.4 Other Retrieval Tasks. So far, dense retrieval has been widely applied to various domain-
specificretrievaltasks,suchasmathematicalIR[ 320],coderetrieval[ 158],andbiomedicalIR[ 162].
In essence, when considering a specific retrieval task, we need to adapt the dense retrievers to
effectively fit domain-specific text data (continual pretraining is often needed), deal with special
contentfeaturesorformats(e.g.,formulasorcode)andresolveotherpossibletrainingissues(e.g.,
lack of training data). Besides, another important topic is cross-modal retrieval [278], where we
consider a retrieval setting across different modalities (e.g., text-to-image retrieval and text-to-
video retrieval). The key to these cross-modal tasks is to establish the fusion or mapping between
different modalities. With the development of vision-language pretraining models exemplified by
CLIP [208], the capacity of learning shared representations across modalities has been largely
enhanced, which can improve the downstream cross-modal retrieval tasks. While, this survey is
mainly focused on text resources for retrieval, and interested readers can refer to Reference [278]
for a detailed discussion.
8.2 Industrial Practice
Besides research-purpose studies, considerable efforts have attempted to deploy dense retrieval
techniques in real systems. We review these works in three typical applications as follows.
In Web search, the term mismatch between search queries and web pages is one of the major
challenges in practice. For enhancing the semantic matching capacity, Liu et al. [151]p r o p o s ea
multi-stagetrainingapproachtooptimizingdenseretrieverbyleveragingbothlarge-scaleweakly
supervised training data (e.g., search logs) and small-scale human labeled training data. Such a
multi-stagetrainingapproachleadstosignificantimprovementcomparedtosingle-stagetraining.
Besides,sincedenseretrievalislesscapableinlexicalmatching,anintegrationofdenseandsparse
retrieval techniques has been employed in Baidu search [151] and Spotify search [255].
In sponsor search [205], the systems should optimize multiple objectives simultaneously, e.g.,
query-advertisement (ad) relevance and clicked-through rate (CTR). Under a traditional
retrieval pipeline, the retriever first retrieves the relevant ads by only considering the relevance
between queries and ads, and then the reranker reorders the retrieved ads according to the CTR
criteron. Fan et al. [56] argue that the separation of relevance learning and CTR prediction tends
to produce sub-optimal performance, and they propose to train a retriever that jointly considers
both relevance and CTR. Specifically, they train the retriever from the clicked logs augmented
by a relevance teacher. The proposed system is calledMobius and has been deployed into Baidu’s
sponsor search. As another solution, Zhang et al. [313] propose to unify knowledge distillation
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.Dense Text Retrieval Based on Pretrained Language Models: A Survey 89:39
and contrastive learning for training a more capable retriever, namely,Uni-retriever. Specifically,
the retriever learns to retrieve high-relevance ads by distilling the knowledge from a relevance
teacher, while learns to retrieve high-CTR ads by contrastive learning. It has been reported that
Uni-Retriever has been included as one of the major multi-path retrievers in Bing’s sponsor
search [313]. Besides, Dodla et al. [55] fuse dense retrievers and non-autoregressive generators
in a multi-task setting (sharing the same encoder), showing a significant performance gain in bid
keyword retrieval.
Inpersonalizedsearchandproductsearch,thepersonalizedfeaturesandproductfeaturesshould
be incorporated into the retrievers. Huang et al. [95] incorporate the embeddings of search con-
text (social relation, location, etc.) as the input of dense retriever. Liu et al. [153] incorporate the
embeddings of product images as the input of dense retriever. In these systems, Huang et al. [95]
and Li et al. [130] also find that hard negatives are important for training dense retrievers (as we
discussedinSection 4.2).Magnanietal.[ 171]designahybriddense-sparsesystemfore-commerce
searchdeployedatWalmart,andtheyselectnegativesbasedonthreestrategiesincludingproduct
type matching, query token matching and MonoBERT-based scoring.
9 CONCLUSION AND REMAINING ISSUES
In this survey, we thoroughly review the recent progress of dense retrieval based on PLMs. As an
important evolution of language intelligence techniques, PLMs empower dense retrieval models
withexcellentmodelingcapacitiestocaptureandrepresenttextsemanticsforrelevancematching.
Oursurveyhasextensivelydiscussedthekeyissuesandthemainstreamsolutionsinfourmajoras-
pectstodevelopdenseretrievalsystems,includingarchitecture,training,indexingandintegration.
Next, we briefly summarize the discussions of this survey and introduce some remaining issues
for dense retrieval.
PLM-based architecture. We have reviewed two kinds of PLM architectures for dense re-
trieval,namely,bi-encoder( efficient yetlesseffective)andcross-encoder( effectiveyetlessefficient ),
which are often used to implement the retriever and reranker, respectively. Since the bi-encoder
has limited capacity in capturing the semantic interaction between query and text, the multi-
representation technique (i.e., learning multiple contextual representations of a text) has been
widelyexploredinenhancingthefine-grainedinteractionmodeling.Althoughdenseretrievalmod-
elshaveachieveddecentretrievalperformanceonseveralbenchmarkdatasets,ithasbeenshown
that their capacities in some specific settings are rather limited, especially the capability on zero-
shotretrieval[ 260].Comparedtosparseretrievers(e.g.,BM25),denseretrieversperformbetterfor
semantic matching, but less well for lexical matching [111, 236]. Thus, there has been increasing
attention in combining the merits of both dense retrievers and sparse retrievers in a unified re-
trieval system [37,139]. Moreover, dense retrieval models rely on latent semantic representations
for relevance matching. It is useful to explore more interpretable mechanisms to understand how
denseretrievalmodelsbehaveandperforminresponsetodifferenttypesofqueries,e.g.,howthey
mimic “bag-of-words” retrieval [161]. In addition, instead of adapting a dense retriever to some
specific dataset, it is meaningful to develop more general retrievers over multiple domains, and
specificarchitecturedesigns(e.g.,mixtureofexperts[ 61])wouldbeworthfurtherexplorationfor
retrieval models. Furthermore, in some cases, dense retrievers need to process very long text, and
length-extrapolatable techniques [252] are required to facilitate the accurate modeling of the text
semantics.
Training approach. Compared with previous neural IR methods, PLM-based dense retrieval
models are more difficult to be optimized given the large number of parameters. Even with the
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.89:40 W. Xin et al.
ever-increasinglabeleddataset,itstillrequiresspecifictrainingtechniquestoeffectivelyoptimize
large retrieval models. We have summarized three major issues to address for improving dense
retrieval systems, i.e., large-scale candidate space, limited relevance judgements, and pretraining
discrepancy. Focused on the three issues, the researchers have made extensive efforts by design-
ingvariouseffectivetrainingstrategies,largelyraisingtheperformancebarofdenseretrieverson
benchmark datasets [65, 111, 207, 289]. In particular, negative selection is a key step in the train-
ing approach, and usingmore negatives (e.g., in- and cross-batch negatives) ormore high-quality
negatives (e.g., sampling hard negatives) typically lead to a better retrieval performance. Besides,
to enhance the capacity of bi-encoder, it is very useful to distill a more capable teacher model
(e.g., cross-encoder) and design suitable pretraining tasks (e.g., representation-enhanced pretrain-
ing). More recently, auto-encoder and contrastive learning-based pretraining methods have been
widely adopted to enhance the semantic representations of texts. As another improvement direc-
tion, it is important to enhance the performance of dense retrievers under low-resourced setting
(e.g., zero-shot retrieval), and data- and parameter-efficient training approaches are useful for en-
hancingthegeneralizationcapacityofdenseretrievers.Furthermore,transferlearningcanbealso
employed to mitigate the scarcity of labeled data by leveraging data-rich domains.
Dense vector index.Without the support of a suitable index structure, the search process (i.e.,
findingthemostclosetextvectorsw.r.t.queryvectors)wouldbeextremelyslowfordenseretrieval.
To tackle this issue, ANNS algorithms [74, 108, 242] have been widely used, achieving highly ef-
ficient retrieval performance (e.g., sublinear time cost [242]). Compared with term-based index,
it is more difficult to maintain the ANNS index for frequent operations of adding, deleting and
updating the text embeddings. For example, when the optimization involves in the update of in-
dexed embeddings, it usually suffers from theindex stalenessissue [79]. Therefore, it is necessary
todevelopefficientonlineupdatealgorithmsforenhancingtheflexibilityofANNSindex.Besides
the effectiveness, the cost for maintenance and use is also important for the practical deployment
of dense retrieval systems [90]. In practice, quantization techniques have become a major techni-
cal approach to alleviate the cost of dense vector index. To fit quantization for the retrieval task,
it is important develop effective approaches to optimize quantization-based vector index, which
become very difficult due to the involved non-differential operation [306,312]. Overall, the dense
vector index should be well suited to both the architecture and optimization of the PLM-based
retrievers, and more research is still required in this line.
Retrieval pipeline.An information-retrieval system typically adopts a pipeline way (consisting
of first-stage retrieval and reranking stages) to recall and rank the texts, which gradually reduces
the search space and refines the top-ranked texts. Generally, it is more complicated to optimize
the entire retrievalpipeline than a single-stage componentin it (e.g., first-stageretriever), though
a joint optimization approach is preferred. As the major difficulty for joint training, these stages
are usually learned according to different optimization goals, based on varied input and output.
In early studies [191], the multiple stages in a retrieval pipeline are separately optimized. Recent
studies propose a series of improved optimization approaches for jointly training the retrieval
pipeline [79, 219]. The basic idea is to let the retriever and the reranker adjust according to the
learnedrelevanceinformationfromeachother.However,itisstillchallengingtooptimizeamulti-
stage retrieval pipeline, especially when it is built in a sparse-dense hybrid way. Besides, it is also
meaningfultostudyhowtoautomaticallyconstructthepipeline(e.g.,whatcomponentsshouldbe
usedfordifferentstages)bylearningfromlabeleddataset.Furthermore,toreducethemaintenance
cost, it is also useful to develop effective approaches (e.g., distillation) to simplify the multi-stage
ranking pipeline by merging consecutive ranking components.
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.Dense Text Retrieval Based on Pretrained Language Models: A Survey 89:41
Recently, large language models (LLMs)[21, 319] have shown more superior performance
than previous relatively small PLMs, whichhas inspiredmore thoughtson the researchparadigm
andtechnicalapproachofinformationretrieval.Despitethepowerfulcapacities,LLMsaremainly
focused on generation-based task solving, and also incur a higher cost on tuning and inference,
makingitdifficulttofullysubstituteforPLMsfordevelopingdenseretrievers.Overall,PLM-based
dense retrievers and LLMs bear reciprocal or mutual relations in recent research [329]. On the
one hand, LLMs are powerful in possessing rich world knowledge and various abilities, which
are beneficial to enhance dense retrievers in multiple aspects, including complex semantic under-
standing [177], ranking refinement [251], search result extraction [291], and low-sourced domain
adaptation[17].Ontheotherhand,LLMsstillrelyonexternalinformationresourceswhendealing
with domain-specific tasks [299] or those outside its knowledge scope [195], where dense retriev-
ers have become necessary tools for finding the relevant evidence [221] or the demonstration
examples[212].Generally,bothresearchdirections(i.e.,LLMsforsearchandsearchforLLMs)are
promising to explore for improving the quality of information-seeking systems for human users.
APPENDICES
A SUPPLEMENTARY CONTENT
In this section, we introduce the available datasets, the evaluation metrics, and the supporting
libraries for dense retrieval.
Table 4. Detailed Statistics of Available Retrieval Datasets
Categorization Domain Dataset #q in train #label in train #q in dev #q in test #instance
Information
retrieval
Web MS MARCO [185] 502,939 532,761 6,980 6,837 8,841,823
Web mMARCO [18] 808,731 — 101,093 — 8,841,823
News TREC-NEWS [246] — — — 57 594,977
Biomedical TREC-COVID [224] — — — 50 171,332
Biomedical NFCorpus [20] 5,922 110,575 324 323 3,633
Twitter Signal-1M [249] — — — 97 2,866,316
Argument Touché-2020 [16] — — — 249 528,155
Argument ArguAna [271] — — — 1,406 8,674
Wikipedia DBPedia [82] — — 67 400 4,635,922
Web ORCAS [43] 10.4M 18.8M — — 3,213,835
Wikipedia EntityQuestions [236] 176,560 186,367 22,068 22,075 —
Web MS MARCO v2 [45] 277,144 284,212 8,184 — 138,364,198
Web DuReaderretrieval [206] 97,343 86,395 2,000 8,948 8,096,668
Question
answering
Wikipedia Natural Questions [116] 152,148 152,148 6,515 3,610 2,681,468
Wikipedia SQuAD [211] 78,713 78,713 8,886 10,570 23,215
Wikipedia TriviaQA [109] 78,785 78,785 8,837 11,313 740K
Wikipedia HotpotQA [298] 85,000 170,000 5,447 7,405 5,233,329
Web WebQuestions [13] 3,417 3,417 361 2,032 —
Web CuratedTREC [11] 1,353 1,353 133 694 —
Finance FiQA-2018 [172] 5,500 14,166 500 648 57,638
Biomedical BioASQ [265] 3,743 35,285 — 497 15,559,157
StackEx. CQADupStack [94] — — — 13,145 457,199
Quora Quora [100] — — 5,000 10,000 522,931
News ArchivalQA [273] 853,644 853,644 106,706 106,706 483,604
Web CCQA [97] 55M 130M — — —
Other tasks
Wikipedia FEVER [261] — 140,085 6,666 6,666 5,416,568
Wikipedia Climate-FEVER [54] — — — 1,535 5,416,593
Scitific SciFact [272] 809 920 — 300 5,183
Scitific SciDocs [41] — — — 1,000 25,657
Here, “q” is the abbreviation of queries, and “instance” denotes a candidate text in the collection.
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.89:42 W. Xin et al.
A.1 Datasets
Compared with traditional retrieval models, dense retrieval models are more data hungry, requir-
ing large-scale labeled datasets to learn the parameters of the PLMs. In recent years, there are
a number of retrieval datasets with relevance judgements released publicly, which significantly
advances the research of dense retrieval. We categorize the available retrieval datasets into three
majorcategoriesaccordingtotheiroriginaltasks,namely,informationretrieval,questionanswer-
ing, and other task settings. The statistics of the available datasets are shown in Table4.
As we can see from Table4, Wikipedia and Web are two major resources for creating these
datasets.Amongthesedatasets, MSMARCO dataset[185]containsalargeamountofquerieswith
annotated relevant passages in Web documents, and NQ [116] dataset contains Google search
queries and documents with paragraphs and answer spans from the top-ranked Wikipedia pages.
Among these datasets, MS MARCO and NQ datasets have been widely used for evaluating dense
retrieval models. A recent study [48] has summarized the promotion effect of MS MARCO on the
progressofdenseretrieval:Neuralmodelscanexplorelarge-scaledatafortraining.Besides,based
on MS MARCO, several variants have been created to enrich the evaluation characteristics on
somespecificaspect,e.g.,themultilingualversionmMARCO[ 18]andtheMSMARCOChameleons
dataset[3](consistingofobstinatequeriesthataredifficulttoanswerbyneuralretrieversandhave
similar query length and distribution of relevance judgements with easier queries). Besides, Sci-
avolino et al. [236] create EntityQuestionsdataset,as a challenging test set for the models trained
on NQ, which contains simple factoid questions about entities from Wikipedia. Another inter-
esting observation is that there are increasingly more domain-specific retrieval datasets, includ-
ing COVID-19 pandemic dataset [224], financial dataset [172], biomedical dataset [265], climate-
specific dataset [54], and scientific dataset [272].
Besides the presented datasets in Table4, several more comprehensive benchmark datasets are
released to evaluate the overall retrieval capability of the retrieval models by aggregating repre-
sentative datasets and conducting diverse evaluation tasks, such asBEIR [260]a n dKILT [198].
Although existing datasets largely improve the training of dense retrievers, in these datasets,
a query typically corresponds to very few relevance judgements. For example, Nogueira et al.
observethatmostofqueriesinMSMARCOdatasetcontainsonlyonelabeledpositive[ 4], whichis
likelytobesmallerthantheactualnumberofrelevantonesinthecollection.Itismainlybecauseit
istime-consumingtoconstructcompleterelevancejudgementsforalargedataset.Theincomplete
relevanceannotationswill leadto severaltrainingissuessuchasfalsenegative, whichpotentially
affects the retrieval performance.
Besides, to date, most of the released datasets are created in English, and it is more difficult to
obtainsufficient labeleddata for training a non-English dense retriever.More recently,DuReader-
retrieval [206] releases a large-scale Chinese dataset consisting of 90K queries from Baidu search
andover8Mpassagesforpassageretrieval.Toenhancetheevaluationquality,DuReader-retrieval
tries to reduce the false negatives in development and testing sets, and also removes the training
queries that are semantically similar to the development and testing queries. Besides, DuReader-
retrieval provides human-translated queries (in English) for cross-lingual retrieval.
A.2 Evaluation Metrics
To evaluate the retrieval capacity of an information-retrieval system, a number of factors need
to be considered [81, 81, 267]: effectiveness, efficiency, diversity, novelty, and so on. This survey
mainly focuses on the effectiveness for the retrieval system.6 We next introduce the commonly
6Note that these metrics can be used in both first-stage retrieval and reranking. However, we only describe the evaluation
metrics from a general ranking perspective without considering specific task scenarios.
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.Dense Text Retrieval Based on Pretrained Language Models: A Survey 89:43
usedevaluationmetricsforranking,includingRecall,Precision,MAP,MRR,andNDCG.Fordense
retrieval tasks, top-ranked texts are more important for evaluation, and therefore cut-off metrics
are often adopted to examine the quality of texts at top positions. Next, we introduce several
commonly used metrics for dense retrieval in detail.
Inthetraditionalretrievalbenchmarks,Recallisthefractionofrelevanttextsthatareactuallyre-
trievedbyaretrievalmodelamongalltherelevantones,andRecall@ k [328]calculatesatruncated
Recall value at thekth position of a retrieved list:
Recall@k = 1
|Q|
|Q|∑
q=1
#retrq,k
#relq
, (11)
where #retrq,k denotes the number of relevant texts w.r.t. queryq retrieved at topk positions by
a retrieval method, and #relq denotes the total number of relevant texts for queryq.H e r e ,w e
average the Recall@k values over the queries from the query setQ.
In dense retrieval, there is a commonly used metric,Top-k Accuracy, and it computes the pro-
portion of queries for which the top-k retrieved texts contain the answers [111], defined as
Accuracy@k = 1
|Q|
|Q|∑
q=1
I(#retrq,k > 0), (12)
whereI(·) is an binary indicator function that only returns 1 when the case is true. In contrast to
traditional TREC benchmarks, mainstream dense retrieval benchmarks, such as NQ, aim to find
answers to queries instead of retrieving all relevant texts. According to References [111, 207], a
retrievedlistisconsideredto accurately solveaquerywhenitcontainstheanswer,notnecessarily
retrieving all the relevant texts.7
Besides, Precision@k [328] calculates the average proportion of relevant texts among the topk
positions over the query setQ:
Precision@k = 1
|Q|
|Q|∑
q=1
#retrq,k
k . (13)
Based on Precision@k,t h eAverage Precision (AP)further averages the precision values at
the positions of each positive text for a query:
APq = 1
#relq
L∑
k=1
Precision@l ×I(q,k), (14)
where Precision@l is the per-query version of the precision value at thelth position,L is the
length ofaretrievedlist and I(q,l) isan indicatorfunctionreturning1 onlywhenthe lthposition
correspondstoarelevanttextforquery q.Furthermore,MAP[ 328]calculatestheaveragePrecision
scores over a set of queriesQ:
MAP = 1
|Q|
|Q|∑
q=1
APq. (15)
Notonlycountingtheoccurrenceofpositivetexts,Normalized DiscountedCumulativeGain
(DCG) [104] further incorporates the position of a relevant text into consideration: it prefers a
7Note that Accuracy@k is also known as Recall@k in some dense retrieval studies [66, 207], which is somehow different
from the definition used in traditional retrieval benchmarks.
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.89:44 W. Xin et al.
ranking that places a relevant text at a higher position:
DCGq@k =
k∑
i=1
2дi −1
log2(i +1), (16)
whereдi is the graded relevance score for theith retrieved text. Based on the above definition of
DCG, NDCG is the sum of the normalized DCG values at a particular rank position:
nDCG@k = 1
|Q|
|Q|∑
q=1
DCGq@k
IDCGq@k, (17)
whereDCG@k andIDCG@k denotediscountedcumulativegainandidealdiscountedcumulative
gain at a particular rank positionk, respectively.
Furthermore, MRR [270] averages the reciprocal of the rank of the first retrieved positive text
over a set of queriesQ:
MRR = 1
|Q|
|Q|∑
q=1
1
rankq
, (18)
where rankq is the position of the first retrieved positive text w.r.t. queryq.
Forfirst-stageretrieval,Recall@ k andAccuracy@k arethemostcommonlyusedmetrics,since
itsmainfocusistorecallasmanyrelevanttextsoranswersaspossibleatatruncationlength;while
for ranking, MRR, NDCG, and MAP are more commonly used in practice. For a comprehensive
discussionaboutIRevaluation,thereadersaresuggestedtoreadmorefocusedreferences[ 81,176,
234,267].
A.3 Code Library Resource
Recently, several open-sourced dense retrieval libraries have been released for research purpose.
As a representative library, Tevatron [69] has developed a modularized framework for building
dense retrieval models based on PLMs via command-line interfaces. It supports a number of im-
portantproceduresinvolvedinacompleteretrievalpipelineincludingtextprocessing,modeltrain-
ing, text encoding, and text retrieval. It can be accessed at the link:https://github.com/TextTron/
Tevatron.
Besides, Pyserini [136] is a toolkit that is designed to facilitate reproducible research for infor-
mation retrieval. Specifically, it supports both sparse retrieval and dense retrieval with Anserini
IR toolkit [296]a n dF A I S S[108]. It also provides the evaluation scripts for the standard IR test
collections. It can be accessed from the link:http://pyserini.io/.
SentenceTransformers [215] is another library that provides an easy way to compute dense
embeddings for sentences and paragraphs based on Transformer-based networks. Specifically, it
integratestheimplementationofSentence-BERT[ 215]andTSDAE[ 276].Itcanbeaccessedatthe
link: https://www.sbert.net/.
Toenhancethevalidationofdenseretrievercheckpoints,Asyncval[ 334]isreleasedtoeaseand
accelerate the checkpoint validation for dense retrieval models. An important merit is that the
training can be decoupled from checkpoint validation with Asyncval. It can be accessible at the
link: https://github.com/ielab/asyncval.
OpenMatch [155] has been originally proposed for neural information retrieval (v1), and ex-
tended (v2) to support dense retrieval on commonly used benchmarks such as MS MARCO and
NQ. It can be accessed at the link:https://github.com/thunlp/OpenMatch.
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.Dense Text Retrieval Based on Pretrained Language Models: A Survey 89:45
MatchZoo[76]isatextmatchinglibrarythatsupportsanumberofneuraltextmatchingmodels,
and allow users to develop new models by providing rich interfaces.It can be accessedat the link:
https://github.com/NTMC-Community/MatchZoo.
As the supporting resource, we also release an open-sourced implementation of dense retriev-
ers based on our previous work RocketQA [207], at the link:https://github.com/PaddlePaddle/
RocketQA, including RocketQA [207], RocketQAPAIR [218], and RocketQAv2 [219]. This soft-
ware also provides an easy-to-use toolkit with pre-built models (including both English and Chi-
nese models) for direct use after the installation. We also aggregate other open-sourced codes
for related dense retrieval papers in our survey site, which can be found in the link:https://
github.com/RUCAIBox/DenseRetrieval.
ACKNOWLEDGMENTS
The authors gratefully appreciate the anonymous reviewers for their valuable and detailed com-
ments, which greatly helped to improve the quality of this article.
REFERENCES
[1] Akiko Aizawa. 2003. An information-theoretic perspective of TF–IDF measures.Info. Process. Manage.39, 1 (2003),
45–65.
[2] ChrisAlberti,DanielAndor,EmilyPitler,JacobDevlin,andMichaelCollins.2019.SyntheticQAcorporageneration
withroundtripconsistency.In Proceedingsofthe57thAnnualMeetingoftheAssociationforComputationalLinguistics .
6168–6173.
[3] NegarArabzadeh,BhaskarMitra,andEbrahimBagheri.2021.MSMARCOchameleons:ChallengingtheMSMARCO
leaderboardwithextremelyobstinatequeries.In Proceedingsofthe30thACMInternationalConferenceonInformation
and Knowledge Management. 4426–4435.
[4] Negar Arabzadeh, Alexandra Vtyurina, Xinyi Yan, and Charles L. A. Clarke. 2021. Shallow pooling for sparse labels.
Retrieved fromhttps://arXiv:2109.00062
[5] Negar Arabzadeh, Xinyi Yan, and Charles L. A. Clarke. 2021. Predicting efficiency/effectiveness trade-offs for dense
vs. sparse retrieval strategy selection. Retrieved fromhttps://arXiv:2109.10739
[6] Akari Asai, Jungo Kasai, Jonathan Clark, Kenton Lee, Eunsol Choi, and Hannaneh Hajishirzi. 2021. XOR QA: Cross-
lingual open-retrieval question answering. InProceedings of the Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies. 547–564.
[7] AkariAsai,XinyanYu,JungoKasai,andHannaHajishirzi.2021.Onequestionansweringmodelformanylanguages
with cross-lingual dense passage retrieval.Adv. Neural Info. Process. Syst.34 (2021).
[8] Martin Aumüller, Erik Bernhardsson, and Alexander Faithfull. 2017. ANN-benchmarks: A benchmarking tool for
approximate nearest-neighbor algorithms. InProceedings of the International Conference on Similarity Search and
Applications (SISAP’17).
[9] RicardoBaeza-YatesandBerthierA.Ribeiro-Neto.2011. ModernInformationRetrieval—TheConceptsandTechnology
Behind Search, 2nd Ed.
[10] VidhishaBalachandran,AshishVaswani,YuliaTsvetkov,andNikiParmar.2021.Simpleandefficientwaystoimprove
REALM. Retrieved fromhttps://arXiv:2104.08710
[11] Petr Baudiš and Jan Šediv`y. 2015. Modeling of the question answering task in the YodaQA system. InProceedings of
the International Conference of the Cross-language Evaluation Forum for European Languages. Springer, 222–228.
[12] YoshuaBengio,RéjeanDucharme,andPascalVincent.2000.Aneuralprobabilisticlanguagemodel.In Proceedingsof
theAdvancesinNeuralInformationProcessingSystems13,PapersfromNeuralInformationProcessingSystems(NIPS’00) .
932–938.
[13] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on Freebase from question-
answer pairs. InProceedings of the Conference on Empirical Methods in Natural Language Processing. 1533–1544.
[14] Michele Bevilacqua, Giuseppe Ottaviano, Patrick Lewis, Wen tau Yih, Sebastian Riedel, and Fabio Petroni. 2022.
Autoregressivesearchengines:Generatingsubstringsasdocumentidentifiers.Retrievedfrom https://abs/2204.10628
[15] Nitin Bhatia and Vandana. 2010. Survey of nearest-neighbor techniques. Retrieved fromhttps://arXiv:1007.0085
[16] Alexander Bondarenko, Maik Fröbe, Meriem Beloucif, Lukas Gienapp, Yamen Ajjour, Alexander Panchenko, Chris
Biemann, Benno Stein, Henning Wachsmuth, Martin Potthast, and Matthias Hagen. 2020. Overview of touché 2020:
Argument retrieval. InProceedings of the Conference and Labs of the Evaluation Forum (CLEF’20).
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.89:46 W. Xin et al.
[17] Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, and Rodrigo Nogueira. 2022. Inpars: Data augmentation for infor-
mation retrieval using large language models. Retrieved fromhttps://arXiv:2202.05144
[18] Luiz Henrique Bonifacio, Israel Campiotti, Roberto Lotufo, and Rodrigo Nogueira. 2021. mMARCO: A multilingual
version of MS MARCO passage ranking dataset. Retrieved fromhttps://arXiv:2108.13897
[19] SebastianBorgeaud,ArthurMensch,JordanHoffmann,TrevorCai,ElizaRutherford,KatieMillican,Georgevanden
Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy, Jacob Menick,
Roman Ring, T. W. Hennigan, Saffron Huang, Lorenzo Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela
Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack W. Rae, Erich Elsen, and L. Sifre.
2021. Improving language models by retrieving from trillions of tokens. Retrieved fromhttps://abs/2112.04426
[20] Vera Boteva, Demian Gholipour, Artem Sokolov, and Stefan Riezler. 2016. A full-text learning to rank dataset for
medical information retrieval. InProceedings of the European Conference on Information Retrieval. Springer, 716–722.
[21] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom
Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark
Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford,Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. InProceedings of the
Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems
(NeurIPS’20).
[22] Yinqiong Cai, Yixing Fan, Jiafeng Guo, Fei Sun, Ruqing Zhang, and Xueqi Cheng. 2021. Semantic models for the
first-stage retrieval: A comprehensive review. Retrieved fromhttps://arXiv:2103.04831
[23] Yinqiong Cai, Jiafeng Guo, Yixing Fan, Qingyao Ai, Ruqing Zhang, and Xueqi Cheng. 2022. Hard negatives or false
negatives: Correcting pooling bias in training neural ranking models. Retrieved fromhttps://arXiv:2209.05072
[24] Arthur Câmara and Claudia Hauff. 2020. Diagnosing BERT with retrieval heuristics. InProceedings of the Advances
in Information Retrieval 42nd European Conference on IR Research (ECIR’20), Vol. 12035. 605–618.
[25] Nicola De Cao, Gautier Izacard, Sebastian Riedel, and Fabio Petroni. 2021. Autoregressive entity retrieval. Retrieved
fromhttps://abs/2010.00904
[26] Zhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and Hang Li. 2007. Learning to rank: From pairwise approach to
listwise approach. InProceedings of the 24th International Conference on Machine Learning (ICML’07). 129–136.
[27] Claudio Carpineto and Giovanni Romano. 2012. A survey of automatic query expansion in information retrieval.
ACM Comput. Surveys44, 1 (2012), 1–50.
[28] Wei-Cheng Chang, Felix X. Yu, Yin-Wen Chang, Yiming Yang, and Sanjiv Kumar. 2020. Pre-training tasks for
embedding-basedlarge-scaleretrieval.In Proceedingsofthe8thInternationalConferenceonLearningRepresentations
(ICLR’20).
[29] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading Wikipedia to answer open-domain
questions. InProceedings of the 55th Annual Meeting of the Association for Computational Linguistics. 1870–1879.
[30] HongshenChen,XiaoruiLiu,DaweiYin,andJiliangTang.2017.Asurveyondialoguesystems:Recentadvancesand
new frontiers.SIGKDD Explor.19, 2 (2017), 25–35.
[31] Jiangui Chen, Ruqing Zhang, Jiafeng Guo, Yixing Fan, and Xueqi Cheng. 2022. GERE: Generative evidence retrieval
for fact verification. InProceedings of the 45th International ACM SIGIR Conference on Research and Development in
Information Retrieval (SIGIR’22). 2184–2189.
[32] Jiangui Chen, Ruqing Zhang, Jiafeng Guo, Yiqun Liu, Yixing Fan, and Xueqi Cheng. 2022. CorpusBrain: Pre-train a
generative retrieval model for knowledge-intensive language tasks. Retrieved fromhttps://abs/2208.07652
[33] Qi Chen, Haidong Wang, Mingqin Li, Gang Ren, Scarlett Li, Jeffery Zhu, Jason Li, Chuanjie Liu, Lintao Zhang, and
Jingdong Wang. 2018.SPTAG: A Library for Fast Approximate Nearest Neighbor Search.
[34] Qi Chen, Bing Zhao, Haidong Wang, Mingqin Li, Chuanjie Liu, Zengzhong Li, Mao Yang, and Jingdong Wang. 2021.
SPANN: Highly efficient billion-scale approximate nearest-neighbor search. InProceedings of the Annual Conference
on Neural Information Processing Systems (NeurIPS’21).
[35] TingChen,SimonKornblith,MohammadNorouzi,andGeoffreyE.Hinton.2020.Asimpleframeworkforcontrastive
learningofvisualrepresentations.In Proceedingsofthe37thInternationalConferenceonMachineLearning(ICML’20) .
1597–1607.
[36] Tao Chen, Mingyang Zhang, Jing Lu, Michael Bendersky, and Marc-Alexander Najork. 2022. Out-of-domain seman-
tics to the rescue! zero-shot hybrid retrieval models. InProceedings of the Advances in Information Retrieval 44th
European Conference on IR Research (ECIR’22).
[37] Xilun Chen, Kushal Lakhotia, Barlas Oğuz, Anchit Gupta, Patrick Lewis, Stan Peshterliev, Yashar Mehdad, Sonal
Gupta, and Wen-tau Yih. 2021. Salient phrase aware dense retrieval: Can a dense retriever imitate a sparse one?
Retrieved fromhttps://arXiv:2110.06918
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.Dense Text Retrieval Based on Pretrained Language Models: A Survey 89:47
[38] Xuanang Chen, Jian Luo, Ben He, Le Sun, and Yingfei Sun. 2022. Towards robust dense retrieval via local ranking
alignment. InProceedings of the 31st International Joint Conference on Artificial Intelligence (IJCAI’22). 1980–1986.
[39] Hao Cheng, Hao Fang, Xiaodong Liu, and Jianfeng Gao. 2022. Task-aware specialization for efficient and robust
dense retrieval for open-domain question answering. Retrieved fromhttps://arXiv:2210.05156
[40] Kyunghyun Cho, Bart van Merriënboer, Dzmitry Bahdanau, and Yoshua Bengio. 2014. On the properties of neural
machine translation: encoder–decoder approaches. InProceedings of the 8th Workshop on Syntax, Semantics and
Structure in Statistical Translation (SSST’14). 103–111.
[41] ArmanCohan,SergeyFeldman,IzBeltagy,DougDowney,andDanielWeld.2020.SPECTER:Document-levelrepre-
sentationlearningusingcitation-informedtransformers.In Proceedingsofthe58thAnnualMeetingoftheAssociation
for Computational Linguistics. 2270–2282.
[42] Alexis Conneau and Douwe Kiela. 2018. SentEval: An evaluation toolkit for universal sentence representations. In
Proceedings of the 11th International Conference on Language Resources and Evaluation (LREC’18).
[43] NickCraswell,DanielCampos,BhaskarMitra,EmineYilmaz,andBodoBillerbeck.2020.ORCAS:20millionclicked
query-document pairs for analyzing search. InProceedings of the 29th ACM International Conference on Information
and Knowledge Management (CIKM’20). 2983–2989.
[44] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, and Daniel Campos. 2021. Overview of the TREC 2020 deep learning
track. Retrieved fromhttps://arXiv:2102.07662
[45] NickCraswell,BhaskarMitra,EmineYilmaz,DanielCampos,andJimmyLin.[n.d.].TREC2021DeepLearningTrack
Guidelines. Retrieved fromhttps://microsoft.github.io/msmarco/TREC-Deep-Learning.html
[46] NickCraswell,BhaskarMitra,EmineYilmaz,DanielCampos,andJimmyLin.2022.OverviewoftheTREC2021deep
learning track. InProceedings of the Text Retrieval Conference (TREC’22).
[47] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen M. Voorhees. 2020. Overview of the TREC
2019 deep learning track. Retrieved fromhttps://arXiv:2003.07820
[48] NickCraswell,BhaskarMitra,EmineYilmaz,DanielFernandoCampos,andJimmyJ.Lin.2021.MSMARCO:Bench-
marking ranking models in the large-data regime.Proceedings of the 44th International ACM SIGIR Conference on
Research and Development in Information Retrieval.
[49] Andrew M. Dai and Quoc V. Le. 2015. Semi-supervised sequence learning. InProceedings of the Advances in Neural
Information Processing Systems 28: Annual Conference on Neural Information Processing Systems (NeurIPS’15). 3079–
3087.
[50] Zhuyun Dai and Jamie Callan. 2020. Context-aware document term weighting for ad hoc search. InProceedings of
the Web Conference (WWW’20). 1897–1907.
[51] ZhuyunDaiandJamieCallan.2020.Context-awaretermweightingforfirststagepassageretrieval.In Proceedingsof
the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’20). 1533–
1536.
[52] Zhuyun Dai, Vincent Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov, Kelvin Guu, Keith B. Hall, and Ming-
Wei Chang. 2022. Promptagator: Few-shot dense retrieval from 8 examples. Retrieved fromhttps://abs/2209.11755
[53] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional
transformers for language understanding. InProceedings of the Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies. 4171–4186.
[54] Thomas Diggelmann, Jordan Boyd-Graber, Jannis Bulian, Massimiliano Ciaramita, and Markus Leippold. 2020.
CLIMATE-FEVER: A dataset for verification of real-world climate claims. Retrieved fromhttps://arXiv:2012.00614
[55] Bhargav Dodla, Akash Kumar Mohankumar, and Amit Singh. 2022. HEARTS: Multi-task fusion of dense retrieval
and non-autoregressive generation for sponsored search. Retrieved fromhttps://abs/2209.05861
[56] Miao Fan, Jiacheng Guo, Shuai Zhu, Shuo Miao, Mingming Sun, and Ping Li. 2019. MOBIUS: Towards the next
generation of query-ad matching in baidu’s sponsored search. InProceedings of the 25th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining (KDD’19). 2509–2517.
[57] Yixing Fan, Xiaohui Xie, Yinqiong Cai, Jia Chen, Xinyu Ma, Xiangsheng Li, Ruqing Zhang, Jiafeng Guo, and Yiqun
Liu. 2021. Pre-training methods in information retrieval. Retrieved fromhttps://arXiv:2111.13853
[58] HuiFang,TaoTao,andChengXiangZhai.2004.Aformalstudyofinformationretrievalheuristics.In Proceedingsof
the International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’04).
[59] Hui Fang and ChengXiang Zhai. 2005. An exploration of axiomatic approaches to information retrieval. InProceed-
ings of the International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’05).
[60] Hui Fang and ChengXiang Zhai. 2006. Semantic term matching in axiomatic approaches to information retrieval.
Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Re-
trieval.
[61] WilliamFedus,BarretZoph,andNoamShazeer.2022.Switchtransformers:Scalingtotrillionparametermodelswith
simple and efficient sparsity.J. Mach. Learn. Res.23, 1 (2022), 5232–5270.
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.89:48 W. Xin et al.
[62] Thibault Formal, Carlos Lassance, Benjamin Piwowarski, and Stéphane Clinchant. 2021. SPLADE v2: Sparse lexical
and expansion model for information retrieval. Retrieved fromhttps://arXiv:2109.10086
[63] ThibaultFormal,BenjaminPiwowarski,andStéphaneClinchant.2021.SPLADE:Sparselexicalandexpansionmodel
for first stage ranking. InProceedings of the 44th International ACM SIGIR Conference on Research and Development
in Information Retrieval. 2288–2292.
[64] ThibaultFormal,BenjaminPiwowarski,andStéphaneClinchant.2021.AwhiteboxanalysisofColBERT.In Proceed-
ings of the European Conference on Information Retrieveal (ECIR’21).
[65] Luyu Gao and Jamie Callan. 2021. Condenser: A pre-training architecture for dense retrieval. InProceedings of the
Conference on Empirical Methods in Natural Language Processing. 981–993.
[66] Luyu Gao and Jamie Callan. 2021. Unsupervised corpus aware language model pre-training for dense passage re-
trieval. Retrieved fromhttps://arXiv:2108.05540
[67] Luyu Gao, Zhuyun Dai, and Jamie Callan. 2021. COIL: Revisit exact lexical match in information retrieval with
contextualized inverted list. InProceedings of the Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies. 3030–3042.
[68] LuyuGao,ZhuyunDai,andJamieCallan.2021.RethinktrainingofBERTrerankersinmulti-stageretrievalpipeline.
InProceedingsoftheAdvancesinInformationRetrieval43rdEuropeanConferenceonIRResearch(ECIR’21) ,Vol.12657.
280–286.
[69] Luyu Gao, Xueguang Ma, Jimmy J. Lin, and Jamie Callan. 2022. Tevatron: An efficient and flexible toolkit for dense
retrieval. Retrieved fromhttps://abs/2203.05765
[70] Luyu Gao, Yunyi Zhang, Jiawei Han, and Jamie Callan. 2021. Scaling deep contrastive learning batch size under
memory limited setup. InProceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP’21). 316–
321.
[71] Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple contrastive learning of sentence embeddings.
In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP’21).
[72] Tiezheng Ge, Kaiming He, Qifa Ke, and Jian Sun. 2013. Optimized product quantization for approximate nearest-
neighbor search. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR’13).
2946–2953.
[73] Daniel Gillick, Sayali Kulkarni, Larry Lansing, Alessandro Presta, Jason Baldridge, Eugene Ie, and Diego Garcia-
Olano. 2019. Learning dense representations for entity retrieval. InProceedings of the 23rd Conference on Computa-
tional Natural Language Learning (CoNLL’19). 528–537.
[74] Aristides Gionis, Piotr Indyk, Rajeev Motwani et al. 1999. Similarity search in high dimensions via hashing. InPro-
ceedings of the International Conference on Very Large Data Bases (VLDB’99), Vol. 99. 518–529.
[75] JiafengGuo,YixingFan,QingyaoAi,andW.BruceCroft.2016.Adeeprelevancematchingmodelforadhocretrieval.
InProceedingsofthe25thACMInternationalConferenceonInformationandKnowledgeManagement(CIKM’16) .55–64.
[76] JiafengGuo,YixingFan,XiangJi,andXueqiCheng.2019.MatchZoo:Alearning,practicing,anddevelopingsystem
forneuraltextmatching.In Proceedingsofthe42ndInternationalACMSIGIRConferenceonResearchandDevelopment
in Information Retrieval (SIGIR’19). 1297–1300.
[77] Jiafeng Guo, Yixing Fan, Liang Pang, Liu Yang, Qingyao Ai, Hamed Zamani, Chen Wu, W. Bruce Croft, and Xueqi
Cheng. 2020. A deep look into neural ranking models for information retrieval.Info. Process. Manage.57, 6 (2020),
102067.
[78] Ruiqi Guo,PhilipSun, Erik Lindgren,QuanGeng,DavidSimcha,FelixChern, andSanjivKumar.2020.Accelerating
large-scale inference with anisotropic vector quantization. InProceedings of the 37th International Conference on
Machine Learning (ICML’20) (Proceedings of Machine Learning Research, Vol. 119). 3887–3896.
[79] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. 2020. REALM: Retrieval-augmented
language model pre-training. Retrieved fromhttps://abs/2002.08909
[80] Kiana Hajebi, Yasin Abbasi-Yadkori, Hossein Shahbazi, and Hong Zhang. 2011. Fast approximate nearest-neighbor
search with k-nearest-neighbor graph. InProceedings of the 22nd International Joint Conference on Artificial Intelli-
gence (IJCAI’11). 1312–1317.
[81] Donna Harman. 2011. Information retrieval evaluation.Synth. Lect. Info. Concepts, Retriev. Serv.3, 2 (2011), 1–119.
[82] FaeghehHasibi,FedorNikolaev,ChenyanXiong,KrisztianBalog,SveinErikBratsberg,AlexanderKotov,andJamie
Callan. 2017. DBpedia-entity v2: A Test collection for entity search. InProceedings of the 40th International ACM
SIGIR Conference on Research and Development in Information Retrieval. 1265–1268.
[83] Junxian He, Graham Neubig, and Taylor Berg-Kirkpatrick. 2021. Efficient nearest-neighbor language models. In
Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP’21).
[84] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick. 2020. Momentum contrast for unsupervised
visualrepresentationlearning.In ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition
(CVPR’20). 9726–9735.
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.Dense Text Retrieval Based on Pretrained Language Models: A Survey 89:49
[85] Matthew Henderson, Rami Al-Rfou, Brian Strope, Yun-Hsuan Sung, László Lukács, Ruiqi Guo, Sanjiv Kumar, Balint
Miklos, and Ray Kurzweil. 2017. Efficient natural language response suggestion for smart reply. Retrieved from
https://arXiv:1705.00652
[86] Jonathan Herzig, Thomas Müller, Syrine Krichene, and Julian Eisenschlos. 2021. Open domain question answering
over tables via dense retrieval. InProceedings of the Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies. 512–519.
[87] Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. 2015. Distilling the knowledge in a neural network. Retrieved
fromhttps://abs/1503.02531
[88] Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory.Neural Comput.9, 8 (1997), 1735–1780.
[89] Sebastian Hofstätter, Sophia Althammer, Michael Schröder, Mete Sertkan, and Allan Hanbury. 2020. Improving effi-
cientneuralrankingmodelswithcross-architectureknowledgedistillation.Retrievedfrom https://arXiv:2010.02666
[90] Sebastian Hofstatter, Nick Craswell, Bhaskar Mitra, Hamed Zamani, and Allan Hanbury. 2022. Are we there yet? A
decisionframeworkforreplacingtermbasedretrievalwithdenseretrievalsystems.Retrievedfrom https://abs/2206.
12993
[91] Sebastian Hofstätter, O. Khattab, Sophia Althammer, Mete Sertkan, and Allan Hanbury. 2022. Introducing neural
bagofwhole-wordswithcolberter:Contextualizedlateinteractionsusingenhancedreduction.Retrievedfrom https:
//abs/2203.13088
[92] Sebastian Hofstätter, Sheng-Chieh Lin, Jheng-Hong Yang, Jimmy Lin, and Allan Hanbury. 2021. Efficiently teaching
an effective dense retriever with balanced topic aware sampling. InProceedings of the 44th International ACM SIGIR
Conference on Research and Development in Information Retrieval. 113–122.
[93] Wu Hong, Zhuosheng Zhang, Jinyuan Wang, and Hai Zhao. 2022. Sentence-aware contrastive learning for open-
domain passage retrieval. InProceedings of the 60th Annual Meeting of the Association for Computational Linguistics.
1062–1074.
[94] Doris Hoogeveen, Karin M. Verspoor, and Timothy Baldwin. 2015. CQADupStack: A benchmark data set for com-
munity question-answering research. InProceedings of the 20th Australasian Document Computing Symposium.1 – 8 .
[95] Jui-Ting Huang, Ashish Sharma, Shuying Sun, Li Xia, David Zhang, Philip Pronin, Janani Padmanabhan, Giuseppe
Ottaviano, and Linjun Yang. 2020. Embedding-based retrieval in facebook search. InProceedings of the 26th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining (KDD’20). 2553–2561.
[96] Po-SenHuang,XiaodongHe,JianfengGao,LiDeng,AlexAcero,andLarryP.Heck. 2013.Learningdeepstructured
semanticmodelsforwebsearchusingclickthroughdata.In Proceedingsofthe22ndACMInternationalConferenceon
Information and Knowledge Management (CIKM’13). 2333–2338.
[97] PatrickHuber,ArmenAghajanyan,BarlasOğuz,DmytroOkhonko,Wen-tauYih,SonalGupta,andXilunChen.2021.
CCQA:Anewweb-scalequestionansweringdatasetformodelpre-training.Retrievedfrom https://arXiv:2110.07731
[98] SamuelHumeau,KurtShuster,Marie-AnneLachaux,andJasonWeston.2020.Poly-encoders:Architecturesandpre-
training strategies for fast and accurate multi-sentence scoring. InProceedings of the 8th International Conference on
Learning Representations (ICLR’20).
[99] Piotr Indyk and Rajeev Motwani. 1998. Approximate nearest neighbors: Towards removing the curse of dimension-
ality. InProceedings of the 30th Annual ACM Symposium on Theory of Computing (STOC’98).
[100] Shankar Iyer, Nikhil Dandekar, and Kornél Csernai. [n.d.]. First Quora Dataset Release: Question Pairs.https://
quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs
[101] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard
Grave. 2021. Towards unsupervised dense information retrieval with contrastive learning. Retrieved fromhttps:
//abs/2112.09118
[102] Gautier Izacard and Edouard Grave. 2021. Distilling knowledge from reader to retriever for question answering. In
Proceedings of the International Conference on Learning Representations.
[103] Gautier Izacard and Edouard Grave. 2021. Leveraging passage retrieval with generative models for open domain
questionanswering.In Proceedingsofthe16thConferenceoftheEuropeanChapteroftheAssociationforComputational
Linguistics: Main Volume. 874–880.
[104] Kalervo Järvelin and Jaana Kekäläinen. 2002. Cumulated gain-based evaluation of IR techniques.ACM Trans. Inf.
Syst.20, 4 (2002), 422–446.
[105] Hervé Jégou, Matthijs Douze, and Cordelia Schmid. 2011. Product quantization for nearest-neighbor search.IEEE
Trans. Pattern Anal. Mach. Intell.33, 1 (2011), 117–128.
[106] Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, and Jong C. Park. 2022. Augmenting document repre-
sentations for dense retrieval with interpolation and perturbation. InProceedings of the 60th Annual Meeting of the
Association for Computational Linguistics. 442–452.
[107] Zongcheng Ji, Zhengdong Lu, and Hang Li. 2014. An information retrieval approach to short text conversation.
Retrieved fromhttps://abs/1408.6988
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.89:50 W. Xin et al.
[108] J. Johnson, M. Douze, and H. Jegou. 2019. Billion-scale similarity search with GPUs.IEEE Trans. Big Data7, 3 (2019),
535–547.
[109] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. TriviaQA: A large scale distantly supervised
challenge dataset for reading comprehension. InProceedings of the 55th Annual Meeting of the Association for Com-
putational Linguistics. 1601–1611.
[110] T.JoyceandR.M.Needham.1958.Thethesaurusapproachtoinformationretrieval. AmericanDocument. 9,3(1958),
192.
[111] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau
Yih.2020.Densepassageretrievalforopen-domainquestionanswering.In ProceedingsoftheConferenceonEmpirical
Methods in Natural Language Processing (EMNLP’20). 6769–6781.
[112] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2020. Generalization through
memorization: Nearest neighbor language models. InProceedings of the 8th International Conference on Learning
Representations (ICLR’20).
[113] Omar Khattab and Matei Zaharia. 2020. ColBERT: Efficient and effective passage search via contextualized late in-
teraction over BERT. InProceedings of the 43rd International ACM SIGIR Conference on Research and Development in
Information Retrieval (SIGIR’20). 39–48.
[114] JonM.Kleinberg.2000.Thesmall-worldphenomenon:Analgorithmicperspective.In Proceedingsofthe32ndAnnual
ACM Symposium on Theory of Computing. 163–170.
[115] BogdanKostić,JulianRisch,andTimoMöller.2021.Multi-modalretrievaloftablesandtextsusingtri-encodermodels.
In Proceedings of the 3rd Workshop on Machine Reading for Question Answering. 82–91.
[116] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle
Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei
Chang,AndrewM.Dai,JakobUszkoreit,QuocLe,andSlavPetrov.2019.Naturalquestions:Abenchmarkforquestion
answering research.Trans. Assoc. Comput. Linguist.7 (2019), 452–466.
[117] Hyunji Lee, Sohee Yang, Hanseok Oh, and Minjoon Seo. 2022. Generative retrieval for long sequences. Retrieved
fromhttps://abs/2204.13596
[118] Jinhyuk Lee, Mujeen Sung, Jaewoo Kang, and Danqi Chen. 2021. Learning dense representations of phrases at scale.
In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International
Joint Conference on Natural Language Processing. 6634–6647.
[119] Jinhyuk Lee,AlexanderWettig,andDanqiChen. 2021.Phrase retrievallearnspassageretrieval,too.Retrievedfrom
https://arXiv:2109.08133
[120] Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open domain
questionanswering.In Proceedingsofthe57thAnnualMeetingoftheAssociationforComputationalLinguistics .6086–
6096.
[121] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoy-
anov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre-training for natural language gener-
ation,translation,andcomprehension.In Proceedingsofthe58thAnnualMeetingoftheAssociationforComputational
Linguistics. 7871–7880.
[122] Patrick Lewis, Barlas Oğuz, Wenhan Xiong, Fabio Petroni, Wen tau Yih, and Sebastian Riedel. 2021. Boosted dense
retriever. Retrieved fromhttps://abs/2112.07771
[123] PatrickLewis,YuxiangWu,LinqingLiu,PasqualeMinervini,HeinrichKüttler,AleksandraPiktus,PontusStenetorp,
andSebastianRiedel.2021.Paq:65millionprobably-askedquestionsandwhatyoucandowiththem.Retrievedfrom
https://arXiv:2102.07033
[124] Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich
Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented
generation for knowledge-intensive NLP tasks. InProceedings of the Advances in Neural Information Processing Sys-
tems 33: Annual Conference on Neural Information Processing Systems (NeurIPS’20).
[125] Hang Li. 2011. Learning to rank for information retrieval and natural language processing.Synth. Lect. Hum. Lang.
Technol.4 (2011), 1–113.
[126] Hang Li, Ahmed Mourad, Shengyao Zhuang, Bevan Koopman, and Guido Zuccon. 2021. Pseudo relevance feedback
with deep language models and dense retrievers: Successes and pitfalls. Retrieved fromhttps://arXiv:2108.11044
[127] Hang Li, Shengyao Zhuang, Ahmed Mourad, Xueguang Ma, Jimmy J. Lin, and G. Zuccon. 2021. Improving query
representations for dense retrieval with pseudo relevance feedback: A reproducibility study. Retrieved fromhttps:
//abs/2112.06400
[128] Jiwei Li, Thang Luong, and Dan Jurafsky. 2015. A hierarchical neural autoencoder for paragraphs and documents.
In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International
Joint Conference on Natural Language Processing. 1106–1115.
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.Dense Text Retrieval Based on Pretrained Language Models: A Survey 89:51
[129] Minghan Li, Ming Li, Kun Xiong, and Jimmy Lin. 2021. Multi-task dense retrieval via model uncertainty fusion for
open-domain question answering. InFindings of the Association for Computational Linguistics (EMNLP’21). 274–287.
[130] Sen Li, Fuyu Lv, Taiwei Jin, Guli Lin, Keping Yang, Xiaoyi Zeng, Xiao-Ming Wu, and Qianli Ma. 2021. Embedding-
basedproductretrievalinTaobaosearch.In Proceedingsofthe27thACMSIGKDDConferenceonKnowledgeDiscovery
and Data Mining. 3181–3189.
[131] WenLi,YingZhang,YifangSun,WeiWang,MingjieLi,WenjieZhang,andXueminLin.2019.Approximatenearest-
neighbor search on high dimensional data—Experiments, analyses, and improvement.IEEE Trans. Knowl. Data Eng.
32, 8 (2019), 1475–1488.
[132] Yizhi Li, Zhenghao Liu, Chenyan Xiong, and Zhiyuan Liu. 2021. More robust dense retrieval with contrastive dual
learning. InProceedings of the ACM SIGIR International Conference on Theory of Information Retrieval. 287–296.
[133] Yongqi Li, Nan Yang, Liang Wang, Furu Wei, and Wenjie Li. 2023. Multiview identifiers enhanced generative re-
trieval. InProceedings of the 61st Annual Meeting of the Association for Computational Linguistics. Association for
Computational Linguistics, 6636–6648.
[134] Zehan Li, Nan Yang, Liang Wang, and Furu Wei. 2022. Learning diverse document representations with deep query
interactions for dense retrieval. Retrieved fromhttps://arXiv:2208.04232
[135] Davis Liang, Peng Xu, Siamak Shakeri, Cicero Nogueira dos Santos, Ramesh Nallapati, Zhiheng Huang, and Bing
Xiang.2020.Embedding-basedzero-shotretrievalthroughquerygeneration.Retrievedfrom https://arXiv:2009.10270
[136] JimmyLin,XueguangMa,Sheng-ChiehLin,Jheng-HongYang,RonakPradeep,andRodrigoNogueira.2021.Pyserini:
A Python toolkit for reproducible information retrieval research with sparse and dense representations. InProceed-
ings of the 44th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval
(SIGIR’21). 2356–2362.
[137] Jimmy Lin, Rodrigo Nogueira, and Andrew Yates. 2021. Pretrained transformers for text ranking: Bert and beyond.
Synth. Lect. Hum. Lang. Technol.14, 4 (2021), 1–325.
[138] Jimmy J. Lin and Xueguang Ma. 2021. A few brief notes on deepimpact, COIL, and a conceptual framework for
information retrieval techniques. Retrieved fromhttps://abs/2106.14807
[139] Sheng-Chieh Lin, Minghan Li, and Jimmy Lin. 2022. Aggretriever: A simple approach to aggregatetextual represen-
tation for robust dense passage retrieval. Retrieved fromhttps://abs/2208.00511
[140] Sheng-ChiehLinandJimmyJ.Lin.2021.Densifyingsparserepresentationsforpassageretrievalbyrepresentational
slicing. Retrieved fromhttps://abs/2112.04666
[141] Sheng-ChiehLin,Jheng-HongYang,andJimmyLin.2020.Distillingdenserepresentationsforrankingusingtightly-
coupled teachers. Retrieved fromhttps://arXiv:2010.11386
[142] ZhenghaoLin,YeyunGong,XiaoLiu,HangZhang,ChenLin,AnleiDong,JianJiao,JingwenLu,DaxinJiang,Rangan
Majumder, et al. 2023. PROD: Progressive distillation for dense retrieval. InProceedings of the ACM Web Conference
2023. 3299–3308.
[143] Alexander Liu and Samuel Yang. 2022. Masked autoencoders as the unified learners for pre-trained sentence repre-
sentation. Retrieved fromhttps://abs/2208.00231
[144] Fangyu Liu, Serhii Havrylov, Yunlong Jiao, Jordan Massiah, and Emine Yilmaz. 2021. Trans-encoder: Unsupervised
sentence-pair modelling through self-and mutual-distillations. Retrieved fromhttps://arXiv:2109.13059
[145] Jiawei Liu, Yangyang Kang, Di Tang, Kaisong Song, Changlong Sun, Xiaofeng Wang, Wei Lu, and Xiaozhong Liu.
2022.Order-disorder: Imitationadversarialattacks for black-boxneural ranking models.Retrievedfrom https://abs/
2209.06506
[146] JiduanLiu,JiahaoLiu,YangYang,JingangWang,WeiWu,DongyanZhao,andRuiYan.2022.GNN-encoder:Learning
adual-encoderarchitectureviagraphneuralnetworksforpassageretrieval.Retrievedfrom https://arXiv:2204.08241
[147] Linqing Liu, Patrick Lewis, Sebastian Riedel, and Pontus Stenetorp. 2021. Challenges in generalization in open do-
main question answering. Retrieved fromhttps://abs/2109.01156
[148] PengfeiLiu,WeizheYuan,JinlanFu,ZhengbaoJiang,HiroakiHayashi,andGrahamNeubig.2023.Pre-train,prompt,
and predict: A systematic survey of prompting methods in natural language processing.ACM Comput. Surveys, 55,
9 (2023), 1–35.
[149] Tie-Yan Liu. 2010. Learning to rank for information retrieval. InProceedings of the 33rd International ACM SIGIR
Conference on Research and Development in Information Retrieval (SIGIR’10). 904.
[150] Ye Liu, Kazuma Hashimoto, Yingbo Zhou, Semih Yavuz, Caiming Xiong, and S Yu Philip. 2021. Dense hierarchi-
cal retrieval for open-domain question answering. InProceedings of the Association for Computational Linguistics
(EMNLP’21). 188–200.
[151] Yiding Liu, Guan Huang, Jiaxiang Liu, Weixue Lu, Suqi Cheng, Yukun Li, Daiting Shi, Shuaiqiang Wang, Zhicong
Cheng, and Dawei Yin. 2021. Pre-trained language model for web-scale retrieval in baidu search.Proceedings of the
27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining.
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.89:52 W. Xin et al.
[152] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettle-
moyer, and Veselin Stoyanov. 2019. RoBERTa: A robustly optimized BERT pretraining approach. Retrieved from
https://arXiv:1907.11692
[153] Yiqun Liu, Kaushik Rangadurai, Yunzhong He, Siddarth Malreddy, Xunlong Gui, Xiaoyi Liu, and Fedor Borisyuk.
2021. Que2Search: Fast and accurate query and document understanding for search at Facebook. InProceedings of
the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining.
[154] ZhengLiuandYingxiaShao.2022.RetroMAE:Pre-trainingretrieval-orientedtransformersviamaskedauto-encoder.
Retrieved fromhttps://abs/2205.12035
[155] ZhenghaoLiu,KaitaoZhang,ChenyanXiong,ZhiyuanLiu,andMaosongSun.2021.OpenMatch:Anopensourceli-
braryforNeu-IRresearch.In Proceedingsofthe44thInternationalACMSIGIRConferenceonResearchandDevelopment
in Information Retrieval (SIGIR’21). 2531–2535.
[156] JingLu,GustavoHernándezÁbrego,JiMa,JianmoNi,andYinfeiYang.2020.Neuralpassageretrievalwithimproved
negative contrast. Retrieved fromhttps://abs/2010.12523
[157] JingLu,GustavoHernándezÁbrego,JiMa,JianmoNi,andYinfeiYang.2021.Multi-stagetrainingwithimprovedneg-
ativecontrastforneuralpassageretrieval.In ProceedingsoftheConferenceonEmpiricalMethodsinNaturalLanguage
Processing. 6091–6103.
[158] ShuaiLu,NanDuan,HojaeHan,DayaGuo,Seung-wonHwang,andAlexeySvyatkovskiy.2022.ReACC:Aretrieval-
augmented code completion framework. InProceedings of the 60th Annual Meeting of the Association for Computa-
tional Linguistics (ACL’22). 6227–6240.
[159] Shuqi Lu, Di He, Chenyan Xiong, Guolin Ke, Waleed Malik, Zhicheng Dou, Paul Bennett, Tie-Yan Liu, and Arnold
Overwijk. 2021. Less is more: Pretrain a strong siamese encoder for dense text retrieval using a weak decoder. In
Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP’21). 2780–2791.
[160] Yuxiang Lu, Yiding Liu, Jiaxiang Liu, Yunsheng Shi, Zhengjie Huang, Shikun Feng, Yu Sun, Hao Tian, Hua Wu,
Shuaiqiang Wang, Dawei Yin, and Haifeng Wang. 2022. ERNIE-search: Bridging cross-encoder with dual-encoder
via self on-the-fly distillation for dense passage retrieval. Retrieved fromhttps://abs/2205.09153
[161] Yi Luan, Jacob Eisenstein, Kristina Toutanova, and Michael Collins. 2021. Sparse, dense, and attentional representa-
tions for text retrieval.Trans. Assoc. Comput. Linguist.9 (2021), 329–345.
[162] Man Luo, Arindam Mitra, Tejas Gokhale, and Chitta Baral. 2022. Improving biomedical information retrieval with
neural retrievers. InProceedings of the 36th AAAI Conference on Artificial Intelligence (AAAI’22), 34th Conference
on Innovative Applications of Artificial Intelligence (IAAI’22), and the 12th Symposium on Educational Advances in
Artificial Intelligence (EAAI’22). 11038–11046.
[163] Ji Ma, Ivan Korotkov, Yinfei Yang, Keith Hall, and Ryan McDonald. 2021. Zero-shot neural passage retrieval via
domain-targeted synthetic question generation. InProceedings of the 16th Conference of the European Chapter of the
Association for Computational Linguistics. 1075–1088.
[164] Xinyu Ma, Jiafeng Guo, Ruqing Zhang, Yixing Fan, and Xueqi Cheng. 2022. Pre-train a discriminative text encoder
for dense retrieval via contrastive span prediction. InProceedings of the 45rd International ACM SIGIR Conference on
Research and Development in Information Retrieval. 848–858.
[165] Xinyu Ma, Jiafeng Guo, Ruqing Zhang, Yixing Fan, Xiang Ji, and Xueqi Cheng. 2021. PROP: Pre-training with rep-
resentative words prediction for ad hoc retrieval. InProceedings of the 14th ACM International Conference on Web
Search and Data Mining. 283–291.
[166] Xinyu Ma, Jiafeng Guo, Ruqing Zhang, Yixing Fan, Yingyan Li, and Xueqi Cheng. 2021. B-PROP: Bootstrapped pre-
trainingwithrepresentativewordspredictionforadhocretrieval.In Proceedingsofthe44thInternationalACMSIGIR
Conference on Research and Development in Information Retrieval (SIGIR’21). 1318–1327.
[167] Xinyu Ma, Ruqing Zhang, Jiafeng Guo, Yixing Fan, and Xueqi Cheng. 2022. A contrastive pre-training approach to
learn discriminative autoencoder for dense retrieval. Retrieved fromhttps://arXiv:2208.09846
[168] Zhengyi Ma, Zhicheng Dou, Wei Xu, Xinyu Zhang, Hao Jiang, Zhao Cao, and Ji-Rong Wen. 2021. Pre-training for
ad hoc retrieval: Hyperlink is also you need. InProceedings of the 30th ACM International Conference on Information
and Knowledge Management. 1212–1221.
[169] Sean MacAvaney, Sergey Feldman, Nazli Goharian, Doug Downey, and Arman Cohan. 2020. ABNIRML: Analyzing
the behavior of neural IR models. Retrieved fromhttps://arXiv:2011.00696
[170] Sean MacAvaney, Andrew Yates, Arman Cohan, and Nazli Goharian. 2019. CEDR: Contextualized embeddings for
document ranking. InProceedings of the 42nd International ACM SIGIR Conference on Research and Development in
Information Retrieval (SIGIR’19). 1101–1104.
[171] Alessandro Magnani, Feng Liu, Suthee Chaidaroon, Sachin Yadav, Praveen Reddy Suram, Ajit Puthenputhussery,
Sijie Chen, Min Xie, Anirudh Kashi, Tony Lee, and Ciya Liao. 2022. Semantic retrieval at walmart. InProceedings of
the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD’22). 3495–3503.
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.Dense Text Retrieval Based on Pretrained Language Models: A Survey 89:53
[172] Macedo Maia, Siegfried Handschuh, André Freitas, Brian Davis, Ross McDermott, Manel Zarrouk, and Alexandra
Balahur. 2018. WWW’18 open challenge: Financial opinion mining and question answering. InProceedings of the
Web Conference (WWW’18). 1941–1942.
[173] Jean Maillard, Vladimir Karpukhin, Fabio Petroni, Wen-tau Yih, Barlas Oğuz, Veselin Stoyanov, and Gargi Ghosh.
2021. Multi-task retrieval for knowledge-intensive tasks. Retrieved fromhttps://arXiv:2101.00117
[174] YuryMalkov,AlexanderPonomarenko,AndreyLogvinov,andVladimirKrylov.2014.Approximatenearest-neighbor
algorithm based on navigable small world graphs.Inf. Syst.45 (2014), 61–68.
[175] Yu A. Malkov and Dmitry A. Yashunin. 2018. Efficient and robust approximate nearest-neighbor search using hier-
archical navigable small world graphs.IEEE Trans. Pattern Anal. Mach. Intell.42, 4 (2018), 824–836.
[176] ChristopherD.Manning,PrabhakarRaghavan,andHinrichSchütze.2008. IntroductiontoInformationRetrieval .Cam-
bridge University Press, Cambridge, UK.
[177] KelongMao,ZhichengDou,HaonanChen,FengranMo,andHongjinQian.2023.Largelanguagemodelsknowyour
contextualsearchintent:Apromptingframeworkforconversationalsearch.Retrievedfrom https://arXiv:2303.06573
[178] Melvin Earl Maron and John Larry Kuhns. 1960. On relevance, probabilistic indexing and information retrieval.J.
ACM 7, 3 (1960), 216–244.
[179] Ken J. McDonell. 1977. An inverted index implementation.Comput. J.20, 2 (1977), 116–123.
[180] Donald Metzler, Yi Tay, Dara Bahri, and Marc Najork. 2021. Rethinking search: Making experts out of dilettantes.
Retrieved fromhttps://abs/2105.02274
[181] Tomás Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. 2013. Distributed representations
of words and phrases and their compositionality. InProceedings of the Advances in Neural Information Processing
Systems 26: 27th Annual Conference on Neural Information Processing Systems (NeurIPS’13). 3111–3119.
[182] Bhaskar Mitra and Nick Craswell. 2017. Neural models for information retrieval. Retrieved fromhttps://arXiv:1705.
01509
[183] Bhaskar Mitra, Fernando Diaz, and Nick Craswell. 2017. Learning to match using local and distributed represen-
tations of text for web search. InProceedings of the 26th International Conference on World Wide Web (WWW’17).
1291–1299.
[184] Arvind Neelakantan, Tao Xu, Raul Puri, Alec Radford, Jesse Michael Han, Jerry Tworek, Qiming Yuan, Nikolas A.
Tezak, Jong Wook Kim, Chris Hallacy, Johannes Heidecke, Pranav Shyam, Boris Power, Tyna Eloundou Nekoul,
Girish Sastry, Gretchen Krueger, David P. Schnurr, Felipe Petroski Such, Kenny Sai-Kin Hsu, Madeleine Thompson,
Tabarak Khan, Toki Sherbakov, Joanne Jang, Peter Welinder, and Lilian Weng. 2022. Text and code embeddings by
contrastive pre-training. Retrieved fromhttps://abs/2201.10005
[185] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. MS
MARCO: A human generated machine reading comprehension dataset. InProceedings of the Workshop on Cognitive
Computation: Integrating Neural and Symbolic Approaches Co-located with the 30th Annual Conference on Neural
Information Processing Systems (NIPS’16).
[186] Jianmo Ni, Gustavo Hern’andez ’Abrego, Noah Constant, Ji Ma, Keith B. Hall, Daniel Matthew Cer, and Yinfei Yang.
2021.Sentence-T5:Scalablesentenceencodersfrompre-trainedtext-to-textmodels.Retrievedfrom https://abs/2108.
08877
[187] Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hern’andez ’Abrego, Ji Ma, Vincent Zhao, Yi Luan, Keith Hall,
Ming-Wei Chang, and Yinfei Yang. 2021. Large dual encoders are generalizable retrievers. Retrieved fromhttps:
//abs/2112.07899
[188] RodrigoNogueiraandKyunghyunCho.2019.Passagere-rankingwithBERT.Retrievedfrom https://abs/1901.04085
[189] Rodrigo Nogueira, Zhiying Jiang, Ronak Pradeep, and Jimmy Lin. 2020. Document ranking with a pretrained
sequence-to-sequence model. InProceedings of the Association for Computational Linguistics (EMNLP’20). 708–718.
[190] RodrigoNogueira,JimmyLin,andAIEpistemic.2019.Fromdoc2querytodoc TTTTTquery. OnlinePreprint 6(2019),
2.
[191] Rodrigo Nogueira, Wei Yang, Kyunghyun Cho, and Jimmy Lin. 2019. Multi-stage document ranking with BERT.
Retrieved fromhttps://abs/1910.14424
[192] Barlas Oguz, Xilun Chen, Vladimir Karpukhin, Stan Peshterliev, Dmytro Okhonko, Michael Schlichtkrull, Sonal
Gupta,YasharMehdad,andScott Yih.2020.Unik-qa: Unifiedrepresentationsofstructured andunstructured knowl-
edge for open-domain question answering. Retrieved fromhttps://arXiv:2012.14610
[193] Barlas Oğuz, Kushal Lakhotia, Anchit Gupta, Patrick Lewis, Vladimir Karpukhin, Aleksandra Piktus, Xilun Chen,
Sebastian Riedel, Wen-tau Yih, Sonal Gupta et al. 2021. Domain-matched pre-training tasks for dense retrieval. Re-
trieved fromhttps://arXiv:2107.13602
[194] LiangPang,JunXu,QingyaoAi,YanyanLan,XueqiCheng,andJirongWen.2020.SetRank:Learningapermutation-
invariant ranking model for information retrieval.Proceedings of the 43rd International ACM SIGIR Conference on
Research and Development in Information Retrieval.
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.89:54 W. Xin et al.
[195] Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu,
WeizhuChenetal.2023.Checkyourfactsandtryagain:Improvinglargelanguagemodelswithexternalknowledge
and automated feedback. Retrieved fromhttps://arXiv:2302.12813
[196] GustavoPenha,ArthurCâmara,andClaudiaHauff.2022.Evaluatingtherobustnessofretrievalpipelineswithquery
variationgenerators.In AdvancesinInformationRetrieval44thEuropeanConferenceonIRResearch(ECIR’22) .397–412.
[197] MatthewE.Peters,MarkNeumann,MohitIyyer,MattGardner,ChristopherClark,KentonLee,andLukeZettlemoyer.
2018. Deep contextualized word representations. InProceedings of the Conference of the North American Chapter of
the Association for Computational Linguistics: Human Language Technologies. 2227–2237.
[198] Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine
Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rocktäschel, and Sebastian Riedel. 2021. KILT:
Abenchmarkforknowledgeintensivelanguagetasks.In ProceedingsoftheConferenceoftheNorthAmericanChapter
of the Association for Computational Linguistics: Human Language Technologies. 2523–2544.
[199] Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller.
2019. Language models as knowledge bases? InProceedings of the Conference on Empirical Methods in Natural Lan-
guageProcessingandthe9thInternationalJointConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP’19) .2463–
2473.
[200] AleksandraPiktus,FabioPetroni,VladimirKarpukhin,DmytroOkhonko,SamuelBroscheit,GautierIzacard,Patrick
Lewis, Barlas Oguz, Edouard Grave, Wen-tau Yih, and Sebastian Riedel. 2021. The web is your oyster—Knowledge-
intensive NLP against a very large web corpus. Retrieved fromhttps://abs/2112.09924
[201] Telmo Pires, Eva Schlinger, and Dan Garrette. 2019. How multilingual is multilingual BERT? InProceedings of the
57th Annual Meeting of the Association for Computational Linguistics. 4996–5001.
[202] Prafull Prakash, Julian Killingback, and Hamed Zamani. 2021. Learning robust dense retrieval models from incom-
pleterelevancelabels.In Proceedingsofthe44thInternationalACMSIGIRConferenceonResearchandDevelopmentin
Information Retrieval. 1728–1732.
[203] YifanQiao,ChenyanXiong,ZhenghaoLiu,andZhiyuanLiu.2019.UnderstandingthebehaviorsofBERTinranking.
Retrieved fromhttps://abs/1904.07531
[204] YifanQiao,ChenyanXiong,ZhenghaoLiu,andZhiyuanLiu.2019.UnderstandingthebehaviorsofBERTinranking.
Retrieved fromhttps://abs/1904.07531
[205] Tao Qin, Wei Chen, and Tie-Yan Liu. 2014. Sponsored search auctions: Recent advances and future directions.ACM
Trans. Intell. Syst. Technol.5, 4 (2014), 60:1–60:34.
[206] Yifu Qiu, Hongyu Li, Yingqi Qu, Ying Chen, Qiaoqiao She, Jing Liu, Hua Wu, and Haifeng Wang. 2022.
DuReader_retrieval: A large-scale chinese benchmark for passage retrieval from web search engine. Retrieved from
https://arXiv:2203.10232
[207] Yingqi Qu, Yuchen Ding, Jing Liu, Kai Liu, Ruiyang Ren, Wayne Xin Zhao, Daxiang Dong, Hua Wu, and Haifeng
Wang.2021.RocketQA:Anoptimizedtrainingapproachtodensepassageretrievalforopen-domainquestionanswer-
ing. InProceedings of the Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies. 5835–5847.
[208] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,GirishSastry,Amanda
Askell,PamelaMishkin,JackClark,GretchenKrueger,andIlyaSutskever.2021.Learningtransferablevisualmodels
fromnaturallanguagesupervision.In Proceedingsofthe38thInternationalConferenceonMachineLearning(ICML’21) .
8748–8763.
[209] Alec Radford and Karthik arasimhan. 2018. Improving language understanding by generative pre-training. Online
preprint (2018).
[210] Colin Raffel, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li,
andPeterJ.Liu.2020.Exploringthelimitsoftransferlearningwithaunifiedtext-to-texttransformer.Retrievedfrom
https://abs/1910.10683
[211] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine
comprehension of text. InProceedings of the Conference on Empirical Methods in Natural Language Processing. 2383–
2392.
[212] Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham.
2023. In-context retrieval-augmented language models. Retrieved fromhttps://arXiv:2302.00083
[213] OriRam,GalShachaf,OmerLevy,JonathanBerant,andAmirGloberson.2021.Learningtoretrievepassageswithout
supervision. Retrieved fromhttps://abs/2112.07708
[214] Revanth Gangi Reddy, Vikas Yadav, Md Arafat Sultan, Martin Franz, Vittorio Castelli, Heng Ji, and Avirup Sil. 2021.
Towards robust neural retrieval models with synthetic pre-training. Retrieved fromhttps://arXiv:2104.07800
[215] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence embeddings using Siamese BERT-networks. In
Proceedings of the Conference on Empirical Methods in Natural Language Processing and the 9th International Joint
Conference on Natural Language Processing (EMNLP-IJCNLP’19). 3982–3992.
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.Dense Text Retrieval Based on Pretrained Language Models: A Survey 89:55
[216] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings using Siamese BERT-networks. In
Proceedings of the Conference on Empirical Methods in Natural Language Processing and the 9th International Joint
Conference on Natural Language Processing (EMNLP-IJCNLP’19). 3982–3992.
[217] Nils Reimers and Iryna Gurevych. 2021. The curse of dense low-dimensional information retrieval for large index
sizes. InProceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th Interna-
tional Joint Conference on Natural Language Processing. 605–611.
[218] Ruiyang Ren, Shangwen Lv, Yingqi Qu, Jing Liu, Wayne Xin Zhao, QiaoQiao She, Hua Wu, Haifeng Wang, and
Ji-Rong Wen. 2021. PAIR: Leveraging passage-centric similarity relation for improving dense passage retrieval. In
Proceedings of the Association for Computational Linguistics (ACL-IJCNLP’21). 2173–2183.
[219] Ruiyang Ren, Yingqi Qu, Jing Liu, Wayne Xin Zhao, Qiaoqiao She, Hua Wu, Haifeng Wang, and Ji-Rong Wen. 2021.
RocketQAv2: A joint training method for dense passage retrieval and passage re-ranking. InProceedings of the Con-
ference on Empirical Methods in Natural Language Processing. 2825–2835.
[220] Ruiyang Ren, Yingqi Qu, Jing Liu, Wayne Xin Zhao, Qifei Wu, Yuchen Ding, Hua Wu, Haifeng Wang, and Ji-Rong
Wen.2023.Athoroughexaminationonzero-shotdenseretrieval.In ProceedingsoftheAssociationforComputational
Linguistics (EMNLP’23).
[221] Ruiyang Ren, Yuhao Wang, Yingqi Qu, Wayne Xin Zhao, Jing Liu, Hao Tian, Hua Wu, Ji-Rong Wen, and Haifeng
Wang. 2023. Investigating the factual knowledge boundary of large language models with retrieval augmentation.
Retrieved fromhttps://arXiv:2307.11019
[222] Ruiyang Ren, Wayne Xin Zhao, Jing Liu, Hua Wu, Ji-Rong Wen, and Haifeng Wang. 2023. TOME: A two-stage
approach for model-based retrieval. InProceedings of the 61st Annual Meeting of the Association for Computational
Linguistics. Association for Computational Linguistics, 6102–6114.
[223] Mehdi Rezagholizadeh, Aref Jafari, Puneeth S. M. Saladi, Pranav Sharma, Ali Saheb Pasand, and Ali Ghodsi. 2022.
Pro-KD: Progressive distillation by following the footsteps of the teacher. InProceedings of the 29th International
Conference on Computational Linguistics (COLING’22). 4714–4727.
[224] KirkRoberts,TasmeerAlam,StevenBedrick,DinaDemner-Fushman,KyleLo,IanSoboroff,EllenVoorhees,LucyLu
Wang, and William R. Hersh. 2020. TREC-COVID: Rationale and structure of an information retrieval shared task
for COVID-19.J. Amer. Med. Info. Assoc.27, 9, 1431–1436.
[225] Stephen Robertson. 2004. Understanding inverse document frequency: on theoretical arguments for IDF.Journal of
documentation 60, 5 (2004), 503–520.
[226] Stephen Robertson and Hugo Zaragoza. 2009.The Probabilistic Relevance Framework: BM25 and Beyond.N o wP u b -
lishers Inc., Hanover, MD.
[227] Stephen E. Robertson, Steve Walker, Susan Jones, Micheline M. Hancock-Beaulieu, Mike Gatford et al. 1995. Okapi
at TREC-3.Nist Special Publ.109 (1995), 109.
[228] Corby Rosset, Bhaskar Mitra, Chenyan Xiong, Nick Craswell, Xia Song, and Saurabh Tiwary. 2019. An axiomatic
approach to regularizing neural ranking models. InProceedings of the 42nd International ACM SIGIR Conference on
Research and Development in Information Retrieval (SIGIR’19), Benjamin Piwowarski, Max Chevalier, Éric Gaussier,
Yoelle Maarek, Jian-Yun Nie, and Falk Scholer (Eds.). ACM, 981–984.
[229] Devendra Sachan, Mostofa Patwary, Mohammad Shoeybi, Neel Kant, Wei Ping, William L. Hamilton, and Bryan
Catanzaro.2021.End-to-endtrainingofneuralretrieversforopen-domainquestionanswering.In Proceedingsofthe
59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on
Natural Language Processing. 6648–6662.
[230] Devendra Singh Sachan, Mike Lewis, Dani Yogatama, Luke Zettlemoyer, Joelle Pineau, and Manzil Zaheer. 2022.
Questions are all you need to train a dense passage retriever. Retrieved fromhttps://arXiv:2206.10658
[231] Gerard Salton. 1962. Some experiments in the generation of word and document associations. InProceedings of the
Computer Conference of the American Federation of Information Processing Societies (AFIPS’62). 234–250.
[232] Gerard Salton and Chris Buckley. 1988. Term-weighting approaches in automatic text retrieval.Inf. Process. Manag.
24 (1988), 513–523.
[233] Gerard Salton, Anita Wong, and Chung-Shu Yang. 1975. A vector space model for automatic indexing.Commun.
ACM 18, 11 (1975), 613–620.
[234] MarkSandersonetal.2010.Testcollectionbasedevaluationofinformationretrievalsystems. FoundationsandTrends
in Information Retrieval4, 4 (2010), 247–375.
[235] Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei Zaharia. 2021. ColBERTv2: Effec-
tive and efficient retrieval via lightweight late interaction. Retrieved fromhttps://arXiv:2112.01488
[236] Christopher Sciavolino, Zexuan Zhong, Jinhyuk Lee, and Danqi Chen. 2021. Simple entity-centric questions chal-
lenge dense retrievers. In Proceedings of the Conference on Empirical Methods in Natural Language Processing
(EMNLP’21).
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.89:56 W. Xin et al.
[237] MinjoonSeo,TomKwiatkowski,AnkurParikh,AliFarhadi,andHannanehHajishirzi.2018.Phrase-indexedquestion
answering: A new challenge for scalable document comprehension. InProceedings of the Conference on Empirical
Methods in Natural Language Processing. 559–564.
[238] Minjoon Seo, Jinhyuk Lee, Tom Kwiatkowski, Ankur Parikh, Ali Farhadi, and Hannaneh Hajishirzi. 2019. Real-time
open-domain question answering with dense-sparse phrase index. InProceedings of the 57th Annual Meeting of the
Association for Computational Linguistics. 4430–4441.
[239] Siamak Shakeri, Cicero Nogueira dos Santos, Henghui Zhu, Patrick Ng, Feng Nan, Zhiguo Wang, Ramesh Nallapati,
and Bing Xiang. 2020. End-to-end synthetic data generation for domain adaptation of question answering systems.
In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP’20). 5445–5460.
[240] TaoShen,XiuboGeng,ChongyangTao,CanXu,XiaolongHuang,BinxingJiao,LinjunYang,andDaxinJiang.2022.
LexMAE: Lexicon-bottlenecked pretraining for large-scale retrieval. Retrieved fromhttps://abs/2208.14754
[241] Xiaoyu Shen, Svitlana Vakulenko, Marco Del Tredici, Gianni Barlacchi, Bill Byrne, and Adrià de Gispert. 2022. Low-
resource dense retrieval for open-domainquestion answering:A comprehensivesurvey. Retrieved fromhttps://abs/
2208.03197
[242] AnshumaliShrivastavaandPingLi.2014.AsymmetricLSH(ALSH)forsublineartimemaximuminnerproductsearch
(MIPS).In AdvancesinNeuralInformationProcessingSystems27:AnnualConferenceonNeuralInformationProcessing
Systems. 2321–2329.
[243] Georgios Sidiropoulos and Evangelos Kanoulas. 2022. Analysing the robustness of dual encoders for dense retrieval
against misspellings. InProceedings of the 45th International ACM SIGIR Conference on Research and Development in
Information Retrieval (SIGIR’22). 2132–2136.
[244] Harsha Vardhan Simhadri, G. R. Williams, Martin Aumüller, Matthijs Douze, Artem Babenko, Dmitry Baranchuk,
Qi Chen, Lucas Hosseini, Ravishankar Krishnaswamy, Gopal Srinivasa, Suhas Jayaram Subramanya, and Jingdong
Wang. 2022. Results of the NeurIPS’21 challenge on billion-scale approximate nearest-neighbor search. Retrieved
fromhttps://abs/2205.03763
[245] Josef Sivic and Andrew Zisserman. 2003. Video google: A text-retrieval approach to object matching in videos.Pro-
ceedings of the 9th IEEE International Conference on Computer Vision. 1470–1477.
[246] IanSoboroff,ShudongHuang,andDonnaHarman.2019.TREC2019newstrackoverview.In ProceedingsoftheText
Retrieval Conference (TREC’19).
[247] Richard Socher, Eric H. Huang, Jeffrey Pennington, Andrew Y. Ng, and Christopher D. Manning. 2011. Dynamic
pooling and unfolding recursive autoencoders for paraphrase detection. InProceedings of the Advances in Neural
Information Processing Systems 24: 25th Annual Conference on Neural Information Processing Systems. 801–809.
[248] Richard Socher, Jeffrey Pennington, Eric H. Huang, Andrew Y. Ng, and Christopher D. Manning. 2011. Semi-
supervised recursive autoencoders for predicting sentiment distributions. InProceedings of the Conference on Em-
pirical Methods in Natural Language Processing. 151–161.
[249] AxelSuarez,DyaaAlbakour,DavidCorney,MiguelMartinez,andJoséEsquivel.2018.Adatacollectionforevaluating
the retrieval of related tweets to news articles. InProceedings of the European Conference on Information Retrieval.
Springer, 780–786.
[250] Suhas Jayaram Subramanya, Fnu Devvrit, Harsha Vardhan Simhadri, Ravishankar Krishnaswamy, and Rohan
Kadekodi. 2019. DiskANN: Fast accurate billion-point nearest-neighbor search on a single node. InProceedings of
the Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Sys-
tems (NeurIPS’19).
[251] WeiweiSun,LingyongYan,XinyuMa,PengjieRen,DaweiYin,andZhaochunRen.2023.IsChatGPTgoodatsearch?
Investigating large language models as re-ranking agent. Retrieved fromhttps://arXiv:2304.09542
[252] Yutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaohan Huang, Alon Benhaim, Vishrav Chaudhary, Xia Song, and
Furu Wei. 2022. A length-extrapolatable transformer. Retrieved fromhttps://arXiv:2212.10554
[253] Yu Sun, Shuohuan Wang, Yu-Kun Li, Shikun Feng, Hao Tian, Hua Wu, and Haifeng Wang. 2020. ERNIE 2.0: A con-
tinual pre-training framework for language understanding. InProceedings of the 34th AAAI Conference on Artificial
Intelligence (AAAI’20), the 32nd Innovative Applications of Artificial Intelligence Conference (IAAI’20), the 10th AAAI
Symposium on Educational Advances in Artificial Intelligence (EAAI’20). 8968–8975.
[254] Weng Lam Tam, Xiao Liu, Kaixuan Ji, Lilong Xue, Xing Zhang, Yuxiao Dong, Jiahua Liu, Maodi Hu, and Jie Tang.
2022. Parameter-efficient prompt tuning makes generalized and calibrated neural text retrievers. Retrieved from
https://abs/2207.07087
[255] Alexandre Tamborrino. [n.d.]. Introducing Natural Language Search for Podcast Episodes.https://engineering.
atspotify.com/2022/03/introducing-natural-language-search-for-podcast-episodes/ .
[256] Hongyin Tang, Xingwu Sun, Beihong Jin, Jingang Wang, Fuzheng Zhang, and Wei Wu. 2021. Improving document
representationsbygeneratingpseudoqueryembeddingsfordenseretrieval.In Proceedingsofthe59thAnnualMeeting
of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language
Processing. 5054–5064.
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.Dense Text Retrieval Based on Pretrained Language Models: A Survey 89:57
[257] Zhengyang Tang, Benyou Wang, and Ting Yao. 2022. DPTDR: Deep prompt tuning for dense passage retrieval. Re-
trieved fromhttps://arXiv:2208.11503
[258] YiTay,VinhQuangTran,MostafaDehghani,JianmoNi,DaraBahri,HarshMehta,ZhenQin,KaiHui,ZheZhao,Jai
Gupta, Tal Schuster, William W. Cohen, and Donald Metzler. 2022. Transformer memory as a differentiable search
index. Retrieved fromhttps://abs/2202.06991
[259] NandanThakur,NilsReimers,JohannesDaxenberger,andIrynaGurevych.2021.AugmentedSBERT:Dataaugmen-
tation method for improving bi-encoders for pairwise sentence scoring tasks. InProceedings of the Conference of the
North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 296–310.
[260] Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych. 2021. BEIR: A heteroge-
nous benchmark for zero-shot evaluation of information retrieval models. Retrieved fromhttps://arXiv:2104.08663
[261] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. FEVER: A large-scale dataset
forfactextractionandVERification.In ProceedingsoftheConferenceoftheNorthAmericanChapteroftheAssociation
for Computational Linguistics: Human Language Technologies. 809–819.
[262] Naftali Tishby and Noga Zaslavsky. 2015. Deep learning and the information bottleneck principle. InProceedings of
the IEEE Information Theory Workshop (ITW’15).1 – 5 .
[263] Nicola Tonellotto. 2022. Lecture notes on neural information retrieval. Retrieved fromhttps://abs/2207.13443
[264] Nicola Tonellotto and Craig Macdonald. 2021. Query embedding pruning for dense retrieval. InProceedings of the
30th ACM International Conference on Information and Knowledge Management. 3453–3457.
[265] George Tsatsaronis, Georgios Balikas, Prodromos Malakasiotis, Ioannis Partalas, Matthias Zschunke, Michael R.
Alvers, Dirk Weissenborn, Anastasia Krithara, Sergios Petridis, Dimitris Polychronopoulos et al. 2015. An overview
oftheBIOASQlarge-scalebiomedicalsemanticindexingandquestionansweringcompetition. BMCBioinform. 16,1
(2015), 1–28.
[266] Aäron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning with contrastive predictive coding.
Retrieved fromhttps://abs/1807.03748 (2018).
[267] Saúl Vargas. 2014. Novelty and diversity enhancement and evaluation in recommender systems and information
retrieval. InProceedings of the 37th International ACM SIGIR Conference on Research and Development in Information
Retrieval (SIGIR’14). 1281.
[268] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanN.Gomez,LukaszKaiser,andIllia
Polosukhin. 2017. Attention is all you need. InProceedings of the Advances in Neural Information Processing Systems
30: Annual Conference on Neural Information Processing Systems. 5998–6008.
[269] Michael Völske, Alexander Bondarenko, Maik Fröbe, Benno Stein, Jaspreet Singh, Matthias Hagen, and Avishek
Anand. 2021. Towards axiomatic explanations for neural ranking models. InProceedings of the ACM SIGIR Interna-
tional Conference on Theory of Information Retrieval. 13–22.
[270] EllenM.Voorheesetal.1999.Thetrec-8questionansweringtrackreport.In ProceedingsoftheTextRetrievalConfer-
ence (TREC’99), Vol. 99. 77–82.
[271] Henning Wachsmuth, Shahbaz Syed, and Benno Stein. 2018. Retrieval of the best counterargument without prior
topicknowledge.In Proceedingsofthe56thAnnualMeetingoftheAssociationforComputationalLinguistics .241–251.
[272] David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine van Zuylen, Arman Cohan, and Hannaneh Ha-
jishirzi. 2020. Fact or fiction: Verifying scientific claims. InProceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP’20). 7534–7550.
[273] Jiexin Wang, Adam Jatowt, and Masatoshi Yoshikawa. 2021. ArchivalQA: A large-scale benchmark dataset for open
domain question answering over archival news collections. Retrieved fromhttps://arXiv:2109.03438
[274] Jingdong Wang and Shipeng Li. 2012. Query-driven iterated neighborhood graph search for large scale indexing.
Proceedings of the 20th ACM International Conference on Multimedia.
[275] Jingdong Wang, Ting Zhang, Jingkuan Song, N. Sebe, and Heng Tao Shen. 2018. A survey on learning to hash.IEEE
Trans. Pattern Anal. Mach. Intell.40 (2018), 769–790.
[276] Kexin Wang, Nils Reimers, and Iryna Gurevych. 2021. TSDAE: Using transformer-based sequential denoising auto-
encoder for unsupervised sentence embedding learning. InProceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP’21).
[277] Kexin Wang, Nandan Thakur, Nils Reimers, and Iryna Gurevych. 2021. GPL: Generative pseudo labeling for unsu-
pervised domain adaptation of dense retrieval. Retrieved fromhttps://arXiv:2112.07577
[278] KaiyeWang,QiyueYin,WeiWang,ShuWu,andLiangWang.2016.Acomprehensivesurveyoncross-modalretrieval.
Retrieved fromhttps://abs/1607.06215 (2016).
[279] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei.
2022. SimLM: Pre-training with representation bottleneck for dense passage retrieval. Retrieved fromhttps://abs/
2207.02578.
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.89:58 W. Xin et al.
[280] Xiao Wang, Craig Macdonald, Nicola Tonellotto, and Iadh Ounis. 2021. Pseudo-relevance feedback for multiple rep-
resentation dense retrieval. Retrieved fromhttps://arXiv:2106.11251
[281] Xiao Wang, Craig Macdonald, Nicola Tonellotto, and Iadh Ounis. 2021. Pseudo-relevance feedback for multiple rep-
resentation dense retrieval. InProceedings of the ACM SIGIR International Conference on the Theory of Information
Retrieval (ICTIR’21). 297–306.
[282] YujingWang,YingyanHou,HaonanWang,ZimingMiao,ShibinWu,HaoSun,QiChen,YuqingXia,ChengminChi,
GuoshuaiZhao,ZhengLiu,XingXie,HaoAllenSun,WeiweiDeng,QiZhang,andMaoYang.2022.Aneuralcorpus
indexer for document retrieval. Retrieved fromhttps://abs/2206.02743
[283] Yumeng Wang, Lijun Lyu, and Avishek Anand. 2022. BERT rankers are brittle: A study using adversarial document
perturbations. InProceedings of the ACM SIGIR International Conference on the Theory of Information Retrieval (IC-
TIR’22). 115–120.
[284] Zhiguo Wang, Patrick Ng, Xiaofei Ma, Ramesh Nallapati, and Bing Xiang. 2019. Multi-passage BERT: A globally
normalized bert model for open-domain question answering. InProceedings of the Conference on Empirical Methods
in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-
IJCNLP’19). 5878–5882.
[285] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and
Quoc V. Le. 2022. Finetuned language models are zero-shot learners. Retrieved fromhttps://abs/2109.01652
[286] Ledell Wu, Fabio Petroni, Martin Josifoski, Sebastian Riedel, and Luke Zettlemoyer. 2020. Scalable zero-shot entity
linking with dense entity retrieval. InProceedings of the Conference on Empirical Methods in Natural Language Pro-
cessing (EMNLP’20). 6397–6407.
[287] Xing Wu, Guangyuan Ma, Meng Lin, Zijia Lin, Zhongyuan Wang, and Songlin Hu. 2022. ConTextual mask auto-
encoder for dense passage retrieval. Retrieved fromhttps://arXiv:2208.07670
[288] ChenyanXiong,ZhuyunDai,JamieCallan,ZhiyuanLiu,andRussellPower.2017.End-to-endneuraladhocranking
with kernel pooling. InProceedings of the 40th International ACM SIGIR Conference on Research and Development in
Information Retrieval. 55–64.
[289] LeeXiong,ChenyanXiong,YeLi,Kwok-FungTang,JialinLiu,PaulN.Bennett,JunaidAhmed,andArnoldOverwijk.
2021. Approximate nearest-neighbor negative contrastive learning for dense text retrieval. InProceedings of the 9th
International Conference on Learning Representations (ICLR’21).
[290] Canwen Xu, Daya Guo, Nan Duan, and Julian McAuley. 2022. LaPraDoR: Unsupervised pretrained dense retriever
for zero-shot text retrieval. Retrieved fromhttps://arXiv:2203.06169
[291] Shicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng, and Tat-seng Chua. 2023. Search-in-the-chain: Towards the
accurate, credible and traceable content generation for complex knowledge-intensive tasks. Retrieved fromhttps:
//arXiv:2304.14732
[292] IkuyaYamada,AkariAsai,andHannanehHajishirzi.2021.Efficientpassageretrievalwithhashingforopen-domain
question answering. InProceedings of the 59th Annual Meeting of the Association for Computational Linguistics and
the 11th International Joint Conference on Natural Language Processing. 979–986.
[293] Ming Yan, Chenliang Li, Chen Wu, Bin Bi, Wei Wang, Jiangnan Xia, and Luo Si. 2019. IDST at TREC 2019 deep
learningtrack:deepcascaderankingwithgeneration-baseddocumentexpansionandpre-trainedlanguagemodeling.
In Proceedings of the 28th Text Retrieval Conference (TREC’19) (NIST Special Publication, Vol. 1250).
[294] Yuanmeng Yan, Rumei Li, Sirui Wang, Fuzheng Zhang, Wei Wu, and Weiran Xu. 2021. ConSERT: A contrastive
framework for self-supervised sentence representation transfer. InProceedings of the 59th Annual Meeting of the
AssociationforComputationalLinguisticsandthe11thInternationalJointConferenceonNaturalLanguageProcessing .
5065–5075.
[295] NanYang,FuruWei,BinxingJiao,DaxingJiang,andLinjunYang.2021.xMoCo:Crossmomentumcontrastivelearn-
ing for open-domain question answering. InProceedings of the 59th Annual Meeting of the Association for Computa-
tional Linguistics and the 11th International Joint Conference on Natural Language Processing. 6120–6129.
[296] Peilin Yang, Hui Fang, and Jimmy Lin. 2017. Anserini: Enabling the use of lucene for information retrieval research.
In Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval.
1253–1256.
[297] Yinfei Yang, Ning Jin, Kuo Lin, Mandy Guo, and Daniel Cer. 2021. Neural retrieval for question answering with
cross-attention supervised data augmentation. InProceedings of the 59th Annual Meeting of the Association for Com-
putational Linguistics and the 11th International Joint Conference on Natural Language Processing. 263–268.
[298] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D.
Manning. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answering. InProceedings of the
Conference on Empirical Methods in Natural Language Processing. 2369–2380.
[299] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. React: Syner-
gizing reasoning and acting in language models. Retrieved fromhttps://arXiv:2210.03629
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.Dense Text Retrieval Based on Pretrained Language Models: A Survey 89:59
[300] DaniYogatama,CypriendeMassond’Autume,andLingpengKong.2021.Adaptivesemiparametriclanguagemodels.
Trans. Assoc. Comput. Linguist.9 (2021), 362–373.
[301] HongChien Yu, Chenyan Xiong, and Jamie Callan. 2021. Improving query representations for dense retrieval with
pseudo relevance feedback. InProceedings of the 30th ACM International Conference on Information and Knowledge
Management. 3592–3596.
[302] Shi Yu, Zhenghao Liu, Chenyan Xiong, Tao Feng, and Zhiyuan Liu. 2021. Few-shot conversational dense retrieval.
In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval
(SIGIR’21). 829–838.
[303] HansiZeng,HamedZamani,andVishwaVinay.2022.Curriculumlearningfordenseretrievaldistillation.In Proceed-
ings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’22).
1979–1983.
[304] ChengXiang Zhai. 2008.Statistical Language Models for Information Retrieval. Morgan & Claypool Publishers.
[305] Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Jiafeng Guo, Min Zhang, and Shaoping Ma. 2021. Interpreting dense retrieval
as mixture of topics. Retrieved fromhttps://arXiv:2111.13957
[306] Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Jiafeng Guo, Min Zhang, and Shaoping Ma. 2021. Jointly optimizing query
encoder and product quantization to improve retrieval performance. InProceedings of the 30th ACM International
Conference on Information and Knowledge Management. 2487–2496.
[307] JingtaoZhan,JiaxinMao,YiqunLiu,JiafengGuo,MinZhang,andShaopingMa.2021.Learningdiscreterepresenta-
tions via constrained clustering for effective and efficient dense retrieval. Retrieved fromhttps://arXiv:2110.05789
[308] Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Jiafeng Guo, Min Zhang, and Shaoping Ma. 2021. Optimizing dense retrieval
model training with hard negatives. InProceedings of the 44th International ACM SIGIR Conference on Research and
Development in Information Retrieval. 1503–1512.
[309] JingtaoZhan,JiaxinMao,YiqunLiu,MinZhang,andShaopingMa.2020.RepBERT:Contextualizedtextembeddings
for first-stage retrieval. Retrieved fromhttps://abs/2006.15498
[310] Jingtao Zhan, Xiaohui Xie, Jiaxin Mao, Yiqun Liu, Min Zhang, and Shaoping Ma. 2022. Evaluating extrapolation
performance of dense retrieval. Retrieved fromhttps://abs/2204.11447
[311] Hang Zhang, Yeyun Gong, Yelong Shen, Jiancheng Lv, Nan Duan, and Weizhu Chen. 2021. Adversarial retriever-
ranker for dense text retrieval. Retrieved fromhttps://arXiv:2110.03611
[312] HanZhang,HongweiShen,YimingQiu,YunjiangJiang,SonglinWang,SulongXu,YunXiao,BoLong,andWen-Yun
Yang. 2021. Joint learning of deep retrieval model and product quantization based embedding index. Retrieved from
https://arXiv:2105.03933
[313] Jianjin Zhang, Zheng Liu, Weihao Han, Shitao Xiao, Rui Zheng, Yingxia Shao, Hao Sun, Hanqing Zhu, Premkumar
Srinivasan,DenvyDeng,QiZhang,andXingXie.2022.Uni-retriever:Towardslearningtheunifiedembeddingbased
retriever in bing sponsored search. Retrieved fromhttps://abs/2202.06212
[314] Kai Zhang, Chongyang Tao, Tao Shen, Can Xu, Xiubo Geng, Binxing Jiao, and Daxin Jiang. 2022. LED: Lexicon-
enlightened dense retriever for large-scale retrieval. Retrieved fromhttps://abs/2208.13661
[315] Michael J. Q. Zhang and Eunsol Choi. 2021. SituatedQA: Incorporating extra-linguistic contexts into QA. Retrieved
fromhttps://arXiv:2109.06157
[316] Shunyu Zhang, Yaobo Liang, Ming Gong, Daxin Jiang, and Nan Duan. 2022. Multi-view document representation
learning for open-domain dense retrieval. InProceedings of the 60th Annual Meeting of the Association for Computa-
tional Linguistics. 5990–6000.
[317] XinyuZhang,XueguangMa,PengShi,andJimmyLin.2021.Mr.TyDi:Amulti-lingualbenchmarkfordenseretrieval.
Retrieved fromhttps://abs/2108.08787
[318] Yanzhao Zhang, Dingkun Long, Guangwei Xu, and Pengjun Xie. 2022. HLATR: Enhance multi-stage text retrieval
with hybrid list aware transformer reranking. Retrieved fromhttps://abs/2205.10569.https://doi.org/10.48550/arXiv.
2205.10569
[319] WayneXinZhao,KunZhou,JunyiLi,TianyiTang,XiaoleiWang,YupengHou,YingqianMin,BeichenZhang,Junjie
Zhang, Zican Dong et al. 2023. A survey of large language models. Retrieved fromhttps://arXiv:2303.18223
[320] WeiZhong,Jheng-HongYang,andJimmyLin.2022.Evaluatingtoken-levelandpassage-leveldenseretrievalmodels
for math information retrieval. Retrieved fromhttps://abs/2203.11163
[321] Chunting Zhou, Jiatao Gu, and Graham Neubig. 2020. Understanding knowledge distillation in non-autoregressive
machine translation. InProceedings of the 8th International Conference on Learning Representations (ICLR’20).
[322] Jiawei Zhou, Xiaoguang Li, Lifeng Shang, Lan Luo, Ke Zhan, Enrui Hu, Xinyu Zhang, Hao Jiang, Zhao Cao, Fan Yu
etal.2022.Hyperlink-inducedpre-trainingforpassageretrievalinopen-domainquestionanswering.In Proceedings
of the 60th Annual Meeting of the Association for Computational Linguistics. 7135–7146.
[323] KunZhou,YeyunGong,XiaoLiu,WayneXinZhao,YelongShen,AnleiDong,JingwenLu,RanganMajumder,Ji-Rong
Wen,NanDuan,andWeizhuChen.2022.SimANS:Simpleambiguousnegativessamplingfordensetextretrieval.In
Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP’22).
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.89:60 W. Xin et al.
[324] KunZhou,BeichenZhang,XinZhao,andJi-RongWen.2022.Debiasedcontrastivelearningofunsupervisedsentence
representations. InProceedings of the Association for Computational Linguistics (ACL’22). 6120–6130.
[325] YuchengZhou,TaoShen,XiuboGeng,ChongyangTao,CanXu,GuodongLong,BinxingJiao,andDaxinJiang.2022.
Towards robust ranker for text retrieval. Retrieved fromhttps://abs/2206.08063
[326] YujiaZhou,JingYao,ZhichengDou,LedellWu,PeitianZhang,andJi-RongWen.2022.Ultron:Anultimateretriever
on corpus with a model-based indexer. Retrieved fromhttps://abs/2208.09257
[327] YujiaZhou,JingYao,ZhichengDou,LedellYuWu,andJirongWen.2022.DynamicRetriever:Apre-trainingmodel-
based IR system with neither sparse nor dense index. Retrieved fromhttps://abs/2203.00537
[328] MuZhu.2004.Recall,precisionandaverageprecision. Dept.Stat.Actuar.Sci.(UniversityofWaterloo,Waterloo) 2,30
(2004), 6.
[329] Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng, Zhicheng Dou, and Ji-Rong
Wen. 2023. Large language models for information retrieval: A survey. Retrieved fromhttps://arXiv:2308.07107
[330] HongleiZhuang,ZhenQin,RolfJagerman,KaiHui,JiMa,JingLu,JianmoNi,XuanhuiWang,andMichaelBendersky.
2022. RankT5: Fine-tuning T5 for text ranking with ranking losses. Retrieved fromhttps://abs/2210.10634. https:
//doi.org/10.48550/arXiv.2210.10634
[331] Shengyao Zhuang, Hang Li, and G. Zuccon. 2022. Implicit feedback for dense passage retrieval: A counterfactual
approach. Retrieved fromhttps://abs/2204.00718
[332] Shengyao Zhuang, Houxing Ren, Linjun Shou, Jian Pei, Ming Gong, Guido Zuccon, and Daxin Jiang. 2022. Bridging
the gap between indexing and retrieval for differentiable search index with query generation. Retrieved fromhttps:
//abs/2206.10128
[333] Shengyao Zhuang and Guido Zuccon. 2021. Dealing with typos for BERT-based passage retrieval and ranking. In
Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP’21). 2836–2842.
[334] Shengyao Zhuang and G. Zuccon. 2022. Asyncval: A toolkit for asynchronously validating dense retriever check-
points during training. Retrieved fromhttps://abs/2202.12510
[335] ShengyaoZhuangandGuidoZuccon.2022.CharacterBERTandself-teachingforimprovingtherobustnessofdense
retrievers on queries with typos. InProceedings of the 45rd International ACM SIGIR Conference on Research and
Development in Information Retrieval. 1444–1454.
[336] Justin Zobel and Alistair Moffat. 2006. Inverted files for text search engines.ACM Comput. Surv.38 (2006), 6.
[337] Justin Zobel, Alistair Moffat, and Kotagiri Ramamohanarao. 1998. Inverted files versus signature files for text index-
ing.ACM Trans. Database Syst.23, 4 (1998), 453–490.
Received 19 January 2023; revised 12 October 2023; accepted 3 December 2023
ACM Transactions on Information Systems, Vol. 42, No. 4, Article 89. Publication date: February 2024.

DEMONSTRATE –SEARCH –PREDICT :
Composing retrieval and language models for knowledge-intensive NLP
Omar Khattab 1 Keshav Santhanam1 Xiang Lisa Li 1 David Hall 1
Percy Liang 1 Christopher Potts 1 Matei Zaharia 1
Abstract
Retrieval-augmented in-context learning has
emerged as a powerful approach for addressing
knowledge-intensive tasks using frozen language
models (LM) and retrieval models (RM). Exist-
ing work has combined these in simple “retrieve-
then-read” pipelines in which the RM retrieves
passages that are inserted into the LM prompt.
To begin to fully realize the potential of frozen
LMs and RMs, we propose DEMONSTRATE –
SEARCH –PREDICT (DSP ), a framework that re-
lies on passing natural language texts in sophisti-
cated pipelines between an LM and an RM. DSP
can express high-level programs that bootstrap
pipeline-aware demonstrations, search for rele-
vant passages, and generate grounded predictions,
systematically breaking down problems into small
transformations that the LM and RM can handle
more reliably. We have written novel DSP pro-
grams for answering questions in open-domain,
multi-hop, and conversational settings, establish-
ing in early evaluations new state-of-the-art in-
context learning results and delivering 37–120%,
8–39%, and 80–290% relative gains against the
vanilla LM (GPT-3.5), a standard retrieve-then-
read pipeline, and a contemporaneous self-ask
pipeline, respectively. We release DSP at https:
//github.com/stanfordnlp/dsp.
1. Introduction
In-context learning adapts a frozen language model (LM) to
tasks by conditioning the LM on a textual prompt including
task instructions and a few demonstrating examples (Mc-
Cann et al., 2018; Radford et al., 2019; Brown et al., 2020).
For knowledge-intensive tasks such as question answering,
fact checking, and information-seeking dialogue, retrieval
models (RM) are increasingly used to augment prompts
1Stanford University. Correspondence to:
Omar Khattab <okhattab@cs.stanford.edu>.
Preprint.
How many storeys are in the castle David Gregory inherited?LM:Castle Gregory has three storeys.❌Hallucinates a fictitious castleRM: “St. Gregory Hotel is a nine-floor boutique hotel in D.C...”LM: St. Gregory Hotel has nine storeys.❌Retrieves a different buildingLM: “Which castle did David Gregory inherit?”RM: “David Gregory inherited Kinnairdy Castle in 1664...”LM: “How many storyes does Kinnairdy Castle have?”RM: “Kinnairdy Castle is a tower house, having five storeys…”LM: Kinnairdy Castlehas fivestoreys.Vanilla LMRetrieve-then-ReadMulti-HopDSP Program
Figure 1.A comparison between three systems based on GPT-
3.5 (text-davinci-002). On its own, the LM often makes false
assertions. An increasingly popular retrieve-then-read pipeline
fails when simple search can’t ﬁnd an answer. In contrast, a task-
aware DSP program successfully decomposes the problem and
produces a correct response. Texts edited for presentation.
with relevant information from a large corpus (Lazaridou
et al., 2022; Press et al., 2022; Khot et al., 2022).
Recent work has shown suchretrieval-augmented in-context
learning to be effective in simple “retrieve-then-read”
pipelines: a query is fed to the RM and the retrieved pas-
sages become part of a prompt that provides context for
the LM to use in its response. In this work, we argue that
the fact that both LMs and RMs consume (and generate or
retrieve) natural language texts creates an opportunity for
much more sophisticated interactions between them. Fully
realizing this would be transformative: frozen LMs and
RMs could serve as infrastructure across tasks, enabling
ML- and domain-experts alike to rapidly build grounded
AI systems at a high level of abstraction and with lower
deployment overheads and annotation costs.
Figure 1 begins to illustrate the power of retrieval-
augmented in-context learning, but also the limitations of
“retrieve-then-read” (Lazaridou et al., 2022; Izacard et al.,
2022). Our query is “How many storeys are in the castle
David Gregory inherited?” When prompted to answer this,
GPT-3.5 (text-davinci-002; Ouyang et al. 2022) makes
up a ﬁctitious castle with incorrect attributes, highlighting
the common observation that knowledge stored in LM pa-
rameters is often unreliable (Shuster et al., 2021; Ishii et al.,
2022). Introducing an RM component helps, as the LM
can ground its responses in retrieved passages, but a rigid
arXiv:2212.14024v2  [cs.CL]  23 Jan 2023DEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models
QHow many storeys are in...Q In which city did Akeem Ellis play in 2017?A Ellesmere PortQ When was the discoverer of Palomar 4 born?A 1889TrainDemonstratedefdemonstrate(x:Example) -> Example:x.demos = annotate(x.train, attempt)returnxdefattempt(d:Example):d= search(d)d= predict(d)if d.pred ==d.answer: returnd1QHow many storeys are in the castle...Q When was the discoverer of Palomar 4 born?A 1889Hop1Who discovered Palomar 4?Psg1Edwin Hubble discovered Palomar 4...Hop2When was Edwin Powell born?Psg2Edwin Powell Hubble (1889–1953) was...Pred1889x : ExampleQ In which city did Akeem Ellis play...A Ellesmere Port... ...PredWaterloo❌Demos“How many storeys are in the castle David Gregory inherited?”QHow many storeys are in the...Demos. . .Hop1Which castle did David Gregory inherit?Psg1David Gregory inherited Kinnairdy Castle...Hop2How many storeys are in Kinnairdy Castle?Psg2Kinnairdy Castle […] having five storeys...QHow many storeys does the.... . .. . .PredFive storeysSearchdefsearch(x:Example) -> Example:x.hop1 =generate(hop_template)(x).predx.psg1 =retrieve(x.hop1, k=1)[0]x.hop2 =generate(hop_template)(x).predx.psg2 =retrieve(x.hop2, k=1)[0]returnx2 Predictdefpredict(x:Example) -> Example:x.context = [x.psg1, x.psg2]x.pred=generate(qa_template)(x).predreturnx3“Five storeys”
Figure 2.A toy example of a DSP program for multi-hop question answering. Given an input question and a 2-shot training set, the
DEMONSTRATE stage programmatically annotates intermediate transformations on the training examples using a form of weak supervision.
Learning from a resulting demonstration, the SEARCH stage decomposes the complex input question and retrieves supporting information
over two retrieval hops. Finally, the PREDICT stage uses the demonstration and retrieved passages to answer the question.
retrieve-then-read strategy fails because the RM cannot ﬁnd
passages that directly answer the question.
We introduce the DEMONSTRATE –SEARCH –PREDICT
(DSP ) framework for in-context learning, which relies en-
tirely on passing natural language text (and scores) be-
tween a frozen RM and LM. DSP introduces a num-
ber of composable functions that bootstrap training exam-
ples (DEMONSTRATE ), gather information from a knowl-
edge corpus ( SEARCH ), and generate grounded outputs
(PREDICT ), using them to systematically unify techniques
from the retrieval-augmented NLP and the in-context learn-
ing literatures (Lee et al., 2019; Khattab et al., 2021a; Anan-
tha et al., 2020; Gao et al., 2022; Izacard et al., 2022; Dohan
et al., 2022; Zelikman et al., 2022; Zhang et al., 2022).
We use DSP to suggest powerful strategies for knowledge-
intensive tasks with compositions of these techniques. This
reveals new conceptual possibilities for in-context learning
in general (§2), and it allows us to present rich programs
that set new state-of-the-art results (§3).
Figure 1 shows the path that a DSP program might take to
arrive at an answer, and Figure 2 illustrates how a deliberate
program achieves this. Instead of asking the LM to answer
this complex question, the program’sSEARCH stage uses the
LM to generate a query “Which castle did David Gregory
inherit?” The RM retrieves a passage saying Gregory inher-
ited the Kinnairdy Castle. After a second search “hop” ﬁnds
the castle’s number of storeys, thePREDICT stage queries
the LM with these passages to answer the original question.
Although this program implements behaviors such as query
generation, it requires no hand-labeled examples of these
intermediate transformations (i.e., of the queries and pas-
sages of both retrieval hops). Instead, the DEMONSTRATE
stage uses labeled question–answer pairs to implement a
form of weak supervision that programmatically annotates
the transformations invoked within SEARCH and PREDICT .
We evaluate severalDSP programs on answering questions
in open-domain, multi-hop, and conversational settings. In
them, we implement novel and reusable transformations
such as bootstrapping annotations for all of our pipelines
with weak supervision (§2.3), reliably rewriting questions to
resolve conversational dependencies and iteratively decom-
pose complex queries with summarization of intermediate
hops (§2.4), and generating grounded responses from mul-
tiple passages with self-consistency (§2.5). We report pre-
liminary results on Open-SQuAD, HotPotQA, and QReCC
using the frozen LM GPT-3.5 and RM ColBERTv2 (Khat-
tab & Zaharia, 2020; Santhanam et al., 2022b) with no
ﬁne-tuning. Our DSP programs deliver 37–120%, 8–39%,
and 80–290% relative gains against corresponding vanilla
LMs, a standard retrieve-then-read pipeline, and a contem-
poraneous self-ask pipeline (Press et al., 2022), respectively.
Future versions of this report will include additional test
tasks and LM choices.
In summary, this work makes the following contributions.
First, we argue that simple task-agnostic pipelines for in-
context learning should give way to deliberate, task-aware
strategies. Second, we show that this shift need not be a
burden: with DSP , such strategies can be easily expressed
as short programs using composable operators. Third, this
composability spawns powerful capacities, like automati-
cally annotating demonstrations for complex pipelines from
end-task labels. Fourth, for three knowledge-intensive tasks,
we implement rich programs that establish state-of-the-art
results for in-context learning.DEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models
2. DEMONSTRATE –SEARCH –PREDICT
We now introduce the DSP framework and show its expres-
sive power by suggesting a number of strategies in which
the LM and RM can come together to tackle complex prob-
lems effectively. We show in §3 that such strategies out-
perform existing in-context learning methods. We begin by
discussing the LM and RM foundation modules on which
DSP is built (§2.1) and then the datatypes and control ﬂow
within DSP (§2.2). Subsequently, we discuss each of the
three inference stages: DEMONSTRATE (§2.3), SEARCH
(§2.4), and PREDICT (§2.5).
2.1. Pretrained Modules: LM and RM
A DSP program deﬁnes the communication between the
language model LM and the retrieval model RM.
Language Model We invoke a frozen language model
LM to conditionally generate (or score) text. For each
invocation, the program prepares a prompt that adapts the
LM to a speciﬁc function (e.g., answering questions or
generating queries). A prompt often includes instructions,
a few demonstrations of the desired behavior, and an input
query to be answered.
As in Figure 2, the LM generates not only: (i) the ﬁnal
answer to the input question (in thePREDICT stage), but also
(ii) intermediate “hop” queries to ﬁnd useful information
for the input question (SEARCH ) as well as (iii) exemplar
queries that illustrate how to produce queries for questions
in the training set (DEMONSTRATE ). This systematic use of
the LM is a hallmark of DSP programs.
Retrieval Model DSP programs also invoke a frozen re-
trieval model RM to retrieve the top-k most “relevant”
text sequences for a given query. The RM can index a
massive set of pre-deﬁned passages for scalable search, and
those passages can be updated without changing the retrieval
parameters. The RM accepts free-form textual inputs and
specializes in estimating the relevance (or similarity) of a
text sequence to a query.
As in Figure 2, the RM is responsible for retrieving (i)
passages for each query generated by the LM (during the
SEARCH stage), but also (ii) passages that are used within
demonstrations (DEMONSTRATE ). In the latter case, the
RM’s contributions are less about providing directly rel-
evant information to the input question and more about
helping the LM adapt to the domain and task.
Though not utilized in this example, the RM is also used in
DSP for functions like retrieving “nearest-neighbor” demon-
strations from task training data (DEMONSTRATE ) and se-
lecting well-grounded generated sequences from the LM
(PREDICT ).
2.2. Datatypes and Control Flow
We have implemented the DSP framework in Python. The
present section introduces the core data types and compos-
able functions provided by the framework. We use illustra-
tive code snippets to ground the examples, and to convey
the power that comes from being able to express complex
interactions between the LM and RM in simple programs.
The Example Datatype To conduct a task, a DSP pro-
gram manipulates one or more instances of the Example
datatype. An Example behaves like a Python dictionary
with multiple ﬁelds. The program is typically provided with
a few training examples. The code snippet below illustrates
this for multi-hop question answering.
1 from dsp import Example
2
3 train = [ Example ( question =" When was the discoverer
of Palomar 4 born ?", answer =" 1889 "),
4 Example ( question ="In which city did Akeem
Ellis play in 2017? ", answer =" Ellesmere Port ")]
This snippet contains two labeled examples, each with a
multi-hop question (e.g., “In which city did Akeem Ellis
play in 2017?”) and its short answer (“Ellesmere Port”).
Arbitrary keys and values are allowed within an Example,
though typical values are strings or lists of strings.
In this task, we are unlikely to ﬁnd an individual passage
that provides the answer to any question. For example, the
ﬁrst training example can probably be resolved only by ﬁrst
answering the question of who discovered Palomar (“Edwin
Hubble”) and then addressing the question of Hubble’s birth
date using different evidence passages. We typically assume
that the human-labeled training data do not include labels
for intermediate transformations (e.g., queries for individual
hops) that would be useful for following these steps, and so
it is the job of the DSP program to discover these strategies
via in-context learning.
A DSP Program The following code snippet is a com-
plete program for resolving multi-hop questions like those
in Figure 1, with help from train examples like those above.
1 def multihop_program ( question : str ) -> str :
2 x = Example ( question = question , train = train )
3 x = multihop_demonstrate (x)
4 x = multihop_search (x)
5 x = multihop_predict (x)
6 return x. answer
7
8 multihop_program (" How many storeys does the castle
David Gregory inherited have ?")
9 # => " five storeys "
The program takes the input (here, a question) and outputs
the system output (its short answer). It starts by creating
an Example for the input question and assigning the train
ﬁeld to the training set from the previous snippet. ProgramsDEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models
invoke and compose DSP primitives (i.e., built-in functions)
to build the DEMONSTRATE , SEARCH , and PREDICT trans-
formations that deﬁne the program.
Transformations A transformation is a function that
takes an Example as input and returns an Example, pop-
ulating new ﬁelds (or modifying existing ﬁelds) in it. This
program invokes three developer-deﬁned transformations,
namely, multihop_demonstrate, multihop_search, and
multihop_predict. Transformations may themselves in-
voke other transformations, and they act analogously to
layers in standard deep neural network (DNN) program-
ming frameworks such as PyTorch, except that they pass
text data instead of tensors between each other and do not
involve backpropagation.
We categorize transformations according to their behavior
(or purpose) under one of the DEMONSTRATE , SEARCH ,
and PREDICT stages. That said, DSP does not impose this
categorization and allows us to deﬁne functions that may
blend these stages. We will discuss each of the three stages
next.
2.3. DEMONSTRATE
It is known that including examples of the desired behavior
from the LM in its prompt typically leads to better perfor-
mance (Brown et al., 2020). In DSP , a demonstration is a
training example that has been prepared to illustrate speciﬁc
desired behaviors from the LM. A DEMONSTRATE transfor-
mation takes as input x of type Example and prepares a list
of demonstrations in x.demos, typically by selecting a sub-
set of the training examples in x.train and bootstrapping
new ﬁelds in them.
Bootstrapping Demonstrations Examples in the train-
ing set typically consist of the input text and the target
output of the task. The DEMONSTRATE stage can aug-
ment a training example by programmatically bootstrapping
annotations for intermediate transformations. In our run-
ning “multi-hop” example, the demonstrations illustrate
three LM-based transformations: (i) how to break down the
input question in order to gather information for answer-
ing it (i.e., ﬁrst-hop retrieval), (ii) how to use information
gathered in an earlier “hop” to ask follow-up questions, and
(iii) how to use the information gathered to answer complex
questions.
1 Examples = list [ Example ]
2 Transformation = Callable [[ Example ],
3 Optional [ Example ]]
4
5 annotate ( train : Examples , fn: Transformation )
6 -> Examples
Akin to a specialized map, the annotate primitive accepts
a user-deﬁned transformation fn and applies it over a list
of training examples. Whenever fn returns an example
(rather than None), annotate caches the intermediate pre-
dictions (i.e., the generated queries and retrieved passages).
These predictions serve as successful demonstrations for the
pipeline transformations. In simple uses, fn may attempt
to answer the example “zero-shot” one or more times. This
is typically done by invoking the SEARCH and PREDICT
stages of the program. When an answer is produced, if
fn assesses it as correct, it returns a populated example in
which the intermediate predictions are present.
Case Study The snippet below deﬁnes the func-
tion multihop_demonstrate, called in Line 3 of
multihop_program, and illustrates the usage of annotate.
1 from dsp import sample , annotate
2
3 def attempt_example (d: Example ):
4 d = d. copy ( demos =[])
5 d = multihop_search (d)
6 d = multihop_predict (d)
7 return d if d. pred == d. answer else None
8
9 def multihop_demonstrate (x: Example ):
10 demos = annotate (x. train , attempt_example )
11 return Example (x, demos = demos )
In Line 10, multihop_demonstrate invokes annotate,
which bootstraps missing ﬁelds in training examples by
caching annotations from attempt_example. The transfor-
mation attempt_example takes a training example d and
attempts to answer it in a zero-shot fashion: it creates a copy
of d with no demonstrations (Line 4; i.e., zero-shot) and
invokes the multi-hop search and predict pipeline (Lines 5
and 6). Each transformation returns an updated version of
d with additional ﬁelds populated. If the pipeline answers
correctly (Line 7), the updated d is returned.
Figure 2 illustrates this behavior. DEMONSTRATE trans-
forms a training question–answer pair to a fully-populated
demonstration, including ﬁelds such as hop1 and hop2 (i.e.,
queries for multi-hop search) as well as psg1 and psg2.
When the LM is later invoked to conduct a transformation,
say, generating a “second-hop” query duringSEARCH , the
psg1 ﬁeld serves as context and the hop2 ﬁeld serves as a
label for this particular training example.
Discussion This simple case study illustrates the power of
composition in the DSP abstraction. Because the pipeline
is a well-deﬁned program in which transformations com-
municate by passing text attached to Examples, a simple
map-and-ﬁlter strategy can leverage the LM and RM to
bootstrap annotations for a full pipeline from end-task la-
bels. This is an extensible strategy, but even in its simplest
form it generalizes the approaches explored recently by Ze-
likman et al. (2022), Wei et al. (2022), Zhang et al. (2022),
and Huang et al. (2022) in which an LM self-generates
chain-of-thought rationales for an individual prompt.DEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models
By bootstrapping pipelines, DEMONSTRATE makes it easy
to explore complex strategies in SEARCH and PREDICT
without writing examples for every transformation. This
includes strategies that are challenging to explore without
custom annotations in traditional retrieval-augmented NLP.
For instance, Khattab et al. (2021a) introduces a pipeline
for multi-hop reasoning that is trained with weak supervi-
sion, extending work by Lee et al. (2019) and Khattab et al.
(2021b). In it, the target 3 or 4 passages that need to re-
trieved must be labeled but the system discovers the best
order of “hops” automatically.
In contrast, DSP allows us to build complex pipelines with-
out labels for intermediate steps, because we can compose
programs out of small transformations. If LM and RM can
accurately process such transformations “zero-shot” (i.e.,
without demonstrations) on at least one or two examples,
these examples can be discovered with end-task labels and
used as demonstrations.
To draw on our earlier analogy with DNN frameworks like
PyTorch, DEMONSTRATE aims to replace the function of
backpropagation in extensible ways by simulating the be-
havior of the program (corresponding to a “forward” pass)
and programmatically learning from errors. In doing this
with frozen models and with only end-task labels, DEMON -
STRATE introduces a high degree of modularity. In partic-
ular, without hand-labeling intermediate transformations,
developers may swap the training domain, update the train-
ing examples, or modify the program’s strategy, and use
annotate to automatically populate all of the intermediate
ﬁelds for demonstrations.
Selecting Demonstrations It is not always possible to ﬁt
all of the training examples in the context window of the
LM. DSP provides three primitives for selecting a subset
of training examples, namely, sample, knn, and crossval.
1 sample ( train : Examples , k: int )
2 -> Examples
3
4 knn ( train : Examples , cast : Callable [[ Example ], str ])
5 -> fn( example : Example , k: int ) # currying
6 -> Examples
7
8 crossval ( train : Examples , n: int , k: int )
9 -> fn( evaluate : Transformation )
10 -> Examples
As a baseline choice, k demonstrations can be randomly
sampled from train using the sample primitive, an ap-
proach used by Brown et al. (2020) and much subsequent
work. We can also leverage theRM’s representations and se-
lect from the training set thek nearest neighbors to the input
text, a strategy explored by Liu et al. (2021). Another strat-
egy is to apply cross-validation to select among a number of
sampled sets of demonstrations (Perez et al., 2021). For ex-
ample, given |train| = 100training examples, crossval
would select n subsets of k = 5examples each, and return
the set with which a transformationevaluate performs best
on the remaining 95 examples.
Compositions & Extensions By manipulating demon-
strations and higher-order transformations, these simple
selection and bootstrapping primitives can be combined to
conduct larger novel strategies. If the training set is very
large (e.g., |train| = 100, 000), we can conduct knn to
ﬁnd the nearest k = 16examples and only annotate these,
arriving at a system that learns incrementally in real-time. If
the training set is moderately large (e.g., |train| = 1000),
we can conduct crossval and cache the performance of all
prompts it evaluates on each training example. At test time,
we can use knn to ﬁnd k = 50similar examples to the test
input and select the prompt that performs best on these k
examples, producing an adaptive system that is informed by
the quality of its pipeline on different types of examples.
2.4. SEARCH
The SEARCH stage gathers passages to support transforma-
tions conducted by the LM. We assume a large knowledge
corpus—e.g., a snippet of Web, Wikipedia, or arXiv—that
is divided into text passages. Providing passages to the LM
facilitates factual responses, enables updating the knowl-
edge store without retraining, and presents a transparency
contract: when in doubt, users can check whether the system
has faithfully used a reliable source in making a prediction.
In the simplest scenarios, SEARCH can directly query the
RM, requesting the top-k passages (from a pre-deﬁned in-
dex) that match an input question. This baseline instantia-
tion of SEARCH simulates retrieval in most open-domain
question answering systems, which implement a “retrieve-
then-read” pipeline, like Lee et al. (2019), Khattab et al.
(2021b), Lazaridou et al. (2022), and many others.
1 from dsp import retrieve
2
3 def simple_search (x):
4 passages = retrieve ( query =x. question , k =2)
5 return passages
SEARCH Strategies In many scenarios, the complexity
of the task demands more sophisticated SEARCH strategies
that empower the RM to ﬁnd relevant passages. Our run-
ning example (Figure 2) is one such scenario, in which we
suspect examples are likely to require multi-hop reasoning
in particular. Other settings, for instance, pose conversa-
tional challenges, in which the information need expressed
by a user can only be resolved by taking into account pre-
vious turns in the conversation, or demand more extensive
planning (Zhong et al., 2022).
In the retrieval-augmented NLP literature, multi-hop
search (Xiong et al., 2020; Khattab et al., 2021a) and con-DEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models
versational search (Del Tredici et al., 2021; Raposo et al.,
2022) pipelines have received much attention. These sys-
tems are typically ﬁne-tuned with many hand-labeled query
“rewrites” (Anantha et al., 2020), “decompositions” (Geva
et al., 2021; Min et al., 2019), or target hops (Yang et al.,
2018; Jiang et al., 2020). Supported with automatic anno-
tations from DEMONSTRATE , the SEARCH stage allows us
to simulate many such strategies and many others in terms
of passing queries, passages, and demonstrations between
the RM and LM. More importantly, SEARCH facilitates our
vision of advanced strategies in which the LM and RM co-
operate to incrementally plan a research path for which the
RM gathers information and the LM identiﬁes next steps.
Case Study Let us build on our running multi-hop exam-
ple as a case study. We can deﬁne multihop_search_v2
(Line 4 in our core program), a slightly more advanced ver-
sion of the SEARCH transformation from Figure 2. This
transformation simulates the iterative retrieval component
of ﬁne-tuned retrieval-augmented systems like IRRR (Qi
et al., 2020), which reads a retrieved passage in every hop
and generates a search query (or a termination condition to
stop hopping), and Baleen (Khattab et al., 2021a), which
summarizes the information from many passages in each
hop for inclusion in subsequent hops.
1 from dsp import generate
2
3 def multihop_search_v2 (x, max_hops =3) :
4 x. hops = []
5
6 for hop in range ( max_hops ):
7 summary , query = generate ( hop_template )(x)
8 x. hops . append (( summary , query ))
9
10 if query == /quotesingle.VarN/A /quotesingle.Var: break
11
12 passages = retrieve ( query , k =5)
13 x. context = [ summary ] + passages
14
15 return x
In multihop_search_v2, Line 7 calls the generate prim-
itive, which invokes the LM to produce a query for each
retrieval hop. The LM is conditioned on a prompt that is
prepared using the hop_template template. (We discuss
prompt templates and thegenerate primitive in §2.5.) Here,
this template may be designed to generate a prompt that has
the following format (e.g., for the second hop).
1 My task is to write a simple query that gathers
information for answering a complex question . I
write N/A if the context contains all
information required .
2
3 { Task demonstrations from x. demos , if any }
4
5 Context : {x. context }
6 Question : {x. question }
7 Summary : Let /quotesingle.Vars summarize the above context .
__{ summary }__
8 Search Query : __{ query }__
As shown, the LM is instructed to read the context re-
trieved in earlier hops and a complex question. It is then
prompted to write: (i) a summary of the supplied con-
text and (ii) a search query that gathers information for
answering that question. The generated text will be ex-
tracted and assigned to the summary and query variables in
(multihop_search_v2; Line 7). On Line 10, we terminate
the hops if the query is “N/A”. Otherwise, Line 12 retrieves
k = 5 passages using the query and Line 13 assigns the
context for the subsequent hop (or for PREDICT ), setting
that to include the summary of all previous hops as well as
the passages retrieved in the ﬁnal hop so far.
Comparison with self-ask It may be instructive to con-
trast this multi-hop DSP program with the recent “self-
ask” (Press et al., 2022) prompting technique, which we
compare against in §3. Self-ask can be thought of as a sim-
ple instantiation of DSP’sSEARCH stage. In it, the LM asks
one or more “follow-up questions”, which are intercepted
and sent to a search engine. The search engine’s answers
are concatenated into the prompt and are used to answer
the question. This is essentially a simpliﬁed simulation of
IRRR (Qi et al., 2020).
As a general framework,DSP can express ideas like self-ask
and many other, more sophisticated pipelines as we discuss
in the present section. More importantly, DSP offers a num-
ber of intrinsic advantages that lead to large empirical gains:
80%–290% over self-ask. For instance, DSP programs are
deeply modular, which among other things means that DSP
programs will annotate and construct their own demonstra-
tions. Thus, they can be developed without labeling any
of the intermediate transformations (e.g., the queries gener-
ated). In addition, the LM prompts constructed by DSP get
automatically updated to align with the training data and re-
trieval corpus provided. In contrast, approaches like self-ask
rely on a hand-written prompt with hard-coded examples.
Moreover, DSP assigns the control ﬂow to an explicit pro-
gram and facilitates design patterns that invoke the LM (or
RM) to conduct small transformations. This allows us to
build steps that are dedicated to generating one or more re-
trieval queries, summarizing multiple passages per hop, and
answering questions. These steps are individually simpler
than the self-ask prompt, yet our multi-hop DSP program
deliberately composes them to build richer pipelines that are
thus more reliable. In contrast, self-ask delegates the con-
trol ﬂow to the LM completions, maintaining state within
the prompt itself and intercepting follow-up questions to
conduct search. We ﬁnd that this paradigm leads to a “self-
distraction” problem (§3.5) that DSP programs avoid.
Fusing Retrieval Results For improved recall and robust-
ness, we can also fuse the retrieval across multiple gen-
erated queries. Fusion has a long history in informationDEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models
retrieval (Fox & Shaw, 1994; Xue & Croft, 2013; Kur-
land & Culpepper, 2018) and sequentially processing multi-
ple queries was explored recently by Gao et al. (2022) for
retroactively attributing text generated by LMs to citations.
Inspired by these, we include afused_retrieval primitive
to DSP to offer a versatile mechanism for interacting with
frozen retrievers. It accepts an optional fusion function that
maps multiple retrieval lists into one. By default, DSP uses
a variant of CombSUM (Fox & Shaw, 1994), assigning each
passage the sum of its probabilities across retrieval lists.
To illustrate, the modiﬁcation below generates n = 10
queries for the transformation multihop_search_v2.
c = generate ( hop_template , n =10) (x)
passages = fused_retrieval (c. queries , k =5)
summary = c. summaries [0] # highest - scoring summary
Compositions & Extensions To illustrate a simple com-
position, we can equip a chatbot with the capacity for con-
versational multi-hop search by combining a query rewriting
step, which produces a query that encompasses all of the
relevant conversational context, with the multi-hop transfor-
mation, as follows.
1 def conversational_multihop_search (x):
2 x. question = generate ( conv_rewriting_template )(x)
3 return multihop_search_v2 (x)
Similar approaches can be used for correcting spelling mis-
takes or implementing pseudo-relevance feedback (Cao
et al., 2008; Wang et al., 2022a), in which retrieved passages
are used to inform a better search query, though this has not
been attempted with pretrained LMs to our knowledge.
2.5. PREDICT
The PREDICT stage generates the system output using
demonstrations (e.g., in x.demos) and passages (e.g., in
x.context). PREDICT tackles the challenges of reliably
solving the downstream task, which integrates much of the
work on in-context learning in general. Within DSP, it also
has the more specialized function of systematically aggre-
gating information across a large number of demonstrations,
passages, and candidate predictions.
Generating Candidates Generally, PREDICT has to pro-
duce one or more candidate predictions for the end-task.
To this end, the basic primitive in PREDICT is generate,
which accepts a Template and (via currying) an Example
and queries the LM to produce one or more completions,
as explored earlier in §2.4. A corresponding primitive that
uses the RM in this stage is rank, which accepts a query
and one or more passages and returns their relevance scores.
1 Template # template : an object that can produce
prompts and parse completions
2
3 generate ( template : Template )
4 -> fn( example : Example )
5 -> Completions # object with keys to access
extracted preds and scores
6
7 rank ( query : str , passages : List [ str ])
8 -> List [ float ] # object with keys to access
passage texts and scores
A Template is an object that can produce prompts, that is,
map an Example to a string, and extract ﬁelds out of com-
pletions. For instance, we can map an example x that has a
question and retrieved passages to the following prompt:
1 My task is to answer questions using Web documents .
2
3 { Task demonstrations from x. demos , if any }
4
5 Context : {x. passage }
6 Question : {x. question }
7 Rationale : Let /quotesingle.Vars think step by step . __{ rationale }__
8 Answer : __{ answer }__
As this illustrates, the LM will be asked to generate a chain-
of-thought rationale (CoT; Wei et al. 2022; Kojima et al.
2022) and an answer, and the generated text will be ex-
tracted back into the rationale and answer keys of each
completion.
Each invocation to the LM can sample multiple candidate
predictions. Selecting a “best” prediction is the subject of
much work on decoding (Wiher et al., 2022; Li et al., 2022),
but a frozen and general-purpose LM may not support cus-
tom modiﬁcations to decoding. Within these constraints, we
present several high-level strategies for selecting predictions
and aggregating information in DSP via the LM and RM.
Selecting Predictions Among multiple candidates, we
can simply extract the most popular prediction. When a CoT
is used to arrive at the answer, this is the self-consistency
method of Wang et al. (2022c), which seeks to identify
predictions at which multiple distinct rationales arrive.
1 from dsp import generate , majority
2
3 def multihop_predict (x):
4 candidates = generate ( template_qa )(x)
5 return x. copy ( answer = majority ( candidates ). answer )
DSP generalizes this in two ways. First, we can sample
multiple “pipelines of transformations” (PoT) within the pro-
gram, rather than locally with “chains of thought” (CoT) in
one transformation. These chains may even invoke different
paths in the program, as illustrated below.DEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models
1 from dsp import branch
2
3 def pipeline (x):
4 return multihop_predict ( multihop_search_v2 (x))
5
6 def PoT_program ( question : str ) -> str :
7 x = Example ( question = question , train = train )
8 x = multihop_demonstrate (x)
9
10 candidates = branch ( pipeline , n=5 , t =0.7) (x)
11 return x. copy ( answer = majority ( candidates ). answer )
In the snippet above, Line 10 invokes the primitivebranch
which samples n different PoTs with a high temperature
(e.g., t = 0 .7) and accumulates their intermediate and
ﬁnal predictions. In this example, our pipeline invokes
multihop_search_v2 (§2.4), which applies a variable num-
ber of retrieval hops depending on the questions generated,
before doing PREDICT . That is, PoT_program potentially
invokes multiple distinct paths in the program (i.e., with dif-
ferent multi-hop queries and number of hops in each) across
branches. It then selects the majority answer overall.
DSP generalizes self-consistency in a second way. When
sampling our CoTs or PoTs provides multiple candidates,
we can select the top- k (e.g., top-4) predictions and then
compare them directly. For instance, we may prompt the
LM to compare these choices as MCQ candidates, a trans-
formation for which DEMONSTRATE can automatically pre-
pare exemplars. This effectively simulates the LM recursion
of Levine et al. (2022), though unlike their approach it does
not require a large training set or updating any (prompt-
tuning) weights. One such implementation is illustrated in
openqa_predict below.
1 def openqa_predict (x):
2 preds = generate ( template_qa , n =20) (x). answers
3 x. choices = most_common ( preds , k =4)
4
5 queries = [f"{x. question } {c}"
6 for c in x. choices ]
7
8 x. passages = fused_retrieval ( queries )
9 x. answer = generate ( TemplateMCQ )(x). answer
10 return x
As an alternative comparison approach, we can invoke the
RM via rank to ﬁnd the prediction that is most grounded in
a retrieved contexts (i.e., most similar to the concatenation
of the retrieved passages) or, given an RM that can score
completions (Krishna et al., 2022), simply the prediction
that has the highest score given the prompt.
Aggregating Information When only a few demonstra-
tions or passages are selected, we can simply concate-
nate them all into the prompt. For instance, GPT-3.5
text-davinci-002 has a context window of 4097 tokens,
which we ﬁnd to be reasonably large for accommodating
several (e.g., 3–5) demonstrations, which individually in-
clude their own passages and rationales.
To deal with a larger number of demonstrations or passages,
we can branch in parallel to process individual subsets
of the passages or demonstrations and then aggregate the
individual answers using one of the scoring methods pre-
sented earlier. Indeed, Lewis et al. (2020) and Lazaridou
et al. (2022) have explored marginalization as a way to com-
bine scores across passages and Le et al. (2022) ensemble
prompts across demonstrations, which can be expressed in
this way.
An alternative aggregation strategy is to accumulate informa-
tion across passages sequentially, rather than independently.
This is effectively how our multi-hop approach works (§2.4).
Such a strategy has also been employed recently by Gao
et al. (2022) for retroactively attributing text generated by
LMs to citations. They generate many queries but instead
of fusion (§2.4), they run their pipeline on each query and
use its outputs to alter the input to subsequent queries.1
3. Evaluation
We now consider how to implement DSP programs for three
diverse knowledge-intensive NLP tasks: open-domain ques-
tion answering (QA), multi-hop QA, and conversational QA.
All of these tasks are “open-domain”, in the sense that sys-
tems are given a short question or participate in a multi-turn
conversation without being granted access to context that
answers these questions.
We build and evaluate intuitive compositions of the func-
tions explored in §2 for each task. We show that, despite
low development effort, the resulting DSP programs exhibit
strong quality and deliver considerable empirical gains over
vanilla in-context learning and a standard retrieve-then-read
pipeline with in-context learning.
3.1. Evaluation Methodology
In this report, we consider one development datasetfor each
of the tasks we consider, namely, the open-domain version
of SQuAD (Rajpurkar et al., 2016; Lee et al., 2019), the
multi-hop HotPotQA (Yang et al., 2018) dataset in the open-
domain “fullwiki” setting, and the conversational question
answering QReCC (Anantha et al., 2020; Vakulenko et al.,
2022) dataset, which we used for developing the DSP ab-
stractions. We report the validation set accuracy on all three
datasets and discuss them in detail §3.5.
Unless otherwise stated, systems are given access to 16-
shot training examples, that is, each DSP program can use
(up to) 16 questions—or conversations, where applicable—
randomly sampled from the respective training set. We
1Though most of the functionality in this section is imple-
mented, the primitives branch, knn, and crossval are currently
work-in-progress.DEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models
subsample the validation and test sets to 1000 questions
(or 400 conversations, where applicable) and report average
quality across ﬁve seeds where each seed ﬁxes a single k-
shot training set of examples. To control the language model
API spending budget, each seed processes one ﬁfth of the
evaluation examples (e.g., 200 questions per seed, for a total
of 1000 unique questions).
We also dedicate held-out test datasets (e.g., Open-
NaturalQuestions; Kwiatkowski et al. 2019) and test tasks
(e.g., claim veriﬁcation, as in FEVER; Thorne et al. 2018)
that we only use for evaluating pre-deﬁned DSP programs
rather than development. We will include these results in a
future version of this report.
3.2. Pretrained Modules
RM We use ColBERTv2 (Santhanam et al., 2022b), a
state-of-the-art retriever based on late interaction (Khattab
& Zaharia, 2020). We choose ColBERTv2 for its highly
effective zero-shot search quality and efﬁcient search (San-
thanam et al., 2022a). However, our DSP programs are
agnostic to how the retriever represents examples or scores
passages, so essentially any retriever can be used.
In addition, by making retrieval a ﬁrst-class construct, DSP
allows us to change or update the search index over time.
We simulate this in our experiments by aligning each of our
datasets with the nearest Wikipedia corpus among the Dec
2016 Wikipedia dump from Chen et al. 2017, the Nov 2017
Wikipedia “abstracts” dump from Yang et al. 2018, and the
Dec 2018 Wikipedia dump from Karpukhin et al. 2020.
LM We use the GPT-3.5 ( text-davinci-002; Brown
et al. 2020; Ouyang et al. 2022) language model. Unless
otherwise stated, we use greedy decoding when generating
n = 1 prediction. We sample with temperature t = 0.7
when n >1, like related work (Wang et al., 2022c).
3.3. Baselines
Vanilla LM The vanilla LM baselines represent the few-
shot in-context learning paradigm used by Brown et al.
(2020). The open-domain QA and multi-hop QA base-
lines randomly sample 16 demonstrations (i.e., all of the
examples available to each program in our evaluation) from
the training set and do not augment these demonstrations
with evidence. Similarly, the conversational QA baseline
samples four conversations. The vanilla baselines do not
search for passages relevant to the input query.
1 def vanilla_LM_QA ( question : str ) -> str :
2 demos = sample ( train , k =16)
3 x = Example ( question = question , demos = demos )
4 return generate ( qa_template )(x). pred
Retrieve-then-Read The “retrieve-then-read” baselines
use the RM to support each example with a potentially rele-
vant passage before submitting the prompt to the LM. This
emulates the pipelines used by state-of-the-art open-domain
question answering systems (Khattab et al., 2021b; Izacard
& Grave, 2020; Hofstätter et al., 2022). In conversational
QA, we concatenate the ﬁrst turn and the ﬁnal question, an
approach that we found to perform much better than simply
using the ﬁnal turn. For multi-hop QA, we retrieve and
concatenate two passages per question.
1 def retrieve_then_read_QA ( question : str ) -> str :
2 demos = sample ( train , k =16)
3 passages = retrieve ( question , k =1)
4 x = Example ( question = question ,
5 passages = passages ,
6 demos = demos )
7 return generate ( qa_template )(x). pred
Self-ask We also compare against self-ask (Press et al.,
2022), a contemporaneous pipeline that can be thought of
as a speciﬁc instantiation of DSP’sSEARCH stage followed
by a simple PREDICT step. For direct comparison with
our methods, we modify the self-ask control ﬂow to query
the same ColBERTv2 index used in our DSP experiments
instead of Google Search. We evaluate two conﬁgurations of
self-ask. The ﬁrst uses the original self-ask prompt template,
which contains four hand-written demonstrations. In the
second conﬁguration, we modify the prompt template to
apply a number of changes that we ﬁnd are empirically
useful for HotPotQA.2
3.4. Proposed DSP Programs
We build on transformations presented in §2. Our programs
for all three tasks have the following structure, illustrated
for open-domain QA.
1 def openqa_program ( question : str ) -> str :
2 x = Example ( question = question , train = train )
3 x = openqa_demonstrate (x)
4 x = openqa_search (x)
5 x = openqa_predict (x)
6 return x. answer
The exception is that the conversational QA program,
2In particular: (i) use ColBERTv2-style passages in the hand-
crafted demonstrations of self-ask (i.e., instead of the original
Google-style snippets), (ii) concatenate 16-shot training examples
from the task (i.e., question–answer pairs) as a preﬁx of the prompt,
(iii) ask the model to generate a short intermediate answer per
retrieval step, and(iv) explicitly ask the model to generate a follow-
up “search query” at each step. We found the ﬁnal item to be
important because self-ask’s default prompt often produces follow-
up questions that are not self-contained (e.g., “what is the name of
the national park?”, which is not an informative search query). We
also ﬁx the casing in the prompt to be consistent.DEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models
Table 1.Development results comparing a task-aware DSP program against baseline vanilla LM and retrieve-then-read LM as well as
recent and contemporaneous in-context learning approaches with and without retrieval. All of our runs use GPT-3.5 and our retrieval-based
rows use ColBERTv2. The results marked with ¶ are collected from related work as of mid-December 2022, and attributed to their
individual sources in the main text. As we discuss in the main text, the marked results are not generally apples-to-apples comparisons,
since they span a variety of evaluation settings. Nonetheless, we report them here as qualitative reference points.
Open-SQuAD HotPotQA QReCC
EM F1 EM F1 F1 nF1
Vanilla LM 16.2 25.6 28.3 36.4 29.8 18.4
No-retrieval LM SoTA 20.2¶ – 33.8 ¶ 44.6¶ – –
Retrieve-then-Read 33.8 46.1 36.9 46.1 31.6 22.2
Self-ask w/ ColBERTv2 Search 9.3 17.2 25.2 33.2 – –
+ Reﬁned Prompt 9.0 15.7 28.6 37.3 – –
Retrieval-augmented LM SoTA 34.0¶ – 35.1 ¶ – – –
Task-aware DSP Program 36.6 49.0 51.4 62.9 35.0 25.3
convqa_program, accepts turns (i.e., a list of strings, rep-
resenting the conversational history) instead of a single
question. Unless otherwise stated, our programs default to
greedy decoding during the DEMONSTRATE stage.
For SEARCH , our open-domain QA program uses the ques-
tion directly for retrieving k = 7 passages and concate-
nates these passages into our QA prompt with CoT. For
PREDICT , it generates n = 20 reasoning chains and uses
self-consistency (SC; Wang et al. 2022c) to select its ﬁnal
prediction. For DEMONSTRATE , our open-domain QA pro-
gram uses the following approach, slightly simpliﬁed for
presentation. In it, the parameter k = 3passed to annotate
requests annotating only three demonstrations, which will
then be used in the prompts.
1 def openqa_demonstrate (x: Example ) -> Example :
2 demos = sample (x. train , k =16)
3
4 def openqa_attempt (d: Example ) -> Example :
5 d. demos = all_but ( demos , d) # all ( raw )
examples different from d
6
7 d = openqa_search (d, k =2)
8 if not passage_match (d): return None # skip
examples where search fails
9
10 d = openqa_predict (d, sc= False )
11 if not answer_match (d): return None # skip
examples where predict fails
12
13 return d
14
15 x. demos = annotate ( demos , openqa_attempt , k =3)
16 return x
Our multi-hop program adopts a very similar approach for
DEMONSTRATE and PREDICT . For SEARCH , it uses the
approach described in §2.4, with the following adjustments.
It uses result fusion across n = 10 queries per hop and,
among the n predictions, uses the summary corresponding
to the largest average log-probability. It uses a ﬁxed number
of hops for HotPotQA, i.e., two hops. In each prompt (i.e.,
each hop and QA), it concatenates the summaries of all
previous hops (i.e., hop 1 onwards) and a total of k = 5
passages divided between the hops (i.e., ﬁve passages from
the ﬁrst hop or two passages from the ﬁrst and three from
the second).
For conversational QA, we use a simple PREDICT which
generates a response with greedy decoding, conditioned
on all of the previous turns of the conversation and ﬁve
retrieved passages. For SEARCH , our conversational QA
pipeline generates n = 10re-written queries (and also uses
the simple query as the retrieve-and-read baseline; §3.3) and
fuses them as in §2.4. We implement DEMONSTRATE simi-
lar to openqa_demonstrate, but sample only four examples
(i.e., four conversational turns; instead of 16 questions as in
open-domain QA) for demonstrating the task for the higher-
order transformation convqa_attempt, which is passed to
annotate (not shown for brevity).
1 def convqa_attempt (d: Example ) -> Example :
2 d. demos = all_but ( demos , d) # all ( raw )
examples that don /quotesingle.Vart intersect with the
conversation of d
3
4 d = convqa_search (d, k =2)
5 if max ( precision (d. answer , p) for p in
d. passages ) < .8: return None # skip examples
where search fails
6
7 d = convqa_predict (d, n =20)
8 if max (F1(c.pred , d. answer ) for c in
d. candidates ) < .75: return None # skip
examples where predict fails out of n =20
attempts
9
10 return d
3.5. Development Datasets & Results
Open-SQuAD We conduct the open-domain version of
SQuAD over the Wikipedia 2016 corpus from Chen et al.
(2017), as processed by Khattab et al. (2021b). We use theDEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models
same train/validation/test splits as in Karpukhin et al. (2020)
and Khattab et al. (2021b).
Table 1 reports the answer EM and F1. The task-awareDSP
program achieves 36.6% EM, outperforming the vanilla LM
baseline by 126% EM relative gains. This indicates the im-
portance of grounding the LM’s predictions in retrieval, and
it shows that state-of-the-art retrievers like ColBERTv2 have
the capacity to do so off-the-shelf. The proposed DSP pro-
gram also achieves relative gains of 8% in EM and 6% in F1
over the retrieve-then-read pipeline, highlighting that non-
trivial gains are possible by aggregating information across
several retrieved passages as we do with self-consistency.
These in-context learning results are competitive with a
number of popular ﬁne-tuned systems. For instance, on
the Open-SQuAD test set, DPR achieves 29.8% EM, well
below our 16-shot DSP program. On the Open-SQuAD
dev set, the powerful Fusion-in-Decoder (Izacard & Grave,
2020) “base” approach achieves approximately 36% (i.e.,
very similar quality to our system) when invoked with ﬁve
retrieved passages. Nonetheless, with the default setting
of reading 100 passages, their system reaches 48% EM in
this evaluation. This may indicate that similar gains are
possible for our DSP program if the PREDICT stage is made
to aggregate information across many more passages.
For comparison, we also evaluate the self-ask pipeline,
which achieves 9.3% EM, suggesting that its ﬁxed pipeline
is ineffective outside its default multi-hop setting. Study-
ing a few examples of its errors reveals that it often de-
composes questions in tangential ways and answers these
questions instead. We refer to this behavior of the LMas
“self-distraction”, and we believe it adds evidence in favor of
our design decisions in DSP . To illustrate self-distraction,
when self-ask is prompted with “When does The Kidnap-
ping of Edgardo Mortara take place?”, it asks “What is The
Kidnapping of Edgardo Mortara“ and then asks when it was
published, a tangential question. Thus, self-ask answers
“1997”, instead of the time The Kidnapping of Edgardo
Mortara takes place (1858).
For reference, Table 1 also reports (as No-retrieval LM
SoTA) the concurrent in-context learning results from Si
et al. (2022) using code-davinci-002, who achieve 20.2%
EM without retrieval and 34.0% EM with retrieval, albeit
on a different sample and split of the SQuAD data. Overall,
their approaches are very similar to the baselines we im-
plement (vanilla LM and retrieve-then-read), though their
retrieval-augmented approach retrieves (and concatenates
into the prompt) 10 passages from a Wikipedia dump.
HotPotQA We use the open-domain “fullwiki” setting
of HotPotQA using its ofﬁcial Wikipedia 2017 “abstracts”
corpus. The HotPotQA test set is hidden, so we reserve
the ofﬁcial validation set for our testing. We sub-divide
the training set into 90%/10% train/validation splits. In the
training (and thus validation) split, we keep only examples
marked as “hard” in the original dataset, which matches the
designation of the ofﬁcial validation and test sets.
We report the ﬁnal answer EM and F1 in Table 1. On
HotPotQA, the task-aware DSP program outperforms the
baselines and existing work by very wide margins, exceed-
ing the vanilla LM, the retrieve-then-read baseline, and the
self-ask pipeline by 82%, 39%, and 80%, respectively, in
EM. This highlights the effectiveness of building up more
sophisticated programs that coordinate the LM and RM for
the SEARCH step.
These results may be pegged against the evaluation on Hot-
PotQA in a number of concurrent papers. We ﬁrst compare
with non-retrieval approaches, though our comparisons must
be tentative due to variation in evaluation methodologies. Si
et al. (2022) achieve 25.2% EM with CoT prompting. With
a “recite-and-answer” technique for PaLM-62B (Chowdh-
ery et al., 2022), Sun et al. (2022) achieve 26.5% EM. Wang
et al. (2022b) achieve 33.8% EM and 44.6 F1 when apply-
ing a self-consistency prompt for PaLM-540B. Next, we
compare with a contemporaneous retrieval-based approach:
Yao et al. (2022) achieve 35.1% EM using a system capable
of searching using a Wikipedia API. All of these approaches
trail our task-aware DSP program, which achieves 51.4%
EM, by large margins.
QReCC We use QReCC (Anantha et al., 2020) in an open-
domain setting over Wikipedia 2018. QReCC does not have
an ofﬁcial development set, so we sub-divide the training
set into 90%/10% train/validation splits. For the ﬁrst ques-
tion in every conversation, we use the rewritten question
as the original question often assumes access to a ground-
truth document. We also ﬁlter low-quality examples from
QReCC.3
We conduct the QReCC conversations in an auto-regressive
manner. At turn t > 1 of a particular conversation, the
system sees its own responses (i.e., not the ground truth
responses) to previous turns of the conversation. We report
the novel-F1 metric (nF1; Paranjape et al. 2022), which
computes the F1 overlap between the system response and
the ground truth while discounting common stopwords and
terms present in the question (or earlier questions). The
results are shown in Table 1, and follow the same general
pattern as SQuAD and HotPotQA.
3We remove conversations that have one or more empty ground-
truth answers and conversations that have only one or two ques-
tions. We also ﬁnd many conversations that include “what other
interesting facts are in this article?”, which conﬂict with the open-
domain formulation and have no well-deﬁned answer. Hence, we
remove any conversation that includes the keywords “other inter-
esting” or “else”, which we found to be markers of low quality.DEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models
4. Conclusion
For a long time, the dominant paradigm for building models
in AI has centered around multiplication of tensor repre-
sentations, and in the deep learning era this has given rise
to highly modular (layer-wise) designs that allow for fast
development and wide exploration. However, these design
paradigms require extensive domain expertise, and even
experts face substantial challenges when it comes to com-
bining different pretrained components into larger systems.
The promise of in-context learning is that we can build com-
plex systems from pretrained components using only natural
language as the medium for giving systems instructions and,
as we argue for, allowing components to communicate with
each other. In this new paradigm, the building blocks are
pretrained models and the core operations are natural lan-
guage instructions and operations on natural language texts.
If we can realize this potential, then we can broaden partici-
pation in AI system development, rapidly prototype systems
for new domains, and maximize the value of specialized
pretrained components.
In the current paper, we introduced the DEMONSTRATE –
SEARCH –PREDICT (DSP ) framework for retrieval aug-
mented in-context learning. DSP consists of a number of
simple, composable functions for implementing in-context
learning systems as deliberate programs—instead of end-
task prompts—for solving knowledge intensive tasks. We
implemented DSP as a Python library and used it to write
programs for Open-SQuAD, HotPotQA, and QReCC. These
programs deliver substantial gains over previous in-context
learning approaches. However, beyond any particular per-
formance number, we argue that the central contribution of
DSP is in helping to reveal a very large space of conceptual
possibilities for in-context learning in general.
Acknowledgements
We thank Ashwin Paranjape, Amir Ziai, and Rick Battle for
valuable discussions and feedback. This work was partially
supported by IBM as a founding member of the Stanford
Institute for Human-Centered Artiﬁcial Intelligence (HAI).
This research was supported in part by afﬁliate members and
other supporters of the Stanford DAWN project—Ant Fi-
nancial, Facebook, Google, and VMware—as well as Cisco,
SAP, and the NSF under CAREER grant CNS-1651570.
Any opinions, ﬁndings, and conclusions or recommenda-
tions expressed in this material are those of the authors and
do not necessarily reﬂect the views of the National Science
Foundation. We thank Giuseppe Attanasio for his public
LATEX GitHub-style Python code formatting gist.4 We also
thank Riley Goodside for his public tips on formatting LM
4https://gist.github.com/g8a9/
07c2be12ae02cfad4aa430d77dc940cb
prompts (at @goodside on Twitter).
References
Anantha, R., Vakulenko, S., Tu, Z., Longpre, S., Pulman,
S., and Chappidi, S. Open-domain question answering
goes conversational via question rewriting.arXiv preprint
arXiv:2010.04898, 2020.
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,
Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
Askell, A., et al. Language models are few-shot learners.
Advances in neural information processing systems, 33:
1877–1901, 2020.
Cao, G., Nie, J.-Y ., Gao, J., and Robertson, S. Selecting
good expansion terms for pseudo-relevance feedback. In
Proceedings of the 31st annual international ACM SIGIR
conference on Research and development in information
retrieval, pp. 243–250, 2008.
Chen, D., Fisch, A., Weston, J., and Bordes, A. Reading
Wikipedia to answer open-domain questions. In Proceed-
ings of the 55th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), pp.
1870–1879, Vancouver, Canada, 2017. Association for
Computational Linguistics. doi: 10.18653/v1/P17-1171.
URL https://aclanthology.org/P17-1171.
Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,
G., Roberts, A., Barham, P., Chung, H. W., Sutton, C.,
Gehrmann, S., et al. Palm: Scaling language modeling
with pathways. arXiv preprint arXiv:2204.02311, 2022.
Del Tredici, M., Barlacchi, G., Shen, X., Cheng, W., and
de Gispert, A. Question rewriting for open-domain con-
versational qa: Best practices and limitations. In Pro-
ceedings of the 30th ACM International Conference on
Information & Knowledge Management, pp. 2974–2978,
2021.
Dohan, D., Xu, W., Lewkowycz, A., Austin, J., Bieber, D.,
Lopes, R. G., Wu, Y ., Michalewski, H., Saurous, R. A.,
Sohl-Dickstein, J., et al. Language model cascades. arXiv
preprint arXiv:2207.10342, 2022.
Fox, E. A. and Shaw, J. A. Combination of multiple searches.
NIST special publication SP, 243, 1994.
Gao, L., Dai, Z., Pasupat, P., Chen, A., Chaganty, A. T., Fan,
Y ., Zhao, V . Y ., Lao, N., Lee, H., Juan, D.-C., et al. At-
tributed text generation via post-hoc research and revision.
arXiv preprint arXiv:2210.08726, 2022.
Geva, M., Khashabi, D., Segal, E., Khot, T., Roth, D., and
Berant, J. Did aristotle use a laptop? a question answeringDEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models
benchmark with implicit reasoning strategies. Transac-
tions of the Association for Computational Linguistics, 9:
346–361, 2021.
Hofstätter, S., Chen, J., Raman, K., and Zamani, H. Fid-
light: Efﬁcient and effective retrieval-augmented text
generation. arXiv preprint arXiv:2209.14290, 2022.
Huang, J., Gu, S. S., Hou, L., Wu, Y ., Wang, X., Yu, H., and
Han, J. Large language models can self-improve. arXiv
preprint arXiv:2210.11610, 2022.
Ishii, Y ., Madotto, A., and Fung, P. Survey of hallucination
in natural language generation. ACM Comput. Surv, 1(1),
2022.
Izacard, G. and Grave, E. Leveraging passage retrieval with
generative models for open domain question answering.
arXiv preprint arXiv:2007.01282, 2020.
Izacard, G., Lewis, P., Lomeli, M., Hosseini, L., Petroni,
F., Schick, T., Dwivedi-Yu, J., Joulin, A., Riedel, S., and
Grave, E. Few-shot learning with retrieval augmented lan-
guage models. arXiv preprint arXiv:2208.03299, 2022.
Jiang, Y ., Bordia, S., Zhong, Z., Dognin, C., Singh, M.,
and Bansal, M. HoVer: A dataset for many-hop fact
extraction and claim veriﬁcation. In Findings of the
Association for Computational Linguistics: EMNLP
2020, pp. 3441–3460, Online, 2020. Association for
Computational Linguistics. doi: 10.18653/v1/2020.
ﬁndings-emnlp.309. URL https://aclanthology.
org/2020.findings-emnlp.309.
Karpukhin, V ., Oguz, B., Min, S., Lewis, P., Wu, L., Edunov,
S., Chen, D., and Yih, W.-t. Dense passage retrieval
for open-domain question answering. In Proceedings
of the 2020 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pp. 6769–6781,
Online, 2020. Association for Computational Linguis-
tics. doi: 10.18653/v1/2020.emnlp-main.550. URL
https://aclanthology.org/2020.emnlp-main.550.
Khattab, O. and Zaharia, M. Colbert: Efﬁcient and effective
passage search via contextualized late interaction over
BERT. In Huang, J., Chang, Y ., Cheng, X., Kamps, J.,
Murdock, V ., Wen, J., and Liu, Y . (eds.),Proceedings
of the 43rd International ACM SIGIR conference on re-
search and development in Information Retrieval, SIGIR
2020, Virtual Event, China, July 25-30, 2020, pp. 39–
48. ACM, 2020. doi: 10.1145/3397271.3401075. URL
https://doi.org/10.1145/3397271.3401075.
Khattab, O., Potts, C., and Zaharia, M. Baleen: Robust
Multi-Hop Reasoning at Scale via Condensed Retrieval.
In Thirty-Fifth Conference on Neural Information Pro-
cessing Systems, 2021a.
Khattab, O., Potts, C., and Zaharia, M. Relevance-guided
supervision for openqa with ColBERT. Transactions of
the Association for Computational Linguistics, 9:929–
944, 2021b.
Khot, T., Trivedi, H., Finlayson, M., Fu, Y ., Richardson, K.,
Clark, P., and Sabharwal, A. Decomposed prompting:
A modular approach for solving complex tasks. arXiv
preprint arXiv:2210.02406, 2022.
Kojima, T., Gu, S. S., Reid, M., Matsuo, Y ., and Iwasawa,
Y . Large language models are zero-shot reasoners.arXiv
preprint arXiv:2205.11916, 2022.
Krishna, K., Chang, Y ., Wieting, J., and Iyyer, M. Rankgen:
Improving text generation with large ranking models.
arXiv preprint arXiv:2205.09726, 2022.
Kurland, O. and Culpepper, J. S. Fusion in information
retrieval: Sigir 2018 half-day tutorial. In The 41st Inter-
national ACM SIGIR Conference on Research & Devel-
opment in Information Retrieval, pp. 1383–1386, 2018.
Kwiatkowski, T., Palomaki, J., Redﬁeld, O., Collins, M.,
Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Devlin,
J., Lee, K., Toutanova, K., Jones, L., Kelcey, M., Chang,
M.-W., Dai, A. M., Uszkoreit, J., Le, Q., and Petrov, S.
Natural questions: A benchmark for question answering
research. Transactions of the Association for Computa-
tional Linguistics, 7:452–466, 2019. doi: 10.1162/tacl_a_
00276. URL https://aclanthology.org/Q19-1026.
Lazaridou, A., Gribovskaya, E., Stokowiec, W., and Grig-
orev, N. Internet-augmented language models through
few-shot prompting for open-domain question answering.
arXiv preprint arXiv:2203.05115, 2022.
Le, N. T., Bai, F., and Ritter, A. Few-shot anaphora reso-
lution in scientiﬁc protocols via mixtures of in-context
experts. arXiv preprint arXiv:2210.03690, 2022.
Lee, K., Chang, M.-W., and Toutanova, K. Latent re-
trieval for weakly supervised open domain question an-
swering. In Proceedings of the 57th Annual Meeting of
the Association for Computational Linguistics, pp. 6086–
6096, Florence, Italy, 2019. Association for Computa-
tional Linguistics. doi: 10.18653/v1/P19-1612. URL
https://aclanthology.org/P19-1612.
Levine, Y ., Dalmedigos, I., Ram, O., Zeldes, Y ., Jan-
nai, D., Muhlgay, D., Osin, Y ., Lieber, O., Lenz, B.,
Shalev-Shwartz, S., et al. Standing on the shoul-
ders of giant frozen language models. arXiv preprint
arXiv:2204.10019, 2022.
Lewis, P. S. H., Perez, E., Piktus, A., Petroni, F., Karpukhin,
V ., Goyal, N., Küttler, H., Lewis, M., Yih, W., Rock-
täschel, T., Riedel, S., and Kiela, D. Retrieval-AugmentedDEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models
Generation for Knowledge-Intensive NLP Tasks. In
Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.,
and Lin, H. (eds.), Advances in Neural Information
Processing Systems 33: Annual Conference on Neural
Information Processing Systems 2020, NeurIPS 2020,
December 6-12, 2020, virtual, 2020. URL https:
//proceedings.neurips.cc/paper/2020/hash/
6b493230205f780e1bc26945df7481e5-Abstract.
html.
Li, X. L., Holtzman, A., Fried, D., Liang, P., Eisner, J.,
Hashimoto, T., Zettlemoyer, L., and Lewis, M. Con-
trastive decoding: Open-ended text generation as opti-
mization. arXiv preprint arXiv:2210.15097, 2022.
Liu, J., Shen, D., Zhang, Y ., Dolan, B., Carin, L., and Chen,
W. What makes good in-context examples for gpt- 3?
arXiv preprint arXiv:2101.06804, 2021.
McCann, B., Keskar, N. S., Xiong, C., and Socher, R.
The natural language decathlon: Multitask learning as
question answering. arXiv:1806.08730, 2018. URL
https://arxiv.org/abs/1806.08730.
Min, S., Zhong, V ., Zettlemoyer, L., and Hajishirzi,
H. Multi-hop reading comprehension through ques-
tion decomposition and rescoring. arXiv preprint
arXiv:1906.02916, 2019.
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright,
C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama,
K., Ray, A., et al. Training language models to fol-
low instructions with human feedback. arXiv preprint
arXiv:2203.02155, 2022.
Paranjape, A., Khattab, O., Potts, C., Zaharia, M., and
Manning, C. D. Hindsight: Posterior-guided Training
of Retrievers for Improved Open-ended Generation. In
International Conference on Learning Representations,
2022. URL https://openreview.net/forum?id=Vr_
BTpw3wz.
Perez, E., Kiela, D., and Cho, K. True few-shot learning
with language models. Advances in Neural Information
Processing Systems, 34:11054–11070, 2021.
Press, O., Zhang, M., Min, S., Schmidt, L., Smith, N. A.,
and Lewis, M. Measuring and narrowing the com-
positionality gap in language models. arXiv preprint
arXiv:2210.03350, 2022.
Qi, P., Lee, H., Sido, O., Manning, C. D., et al. Retrieve,
rerank, read, then iterate: Answering open-domain ques-
tions of arbitrary complexity from text. arXiv preprint
arXiv:2010.12527, 2020. URL https://arxiv.org/
abs/2010.12527.
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D.,
Sutskever, I., et al. Language models are unsupervised
multitask learners. OpenAI blog, 1(8):9, 2019.
Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. SQuAD:
100,000+ questions for machine comprehension of text.
In Proceedings of the 2016 Conference on Empirical
Methods in Natural Language Processing, pp. 2383–
2392, Austin, Texas, 2016. Association for Computa-
tional Linguistics. doi: 10.18653/v1/D16-1264. URL
https://aclanthology.org/D16-1264.
Raposo, G., Ribeiro, R., Martins, B., and Coheur, L. Ques-
tion rewriting? assessing its importance for conversa-
tional question answering. In European Conference on
Information Retrieval, pp. 199–206. Springer, 2022.
Santhanam, K., Khattab, O., Potts, C., and Zaharia, M.
PLAID: An Efﬁcient Engine for Late Interaction Re-
trieval. arXiv preprint arXiv:2205.09707, 2022a.
Santhanam, K., Khattab, O., Saad-Falcon, J., Potts, C.,
and Zaharia, M. ColBERTv2: Effective and efﬁcient
retrieval via lightweight late interaction. In Proceedings
of the 2022 Conference of the North American Chapter of
the Association for Computational Linguistics: Human
Language Technologies, pp. 3715–3734, Seattle, United
States, July 2022b. Association for Computational Lin-
guistics. doi: 10.18653/v1/2022.naacl-main.272. URL
https://aclanthology.org/2022.naacl-main.272.
Shuster, K., Poff, S., Chen, M., Kiela, D., and Weston, J.
Retrieval augmentation reduces hallucination in conver-
sation. arXiv preprint arXiv:2104.07567, 2021.
Si, C., Gan, Z., Yang, Z., Wang, S., Wang, J., Boyd-Graber,
J., and Wang, L. Prompting gpt-3 to be reliable. arXiv
preprint arXiv:2210.09150, 2022.
Sun, Z., Wang, X., Tay, Y ., Yang, Y ., and Zhou, D.
Recitation-augmented language models. arXiv preprint
arXiv:2210.01296, 2022.
Thorne, J., Vlachos, A., Christodoulopoulos, C., and Mit-
tal, A. FEVER: a large-scale dataset for fact extrac-
tion and VERiﬁcation. In Proceedings of the 2018 Con-
ference of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Language
Technologies, Volume 1 (Long Papers), pp. 809–819,
New Orleans, Louisiana, 2018. Association for Compu-
tational Linguistics. doi: 10.18653/v1/N18-1074. URL
https://aclanthology.org/N18-1074.
Vakulenko, S., Kiesel, J., and Fröbe, M. SCAI-QReCC
shared task on conversational question answering. In Pro-
ceedings of the Thirteenth Language Resources and Eval-
uation Conference, pp. 4913–4922, Marseille, France,DEMONSTRATE –SEARCH –PREDICT : Composing retrieval and language models
June 2022. European Language Resources Association.
URL https://aclanthology.org/2022.lrec-1.525.
Wang, X., Macdonald, C., Tonellotto, N., and Ounis, I.
Colbert-prf: Semantic pseudo-relevance feedback for
dense passage and document retrieval. ACM Transac-
tions on the Web, 2022a.
Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., and
Zhou, D. Rationale-augmented ensembles in language
models. arXiv preprint arXiv:2207.00747, 2022b.
Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E.,
and Zhou, D. Self-consistency improves chain of
thought reasoning in language models. arXiv preprint
arXiv:2203.11171, 2022c.
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E.,
Le, Q., and Zhou, D. Chain of thought prompting elic-
its reasoning in large language models. arXiv preprint
arXiv:2201.11903, 2022.
Wiher, G., Meister, C., and Cotterell, R. On decoding
strategies for neural text generators. arXiv preprint
arXiv:2203.15721, 2022.
Xiong, W., Li, X. L., Iyer, S., Du, J., Lewis, P., Wang,
W. Y ., Mehdad, Y ., Yih, W.-t., Riedel, S., Kiela, D., et al.
Answering complex open-domain questions with multi-
hop dense retrieval. arXiv preprint arXiv:2009.12756,
2020. URL https://arxiv.org/abs/2009.12756.
Xue, X. and Croft, W. B. Modeling reformulation using
query distributions. ACM Transactions on Information
Systems (TOIS), 31(2):1–34, 2013.
Yang, Z., Qi, P., Zhang, S., Bengio, Y ., Cohen, W. W.,
Salakhutdinov, R., and Manning, C. D. Hotpotqa: A
dataset for diverse, explainable multi-hop question an-
swering. arXiv preprint arXiv:1809.09600, 2018.
Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan,
K., and Cao, Y . React: Synergizing reasoning and acting
in language models. arXiv preprint arXiv:2210.03629,
2022.
Zelikman, E., Wu, Y ., and Goodman, N. D. Star: Boot-
strapping reasoning with reasoning. arXiv preprint
arXiv:2203.14465, 2022.
Zhang, Z., Zhang, A., Li, M., and Smola, A. Automatic
chain of thought prompting in large language models.
arXiv preprint arXiv:2210.03493, 2022.
Zhong, V ., Shi, W., Yih, W.-t., and Zettlemoyer, L. Romqa:
A benchmark for robust, multi-evidence, multi-answer
question answering. arXiv preprint arXiv:2210.14353,
2022.

Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 881–893
November 12-16, 2024 ©2024 Association for Computational Linguistics
Retrieval Augmented Generation or Long-Context LLMs?
A Comprehensive Study and Hybrid Approach
Zhuowan Li1 Cheng Li1 Mingyang Zhang1
Qiaozhu Mei2* Michael Bendersky1
1 Google DeepMind 2 University of Michigan
1 {zhuowan,chgli,mingyang,bemike}@google.com 2 qmei@umich.edu
Abstract
Retrieval Augmented Generation (RAG) has
been a powerful tool for Large Language
Models (LLMs) to efﬁciently process overly
lengthy contexts. However, recent LLMs like
Gemini-1.5 and GPT-4 show exceptional ca-
pabilities to understand long contexts directly.
We conduct a comprehensive comparison be-
tween RAG and long-context(LC) LLMs, aim-
ing to leverage the strengths of both. We
benchmark RAG and LC across various public
datasets using three latest LLMs. Results re-
veal that when resourced sufﬁciently, LC con-
sistently outperforms RAG in terms of aver-
age performance. However, RAG’s signiﬁ-
cantly lower cost remains a distinct advantage.
Based on this observation, we propose S ELF -
ROUTE , a simple yet effective method that
routes queries to RAG or LC based on model
self-reﬂection. S ELF -ROUTE signiﬁcantly re-
duces the computation cost while maintaining
a comparable performance to LC. Our ﬁndings
provide a guideline for long-context applica-
tions of LLMs using RAG and LC.
1 Introduction
Retrieval augmented generation (RAG)has been
shown to be a both effective and efﬁcient approach
for large language models (LLMs) to leverage ex-
ternal knowledge. RAG retrieves relevant informa-
tion based on the query and then prompts an LLM
to generate a response in the context of the retrieved
information. This approach signiﬁcantly expands
LLM’s access to vast amounts of information at a
minimal cost.
However, recent LLMs like Gemini and GPT-4
have demonstrated exceptional capabilities in un-
derstanding long contexts directly. For example,
Gemini 1.5 can process up to 1 million tokens (Reid
et al., 2024). This prompts the need for a system-
atic comparison between long-context (LC) LLMs
*Visiting researcher to Google DeepMind.
performance
(a)cost
(b)
Figure 1: While long-context LLMs (LC) surpass RAG
in long-context understanding, RAG is signiﬁcantly
more cost-efﬁcient. Our approach, SELF -ROUTE , com-
bining RAG and LC, achieves comparable performance
to LC at a much lower cost.
and RAG: on one hand, RAG conceptually acts as
a prior, regularizing the attention of LLMs onto
retrieved segments, thus avoiding the distraction of
the irrelevant information and saving unnecessary
attention computations; on the other hand, large-
scale pretraining may enable LLMs to develop even
stronger long-context capabilities. Therefore, we
are motivated to compare RAG and LC, evaluating
both their performance and efﬁciency.
In this work, we systematically benchmark RAG
and LC on various public datasets, gaining a com-
prehensive understanding of their pros and cons,
and ultimately combining them to get the best of
both worlds. Different from ﬁndings in previous
work (Xu et al., 2023), we ﬁnd that LC consistently
outperform RAG in almost all settings (when re-
sourced sufﬁciently). This demonstrates the su-
perior progress of recent LLMs in long-context
understanding.
Despite the suboptimal performance, RAG re-
mains relevant due to its signiﬁcantly lower compu-
tational cost. In contrast to LC, RAG signiﬁcantly
decreases the input length to LLMs, leading to re-
881duced costs, as LLM API pricing is typically based
on the number of input tokens. (Google, 2024; Ope-
nAI, 2024b)1. Moreover, our analysis reveals that
the predictions from LC and RAG are identical for
over 60% of queries. For these queries, RAG can
reduce cost without sacriﬁcing performance.
Based on this observation, we propose SELF -
ROUTE , a simple yet effective method that routes
various queries to RAG or LC based on model self-
reﬂection. With SELF -ROUTE , we signiﬁcantly re-
duce the cost while achieving overall performance
comparable to LC. For example, the cost is reduced
by 65% for Gemini-1.5-Pro and 39% for GPT-4O.
Fig. 1 shows the comparisons of LC, RAG and
SELF -ROUTE using three recent LLMs: GPT-4O,
GPT-3.5-Turbo and Gemini-1.5-Pro. In addition to
quantitative evaluation, we provide a comprehen-
sive analysis comparing RAG and LC, including
common failure patterns of RAG, the trade-offs
between cost and performance, and the results on
additional synthetic datasets. Our analysis serves
as a starting point, inspiring future improvements
of RAG, and as a empirical guide for building long-
context applications using RAG and LC.
2 Related Work
Long-context LLMs. There has long been ef-
forts for enabling LLMs to handle long contexts
(Guo et al., 2022; Beltagy et al., 2020; Chen et al.,
2023b). While recent LLMs like Gemini-1.5 (Reid
et al., 2024), GPT-4 (Achiam et al., 2023), Claude-
3 (Anthropic, 2024) achieve signiﬁcantly larger
context window size, long-context prompting is
still expensive due to the quadratic computation
cost of transformers regarding to the input token
numbers. Recent work proposes methods to reduce
cost by prompt compression (Jiang et al., 2023),
model distillation (Hsieh et al., 2023), or LLM cas-
cading (Chen et al., 2023a).
Retrieval-augmented generation. Augmenting
LLMs with relevant information retrieved from var-
ious sources (Lewis et al., 2020) has been success-
ful in complementing LLMs with external knowl-
edge. RAG achieves good performance on tasks
like language modeling (Khandelwal et al., 2019;
Shi et al., 2023) and QA (Guu et al., 2020; Izacard
and Grave, 2020), with a signiﬁcantly lower compu-
tation cost (Borgeaud et al., 2022). Related to but
different from our work, recently works augment
1While retrieval may introduce extra cost, retrieval system
is much easier to set up and can be hosted on customer side.
RAG with correction (Yan et al., 2024), critique
(Asai et al., 2023), veriﬁcation (Li et al., 2023), or
adaptive search (Wang et al., 2023; Cheng et al.,
2024; Jeong et al., 2024) to improve retrieval qual-
ity on knowledge-intensive tasks.
Long-context evaluation. Evaluating long-
context models is challenging due to the difﬁculty
in collecting and analyzing long texts. Recent re-
searchers propose both synthetic tests like needle-
in-a-haystack (Greg Kamradt, 2023), Ruler (Hsieh
et al., 2024), or Counting Stars (Song et al., 2024),
and real datasets including LongBench (Bai et al.,
2023), ∞Bench (Zhang et al., 2024), L-Eval (An
et al., 2023), and others (Shaham et al., 2022; Yuan
et al., 2024; Maharana et al., 2024). Evaluating
on these datasets, recent works study the perfor-
mance degradation over various context lengths
(Levy et al., 2024; Hsieh et al., 2024), the lost-
in-the-middle phenomenon (Liu et al., 2024), and
explore solutions (Kuratov et al., 2024). Related
to our work, Xu et al. (2023) compare RAG and
long-context prompting and ﬁnd that long-context
models still lags behind RAG. This is different
from our ﬁndings, possibly due to consideration of
stronger LLMs and longer contexts in our work.
3 Benchmarking RAG versus LC
3.1 Datasets and metrics
We evaluate on a subset of datasets from Long-
Bench (Bai et al., 2023) and ∞Bench (Zhang et al.,
2024), which are recent benchmarks containing a
collection of new and existing datasets for LLM
evaluation, covering both synthetic and real texts in
multiple languages. LongBench contains a collec-
tion of 21 datasets, with an average context length
of 7k words. ∞Bench consists of even longer con-
texts with an average length of 100k tokens.
Among the datasets, we mainly focus on tasks
that are (a) in English, (b) real, and (c) query-based
(e.g. summarization tasks do not contain queries
for retrieving relevant information). This results in
7 datasets from LongBench including NarrativeQA
(Koˇcisk`y et al., 2018), Qasper (Dasigi et al., 2021),
MultiFieldQA (Bai et al., 2023), HotpotQA (Yang
et al., 2018), 2WikiMultihopQA (Ho et al., 2020),
MuSiQue (Trivedi et al., 2022), QMSum (Zhong
et al., 2021); and 2 datasets from ∞Bench includ-
ing En.QA and EN.MC. Please refer to Appendix A
for more details. Additionally, in Sec. 5.4, we will
provide an ablation a synthetic datasets PassKey
from ∞Bench.
882For evaluation metrics, we report F1 scores for
the open-ended QA tasks, accuracy for the multi-
choice QA tasks, and ROUGE score for the sum-
marization tasks.
3.2 Models and Retrievers
Three latest LLMs are evaluated, including Gemini-
1.5-Pro (Reid et al., 2024), GPT-4O (OpenAI,
2024a), and GPT-3.5-Turbo (OpenAI, 2023) 2.
Gemini-1.5-Pro is a recent long-context LLM from
Google, supporting up to 1 million tokens. GPT-
4O, the newest lightweight yet strong LLM from
OpenAI, supports 128k tokens. GPT-3.5-Turbo
supports 16k tokens.
Two retrievers are used in our study: Contriever
(Izacard et al., 2021), which is a contrastively
trained dense retriever outperforming BM25 on
BEIR datasets, and Dragon (Lin et al., 2023), which
is a recent generalizable dense retriever achieving
high performance in both supervised and zero-shot
settings without complex late interaction. Follow-
ing (Xu et al., 2023), we divide long contexts into
chunks of 300 words, and select the top k chunks
(default k = 5) based on the cosine similarity of
the query embedding and the chunk embeddings.
The chunks are ordered by the similarity scores,
with the chunk index prepended at the beginning.
Since black-box LLMs are pretrained on un-
known datasets, the leakage of evaluation datasets
may occur. Especially, some of the evaluation
datasets are based on Wikipedia, which has likely
been seen by LLMs during during. In some cases,
we ﬁnd that model may predict the correct answer
using exactly the same words as the groundtruth
(e.g. “meticulously”), even when they do not appear
in the provided context. In our experiment, we try
mitigating this issue by prompting the model to an-
swer “based only on the provided passage”
for both RAG and LC. It remains an open ques-
tion how to address the data leakage issue in LLM
evaluation.
3.3 Benchmarking results
We benchmark the performance of LC and RAG
across the nine datasets, using three recent LLMs:
Gemini-1.5-Pro, GPT-4O and GPT-3.5-Turbo.
Tab. 1 presents the results using the Contriever
retriever, where rows *-1 and rows *-2 present the
benchmarking results for LC and RAG respectively.
Results using the Dragon retriever will be discussed
2gpt-3.5-turbo-0125, gpt-4o-2024-05-13
in Sec. 5.3 and Tab. 2.
As shown in Tab. 1, LC consistently outperforms
RAG for all the three models, with a signiﬁcant
margin. On average, LC surpasses RAG by 7.6%
for Gemini-1.5-Pro, 13.1% for GPT-4O, and 3.6%
for GPT-3.5-Turbo. Noticeably, the performance
gap is more signiﬁcant for the more recent mod-
els (GPT-4O and Gemini-1.5-Pro) compared to
GPT-3.5-Turbo, highlighting the exceptional long-
context understanding capacity of the latest LLMs.
However, there is an exception observed on the
two longer datasets from ∞Bench (i.e., En.QA and
En.MC), where RAG achieves higher performance
than LC for GPT-3.5-Turbo. This result deviates
from the overall trend, likely due to the signiﬁcantly
longer context in these datasets (147k words on av-
erage) compared with the limited context window
(16k) of GPT-3.5-Turbo. This ﬁnding highlights
the effectiveness of RAG when the input text con-
siderably exceeds the model’s context window size,
emphasizing a speciﬁc use case of RAG.
4 Self-Route
4.1 Motivation
As demonstrated in Sec. 3, RAG lags behind long-
context LLMs in terms of performance. However,
despite this performance gap, we surprisingly ﬁnd
a high degree of overlap in their predictions, as
illustrated in Fig. 2.
𝑆!"#	−	𝑆$%
Figure 2: Distribution of the difference of predic-
tion scores between RAG and LC (computed w.r.t.
groundtruth labels). RAG and LC predictions are
highly identical, for both correct and incorrect ones.
Fig. 2 displays the distribution of the differences
between RAG prediction scores SRAG and LC pre-
diction scores SLC, speciﬁcally SRAG −SLC (the
scores are multiplied by 100 to be scaled to 1-100).
These scores S represent the evaluation of model
predictions against the groundtruth. Notably, for
883Avg Narr Qasp Mult Hotp 2Wiki Musi Sum En.QA En.MC
1-1 LC 49.70 32.76 47.83 52.33 61.85 62.9640.22 20.73 43.08 85.57
Gemini-1.5-Pro
1-2 RAG 37.33 22.54 44.68 49.53 48.36 54.24 26.56 19.51 19.46 51.09
1-3 S ELF-ROUTE 46.41 28.32 45.23 51.47 55.18 62.68 40.66 19.77 37.51 76.86
1-4 answerable %76.78 73.00 85.00 96.67 84.50 81.00 58.50 93.50 56.41 62.45
1-5 token % 38.39 23.07 49.93 36.88 32.97 53.49 56.14 17.96 42.25 32.84
GPT-4O
2-1 LC 48.67 32.78 44.54 55.28 62.42 70.69 41.65 21.92 32.36 76.42
2-2 RAG 32.60 18.05 46.02 50.74 36.86 50.21 16.09 19.97 14.43 41.05
2-3 S ELF-ROUTE 48.89 31.36 47.99 53.17 62.14 70.14 41.69 21.31 34.95 77.29
2-4 answerable %57.36 44.00 67.50 94.00 52.50 62.00 30.00 92.00 27.07 47.16
2-5 token % 61.40 66.40 72.25 39.65 65.79 77.05 85.00 20.26 73.01 53.21
GPT-3.5-Turbo
3-1 LC 32.07 23.34 42.96 49.19 45.33 41.04 17.92 19.61 14.73 34.50
3-2 RAG 30.33 18.22 38.15 49.21 37.84 35.16 16.41 18.94 15.39 43.67
3-3 S ELF-ROUTE 35.32 24.06 38.65 52.07 47.28 44.62 34.44 19.88 22.03 44.54
3-4 answerable %74.10 71.50 80.00 91.33 68.50 69.00 47.00 93.50 50.43 95.63
3-5 token % 38.85 20.56 55.08 35.29 48.70 65.91 65.08 16.40 38.17 4.50
Table 1: Results of Gemini-1.5-Pro, GPT-3.5-Turbo, and GPT-4O using the Contriever retriever. LC consistently
outperforms RAG, while SELF -ROUTE achieves performance comparable to LC using much less tokens.
most queries, RAG scores and LC scores are highly
similar. In fact, for 63% queries, the model pre-
dictions are exactly identical; and for 70% queries,
the score difference is less than 10 (absolute value).
Interestingly, the identical predictions are not nec-
essarily correct, as shown by the varying colors rep-
resenting the average score, i.e., (SRAG + SLC)/2.
This observation suggests that RAG and LC tend
to make not only the same correct predictions but
also similar errors.
This ﬁnding motivates us to leverage RAG for
the majority of queries, reserving computationally
more expensive LC for a small subset of queries
where it truly excels. By doing so, RAG can signif-
icantly reduce computational costs without sacriﬁc-
ing overall performance.
4.2 Self-Route
Based on the above motivation, we propose SELF -
ROUTE , a simple yet effective method combining
RAG and LC to reduce cost while maintaining a
performance comparable to LC. SELF -ROUTE uti-
lizes LLM itself to route queries based on self-
reﬂection, under the assumption that LLMs are
well-calibrated in predicting whether a query is
answerable given provided context.
Concretely, our method consists of two steps: a
RAG-and-Route step and a long-context prediction
step. In the ﬁrst step, we provide the query and
the retrieved chunks to the LLM, and prompt it to
predict whether the query is answerable and, if so,
generate the answer. This is similar to standard
RAG, with one key difference: the LLM is given
the option to decline answering with the prompt
“Write unanswerable if the query can not
be answered based on the provided text” .
For the queries deemed answerable, we accept the
RAG prediction as the ﬁnal answer. For the queries
deemed unanswerable, we proceed to the second
step, providing the full context to the long-context
LLMs to obtain the ﬁnal prediction (i.e., LC).
As our results will demonstrate, most queries can
be solved by the ﬁrst RAG-and-Route step ( e.g.,
82% for Gemini-1.5-Pro), with only a small por-
tion requiring the following long-context prediction
step. Since the RAG-and-Route step only needs
the retrieved chunks ( e.g., 1.5k tokens) as input,
which is signiﬁcantly shorter than the full contexts
(e.g., 10k - 100k tokens), the overall computation
cost is substantially reduced. Detailed token count
analysis will be provided in the results.
4.3 Results
Rows *-3 to *-5 in Tab. 1 present the results of our
method, utilizing the three LLMs. Rows *-3 report
the performance. Rows *-4 show the percentage
of answerable queries, as predicted in the RAG-
and-Route step. Rows *-5 display the percentage
of tokens used by our method, compared to that
of LC. In terms of performance (rows *-3), SELF -
ROUTE signiﬁcantly outperforms RAG, achieving
results comparable to LC. Across all three models,
SELF -ROUTE surpasses RAG (rows *-2) by over
5%. Compared to LC (rows *-1), there is a slight
performance drop for GPT-4O (-0.2%) and Gemini-
1.5-Pro (-2.2%), but an improvement for GPT-3.5-
Turbo (+1.7%).
All three LLMs consistently route more than half
884of queries towards RAG, as shown in rows *-4. For
Gemini-1.5-Pro, the answerable percentage even
reaches 81.74% (row 1-4). This indicates that RAG
may answer most queries without the need for LC,
conﬁrming our initial motivation.
Due to the high answerable rate, the number of
tokens required is signiﬁcantly reduced (rows *-
5). For example, GPT-4O uses only 61% tokens
while achieving comparable performance (46.83)
with LC (47.04), Gemini-1.5-Pro uses 38.6% of
the tokens. Since the computation cost of the
transformer-based LLMs is quadratic to token
count, and most LLM APIs charge based on token
count (OpenAI, 2024b; Google, 2024), this lower
token count translates to substantial cost savings.
On longer datasets, the advantage of our method
is more pronounced for OpenAI models, but less
signiﬁcant for Gemini. For instance, for GPT-4O,
SELF -ROUTE outperforms LC by 2.3% and 7.4%
respectively on EN.QA and EN.MC, which contain
longer contexts. For GPT-3.5-Turbo, the advantage
margins are even larger. However, for Gemini-
1.5-Pro, the performance is lower than LC. These
different behaviors are possibly due to the differ-
ence in LLM alignments, i.e., OpenAI models are
more likely to reject answering using RAG, leading
to a lower answerable percentage but higher accu-
racy, which results in a different performance-cost
trade-off compared with Gemini-1.5-Pro.
5 Analysis
5.1 Ablations of k
Both RAG and SELF -ROUTE relies on the top- k
retrieved text chunks. The larger k is, the longer
context are fed into LLMs for RAG prediction as
well as routing, resulting in different costs versus
performances. To study the inﬂuence ofk, in Fig. 3,
we plot the performance and cost (i.e. input token
percentage) curves when different ks are used.
In terms of performance, for both RAG and
SELF -ROUTE , a larger k leads to better perfor-
mance. While k increases, more and more chunks
are fed into the LLMs, thus the performance grad-
ually improves to approach LC. As can be seen in
from the curves, the advantage of SELF -ROUTE
is the most signiﬁcant for smaller k. For example,
when k = 1, RAG gets from 20.24% while SELF -
ROUTE gets 37.9%, while when k is larger than 50,
all three methods get similar performance.
However, the trend of cost is not monotonous
for SELF -ROUTE . As seen, the cost reaches its
performance
top-k
token percentage
(a)
(b)
top-k
Figure 3: Trade-off curves between (a) model perfor-
mance and (b) token percentage as a function of k.
minimum at k = 5. This is because when k in-
creases, the cost of RAG (and routing) increases,
but more queries are routed to RAG from LC, thus
the overall cost may decrease. The sweet point ofk
might be different for each dataset, e.g. on average,
k = 5has the lowest cost as shown in the curves,
but on some datasets, especially ones that contain
extractive questions which does not need multi-hop
reasoning (like NarrativeQA and QMSum), k = 1
leads to the lowest cost. This indicates that the opti-
mal k depends on the nature of the task, as well as
the performance requirement. We encourage future
researchers to look for different ks when applying
our method to various applications.
5.2 Why does RAG fail?
To gain a better understanding of why RAG lags
behind LC, we analyze the failure reasons for the
examples that cannot be answered by RAG. We
ﬁrst manually check some examples for which our
RAG-and-Route step predicts “unanswerable” and
summarize four typical failure reasons, then prompt
LLM to classify all the examples.
The four reasons include: (A) The query requires
multi-step reasoning so the results of previous steps
are needed to retrieve information for later steps,
e.g. “What nationality is the performer of
song XXX”. (B) The query is general, e.g. “What
does the group think about XXX” , which is
challenging for the retriever to formulate a good
query. (C) The query is long and complex, which
is challenging for the retriever to understand. How-
ever, answering this kind of questions is arguably,
885Avg Narr Qasp Mult Hotp 2Wiki Musi Sum En.QA En.MC
1 LC 49.70 32.76 47.83 52.33 61.85 62.96 40.22 20.73 43.08 85.57
Dragon
2 RAG 38.09 21.91 44.33 53.08 51.61 50.05 30.47 19.93 21.25 50.22
3 combine 46.81 28.50 43.82 54.62 56.58 60.62 40.66 20.07 37.79 78.60
4 RAG ratio 77.88 74.00 84.00 97.33 86.00 77.00 66.00 95.50 61.25 59.83
5 Token ratio 37.87 19.31 54.15 34.78 32.64 55.65 48.16 16.64 38.71 40.83
Table 2: Results for Gemini-1.5-Pro using Dragon retriever.
an advantage of LLMs. (D) The query is implicit,
demanding a thorough understanding of the en-
tire context. For instance, in a lengthy conversa-
tional narrative about a space voyage, a question
like “What caused the shadow behind the
spaceship?” requires readers to connect the dots
and deduce the answer, as there is no explicit men-
tion of the shadow when the cause is revealed.
number of queries
Figure 4: Distribution of typical RAG failure reasons.
Using these reasons, we prompt Gemini-1.5-Pro
with few-shot in-context examples that we man-
ually annotated, to classify all the unanswerable
examples into these four categories, plus an “other"
option. Fig. 4 shows the distribution of failure rea-
sons on the seven datasets in LongBench. Each
dataset may contain different number of RAG fail-
ure cases, resulting in various bar heights. The
distribution patterns are consistent with the nature
of the datasets. For example, the three Wikipedia-
based multi-hop reasoning datasets (HotpotQA,
2WikiMQA, MuSiQue) are challenging for RAG
because of multi-step retrieval as shown in blue.
For NarrativeQA, which are long stories containing
a lot of dialogues, most failure cases are due to im-
plicit queries that requires understanding the whole
context (shown in green). For QMSum, which is a
summarization dataset contains open-ended ques-
tions, failures are mostly due to general queries
(shown in red). We manually checked the exam-
ples classiﬁed as “others” and ﬁnd that most of
them are actually multi-step questions, often with
ambiguities, which poses challenges for answering.
We hope this failure analysis inspires future im-
provements of RAG. For example, engaging chain-
of-thought (Wei et al., 2022) into RAG may help ad-
dress the multi-step questions, and revisiting query
understanding techniques like query expansion (Lv
and Zhai, 2009; Zhai and Lafferty, 2001) may help
with the general queries and complex queries. We
are also glad to see recent efforts towards the direc-
tion (Chan et al., 2024; Ma et al., 2023).
5.3 Different retrievers
The results using a retriever, Dragon, is shown in
Tab. 2 based on Gemini-1.5-Pro. As can be seen,
the results are consistent with Contriever, for all
of LC, RAG, and SELF -ROUTE , showing that our
ﬁndings are generalizable across retrievers.
5.4 Results on synthetic data
In this study, we mainly focus on real datasets, with
a consideration that results on synthetic data, which
are artiﬁcially created by researchers, may subject
to dataset artifacts. We notice some methods that
researchers adopted to create synthetic long context
datasets may unconsciously, but largely, inﬂuence
the performance comparison between RAG and LC.
For example, here we describe the results on the
“PassKey” dataset in∞Bench and its variations.
This “PassKey” dataset presents a needle-in-a-
haystack test, where a sentence with a passkey
(e.g. “the passkey is 123456”) is hidden within
chunks of irrelevant text, and the model is asked
to answer the question “What is the passkey”.
The task requires strong retrieval capability. On
this dataset, RAG achieves 80.34% accuracy, out-
performing LC, which gets 65.25% using Gemini-
1.5-Pro. However, if the query is slightly modi-
ﬁed as “What is the special token hidden
inside the texts”, RAG accuracy sharply drops
to only 4.58%, while LC keeps roughly the same
(69.32%). Another example: if the chunks contain
two passkeys and the query is “Which passkey
is larger? First or second?” , then RAG
(47.63%) under-performs LC (64.24%) as well.
886RAG LC
Original 80.34 65.25
Variant-1: “special token” 4.58 69.32
Variant-2: “which is larger” 47.63 64.24
Table 3: Synthetic dataset may unconsciously contain
artifacts that inﬂuence the comparison results.
Tab. 3 summarizes the results, which demonstrates
that the evaluation highly subjects to artifacts in
dataset construction, showing limitation of syn-
thetic testing.
5.5 Exclusion of LLM’s internal knowledge
Ideally, the comparison in this paper should ex-
clude the model’s internal knowledge (i.e., para-
metric knowledge) so that the model’s performance
are solely based on its capability to understand long
contexts. In our study, this internal knowledge is
excluded by utilizing the prompt “based only on
the provided passage”, which we empirically ﬁnd is
a simple yet effective method. Here we discuss the
effectiveness of this method, as well as alternative
methods to exclude external knowledge.
First, we validate the effectiveness of the sim-
ple prompt “based only on the provided passage”.
Tab. 4 compares the performance (long-context) of
Gemini-1.5-Pro with and without this prompt. As
shown, using this prompt consistently limits the
model’s performance (average performance drops
from 50.57 to 45.53), which indicates that using
this simple instruction can already effectively limit
the usage of the model’s parametric knowledge.
without
"based only on ..."
with
"based only on ..."
NarrativeQA 36.35 32.76
Qasper 50.69 47.83
MultiFieldQA 56.07 52.33
HotpotQA 66.47 61.85
2WikiMQA 68.97 62.96
Musique 54.56 40.22
QMSum 20.87 20.73
En.QA 49.20 43.08
En.MC 90.83 85.57
Avg 50.57 45.53
Table 4: Comparison of the long-context performance
of Gemini-1.5-Pro, using the prompt with and without
“based only on the provided passage”.
Second, as an alternative method to exclude
internal knowledge, we remove the questions
where the model can correctly answer without
any contexts (i.e., commonsense questions), and
report the model’s performance only on the non-
commonsense questions. Tab. 5 shows the perfor-
mance of Gemini-1.5-Pro and GPT-3.5-Turbo on
all the questions from the MuSiQue dataset, as well
as their performance on the non-commonsense sub-
set3. As shown, after excluding the commonsense
questions, the trend remains the same.
all questions w/o commonsense
Gemini GPT-3.5 Gemini GPT-3.5
# questions 200 200 133 150
LC 40.22 17.92 31.76 13.00
RAG 26.56 16.41 15.51 13.05
Self-Route 40.66 34.44 31.32 19.76
answerable % 58.50 47.00 52.63 45.33
token % 56.14 65.08 48.46 53.43
Table 5: Results on MuSiQue on all questions, and on
the subset of non-commonsense questions (i.e., exclud-
ing questions that can be answered without contexts).
That said, a more thorough study to explore vari-
ous methods for controlling the usage of model’s
internal knowledge, and to study the source of in-
ternal knowledge (e.g. LLM"s world knowledge
or dataset leakage), will be valuable future work,
which we hope can be further investigated.
6 conclusion
This paper presents a comprehensive comparison of
RAG and LC, highlighting the trade-offs between
performance and computational cost. While LC
demonstrate superior performance in long-context
understanding, RAG remains a viable option due
to its lower cost and advantages when the input
considerably exceeds the model’s context window
size. Our proposed method, which dynamically
routes queries based on model self-reﬂection, ef-
fectively combines the strengths of both RAG and
LC, achieving comparable performance to LC at a
signiﬁcantly reduced cost. We believe our ﬁndings
contribute valuable insights for the practical appli-
cation of long-context LLMs and pave the way for
future research in optimizing RAG techniques.
3Different models may learn different internal knowledge,
resulting in different numbers of non-commonsense ques-
tions. For example, GPT-3.5-Turbo gets 14.53 performance on
MuSiQue while Gemini-1.5-Pro gets 23.58 using only internal
knowledge,
887Acknowledgements
We would like to thank Weize Kong, Tao Chen, Jef-
frey Dudek and Spurthi Amba Hombaiah for their
helpful comments and suggestions, as well as the
anonymous reviewers for the valuable discussions.
References
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama
Ahmad, Ilge Akkaya, Florencia Leoni Aleman,
Diogo Almeida, Janko Altenschmidt, Sam Altman,
Shyamal Anadkat, et al. 2023. Gpt-4 technical re-
port. arXiv preprint arXiv:2303.08774.
Chenxin An, Shansan Gong, Ming Zhong, Mukai
Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu.
2023. L-eval: Instituting standardized evaluation
for long context language models. arXiv preprint
arXiv:2307.11088.
Anthropic. 2024. Claude 3.5 sonnet. https://www.
anthropic.com/news/claude-3-5-sonnet/.
Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and
Hannaneh Hajishirzi. 2023. Self-rag: Learning to re-
trieve, generate, and critique through self-reﬂection.
arXiv preprint arXiv:2310.11511.
Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu,
Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao
Liu, Aohan Zeng, Lei Hou, et al. 2023. Longbench:
A bilingual, multitask benchmark for long context
understanding. arXiv preprint arXiv:2308.14508.
Iz Beltagy, Matthew E Peters, and Arman Cohan.
2020. Longformer: The long-document transformer.
arXiv preprint arXiv:2004.05150.
Sebastian Borgeaud, Arthur Mensch, Jordan Hoff-
mann, Trevor Cai, Eliza Rutherford, Katie Milli-
can, George Bm Van Den Driessche, Jean-Baptiste
Lespiau, Bogdan Damoc, Aidan Clark, et al. 2022.
Improving language models by retrieving from tril-
lions of tokens. In International conference on ma-
chine learning, pages 2206–2240. PMLR.
Chi-Min Chan, Chunpu Xu, Ruibin Yuan, Hongyin
Luo, Wei Xue, Yike Guo, and Jie Fu. 2024. Rq-rag:
Learning to reﬁne queries for retrieval augmented
generation. arXiv preprint arXiv:2404.00610.
Lingjiao Chen, Matei Zaharia, and James Zou. 2023a.
Frugalgpt: How to use large language models while
reducing cost and improving performance. arXiv
preprint arXiv:2305.05176.
Shouyuan Chen, Sherman Wong, Liangjian Chen, and
Yuandong Tian. 2023b. Extending context window
of large language models via positional interpolation.
arXiv preprint arXiv:2306.15595.
Qinyuan Cheng, Xiaonan Li, Shimin Li, Qin Zhu,
Zhangyue Yin, Yunfan Shao, Linyang Li, Tianxiang
Sun, Hang Yan, and Xipeng Qiu. 2024. Uniﬁed
active retrieval for retrieval augmented generation.
arXiv preprint arXiv:2406.12534.
Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Co-
han, Noah A Smith, and Matt Gardner. 2021. A
dataset of information-seeking questions and an-
swers anchored in research papers. arXiv preprint
arXiv:2105.03011.
Google. 2024. Gemini pricing. https://ai.google.
dev/pricing.
Greg Kamradt. 2023. Needle in a haystack - pressure
testing llms. https://github.com/gkamradt/
LLMTest_NeedleInAHaystack.
Mandy Guo, Joshua Ainslie, David C Uthus, Santi-
ago Ontanon, Jianmo Ni, Yun-Hsuan Sung, and Yin-
fei Yang. 2022. Longt5: Efﬁcient text-to-text trans-
former for long sequences. In Findings of the Associ-
ation for Computational Linguistics: NAACL 2022,
pages 724–736.
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pa-
supat, and Mingwei Chang. 2020. Retrieval aug-
mented language model pre-training. In Interna-
tional conference on machine learning, pages 3929–
3938. PMLR.
Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara,
and Akiko Aizawa. 2020. Constructing a multi-hop
qa dataset for comprehensive evaluation of reason-
ing steps. arXiv preprint arXiv:2011.01060.
Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shan-
tanu Acharya, Dima Rekesh, Fei Jia, and Boris Gins-
burg. 2024. Ruler: What’s the real context size of
your long-context language models? arXiv preprint
arXiv:2404.06654.
Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh,
Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner,
Ranjay Krishna, Chen-Yu Lee, and Tomas Pﬁster.
2023. Distilling step-by-step! outperforming larger
language models with less training data and smaller
model sizes. arXiv preprint arXiv:2305.02301.
Gautier Izacard, Mathilde Caron, Lucas Hosseini, Se-
bastian Riedel, Piotr Bojanowski, Armand Joulin,
and Edouard Grave. 2021. Unsupervised dense in-
formation retrieval with contrastive learning. arXiv
preprint arXiv:2112.09118.
Gautier Izacard and Edouard Grave. 2020. Lever-
aging passage retrieval with generative models for
open domain question answering. arXiv preprint
arXiv:2007.01282.
Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju
Hwang, and Jong C Park. 2024. Adaptive-rag:
Learning to adapt retrieval-augmented large lan-
guage models through question complexity. In Pro-
ceedings of the 2024 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies (Vol-
ume 1: Long Papers), pages 7029–7043.
888Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng
Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2023.
Longllmlingua: Accelerating and enhancing llms
in long context scenarios via prompt compression.
arXiv preprint arXiv:2310.06839.
Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke
Zettlemoyer, and Mike Lewis. 2019. Generalization
through memorization: Nearest neighbor language
models. arXiv preprint arXiv:1911.00172.
Tomáš Ko ˇcisk`y, Jonathan Schwarz, Phil Blunsom,
Chris Dyer, Karl Moritz Hermann, Gábor Melis, and
Edward Grefenstette. 2018. The narrativeqa reading
comprehension challenge. Transactions of the Asso-
ciation for Computational Linguistics, 6:317–328.
Yuri Kuratov, Aydar Bulatov, Petr Anokhin, Dmitry
Sorokin, Artyom Sorokin, and Mikhail Burtsev.
2024. In search of needles in a 10m haystack: Recur-
rent memory ﬁnds what llms miss. arXiv preprint
arXiv:2402.10790.
Mosh Levy, Alon Jacoby, and Yoav Goldberg. 2024.
Same task, more tokens: the impact of input length
on the reasoning performance of large language
models. arXiv preprint arXiv:2402.14848.
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio
Petroni, Vladimir Karpukhin, Naman Goyal, Hein-
rich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-
täschel, et al. 2020. Retrieval-augmented generation
for knowledge-intensive nlp tasks. Advances in Neu-
ral Information Processing Systems, 33:9459–9474.
Xiaonan Li, Changtai Zhu, Linyang Li, Zhangyue
Yin, Tianxiang Sun, and Xipeng Qiu. 2023. Lla-
trieval: Llm-veriﬁed retrieval for veriﬁable genera-
tion. arXiv preprint arXiv:2311.07838.
Sheng-Chieh Lin, Akari Asai, Minghan Li, Barlas
Oguz, Jimmy Lin, Yashar Mehdad, Wen-tau Yih,
and Xilun Chen. 2023. How to train your dragon:
Diverse augmentation towards generalizable dense
retrieval. arXiv preprint arXiv:2302.07452.
Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paran-
jape, Michele Bevilacqua, Fabio Petroni, and Percy
Liang. 2024. Lost in the middle: How language
models use long contexts. Transactions of the Asso-
ciation for Computational Linguistics, 12:157–173.
Yuanhua Lv and ChengXiang Zhai. 2009. Adaptive
relevance feedback in information retrieval. In Pro-
ceedings of the 18th ACM conference on Informa-
tion and knowledge management, pages 255–264.
Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao,
and Nan Duan. 2023. Query rewriting for retrieval-
augmented large language models. arXiv preprint
arXiv:2305.14283.
Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov,
Mohit Bansal, Francesco Barbieri, and Yuwei
Fang. 2024. Evaluating very long-term conver-
sational memory of llm agents. arXiv preprint
arXiv:2402.17753.
OpenAI. 2023. Gpt-3.5-turbo. https://platform.
openai.com/docs/models/gpt-3-5-turbo.
OpenAI. 2024a. Gpt-4o. https://openai.com/
index/hello-gpt-4o/.
OpenAI. 2024b. Openai-api pricing. https://
platform.openai.com/docs/overview.
Machel Reid, Nikolay Savinov, Denis Teplyashin,
Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste
Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan
Firat, Julian Schrittwieser, et al. 2024. Gemini
1.5: Unlocking multimodal understanding across
millions of tokens of context. arXiv preprint
arXiv:2403.05530.
Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori
Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor
Geva, Jonathan Berant, et al. 2022. Scrolls: Stan-
dardized comparison over long language sequences.
arXiv preprint arXiv:2201.03533.
Weijia Shi, Sewon Min, Michihiro Yasunaga, Min-
joon Seo, Rich James, Mike Lewis, Luke Zettle-
moyer, and Wen-tau Yih. 2023. Replug: Retrieval-
augmented black-box language models. arXiv
preprint arXiv:2301.12652.
Mingyang Song, Mao Zheng, and Xuan Luo. 2024.
Counting-stars: A simple, efﬁcient, and reasonable
strategy for evaluating long-context large language
models. arXiv preprint arXiv:2403.11802.
Harsh Trivedi, Niranjan Balasubramanian, Tushar
Khot, and Ashish Sabharwal. 2022. Musique: Mul-
tihop questions via single-hop question composition.
Transactions of the Association for Computational
Linguistics, 10:539–554.
Yile Wang, Peng Li, Maosong Sun, and Yang Liu. 2023.
Self-knowledge guided retrieval augmentation for
large language models. In Findings of the Associ-
ation for Computational Linguistics: EMNLP 2023,
pages 10303–10315.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,
et al. 2022. Chain-of-thought prompting elicits
reasoning in large language models. Advances in
neural information processing systems, 35:24824–
24837.
Peng Xu, Wei Ping, Xianchao Wu, Lawrence
McAfee, Chen Zhu, Zihan Liu, Sandeep Subra-
manian, Evelina Bakhturina, Mohammad Shoeybi,
and Bryan Catanzaro. 2023. Retrieval meets long
context large language models. arXiv preprint
arXiv:2310.03025.
Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, and Zhen-Hua
Ling. 2024. Corrective retrieval augmented gener-
ation. arXiv preprint arXiv:2401.15884.
889Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-
gio, William W Cohen, Ruslan Salakhutdinov, and
Christopher D Manning. 2018. Hotpotqa: A dataset
for diverse, explainable multi-hop question answer-
ing. arXiv preprint arXiv:1809.09600.
Tao Yuan, Xuefei Ning, Dong Zhou, Zhijie Yang,
Shiyao Li, Minghui Zhuang, Zheyue Tan, Zhuyu
Yao, Dahua Lin, Boxun Li, et al. 2024. Lv-eval: A
balanced long-context benchmark with 5 length lev-
els up to 256k. arXiv preprint arXiv:2402.05136.
Chengxiang Zhai and John Lafferty. 2001. Model-
based feedback in the language modeling approach
to information retrieval. In Proceedings of the tenth
international conference on Information and knowl-
edge management, pages 403–410.
Xinrong Zhang, Yingfa Chen, Shengding Hu, Zi-
hang Xu, Junhao Chen, Moo Khai Hao, Xu Han,
Zhen Leng Thai, Shuo Wang, Zhiyuan Liu, et al.
2024. Inﬁnity bench: Extending long context
evaluation beyond 100k tokens. arXiv preprint
arXiv:2402.13718.
Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia
Mutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli
Celikyilmaz, Yang Liu, Xipeng Qiu, et al. 2021.
Qmsum: A new benchmark for query-based multi-
domain meeting summarization. arXiv preprint
arXiv:2104.05938.
890A Dataset details
We evaluate on 7 datasets from LongBench (Bai et al., 2023). NarrativeQA (Koˇcisk`y et al., 2018) is
a question answering dataset, where the context is a long story like a novel or a movie script. Qasper
(Dasigi et al., 2021) focuses on question answering over academic NLP papers and is annotated by
NLP practitioners. MultiFieldQA, originally proposed in LongBench, contains human-annotated QA
over documents and articles from multiple sources, including legal documents, government reports,
encyclopedias, academic papers, etc. HotpotQA (Yang et al., 2018) contains two-hop questions written
by native English speakers that requires reasoning over two related Wikipedia paragraphs in the long
context. 2WikiMultihopQA (Ho et al., 2020) contains up to 5-hop questions that are synthesized through
manually designed templates, ensuring that they cannot be solved through shortcuts. The questions in
MuSiQue (Trivedi et al., 2022) are up to 4-hop, ﬁrst constructed from single-hop question compositions,
and then paraphrased by annotators for linguistic diversity. QMSum (Zhong et al., 2021) is a query-based
summarization dataset over meeting scripts from multiple domains.
We evaluate on 2 datasets from ∞Bench (Zhang et al., 2024). En.QA contains human-annotated
question-answer pairs for long novels, with key entity names manually replaced in order to avoid
knowledge leakage due to model pretraining. EN.MC is annotated similarly to En.QA, but differs in that
the model is presented with four challenging answer choices written by the annotators.
Tab. 6 shows the details of the datasets, including the number of queries in each evaluation dataset and
the average context length (i.e. number of words).
Num. Query Avg. Length
LongBench
(Bai et al., 2023)
NarrativeQA 200 18,395
Qasper 200 3,599
MultiFieldQA 150 4,539
HotpotQA 200 9,133
2WikiMultihopQA 200 4,873
MuSiQue 200 11,196
QMSum 200 10,533
∞Bench
(Zhang et al., 2024)
En.QA 351 150,374
En.MC 229 142,622
Table 6: Dataset statistics.
B Ablations of k
Tab. 7 shows the performance and token ratio for differentk, which corresponds to Fig. 3. The performance
of LC, which serves as an upper bound, is 45.53. The token ratio is computed the token counts for RAG
or SELF -ROUTE divided the number of tokens required by LC.
performance token ratio
top-k RAG Self-Route RAG Self-Route
1 20.24 41.35 5.26 39.64
5 37.92 43.33 17.02 38.63
10 41.20 44.38 42.42 53.66
50 44.06 45.19 95.29 102.97
100 44.12 45.23 100.32 106.59
Table 7: Performance and token ratio for different k. This table corresponds to Fig. 3.
891C Prompts
Tab. 9 shows the prompts for each dataset in our study. The prompts are modiﬁed from the released
prompts as in LongBench (Bai et al., 2023) and ∞Bench (Zhang et al., 2024). Tab. 8 shows the prompts
used in the failure case study as in Sec. 5.2.
You are given some text chunks from an article, and a question. The text chunks are retrieved by an external retriever.
Now:
(1) Tell whether the question can be answered based only on the provided text chunks.
(2) If the question can be answered, answer the question based on the texts as concisely as you can, using a single
phrase if possible.
(3) If the question cannot be answered, choose the reason from the following:
A. The question needs multistep reasoning, thus it is hard to retrieve all the relevant chunks. For example, "What
nationality is the performer of song You Can?" contains two steps: ﬁnd the performer, then ﬁnd the nationality of the
performer. Other examples include "Where does the director of ﬁlm Wine Of Morning work at?", "What is another
notable work made by the author of Miss Sara Sampson?"
B. The question is a general query, thus it is hard to retrieve relevant chunks. For example, "What did the group
think about Dave leaving?" is general because the group may include multiple persons, and they can have different
thinkings.
C. The question is long and complex, which is hard for the retriever to encode it to retrieve relevant chunks. For
example, "What did Julie Morgan elaborate on the online survey when talking about the evaluations on the legitimacy
of the children’s rights, protection and demands?", "The Huskies football team were invited to the Alamo Bowl where
they were defeated by a team coached by Art Briles and who played their home games at what stadium?"
D. The question is not explicit and requires comprehensive understanding of the whole story and cannot be solved
using retrieval-augmented generation. For example, "What caused the shadow behind Koerber’s ship?" needs a
comprehensive understanding of the whole story. Another example like "How many words are there in the article"
also requires the complete article.
E. Others.
Keep the above reasons in mind, and choose the most possible reason if you think the question cannot be answered
based on the text. Output the results in JSON format.
{in_context_examples}
Text: {context}
Question: {input}
Answer:
Table 8: Prompt for the failure case analysis.
892NarrativeQAYou are given a story, which can be either a novel or a movie script, and a question. Answer the question
as concisely as you can, using a single phrase if possible. Do not provide any explanation. If the question
cannot be answered based on the information in the article, write “unanswerable”. Story: {context} Now,
answer the question based on the story as concisely as you can, using a single phrase if possible. Do not
provide any explanation. If the question cannot be answered based on the information in the article, write
“unanswerable”. Question: {input} Answer:
Qasper You are given a scientiﬁc article and a question. Answer the question as concisely as you can, using a single
phrase or sentence if possible. If the question cannot be answered based on the information in the article,
write “unanswerable”. If the question is a yes/no question, answer “yes”, “no”, or “unanswerable”. Do not
provide any explanation. Article: {context} Answer the question based on the above article as concisely
as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the
information in the article, write “unanswerable”. If the question is a yes/no question, answer “yes”, “no”, or
“unanswerable”. Do not provide any explanation. Question: input Answer:
MultiFQA Read the following text and answer brieﬂy. {context} Now, answer the following question based on the above
text, only give me the answer and do not output any other words. If the question cannot be answered based
on the information in the article, write “unanswerable”. Question: {input} Answer:
HotpotQA Answer the question based on the given passages. Only give me the answer and do not output any other
words. If the question cannot be answered based on the information in the article, write “unanswerable”. The
following are given passages. {context} Answer the question based on the given passages. Only give me the
answer and do not output any other words. If the question cannot be answered based on the information in
the article, write “unanswerable”. Question: {input} Answer:
2WikiMQA Answer the question based on the given passages. Only give me the answer and do not output any other
words. If the question cannot be answered based on the information in the article, write “unanswerable”. The
following are given passages. {context} Answer the question based on the given passages. Only give me the
answer and do not output any other words. If the question cannot be answered based on the information in
the article, write “unanswerable”. Question: {input} Answer:
MuSiQue Answer the question based on the given passages. Only give me the answer and do not output any other
words. If the question cannot be answered based on the information in the article, write “unanswerable”. The
following are given passages. {context} Answer the question based on the given passages. Only give me the
answer and do not output any other words. If the question cannot be answered based on the information in
the article, write “unanswerable”. Question: {input} Answer:
QMSum You are given a meeting transcript and a query containing a question or instruction. Answer the query in
one or more sentences. If the question cannot be answered based on the information in the article, write
“unanswerable”. Transcript: {context} Now, answer the query based on the above meeting transcript in
one or more sentences. If the question cannot be answered based on the information in the article, write
“unanswerable”. Query: {input} Answer:
EN.QA Read the book and answer the question. Be very concise in your answer. If the question cannot be answered
based on the information in the article, write “unanswerable”. {context} Question: {input} Only give me the
answer and do not output any other words. If the question cannot be answered based on the information in
the article, write “unanswerable”. Answer:
EN.MC Read the book and answer the question. If the question cannot be answered based on the information in the
article, write “unanswerable”. {context} Question: {input} {all_classes} Only output the letter of the correct
answer and do not output any other words. If the question cannot be answered based on the information in
the article, write “unanswerable”. The letter of the correct answer is
Table 9: Prompts for each dataset.
893

Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 7969–7992
December 6-10, 2023 ©2023 Association for Computational Linguistics
Active Retrieval Augmented Generation
Zhengbao Jiang1∗ Frank F. Xu1∗ Luyu Gao1∗ Zhiqing Sun1∗ Qian Liu2
Jane Dwivedi-Yu3 Yiming Yang1 Jamie Callan1 Graham Neubig1
1Language Technologies Institute, Carnegie Mellon University
2Sea AI Lab 3FAIR, Meta
{zhengbaj,fangzhex,luyug,zhiqings,gneubig}@cs.cmu.edu
Abstract
Despite the remarkable ability of large lan-
guage models (LMs) to comprehend and gen-
erate language, they have a tendency to hal-
lucinate and create factually inaccurate out-
put. Augmenting LMs by retrieving informa-
tion from external knowledge resources is one
promising solution. Most existing retrieval aug-
mented LMs employ a retrieve-and-generate
setup that only retrieves information once based
on the input. This is limiting, however, in
more general scenarios involving generation
of long texts, where continually gathering in-
formation throughout generation is essential. In
this work, we provide a generalized view of ac-
tive retrieval augmented generation, methods
that actively decide when and what to retrieve
across the course of the generation. We propose
Forward-Looking Active REtrieval augmented
generation (FLARE), a generic method which
iteratively uses a prediction of the upcoming
sentence to anticipate future content, which is
then utilized as a query to retrieve relevant doc-
uments to regenerate the sentence if it contains
low-confidence tokens. We test FLARE along
with baselines comprehensively over 4 long-
form knowledge-intensive generation tasks/-
datasets. FLARE achieves superior or compet-
itive performance on all tasks, demonstrating
the effectiveness of our method.1
1 Introduction
Generative language models (LMs) (Brown et al.,
2020; Ouyang et al., 2022; OpenAI, 2023; Chowd-
hery et al., 2022; Zhang et al., 2022; Touvron et al.,
2023; Zhao et al., 2023) have become a founda-
tional component in natural language processing
(NLP) systems with their remarkable abilities. Al-
though LMs have memorized some world knowl-
edge during training (Petroni et al., 2019; Roberts
et al., 2020; Jiang et al., 2020), they still tend to
∗Lead contributors.
1Code and datasets are available athttps://github.com/
jzbjyb/FLARE.
hallucinate and create imaginary content (Maynez
et al., 2020; Zhou et al., 2021). Augmenting LMs
with retrieval components that look up relevant in-
formation from external knowledge resources is a
promising direction to address hallucination (Khan-
delwal et al., 2020; Izacard et al., 2022).
Retrieval augmented LMs commonly use a
retrieve-and-generate setup where they retrieve doc-
uments based on the user’s input, and then generate
a complete answer conditioning on the retrieved
documents (Chen et al., 2017; Guu et al., 2020;
Lewis et al., 2020; Izacard and Grave, 2021; Sachan
et al., 2021; Lee et al., 2021; Jiang et al., 2022;
Izacard et al., 2022; Nakano et al., 2021; Qian
et al., 2023; Lazaridou et al., 2022; Shi et al., 2023).
These single-time retrieval augmented LMs outper-
form purely parametric LMs, particularly for short-
form knowledge-intensive generation tasks such
as factoid question answering (QA) (Kwiatkowski
et al., 2019; Joshi et al., 2017), where the informa-
tion needs are clear in the user’s input, and it is
sufficient to retrieve relevant knowledge once solely
based on the input.
Increasingly powerful large LMs have also
demonstrated abilities in more complex tasks that
involve generating long-form output, such as long-
form QA (Fan et al., 2019; Stelmakh et al., 2022),
open-domain summarization (Cohen et al., 2021;
Hayashi et al., 2021; Giorgi et al., 2022), and
(chain-of-thought; CoT) reasoning (Wei et al.,
2022; Ho et al., 2020; Geva et al., 2021; Hendrycks
et al., 2020). In contrast to short-form generation,
long-form generation presents complex informa-
tion needs that are not always evident from the in-
put alone. Similar to how humans gradually gather
information as we create content such as papers,
essays, or books, long-form generation with LMs
would require gathering multiple pieces of knowl-
edge throughout the generation process . For ex-
ample, to generate a summary about a particular
topic, the initial retrieval based on the topic name
7969Generate a summary about Joe Biden.
Search results:   !![1]: …[2]: …
Joe Biden (born November 20, 1942) is the 46th president of the United States.Joe Biden (born November 20, 1942) is the 46th president of the United States.
He graduated from the University of Delaware in 1965 with a Bachelor of Arts in history and political science.
Joe Biden attended the University of Pennsylvania, where he earned a law degree.
Retriever
InputStep 1
Search results:   !"![1]: …[2]: …
"####
"#$
#$
$
Step 2
Joe Biden announced his candidacy for the 2020 presidential election on April 25, 2019.
Joe Biden announced his candidacy for the 2020 presidential election on August 18, 2019."#%
#%
Step 3
Search results:   !""[1]: …[2]: …Retrieveddocuments
LM
Generation
$
%$
%%
Figure 1: An illustration of forward-looking active retrieval augmented generation (FLARE). Starting with the user
input x and initial retrieval results Dx, FLARE iteratively generates a temporary next sentence (shown in gray
italic) and check whether it contains low-probability tokens (indicated with underline). If so (step 2 and 3), the
system retrieves relevant documents and regenerates the sentence.
(e.g., Joe Biden) may not cover all aspects and de-
tails. It is crucial to retrieve extra information as
needed during generation, such as when generat-
ing a certain aspect (e.g., Joe Biden’s education
history) or a specific detail (e.g., the date of Joe
Biden’s presidential campaign announcement).
Several attempts have been made to retrieve mul-
tiple times throughout generation. These attempts
include methods that passively use the past context
to retrieve additional information at a fixed interval
(Khandelwal et al., 2020; Borgeaud et al., 2022;
Ram et al., 2023; Trivedi et al., 2022) which might
not accurately reflect what LMs intend to gener-
ate in the future or retrieve at inappropriate points.
Some works in multihop QA decompose the full
question into sub-questions, each of which is used
to retrieve extra information (Press et al., 2022; Yao
et al., 2022; Khot et al., 2022; Khattab et al., 2022).
We ask the following question: can we create a
simple and generic retrieval augmented LM thatac-
tively decides when and what to retrievethroughout
the generation process, and are applicable to a va-
riety of long-form generation tasks? We provide a
generalized view of active retrieval augmented gen-
eration. Our hypothesis regarding when to retrieve
is that LMs should retrieve information only when
they lack the required knowledge to avoid unneces-
sary or inappropriate retrieval that occurs in passive
retrieval augmented LMs (Khandelwal et al., 2020;
Borgeaud et al., 2022; Ram et al., 2023; Trivedi
et al., 2022). Given the observation that large LMs
tend to be well-calibrated and low probability/con-
fidence often indicates a lack of knowledge (Ka-
davath et al., 2022), we adopt an active retrieval
strategy that only retrieves when LMs generate low-
probability tokens. When deciding what to retrieve,
it is important to consider what LMs intend to gen-
erate in the future, as the goal of active retrieval is to
benefit future generations. Therefore, we propose
anticipating the future by generating a temporary
next sentence, using it as a query to retrieve rel-
evant documents, and then regenerating the next
sentence conditioning on the retrieved documents.
Combining the two aspects, we propose Forward-
Looking Active REtrieval augmented generation
(FLARE), as illustrated in Figure 1. FLARE iter-
atively generates a temporary next sentence, use
it as the query to retrieve relevant documents if it
contains low-probability tokens and regenerate the
next sentence until reaches the end.
FLARE is applicable to any existing LMs at
inference time without additional training. Con-
7970sidering the impressive performance achieved by
GPT-3.5 (Ouyang et al., 2022) on a variety of
tasks, we examine the effectiveness of our meth-
ods on text-davinci-003. We evaluate FLARE
on 4 diverse tasks/datasets involving generating
long outputs, including multihop QA (2WikiMul-
tihopQA), commonsense reasoning (StrategyQA),
long-form QA (ASQA), and open-domain summa-
rization (WikiAsp) (Ho et al., 2020; Geva et al.,
2021; Stelmakh et al., 2022; Hayashi et al., 2021).
Over all tasks, FLARE achieves superior or com-
petitive performance compared to single-time and
multi-time retrieval baselines, demonstrating the
effectiveness and generalizability of our method.
2 Retrieval Augmented Generation
We formally define single-time retrieval augmented
generation and propose the framework of active
retrieval augmented generation.
2.1 Notations and Definitions
Given a user input x and a document corpus D=
{di}|D|
i=1 (such as all Wikipedia articles), the goal of
retrieval augmented LMs is to generate the answer
y = [s1,s2,..., sm] = [w1,w2,...,w n] containing
m sentences or n tokens leveraging information
retrieved from the corpus.
In retrieval augmented LM, the LM typically
pairs with a retriever that can retrieve a list of
documents Dq = ret(q) for a query q; the LM
conditions on both the user input x and retrieved
documents Dq to generate the answer. Since we
focus on examining various methods of determin-
ing when and what to retrieve, we follow exist-
ing methods (Ram et al., 2023; Trivedi et al.,
2022) to prepend the retrieved documents before
the user input to aid future generation for both
baselines and our method for fair comparisons:
y = LM([Dq,x]), where [·,·] is concatenation fol-
lowing the specified order.
2.2 Single-time Retrieval Augmented
Generation
The most common choice is to directly use the user
input as the query for retrieval and generate the
complete answer at once y = LM([Dx,x]).
2.3 Active Retrieval Augmented Generation
To aid long-form generation with retrieval, we pro-
pose active retrieval augmented generation. It is a
generic framework that actively decides when and
what to retrieve through the generation process,
resulting in the interleaving of retrieval and genera-
tion. Formally, at step t(t≥1), the retrieval query
qt is formulated based on both the user input x and
previously generated output y<t = [y0,..., yt−1]:
qt = qry(x,y<t),
where qry(·) is the query formulation function. At
the beginning (t = 1), the previous generation is
empty (y<1 = ∅), and the user input is used as the
initial query (q1 = x). Given retrieved documents
Dqt, LMs continually generate the answer until the
next retrieval is triggered or reaches the end:
yt = LM([Dqt,x,y<t]),
where yt represents the generated tokens at the cur-
rent step t, and the input to LMs is the concatena-
tion of the retrieved documents Dqt, the user input
x, and the previous generation y<t. We discard
previously retrieved documents ∪t′<tDqt′ and only
use the retrieved documents from the current step
to condition the next generation to prevent reaching
the input length limit of LMs.
3 FLARE: Forward-Looking Active
REtrieval Augmented Generation
Our intuition is that (1) LMs should only retrieve
information when they do not have the necessary
knowledge to avoid unnecessary or inappropriate
retrieval, and (2) the retrieval queries should reflect
the intents of future generations. We propose two
forward-looking active retrieval augmented gener-
ation (FLARE) methods to implement the active
retrieval augmented generation framework. The
first method prompts the LM to generate retrieval
queries when necessary while generating the an-
swer using retrieval-encouraging instructions, de-
noted as FLAREinstruct. The second method directly
uses the LM’s generation as search queries, denoted
as FLAREdirect, which iteratively generates the next
sentence to gain insight into the future topic, and
if uncertain tokens are present, retrieves relevant
documents to regenerate the next sentence.
3.1 FLARE with Retrieval Instructions
Inspired by Toolformer (Schick et al., 2023), a
straightforward way of expressing information
needs for retrieval is to generate “[Search(query)]”
when additional information is needed (Schick
et al., 2023), e.g., “The colors on the flag of
Ghana have the following meanings. Red is for
[Search(Ghana flag red meaning)] the blood of mar-
tyrs, ...” When working with GPT-3.5 models that
7971Search results:   !![1]: …[2]: …
Joe Biden attended
Search results:   !"![1]: …[2]: …Search results:   !""[1]: …[2]: …
[Search(Joe Biden University)]
[Search(Joe Biden degree)]the University of Pennsylvania, where he earned
a law degree.
Generate a summary about Joe Biden.Input$
&$
&#%$
&%%%Generation
Retriever
$
%$
%%
Figure 2: An illustration of forward-looking active re-
trieval augmented generation with retrieval instructions
(FLAREinstruct). It iteratively generates search queries
(shown in gray italic) to retrieve relevant information to
aid future generations.
offer only API access, we elicit such behavior by
few-shot prompting (Brown et al., 2020).
Specifically, for a downstream task, we place
the search-related instruction and exemplars at the
beginning as skill 1, followed by the instruction and
exemplars of the downstream task as skill 2. Given
a test case, we ask LMs to combine skills 1 and 2 to
generate search queries while performing the task.
The structure of the prompt is shown in Prompt 3.1,
and full details can be found in Prompt D.3.
Prompt 3.1: retrieval instructions
Skill 1. An instruction to guide LMs to generate search
queries.
Several search-related exemplars.
Skill 2. An instruction to guide LMs to perform a
specific downstream task (e.g., multihop QA).
Several task-related exemplars.
An instruction to guide LMs to combine skills 1
and 2 for the test case.
The input of the test case.
As shown in Figure 2, when the LM generates
“[Search(query)]” (shown in gray italic), we stop
the generation and use the query terms to retrieve
relevant documents, which are prepended before
the user input to aid future generation until the
next search query is generated or reaches the end.
Additional implementation details are included in
Appendix A.
3.2 Direct FLARE
Since we cannot fine-tune black-box LMs, we
found queries generated by FLAREinstruct through
retrieval instructions might not be reliable. There-
fore, we propose a more direct way of forward-
looking active retrieval that uses the next sentence
to decide when and what to retrieve.
3.2.1 Confidence-based Active Retrieval
As shown in Figure 1, at step t, we first generate a
temporary next sentence ˆst = LM([x,y<t]) with-
out conditioning on retrieved documents. Then we
decide whether to trigger retrieval and formulate
queries based on ˆst. If the LM is confident aboutˆst,
we accept it without retrieving additional informa-
tion; if not, we use ˆst to formulate search queries
qt to retrieve relevant documents, and then regen-
erate the next sentence st. The reason we utilize
sentences as the basis of our iteration is due to their
significance as semantic units that are neither too
short nor too lengthy like phrases and paragraphs.
However, our approach can also utilize phrases or
paragraphs as the basis.
Since LMs tend to be well-calibrated that low
probability/confidence often indicates a lack of
knowledge (Jiang et al., 2021; Kadavath et al.,
2022; Varshney et al., 2022), we actively trigger
retrieval if any token of ˆst has a probability lower
than a threshold θ ∈[0,1]. θ = 0means retrieval
is never triggered, while θ = 1triggers retrieval
every sentence.
yt =
{
ˆst if all tokens of ˆst have probs ≥θ
st = LM([Dqt,x,y<t]) otherwise
where the query qt is formulated based on ˆst.
3.2.2 Confidence-based Query Formulation
One way to perform retrieval is to directly use the
next sentence ˆst as the query qt. This shares a sim-
ilar spirit with methods that use generated hypo-
thetical titles or paragraphs from LMs as retrieval
queries or evidences (Gao et al., 2022; Sun et al.,
2022; Yu et al., 2022; Mao et al., 2021). We gen-
eralize such techniques to long-form generation
where active information access is essential.
We found retrieving with the next sentence
achieves significantly better results than with the
previous context, as shown later in subsection 6.2.
However, it has a risk of perpetuating errors con-
tained in it. For example, if the LM produces the
sentence “Joe Biden attended the University of
Pennsylvania” instead of the correct fact that he
attended the University of Delaware, using this er-
roneous sentence as a query might retrieve mislead-
7972Joe Biden attended the University of Pennsylvania, where he earned a law degree.
Ask a question to which the answer is “the University of Pennsylvania”Ask a question to which the answer is “a law degree”
What university did Joe Biden attend?What degree did Joe Biden earn?
implicit query by maskingexplicit query by question generationJoe Biden attended  , where he earned  .
LM such as ChatGPT
Figure 3: Implicit and explicit query formulation. To-
kens with low probabilities are marked with underlines.
ing information. We propose two simple methods
to overcome this issue as illustrated in Figure 3.
Masked sentences as implicit queries. The first
method masks out low-confidence tokens inˆst with
probabilities below a threshold β ∈[0,1], where a
higher βresults in more aggressive masking. This
removes potential distractions from the sentence to
improve retrieval accuracy.
Generated questions as explicit queries. An-
other method is to generate explicit questions that
target the low-confident span in ˆst. For example, if
the LM is uncertain about “the University of Penn-
sylvania”, a question like “Which university did
Joe Biden attend?” can help retrieve relevant in-
formation. Self-ask (Press et al., 2022) achieved
this by manually inserting follow-up questions
into downstream task exemplars as shown later
in Prompt D.2, which requires task-specific annota-
tion efforts. Instead, we developed a universal ap-
proach that generates questions for low-confidence
spans without additional annotation. Specifically,
We first extract all spans from ˆst with probabilities
below β. For each extracted span z, we prompt
gpt-3.5-turbo to generate a question qt,z that
can be answered with the span:
Prompt 3.2: zero-shot question generation
User input x.
Generated output so far y≤t.
Given the above passage, ask a question to which
the answer is the term/entity/phrase “z”.
We retrieve using each generated question and
interleave the returned documents into a single
ranking list to aid future generations. In summary,
queries qt are formulated based on ˆst as follows:
qt =
{
∅ if all tokens of ˆst have probs ≥θ
mask(ˆst) or qgen(ˆst) otherwise
3.3 Implementation Details
Base LM We validate our method on one of the
most advanced GPT-3.5 LMs text-davinci-003
by iteratively querying their API.2
Document corpus and retrievers. Since we fo-
cus on the integration of retrieval and generation,
we use off-the-shelf retrievers that take queries
as inputs and return a list of relevant documents.
For datasets that mainly rely on knowledge from
Wikipedia, we use the Wikipedia dump from
Karpukhin et al. (2020) and employ BM25 (Robert-
son and Zaragoza, 2009) as the retriever. For
datasets that rely on knowledge from the open web,
we use the Bing search engine as our retriever.3
Retrieved document formatting. Multiple re-
trieved documents are linearized according to their
ranking and then added to the beginning of the user
input using Prompt D.1.
Other implementation details such as sentence to-
kenization and efficiency are included Appendix A.
4 Multi-time Retrieval Baselines
Existing passive multi-time retrieval augmented
LMs can also be formulated using our framework
(subsection 2.3). In this section, we formally in-
troduce three baseline categories based on when
and what to retrieve. These baselines are not exact
reproductions of the corresponding paper because
many design choices differ which makes direct
comparisons impossible. We implemented them
using the same settings, with the only variation
being when and what to retrieve.
Previous-window approaches trigger retrieval
every ltokens, where lrepresents the window size.
Generated tokens from the previous window are
used as the query:
qt = yt−1 (t≥2),
yt = [w(t−1)l+1,...,w tl].
Some existing methods in this category are RETRO
(Borgeaud et al., 2022), IC-RALM (Ram et al.,
2https://api.openai.com/v1/completions April 23.
3https://www.microsoft.com/en-us/bing/apis/
bing-web-search-api
79732023), which retrieve every few tokens, and KNN-
LM (Khandelwal et al., 2020), which retrieves ev-
ery token.4 We follow Ram et al. (2023) to use a
window size of l= 16.
Previous-sentence approaches trigger retrieval
every sentence and use the previous sentence as the
query, and IRCoT (Trivedi et al., 2022) belongs to
this category:
qt = yt−1 (t≥2),
yt = st.
Question decomposition approaches manually
annotated task-specific exemplars to guide LMs
to generate decomposed sub-questions while pro-
ducing outputs. For example, self-ask (Press et al.,
2022), a method in this category, manually inserts
sub-questions in exemplars using Prompt D.2. For
the test case, retrieval is triggered dynamically
whenever the model generates a sub-question.
The aforementioned approaches can retrieve ad-
ditional information while generating. However,
they have notable drawbacks: (1) Using previously
generated tokens as queries might not reflect what
LMs intend to generate in the future. (2) Retriev-
ing information at a fixed interval can be inefficient
because it might occur at inappropriate points. (3)
Question decomposition approaches require task-
specific prompt engineering, which restricts their
generalizability in new tasks.
5 Experimental Setup
We evaluate the effectiveness of FLARE on 4 di-
verse knowledge-intensive tasks using few-shot in-
context learning (Radford et al., 2019; Brown et al.,
2020; Liu et al., 2023). We follow previous works
(Trivedi et al., 2022) to sub-sample at most 500
examples from each dataset due to the cost of run-
ning experiments. Datasets, metrics, and settings
are summarized in Table 7 of Appendix B. The
hyperparameters of FLARE are selected based on
the development set and listed in Table 9. FLARE
refers to FLAREdirect if not specifically stated.
Multihop QA The goal of multihop QA is to
answer complex questions through information re-
trieval and reasoning. We use 2WikiMultihopQA
(Ho et al., 2020) which contains 2-hop complex
4Since KNN-LM uses the contextualized representation
corresponding to the current decoding position to retrieve rel-
evant information which encodes all previous tokens. Strictly
speaking, qt should be y<t.
questions sourced from Wikipedia articles that re-
quire composition, comparison, or inference, e.g.,
“Why did the founder of Versus die?” We follow
Wang et al. (2022) to generate both the chain-of-
thought and the final answer. Experimental setting
details are included in Appendix B.
We use regular expressions to extract the final
answer from the output and compare it with the ref-
erence answer using exact match (EM), and token-
level F1, precision, and recall.
Commonsense reasoning Commonsense reason-
ing requires world and commonsense knowledge
to generate answers. We use StrategyQA (Geva
et al., 2021) which is a collection of crowdsourced
yes/no questions, e.g., “Would a pear sink in wa-
ter?” We follow Wei et al. (2022) to generate both
the chain-of-thought and the final yes/no answer.
Details are included in Appendix B.
We extract the final answer and match it against
the gold answer using exact match.
Long-form QA Long-form QA aims to generate
comprehensive answers to questions seeking com-
plex information (Fan et al., 2019; Stelmakh et al.,
2022). We use ASQA (Stelmakh et al., 2022) as our
testbed where inputs are ambiguous questions with
multiple interpretations, and outputs should cover
all of them. For example, “Where do the Philadel-
phia Eagles play their home games?” could be
asking about the city, sports complex, or stadium.
We found in many cases it is challenging even for
humans to identify which aspect of the question
is ambiguous. Therefore, we created another set-
ting (ASQA-hint) where we provide a brief hint
to guide LMs to stay on track when generating an-
swers. The hint for the above case is “This question
is ambiguous in terms of which specific location or
venue is being referred to.” Experimental setting
details are included in Appendix B.
We use metrics from Stelmakh et al. (2022), in-
cluding EM, RoBERTa-based QA score (Disambig-
F1), ROUGE (Lin, 2004), and an overall score com-
bining Disambig-F1 and ROUGE (DR).
Open-domain summarization The goal of open-
domain summarization is to generate a comprehen-
sive summary about a topic by gathering informa-
tion from open web (Giorgi et al., 2022). We use
WikiAsp (Hayashi et al., 2021) which aims to gen-
erate aspect-based summaries about entities from
20 domains in Wikipedia, e.g., “Generate a sum-
mary about Echo School (Oregon) including the
79740.0
20.0
40.0
60.0
80.0
2WikiMultihopQA StrategyQA ASQA ASQA-hint WikiAsp
No ret. Single-time ret. Previous-window ret. Forward-Looking Active REtrieval augmented generation (FLARE)
Figure 4: Comparision between FLARE and baselines across all tasks/datasets. We report the primary metric for
each dataset: EM for 2WikiMultihopQA, StrategyQA, and ASQA, and UniEval for WikiAsp.
following aspects: academics, history.” Experimen-
tal setting details are included in Appendix B.
Metrics include ROUGE, named entity-based F1,
and UniEval (Zhong et al., 2022) which measures
factual consistency.
6 Experimental Results
We first report overall results across 4 tasks/datasets
and compare the performance of FLARE with all
the baselines introduced in section 4. We then
run ablation experiments to study the efficacy of
various design choices of our method.
6.1 Comparison with Baselines
Overall results. The overall performance of
FLARE and baseline across all tasks/datasets are
reported in Figure 4. FLARE outperforms all base-
line on all tasks/datasets, indicating that FLARE
is a generic method that can effectively retrieve
additional information throughout the generation.
Among various tasks, multihop QA shows the
most significant improvement. This is largely due
to the task’s clear definition and specific objective
of producing the final answer through a 2-hop rea-
soning process, which makes it easier for LMs to
generate on-topic output. In contrast, ASQA and
WikiAsp are more open-ended, which increases the
difficulty of both generation and evaluation. The
improvement on ASQA-hint is larger than that of
ASQA because identifying ambiguous aspects is
challenging even for humans in many cases, and
providing a generic hint helps LMs to stay on topic.
Thorough comparisons with baselines. The per-
formance of all baselines on 2WikiMultihopQA
are reported in Table 1. FLARE outperforms all
baselines by a large margin, which confirms that
forward-looking active retrieval is highly effective.
Most multi-time retrieval augmented approaches
outperform single-time retrieval but with different
Methods EM F 1 Prec. Rec.
No retrieval 28.2 36.8 36.5 38.6
Single-time retrieval 39.4 48.8 48.6 51.5
Multi-time retrieval
Previous-window 43.2 52.3 51.7 54.5
Previous-sentence 39.0 49.2 48.9 51.8
Question decomposition 47.8 56.4 56.1 58.6
FLAREinstruct (ours) 42.4 49.8 49.1 52.5
FLAREdirect (ours) 51.0 59.7 59.1 62.6
Table 1: FLARE and baselines on 2WikiMultihopQA.
Previous-window (Borgeaud et al., 2022; Ram et al.,
2023), previous-sentence (Trivedi et al., 2022), and ques-
tion decomposition (Press et al., 2022; Yao et al., 2022)
methods are reimplemented for fair comparisons.
margins. The improvement of retrieving using the
previous sentence is relatively small which we hy-
pothesize is mainly because the previous sentence
often describes entities or relations different from
those in the next sentence in 2WikiMultihopQA.
While the previous-window approach might use
the first half of a sentence to retrieve information
potentially helpful for generating the second half.
Among all baselines, the question decomposition
approach (Press et al., 2022) achieves the best per-
formance. which is not surprising since the in-
context exemplars manually annotated with decom-
posed sub-questions (Prompt D.2) guide LMs to
generate sub-questions that align with the topic/in-
tent of future generations. FLARE outperforms
this baseline, indicating that manual exemplar an-
notation is not necessary for effective future-aware
retrieval. The gap between FLAREinstruct and ques-
tion decomposition is large, indicating that teaching
LMs to generate search queries using task-generic
retrieval instructions and exemplars is challenging.
We report all metrics for the other datasets in
Table 2. FLARE outperforms baselines with re-
spect to all metrics. Retrieval using the previ-
7975Datasets StrategyQA ASQA ASQA-hint WikiAsp
Metrics EM EM D-F 1 R-L DR EM D-F 1 R-L DR UniEval E-F1 R-L
No retrieval 72.9 33.8 24.2 33.3 28.4 40.1 32.5 36.4 34.4 47.1 14.1 26.4
Single-time retrieval 68.6 40.0 27.1 34.0 30.4 43.2 34.8 37.4 36.0 52.4 17.4 26.9
Multi-time retrieval
Previous-window 71.2 39.9 27.0 34.3 30.4 43.7 35.7 37.5 36.6 51.8 18.1 27.3
Previous-sentence 71.0 39.9 27.9 34.3 30.9 44.7 35.9 37.5 36.7 52.6 17.8 27.2
FLARE (ours) 77.3 41.3 28.2 34.3 31.1 46.2 36.7 37.7 37.2 53.4 18.9 27.6
Table 2: Comparison between FLARE and baselines on StrategyQA, ASQA, ASQA-hint, and WikiAsp. D-F1 is
Disambig-F1, R-L is ROUGE-L, and E-F1 is named entity-based F1.
2WikiMultihopQA ASQA-hint
EM F 1 Prec. Rec. EM D-F 1 R-L DR
Previous 39.0 49.2 48.9 51.8 42.5 34.1 36.9 35.5
Next 48.8 57.6 57.1 60.5 45.9 35.7 37.5 36.6
Table 3: A head-to-head comparison between using the
previous sentence and the next sentence for retrieval.
#Tokens EM F 1 Prec. Rec.
16 43.2 52.3 51.7 54.5
32 43.6 52.4 52.0 55.0
48 40.0 49.3 49.0 52.0
All 39.0 48.5 48.2 51.1
Table 4: Previous-window approaches using different
numbers of tokens as queries.
ous window underperforms single-time retrieval
on ASQA, which we hypothesize is because the
previous window does not accurately reflect future
intent. Since we focus on evaluating factuality, met-
rics with an emphasis on factual content (such as
EM, Disambig-F1, UniEval) are more reliable than
metrics computed over all tokens (ROUGE-L).
6.2 Ablation Study
Importance of forward-looking retrieval. We
first validate that forward-looking retrieval is more
effective than past-context-based retrieval. We run
ablation experiments on 2WikiMultihopQA and
ASQA-hint comparing retrieval using the previ-
ous versus the next sentence. Specifically, both
methods retrieve every sentence and directly use
the complete previous/next sentence as queries. As
shown in Table 3, using the next sentence to retrieve
is clearly better than using the previous sentence,
confirming our hypothesis.
We also run previous-window approaches using
different numbers of past tokens as queries. As
shown in Table 4, using too many tokens (>32) in
%steps/sentences with retrieval
0.0
20.0
40.0
60.0
80.0
0.0 25.0 50.0 75.0 100.0
2WikiMultihopQA StrategyQA
Figure 5: Performance (EM) of FLARE with respect
to the percentage of steps/sentences with retrieval on
2WikiMultihopQA and StrategyQA.
the past hurts the performance, further confirming
our hypothesis that previous context might not be
relevant to intent of future generations.
Importance of active retrieval. Next, we inves-
tigate how active retrieval threshold θaffects per-
formance. To alter our method from not retrieving
to retrieving every sentence, we adjust the confi-
dence threshold θthat determines when to trigger
retrieval from 0 to 1. We then calculate the pro-
portion of steps/sentences where retrieval is acti-
vated, and present the performance based on it. As
shown in Figure 5, on 2WikiMultihopQA, the per-
formance plateaus when the retrieval percentage
exceeds 60%, indicating that retrieval when LMs
are confident is not necessary. On StrategyQA, the
performance drops when the retrieval percentage
exceeds 50%, indicating that unnecessary retrieval
can introduce noise and impede the original gen-
eration process. We found triggering retrieval for
40%-80% of sentences usually leads to a good per-
formance across tasks/datasets.
Effectiveness of different query formulation
methods We study implicit query formation by
masking and explicit query formulation through
question generation. In Table 5, we compare the
performance of FLARE with different masking
7976β EM F 1 Prec. Rec.
0.0 0.488 0.576 0.571 0.605
0.2 0.498 0.588 0.582 0.616
0.4 0.510 0.597 0.591 0.627
0.6 0.506 0.593 0.586 0.622
Table 5: Performance of FLARE with respect to the
masking threshold βon 2WikiMultihopQA.
ASQA-hint WikiAsp
EM D-F 1 R-L DR UniEval E-F 1 R-L
Implicit 45.7 36.9 37.7 37.3 53.4 18.8 27.7
Explicit 46.2 36.7 37.7 37.2 53.4 18.9 27.6
Table 6: A comparison between implicit and explicit
query formulation methods in FLARE.
thresholds β. Retrieving directly with the complete
sentence (β = 0) is worse than masking tokens
with low probabilities, confirming our hypothesis
that low-confidence erroneous tokens can distract
retrievers. We compare implicit and explicit query
formulation methods in Table 6. Performances of
both methods are similar, indicating that both meth-
ods can effectively reflect information needs.
7 Related Work
We refer to subsection 2.2 and section 4 for ex-
tensively discussion on single-time and multi-time
retrieval augmented LMs, which is the most rele-
vant area to this paper.
Iterative and adaptive retrieval Iterative re-
trieval and refinement has been studied in both
text and code generation tasks (Peng et al., 2023;
Zhang et al., 2023; Zemlyanskiy et al., 2022; Yu
et al., 2023). FLARE differs from these methods in
the granularity of generation and retrieval strategies.
Adaptive retrieval has been studied in single-time
retrieval scenarios based on either question pop-
ularity or generation probabilities (Mallen et al.,
2022; Li et al., 2023), while we focus on long-form
generation requiring active information access.
Browser-enhanced LMs WebGPT (Nakano
et al., 2021) and WebCPM (Qin et al., 2023) train
LMs to interact with browser to enhance factuality
using reinforcement learning or supervised train-
ing where multiple queries can be triggered before
generation. FLARE is built on text-based retrievers
but can be combined with a browser to potentially
improve retrieval quality.
8 Conclusion
To aid long-form generation with retrieval aug-
mentation, we propose an active retrieval aug-
mented generation framework that decides when
and what to retrieve during generation. We imple-
ment this framework with forward-looking active
retrieval that iteratively uses the upcoming sentence
to retrieve relevant information if it contains low-
confidence tokens and regenerates the next sen-
tence. Experimental results on 4 tasks/datasets
demonstrate the effectiveness of our methods. Fu-
ture directions include better strategies for active
retrieval and developing efficient LM architectures
for active information integration.
9 Limitations
We also conduct experiments on Wizard of
Wikipedia (Dinan et al., 2019) and ELI5 (Fan et al.,
2019), and found that FLARE did not provide sig-
nificant gains. Wizard of Wikipedia is a knowledge-
intensive dialogue generation dataset where the out-
put is relatively short (∼20 tokens on average) so
retrieving multiple disparate pieces of information
might not be necessary. ELI5 (Fan et al., 2019)
is a long-form QA dataset requiring in-depth an-
swers to open-ended questions. Due to issues men-
tioned in Krishna et al. (2021) such as difficulties
of grounding generation in retrieval and evalua-
tion, both single-time retrieval and FLARE did not
provide significant gains over not using retrieval.
From an engineering perspective, interleaving gen-
eration and retrieval with a naive implementation
increases both overheads and the cost of generation.
LMs need to be activated multiple times (once for
each retrieval) and a caching-free implementation
also requires recomputing the previous activation
each time after retrieval. This issue can be poten-
tially alleviated with special architectural designs
that encode the retrieved documents Dqt and the
input/generation (x/y<t) independently.
Acknowledgements
This work was supported in part by a grant from
the Singapore Defence Science and Technology
Agency and the IBM PhD Fellowship. We thank
Chunting Zhou, Amanda Bertsch, Uri Alon, Hi-
roaki Hayashi, Harsh Trivedi, Patrick Lewis, Timo
Schick, Kaixin Ma, Shuyan Zhou, and Songwei Ge
for their insightful discussions and help with the
experiments.
7977References
Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,
Trevor Cai, Eliza Rutherford, Katie Millican, George
van den Driessche, Jean-Baptiste Lespiau, Bogdan
Damoc, Aidan Clark, Diego de Las Casas, Aurelia
Guy, Jacob Menick, Roman Ring, Tom Hennigan,
Saffron Huang, Loren Maggiore, Chris Jones, Albin
Cassirer, Andy Brock, Michela Paganini, Geoffrey
Irving, Oriol Vinyals, Simon Osindero, Karen Si-
monyan, Jack W. Rae, Erich Elsen, and Laurent Sifre.
2022. Improving language models by retrieving from
trillions of tokens. In International Conference on
Machine Learning, ICML 2022, 17-23 July 2022, Bal-
timore, Maryland, USA, volume 162 of Proceedings
of Machine Learning Research , pages 2206–2240.
PMLR.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, Sandhini Agarwal, Ariel Herbert-V oss,
Gretchen Krueger, Tom Henighan, Rewon Child,
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
Clemens Winter, Christopher Hesse, Mark Chen, Eric
Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei.
2020. Language models are few-shot learners. In Ad-
vances in Neural Information Processing Systems 33:
Annual Conference on Neural Information Process-
ing Systems 2020, NeurIPS 2020, December 6-12,
2020, virtual.
Danqi Chen, Adam Fisch, Jason Weston, and Antoine
Bordes. 2017. Reading wikipedia to answer open-
domain questions. In Proceedings of the 55th Annual
Meeting of the Association for Computational Lin-
guistics, ACL 2017, Vancouver, Canada, July 30 -
August 4, Volume 1: Long Papers, pages 1870–1879.
Association for Computational Linguistics.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton,
Sebastian Gehrmann, Parker Schuh, Kensen Shi,
Sasha Tsvyashchenko, Joshua Maynez, Abhishek
Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-
odkumar Prabhakaran, Emily Reif, Nan Du, Ben
Hutchinson, Reiner Pope, James Bradbury, Jacob
Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,
Toju Duke, Anselm Levskaya, Sanjay Ghemawat,
Sunipa Dev, Henryk Michalewski, Xavier Garcia,
Vedant Misra, Kevin Robinson, Liam Fedus, Denny
Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim,
Barret Zoph, Alexander Spiridonov, Ryan Sepassi,
David Dohan, Shivani Agrawal, Mark Omernick, An-
drew M. Dai, Thanumalayan Sankaranarayana Pil-
lai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,
Rewon Child, Oleksandr Polozov, Katherine Lee,
Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark
Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy
Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,
and Noah Fiedel. 2022. Palm: Scaling language mod-
eling with pathways. CoRR, abs/2204.02311.
Nachshon Cohen, Oren Kalinsky, Yftah Ziser, and
Alessandro Moschitti. 2021. Wikisum: Coherent
summarization dataset for efficient human-evaluation.
In Proceedings of the 59th Annual Meeting of the As-
sociation for Computational Linguistics and the 11th
International Joint Conference on Natural Language
Processing, ACL/IJCNLP 2021, (Volume 2: Short
Papers), Virtual Event, August 1-6, 2021, pages 212–
219. Association for Computational Linguistics.
Emily Dinan, Stephen Roller, Kurt Shuster, Angela
Fan, Michael Auli, and Jason Weston. 2019. Wizard
of wikipedia: Knowledge-powered conversational
agents. In 7th International Conference on Learning
Representations, ICLR 2019, New Orleans, LA, USA,
May 6-9, 2019. OpenReview.net.
Angela Fan, Yacine Jernite, Ethan Perez, David Grang-
ier, Jason Weston, and Michael Auli. 2019. ELI5:
long form question answering. In Proceedings of
the 57th Conference of the Association for Compu-
tational Linguistics, ACL 2019, Florence, Italy, July
28- August 2, 2019, Volume 1: Long Papers, pages
3558–3567. Association for Computational Linguis-
tics.
Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan.
2022. Precise zero-shot dense retrieval without rele-
vance labels. CoRR, abs/2212.10496.
Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,
Dan Roth, and Jonathan Berant. 2021. Did aristotle
use a laptop? a question answering benchmark with
implicit reasoning strategies. Transactions of the
Association for Computational Linguistics , 9:346–
361.
John M. Giorgi, Luca Soldaini, Bo Wang, Gary D.
Bader, Kyle Lo, Lucy Lu Wang, and Arman Co-
han. 2022. Exploring the challenges of open
domain multi-document summarization. CoRR,
abs/2212.10526.
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-
pat, and Ming-Wei Chang. 2020. REALM: retrieval-
augmented language model pre-training. CoRR,
abs/2002.08909.
Hiroaki Hayashi, Prashant Budania, Peng Wang, Chris
Ackerson, Raj Neervannan, and Graham Neubig.
2021. Wikiasp: A dataset for multi-domain aspect-
based summarization. Trans. Assoc. Comput. Lin-
guistics, 9:211–225.
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
2020. Measuring massive multitask language under-
standing. CoRR, abs/2009.03300.
Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara,
and Akiko Aizawa. 2020. Constructing A multi-hop
QA dataset for comprehensive evaluation of reason-
ing steps. In Proceedings of the 28th International
Conference on Computational Linguistics, COLING
2020, Barcelona, Spain (Online), December 8-13,
79782020, pages 6609–6625. International Committee on
Computational Linguistics.
Gautier Izacard and Edouard Grave. 2021. Leveraging
passage retrieval with generative models for open do-
main question answering. In Proceedings of the 16th
Conference of the European Chapter of the Associ-
ation for Computational Linguistics: Main Volume,
EACL 2021, Online, April 19 - 23, 2021, pages 874–
880. Association for Computational Linguistics.
Gautier Izacard, Patrick S. H. Lewis, Maria Lomeli,
Lucas Hosseini, Fabio Petroni, Timo Schick, Jane
Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and
Edouard Grave. 2022. Few-shot learning with
retrieval augmented language models. CoRR,
abs/2208.03299.
Zhengbao Jiang, Jun Araki, Haibo Ding, and Graham
Neubig. 2021. How can we know When language
models know? on the calibration of language mod-
els for question answering. Trans. Assoc. Comput.
Linguistics, 9:962–977.
Zhengbao Jiang, Luyu Gao, Jun Araki, Haibo Ding,
Zhiruo Wang, Jamie Callan, and Graham Neubig.
2022. Retrieval as attention: End-to-end learning
of retrieval and reading within a single transformer.
CoRR, abs/2212.02027.
Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham
Neubig. 2020. How can we know what language
models know. Trans. Assoc. Comput. Linguistics ,
8:423–438.
Mandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke
Zettlemoyer. 2017. Triviaqa: A large scale distantly
supervised challenge dataset for reading comprehen-
sion. In Proceedings of the 55th Annual Meeting of
the Association for Computational Linguistics, ACL
2017, Vancouver, Canada, July 30 - August 4, Volume
1: Long Papers, pages 1601–1611. Association for
Computational Linguistics.
Saurav Kadavath, Tom Conerly, Amanda Askell, Tom
Henighan, Dawn Drain, Ethan Perez, Nicholas
Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli
Tran-Johnson, Scott Johnston, Sheer El Showk, Andy
Jones, Nelson Elhage, Tristan Hume, Anna Chen,
Yuntao Bai, Sam Bowman, Stanislav Fort, Deep
Ganguli, Danny Hernandez, Josh Jacobson, Jack-
son Kernion, Shauna Kravec, Liane Lovitt, Ka-
mal Ndousse, Catherine Olsson, Sam Ringer, Dario
Amodei, Tom Brown, Jack Clark, Nicholas Joseph,
Ben Mann, Sam McCandlish, Chris Olah, and Jared
Kaplan. 2022. Language models (mostly) know what
they know. CoRR, abs/2207.05221.
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick
S. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen,
and Wen-tau Yih. 2020. Dense passage retrieval for
open-domain question answering. In Proceedings of
the 2020 Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP 2020, Online,
November 16-20, 2020, pages 6769–6781. Associa-
tion for Computational Linguistics.
Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke
Zettlemoyer, and Mike Lewis. 2020. Generalization
through memorization: Nearest neighbor language
models. In 8th International Conference on Learning
Representations, ICLR 2020, Addis Ababa, Ethiopia,
April 26-30, 2020. OpenReview.net.
Omar Khattab, Keshav Santhanam, Xiang Lisa Li,
David Hall, Percy Liang, Christopher Potts, and
Matei Zaharia. 2022. Demonstrate-search-predict:
Composing retrieval and language models for
knowledge-intensive NLP. CoRR, abs/2212.14024.
Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu,
Kyle Richardson, Peter Clark, and Ashish Sabharwal.
2022. Decomposed prompting: A modular approach
for solving complex tasks. CoRR, abs/2210.02406.
Kalpesh Krishna, Aurko Roy, and Mohit Iyyer. 2021.
Hurdles to progress in long-form question answering.
In North American Association for Computational
Linguistics.
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-
field, Michael Collins, Ankur P. Parikh, Chris Alberti,
Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-
ton Lee, Kristina Toutanova, Llion Jones, Matthew
Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob
Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-
ral questions: a benchmark for question answering
research. Trans. Assoc. Comput. Linguistics, 7:452–
466.
Angeliki Lazaridou, Elena Gribovskaya, Wojciech
Stokowiec, and Nikolai Grigorev. 2022. Internet-
augmented language models through few-shot
prompting for open-domain question answering.
CoRR, abs/2203.05115.
Haejun Lee, Akhil Kedia, Jongwon Lee, Ashwin Paran-
jape, Christopher D. Manning, and Kyoung-Gu Woo.
2021. You only need one model for open-domain
question answering. CoRR, abs/2112.07381.
Patrick S. H. Lewis, Ethan Perez, Aleksandra Pik-
tus, Fabio Petroni, Vladimir Karpukhin, Naman
Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih,
Tim Rocktäschel, Sebastian Riedel, and Douwe
Kiela. 2020. Retrieval-augmented generation for
knowledge-intensive NLP tasks. In Advances in Neu-
ral Information Processing Systems 33: Annual Con-
ference on Neural Information Processing Systems
2020, NeurIPS 2020, December 6-12, 2020, virtual.
Junyi Li, Tianyi Tang, Wayne Xin Zhao, Jingyuan Wang,
Jian-Yun Nie, and Ji-Rong Wen. 2023. The web can
be your oyster for improving large language models.
CoRR, abs/2305.10998.
Chin-Yew Lin. 2004. ROUGE: A package for auto-
matic evaluation of summaries. In Text Summariza-
tion Branches Out, pages 74–81, Barcelona, Spain.
Association for Computational Linguistics.
7979Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,
Hiroaki Hayashi, and Graham Neubig. 2023. Pre-
train, prompt, and predict: A systematic survey of
prompting methods in natural language processing.
ACM Comput. Surv., 55(9):195:1–195:35.
Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,
Hannaneh Hajishirzi, and Daniel Khashabi. 2022.
When not to trust language models: Investigating
effectiveness and limitations of parametric and non-
parametric memories. CoRR, abs/2212.10511.
Yuning Mao, Pengcheng He, Xiaodong Liu, Yelong
Shen, Jianfeng Gao, Jiawei Han, and Weizhu Chen.
2021. Generation-augmented retrieval for open-
domain question answering. In Proceedings of the
59th Annual Meeting of the Association for Compu-
tational Linguistics and the 11th International Joint
Conference on Natural Language Processing, ACL/I-
JCNLP 2021, (Volume 1: Long Papers), Virtual Event,
August 1-6, 2021, pages 4089–4100. Association for
Computational Linguistics.
Joshua Maynez, Shashi Narayan, Bernd Bohnet, and
Ryan McDonald. 2020. On faithfulness and factu-
ality in abstractive summarization. In Proceedings
of the 58th Annual Meeting of the Association for
Computational Linguistics, pages 1906–1919, On-
line. Association for Computational Linguistics.
Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,
Long Ouyang, Christina Kim, Christopher Hesse,
Shantanu Jain, Vineet Kosaraju, William Saunders,
Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen
Krueger, Kevin Button, Matthew Knight, Benjamin
Chess, and John Schulman. 2021. Webgpt: Browser-
assisted question-answering with human feedback.
CoRR, abs/2112.09332.
OpenAI. 2023. GPT-4 technical report. CoRR,
abs/2303.08774.
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-
roll L. Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, John
Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,
Maddie Simens, Amanda Askell, Peter Welinder,
Paul F. Christiano, Jan Leike, and Ryan Lowe. 2022.
Training language models to follow instructions with
human feedback. CoRR, abs/2203.02155.
Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng,
Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou
Yu, Weizhu Chen, and Jianfeng Gao. 2023. Check
your facts and try again: Improving large language
models with external knowledge and automated feed-
back. CoRR, abs/2302.12813.
Fabio Petroni, Tim Rocktäschel, Sebastian Riedel,
Patrick S. H. Lewis, Anton Bakhtin, Yuxiang Wu,
and Alexander H. Miller. 2019. Language mod-
els as knowledge bases? In Proceedings of the
2019 Conference on Empirical Methods in Natu-
ral Language Processing and the 9th International
Joint Conference on Natural Language Processing,
EMNLP-IJCNLP 2019, Hong Kong, China, Novem-
ber 3-7, 2019 , pages 2463–2473. Association for
Computational Linguistics.
Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,
Noah A Smith, and Mike Lewis. 2022. Measuring
and narrowing the compositionality gap in language
models. arXiv preprint arXiv:2210.03350.
Hongjing Qian, Yutao Zhu, Zhicheng Dou, Haoqi Gu,
Xinyu Zhang, Zheng Liu, Ruofei Lai, Zhao Cao,
Jian-Yun Nie, and Ji-Rong Wen. 2023. Webbrain:
Learning to generate factually correct articles for
queries by grounding on large web corpus. CoRR,
abs/2304.04358.
Yujia Qin, Zihan Cai, Dian Jin, Lan Yan, Shihao
Liang, Kunlun Zhu, Yankai Lin, Xu Han, Ning Ding,
Huadong Wang, Ruobing Xie, Fanchao Qi, Zhiyuan
Liu, Maosong Sun, and Jie Zhou. 2023. Webcpm: In-
teractive web search for chinese long-form question
answering. CoRR, abs/2305.06849.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners. OpenAI
Blog, 1(8).
Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,
Amnon Shashua, Kevin Leyton-Brown, and Yoav
Shoham. 2023. In-context retrieval-augmented lan-
guage models. arXiv preprint arXiv:2302.00083.
Adam Roberts, Colin Raffel, and Noam Shazeer. 2020.
How much knowledge can you pack into the param-
eters of a language model? In Proceedings of the
2020 Conference on Empirical Methods in Natural
Language Processing, EMNLP 2020, Online, Novem-
ber 16-20, 2020, pages 5418–5426. Association for
Computational Linguistics.
Stephen E. Robertson and Hugo Zaragoza. 2009. The
probabilistic relevance framework: BM25 and be-
yond. Found. Trends Inf. Retr., 3(4):333–389.
Devendra Singh Sachan, Siva Reddy, William L. Hamil-
ton, Chris Dyer, and Dani Yogatama. 2021. End-to-
end training of multi-document reader and retriever
for open-domain question answering. In Advances
in Neural Information Processing Systems 34: An-
nual Conference on Neural Information Processing
Systems 2021, NeurIPS 2021, December 6-14, 2021,
virtual, pages 25968–25981.
Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta
Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola
Cancedda, and Thomas Scialom. 2023. Toolformer:
Language models can teach themselves to use tools.
Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon
Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and
Wen-tau Yih. 2023. REPLUG: retrieval-augmented
black-box language models. CoRR, abs/2301.12652.
7980Ivan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-
Wei Chang. 2022. ASQA: factoid questions meet
long-form answers. In Proceedings of the 2022 Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP 2022, Abu Dhabi, United Arab
Emirates, December 7-11, 2022, pages 8273–8288.
Association for Computational Linguistics.
Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, and
Denny Zhou. 2022. Recitation-augmented language
models. CoRR, abs/2210.01296.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, Aurélien Rodriguez, Armand Joulin, Edouard
Grave, and Guillaume Lample. 2023. Llama: Open
and efficient foundation language models. CoRR,
abs/2302.13971.
Harsh Trivedi, Niranjan Balasubramanian, Tushar
Khot, and Ashish Sabharwal. 2022. Interleav-
ing retrieval with chain-of-thought reasoning for
knowledge-intensive multi-step questions. CoRR,
abs/2212.10509.
Neeraj Varshney, Man Luo, and Chitta Baral. 2022. Can
open-domain QA reader utilize external knowledge
efficiently like humans? CoRR, abs/2211.12707.
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V .
Le, Ed H. Chi, and Denny Zhou. 2022. Self-
consistency improves chain of thought reasoning in
language models. CoRR, abs/2203.11171.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022.
Chain of thought prompting elicits reasoning in large
language models. CoRR, abs/2201.11903.
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak
Shafran, Karthik Narasimhan, and Yuan Cao. 2022.
React: Synergizing reasoning and acting in language
models. CoRR, abs/2210.03629.
Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu,
Mingxuan Ju, Soumya Sanyal, Chenguang Zhu,
Michael Zeng, and Meng Jiang. 2022. Generate
rather than retrieve: Large language models are
strong context generators. CoRR, abs/2209.10063.
Wenhao Yu, Zhihan Zhang, Zhenwen Liang, Meng
Jiang, and Ashish Sabharwal. 2023. Improving lan-
guage models via plug-and-play retrieval feedback.
CoRR, abs/2305.14002.
Yury Zemlyanskiy, Michiel de Jong, Joshua Ainslie,
Panupong Pasupat, Peter Shaw, Linlu Qiu, Sumit
Sanghai, and Fei Sha. 2022. Generate-and-retrieve:
Use your predictions to improve retrieval for seman-
tic parsing. In Proceedings of the 29th International
Conference on Computational Linguistics, COLING
2022, Gyeongju, Republic of Korea, October 12-17,
2022, pages 4946–4951. International Committee on
Computational Linguistics.
Fengji Zhang, Bei Chen, Yue Zhang, Jin Liu, Daoguang
Zan, Yi Mao, Jian-Guang Lou, and Weizhu Chen.
2023. Repocoder: Repository-level code completion
through iterative retrieval and generation. CoRR,
abs/2303.12570.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel
Artetxe, Moya Chen, Shuohui Chen, Christopher De-
wan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-
haylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel
Simig, Punit Singh Koura, Anjali Sridhar, Tianlu
Wang, and Luke Zettlemoyer. 2022. Opt: Open
pre-trained transformer language models. ArXiv,
abs/2205.01068.
Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,
Xiaolei Wang, Yupeng Hou, Yingqian Min, Be-
ichen Zhang, Junjie Zhang, Zican Dong, Yifan Du,
Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao
Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang
Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen.
2023. A survey of large language models. CoRR,
abs/2303.18223.
Ming Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu
Jiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and
Jiawei Han. 2022. Towards a unified multi-
dimensional evaluator for text generation. In Pro-
ceedings of the 2022 Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP 2022,
Abu Dhabi, United Arab Emirates, December 7-11,
2022, pages 2023–2038. Association for Computa-
tional Linguistics.
Chunting Zhou, Graham Neubig, Jiatao Gu, Mona Diab,
Francisco Guzmán, Luke Zettlemoyer, and Marjan
Ghazvininejad. 2021. Detecting hallucinated content
in conditional neural sequence generation. In Find-
ings of the Association for Computational Linguis-
tics: ACL-IJCNLP 2021, pages 1393–1404, Online.
Association for Computational Linguistics.
7981A FLARE Implementation Details
FLAREinstruct implementation details We
found that LMs can effectively combine retrieval
and downstream task-related skills and generate
meaningful search queries while performing the
task. However, there are two issues: (1) LMs tend
to generate fewer search queries than necessary.
(2) Generating excessive search queries can
disrupt answer generation and adversely affect
performance. We address these issues using two
methods respectively. First, we increase the logit
of the token “[” by 2.0 to improve the chances
of LMs generating “[Search(query)]”. Second,
whenever LMs generate a search query, we use it
to retrieve relevant information, promptly remove
it from the generation, and generate the next few
tokens while forbidding “[” by adding a large
negative value to the logit of “[”.
The initial query of FLARE. FLARE starts
with the user input x as the initial query to re-
trieve documents to generate the first sentence
ˆs1 = LM([Dx,x]) to bootstrap the iterative gener-
ation process. For the following steps, the tempo-
rary forward-looking sentence is generated without
retrieved documents.
Sentence tokenization. For each step t, we gen-
erate 64 tokens which are longer than most sen-
tences, and use NLTK sentence tokenizer 5 to ex-
tract the first sentence and discard the rest.
Efficiency As shown in subsection 6.2, on aver-
age retrieval is triggered for 30% ∼60% of sen-
tences depending on downstream tasks. In compar-
ision, KNN-LM (Khandelwal et al., 2020) retrieves
every token, RETRO or IC-RALM (Borgeaud et al.,
2022; Ram et al., 2023) retrievers every 4∼32 to-
kens, and IRCoT (Trivedi et al., 2022) retrieves
every sentence. Compared to single-time retrieval,
however, interleaving retrieval and generation with
a naive implementation indeed increases overheads,
which we discuss in the limitation section (sec-
tion 9).
B Datasets and Settings
Datasets, metrics, and experimental settings are
summarized in Table 7.
5https://www.nltk.org/api/nltk.tokenize.
PunktSentenceTokenizer.html
Multihop QA For “Why did the founder of Ver-
sus die?”, the output we aim to generate is “The
founder of Versus was Gianni Versace. Gianni Ver-
sace was shot and killed on the steps of his Miami
Beach mansion on July 15, 1997. So the answer
is shot.” We use 8 exemplars from Trivedi et al.
(2022) listed in Prompt D.4 for in-context learn-
ing, BM25 as the retriever, and Wikipedia articles
as the retrieval corpus. Similar to the observation
in Trivedi et al. (2022), we found incorporating
retrieval results for exemplars improves the per-
formance, we use the input x of each exemplar to
retrieve several documents and then add them using
the format in Prompt D.1. We found increasing the
number of retrieval documents often increases per-
formance. Therefore, we use the maximum number
of documents that can fit within the input length
limit of text-davinci-003, which is 2 for 2Wiki-
MultihopQA.
Commonsense Reasoning For “Would a pear
sink in water?”, the output we aim to generate is
“The density of a pear is about 0.6g/cm3, which is
less than water. Objects less dense than water float.
Thus, a pear would float. So the final answer is no.”
We use 6 exemplars from Wei et al. (2022) listed in
Prompt D.5, BM25 on the Wikipedia corpus, and 3
retrieved documents to run experiments.
Long-form QA For “Where do the Philadelphia
Eagles play their home games?”, the output we
aim to generate is “We need to consider the dif-
ferent possible locations or venues that could be
considered the home field of the Philadelphia Ea-
gles. These include the city, the sports complex,
or the stadium. Therefore, this question has 3 in-
terpretations and the answers are: (1) The city is
Philadelphia. (2) The sports complex is the South
Philadelphia Sports Complex. (3) The stadium is
the Lincoln Financial Field stadium.” For both the
original setting (ASQA) and the setting with hints
(ASQA-hint), we manually annotate 8 exemplars
(Prompt D.6 and D.8), use BM25 on the Wikipedia
corpus, and 3 retrieved documents to run experi-
ments.
Open-domain Summarization The original
WikiAsp dataset is designed for multi-document
summarization and provides a list of references to
systems. We converted it into the open-domain
setting by removing the associated references and
instead gathering information from the open web.
For “Generate a summary about Echo School (Ore-
7982gon) including the following aspects: academics,
history.”, the output we aim to generate is “# Aca-
demics. In 2008, 91% of the school’s seniors re-
ceived their high school diploma... # History. The
class of 2008 was the 100th class in the school’s
history.” where # is used to indicate aspects. We
manually annotate 4 exemplars (Prompt D.10), and
use the Bing search engine to retrieve 5 documents
from the open web. To avoid leaking, we exclude
several Wikipedia-related domains listed in Table 8
from Bing’s search results.
C Hyperparameters
Hyperparameters of FLARE on different datasets
are listed in Table 9.
D Prompts and Few-shot exemplars
The prompt used to linearize multiple documents
is shown in Prompt D.1. The prompt used in self-
ask (Press et al., 2022) is shown in Prompt D.2.
Prompts and exemplars of different tasks/datasets
are shown in Prompt D.3, D.4, D.5, D.6, D.8, and
D.10, respectively.
Prompt D.1: document formatting
Search results:
[1] Document 1
[2] Document 2
...
The user input x
Prompt D.2: multihop QA with self-ask
Question: Who lived longer, Theodor Haecker or Harry
Vaughan Watkins?
Are follow up questions needed here: Yes.
Follow up: How old was Theodor Haecker when he died?
Intermediate answer: Theodor Haecker was 65 years old
when he died.
Follow up: How old was Harry Vaughan Watkins when he
died?
Intermediate answer: Harry Vaughan Watkins was 69 years
old when he died.
So the final answer is: Harry Vaughan Watkins.
7983Settings 2WikiMultihopQA StrategyQA ASQA WikiAsp
(Ho et al., 2020) (Geva et al., 2021) (Stelmakh et al., 2022) (Hayashi et al., 2021)
Dataset statistics
Task multihop QA commonsense QA long-form QA open-domain summarization
#Examples 500 229 500 500
Evaluation settings
Metrics EM, F 1, Prec., Rec. EM EM, Disambig-F 1, ROUGE, DR UniEval, entity-F1, ROUGE
Retrieval settings
Corpus Wikipedia Wikipedia Wikipedia open web
Retriever BM25 BM25 BM25 Bing
Top-k 2 3 3 5
Prompt format
#Exemplars 8 6 8 4
Ret. for exemplars ✓ ✗ ✗ ✗
Table 7: Dataset statistics and experimental settings of different tasks.
wikipedia.org, wikiwand.com, wiki2.org, wikimedia.org
Table 8: Wikipedia-related domains excluded from Bing’s search results.
Dataset θ β Query formulation Combine single- & multi-time retrieval
2WikiMultihopQA 0.8 0.4 implicit ✗
StrategyQA 0.4 0.4 implicit ✗
ASQA & ASQA-hint 0.8 0.4 explicit ✓
WikiAsp 0.8 0.4 explicit ✓
Table 9: Hyperparameters of FLARE on different datasets.
7984Prompt D.3: retrieval instructions for 2WikiMultihopQA
Skill 1. Use the Search API to look up relevant information by writing “[Search(term)]” where “term” is the search term you
want to look up. For example:
Question: But what are the risks during production of nanomaterials?
Answer (with Search): [Search(nanomaterial production risks)] Some nanomaterials may give rise to various kinds of lung
damage.
Question: The colors on the flag of Ghana have the following meanings.
Answer (with Search): Red is for [Search(Ghana flag red meaning)] the blood of martyrs, green for forests, and gold for
mineral wealth.
Question: Metformin is the first-line drug for what?
Answer (with Search): [Search(Metformin first-line drug)] patients with type 2 diabetes and obesity.
Skill 2. Answer questions by thinking step-by-step. First, write out the reasoning steps, then draw the conclu-
sion. For example:
Question: When did the director of film Hypocrite (Film) die?
Answer (with step-by-step): The film Hypocrite was directed by Miguel Morayta. Miguel Morayta died on 19 June 2013. So
the answer is 19 June 2013.
Question: Are both Kurram Garhi and Trojkrsti located in the same country?
Answer (with step-by-step): Kurram Garhi is located in the country of Pakistan. Trojkrsti is located in the country of
Republic of Macedonia. Thus, they are not in the same country. So the answer is no.
Question: Do director of film Coolie No. 1 (1995 Film) and director of film The Sensational Trial have the same
nationality?
Answer (with step-by-step): Coolie No. 1 (1995 film) was directed by David Dhawan. The Sensational Trial was directed by
Karl Freund. David Dhawan’s nationality is India. Karl Freund’s nationality is Germany. Thus, they do not have the same
nationality. So the answer is no.
Question: Who is Boraqchin (Wife Of Ögedei)’s father-in-law?
Answer (with step-by-step): Boraqchin is married to Ögedei Khan. Ögedei Khan’s father is Genghis Khan. Thus, Boraqchin’s
father-in-law is Genghis Khan. So the answer is Genghis Khan.
Question: Who was born first out of Martin Hodge and Ivania Martinich?
Answer (with step-by-step): Martin Hodge was born on 4 February 1959. Ivania Martinich was born on 25 July 1995. Thus,
Martin Hodge was born first. So the answer is Martin Hodge.
Question: When did the director of film Laughter In Hell die?
Answer (with step-by-step): The film Laughter In Hell was directed by Edward L. Cahn. Edward L. Cahn died on August 25,
1963. So the answer is August 25, 1963.
Question: Which film has the director died later, The Gal Who Took the West or Twenty Plus Two?
Answer (with step-by-step): The film Twenty Plus Two was directed by Joseph M. Newman. The Gal Who Took
the West was directed by Frederick de Cordova. Joseph M. Newman died on January 23, 2006. Fred de Cordova
died on September 15, 2001. Thus, the person to die later from the two is Twenty Plus Two. So the answer is Twenty Plus Two.
Question: Who is the grandchild of Krishna Shah (Nepalese Royal)?
Answer (with step-by-step): Krishna Shah has a child named Rudra Shah. Rudra Shah has a child named Prithvipati Shah.
Thus, Krishna Shah has a grandchild named Prithvipati Shah. So the answer is Prithvipati Shah.
Now, combine the aforementioned two skills. First, write out the reasoning steps, then draw the conclusion,
where the reasoning steps should also utilize the Search API “[Search(term)]” whenever possible.
Question: Where did Minbyauk Thihapate’s wife die?
Answer (with step-by-step & Search):
7985Prompt D.4: exemplars of 2WikiMultihopQA
Question: When did the director of film Hypocrite (Film) die?
Answer: The film Hypocrite was directed by Miguel Morayta. Miguel Morayta died on 19 June 2013. So the answer is 19
June 2013.
Question: Are both Kurram Garhi and Trojkrsti located in the same country?
Answer: Kurram Garhi is located in the country of Pakistan. Trojkrsti is located in the country of Republic of Macedonia.
Thus, they are not in the same country. So the answer is no.
Question: Do director of film Coolie No. 1 (1995 Film) and director of film The Sensational Trial have the same
nationality?
Answer: Coolie No. 1 (1995 film) was directed by David Dhawan. The Sensational Trial was directed by Karl Freund. David
Dhawan’s nationality is India. Karl Freund’s nationality is Germany. Thus, they do not have the same nationality. So the
answer is no.
Question: Who is Boraqchin (Wife Of Ögedei)’s father-in-law?
Answer: Boraqchin is married to Ögedei Khan. Ögedei Khan’s father is Genghis Khan. Thus, Boraqchin’s father-in-law is
Genghis Khan. So the answer is Genghis Khan.
Question: Who was born first out of Martin Hodge and Ivania Martinich?
Answer: Martin Hodge was born on 4 February 1959. Ivania Martinich was born on 25 July 1995. Thus, Martin Hodge was
born first. So the answer is Martin Hodge.
Question: When did the director of film Laughter In Hell die?
Answer: The film Laughter In Hell was directed by Edward L. Cahn. Edward L. Cahn died on August 25, 1963. So the
answer is August 25, 1963.
Question: Which film has the director died later, The Gal Who Took the West or Twenty Plus Two?
Answer: The film Twenty Plus Two was directed by Joseph M. Newman. The Gal Who Took the West was directed by
Frederick de Cordova. Joseph M. Newman died on January 23, 2006. Fred de Cordova died on September 15, 2001. Thus,
the person to die later from the two is Twenty Plus Two. So the answer is Twenty Plus Two.
Question: Who is the grandchild of Krishna Shah (Nepalese Royal)?
Answer: Krishna Shah has a child named Rudra Shah. Rudra Shah has a child named Prithvipati Shah. Thus, Krishna Shah
has a grandchild named Prithvipati Shah. So the answer is Prithvipati Shah.
Question: Which country the director of film Citizen Mavzik is from?
Answer:
7986Prompt D.5: exemplars of StrategyQA
Generate a yes or no answer to the following question.
Question: Do hamsters provide food for any animals?
Answer: Hamsters are prey animals. Prey are food for predators. Thus, hamsters provide food for some animals. So the final
answer is yes.
Generate a yes or no answer to the following question.
Question: Could Brooke Shields succeed at University of Pennsylvania?
Answer: Brooke Shields went to Princeton University. Princeton University is about as academically rigorous as the Uni-
versity of Pennsylvania. Thus, Brooke Shields could also succeed at the University of Pennsylvania. So the final answer is yes.
Generate a yes or no answer to the following question.
Question: Hydrogen’s atomic number squared exceeds number of Spice Girls?
Answer: Hydrogen has an atomic number of 1. 1 squared is 1. There are 5 Spice Girls. Thus, Hydrogen’s atomic number
squared is less than 5. So the final answer is no.
Generate a yes or no answer to the following question.
Question: Is it common to see frost during some college commencements?
Answer: College commencement ceremonies can happen in December, May, and June. December is in the winter, so there
can be frost. Thus, there could be frost at some commencements. So the final answer is yes.
Generate a yes or no answer to the following question.
Question: Could a llama birth twice during War in Vietnam (1945-46)?
Answer: The War in Vietnam was 6 months. The gestation period for a llama is 11 months, which is more than 6 months.
Thus, a llama could not give birth twice during the War in Vietnam. So the final answer is no.
Generate a yes or no answer to the following question.
Question: Would a pear sink in water?
Answer: The density of a pear is about 0.6g/cm3, which is less than water. Objects less dense than water float. Thus, a pear
would float. So the final answer is no.
Generate a yes or no answer to the following question.
Question: Did Spiderman fight against Falcon in the MCU?
Answer:
7987Prompt D.6: exemplars of ASQA
Given an ambiguous question, figure out its interpretations and answer them one by one.
Question: Who played bonnie in gone with the wind?
Answer: This question is ambiguous in terms of which version or adaptation of Gone with the Wind is being referred to. In
order to figure out its interpretations, we need to consider different versions or adaptations of Gone with the Wind. Gone with
the Wind has two versions or adaptations: the 1939 film Gone with the Wind or the 2008 musical Gone with the Wind.
Therefore, this question has 2 interpretations: (1) Who played Bonnie in the 1939 film Gone with the Wind? (2) Who played
Bonnie in the 2008 musical Gone with the Wind? The answers to all interpretations are: (1) The 1939 film Gone with the
Wind’s character Bonnie was played by Eleanore Cammack "Cammie" King. (2) The 2008 musical Gone with the Wind’s
character Bonnie was played by Leilah de Meza.
Given an ambiguous question, figure out its interpretations and answer them one by one.
Question: What is the second largest city in the usa?
Answer: This question is ambiguous in terms of the criteria being used to determine the second largest city in the USA.
In order to figure out its interpretations, we need to consider different criteria to determine a city’s size. City size can be
measured by two criteria: population or area. Therefore, this question has 2 interpretations: (1) What is the second largest
city in the USA by population? (2) What is the second largest city in the USA by area? The answers to all interpretations are:
(1) The second largest city in the USA by population is Los Angeles, California. (2) The second largest city in the USA by
area is Juneau, Alaska.
Given an ambiguous question, figure out its interpretations and answer them one by one.
Question: When was bohemian rhapsody released as a single?
Answer: This question is ambiguous in terms of which country’s release of the single is being referred to. In order to figure
out its interpretations, we need to consider different countries where Bohemian Rhapsody is released. Bohemian Rhapsody
was released in the United Kingdom and in the United States on different dates. Therefore, this question has 2 interpretations:
(1) When was Bohemian Rhapsody released as a single in the United Kingdom? (2) When was Bohemian Rhapsody released
as a single in the United States? The answers to all interpretations are: (1) Bohemian Rhapsody was released as a single in the
United Kingdom on 31 October 1975. (2) Bohemian Rhapsody was released as a single in the United States on December 1975.
Given an ambiguous question, figure out its interpretations and answer them one by one.
Question: Where do the philadelphia eagles play their home games?
Answer: This question is ambiguous in terms of which specific location or venue is being referred to. In order to figure out its
interpretations, we need to consider the different possible locations or venues that could be considered the home field of the
Philadelphia Eagles. These include the city, the sports complex, or the stadium. Therefore, this question has 3 interpretations:
(1) What city do the Philadelphia Eagles play their home games? (2) In what sports complex do the Philadelphia Eagles play
their home games? (3) What stadium do the Philadelphia Eagles play their home games? The answers to all interpretations
are: (1) Philadelphia Eagles play their home games in the city Philadelphia. (2) Philadelphia Eagles play their home games in
the South Philadelphia Sports Complex. (3) Philadelphia Eagles play their home games in the Lincoln Financial Field stadium.
Given an ambiguous question, figure out its interpretations and answer them one by one.
Question: When did xbox one come out in australia?
Answer: This question is ambiguous in terms of which specific version of the Xbox One is being referred to. In order to
figure out its interpretations, we need to consider the different versions of the Xbox One that have been released. Xbox One
has two versions: the Xbox One video game console or the Xbox One X high-end model. Therefore, this question has 2
interpretations: (1) When did the Xbox One release in Australia? (2) When did the Xbox One X release in Australia? The
answers to all interpretations are: (1) The Xbox One video game console was released in Australia on November 22, 2013.
(2) The Xbox One X video game console was released in Australia on November 7, 2017.
Given an ambiguous question, figure out its interpretations and answer them one by one.
Question: When does the movie summer of 84 come out?
Answer: This question is ambiguous in terms of which release of the movie is being referred to. In order to figure out its
interpretations, we need to consider different releases of the movie Summer of ’84. The movie Summer of ’84 is first released
at the Sundance Festival before it’s released throughout the US. Therefore, this question has 2 interpretations: (1) When
did the movie Summer of ’84 first release at the Sundance Festival? (2) When did the movie Summer of ’84 first release
throughout the US? The answers to all interpretations are: (1) Summer of ’84 was released at the Sundance Festival on
January 22, 2018. (2) Summer of ’84 was released throughout the US on August 10, 2018.
7988Prompt D.7: exemplars of ASQA (cont.)
Given an ambiguous question, figure out its interpretations and answer them one by one.
Question: What was roy orbison’s first number one hit?
Answer: This question is ambiguous in terms of which specific chart or region is being referred to. In order to figure out its
interpretations, we need to consider the different charts and regions where Roy Orbison’s music was popular. Roy Orbison is
popular in both the US Hot 100 and Canada, and the UK and Ireland. Therefore, this question has 2 interpretations: (1) What
was Roy Orbison’s first number one hit in the US Hot 100 and Canada? (2) What was Roy Orbison’s first number one hit in
the UK and Ireland? The answers to all interpretations are: (1) Running Scared was the first number one hit for Roy Orbison
in the US Hot 100 and Canada. (2) Only the Lonely (Know the Way I Feel) was the first number one hit for Roy Orbison in
the UK and Ireland.
Given an ambiguous question, figure out its interpretations and answer them one by one.
Question: What is the criminal’s name in the breakfast club?
Answer: This question is ambiguous in terms of which specific name is being referred to - the character’s name or the actor’s
name. In order to figure out its interpretations, we need to consider both possibilities: the character’s name or the actor’s
name. Therefore, this question has 2 interpretations: (1) What is the criminal’s character name in The Breakfast Club? (2)
What is the the name of the actor who played the criminal in The Breakfast Club? The answers to all interpretations are: (1)
John Bender was the name of the criminal’s character in The Breakfast Club. (2) Judd Nelson was the actor of the criminal in
The Breakfast Club.
Given an ambiguous question, figure out its interpretations and answer them one by one.
Question: How many state parks are there in virginia?
Answer:
7989Prompt D.8: exemplars of ASQA-hint
Given an ambiguous question and a hint on which aspect of the question is ambiguous, figure out its interpretations and
answer them one by one.
Question: Who played bonnie in gone with the wind?
Hint: This question is ambiguous in terms of which version or adaptation of Gone with the Wind is being referred to.
Answer: In order to figure out its interpretations, we need to consider different versions or adaptations of Gone with the Wind.
Gone with the Wind has two versions or adaptations: the 1939 film Gone with the Wind or the 2008 musical Gone with the
Wind. Therefore, this question has 2 interpretations: (1) Who played Bonnie in the 1939 film Gone with the Wind? (2) Who
played Bonnie in the 2008 musical Gone with the Wind? The answers to all interpretations are: (1) The 1939 film Gone with
the Wind’s character Bonnie was played by Eleanore Cammack "Cammie" King. (2) The 2008 musical Gone with the Wind’s
character Bonnie was played by Leilah de Meza.
Given an ambiguous question and a hint on which aspect of the question is ambiguous, figure out its interpreta-
tions and answer them one by one.
Question: What is the second largest city in the usa?
Hint: This question is ambiguous in terms of the criteria being used to determine the second largest city in the USA.
Answer: In order to figure out its interpretations, we need to consider different criteria to determine a city’s size. City size can
be measured by two criteria: population or area. Therefore, this question has 2 interpretations: (1) What is the second largest
city in the USA by population? (2) What is the second largest city in the USA by area? The answers to all interpretations are:
(1) The second largest city in the USA by population is Los Angeles, California. (2) The second largest city in the USA by
area is Juneau, Alaska.
Given an ambiguous question and a hint on which aspect of the question is ambiguous, figure out its interpreta-
tions and answer them one by one.
Question: When was bohemian rhapsody released as a single?
Hint: This question is ambiguous in terms of which country’s release of the single is being referred to.
Answer: In order to figure out its interpretations, we need to consider different countries where Bohemian Rhapsody is
released. Bohemian Rhapsody was released in the United Kingdom and in the United States on different dates. Therefore,
this question has 2 interpretations: (1) When was Bohemian Rhapsody released as a single in the United Kingdom? (2) When
was Bohemian Rhapsody released as a single in the United States? The answers to all interpretations are: (1) Bohemian
Rhapsody was released as a single in the United Kingdom on 31 October 1975. (2) Bohemian Rhapsody was released as a
single in the United States on December 1975.
Given an ambiguous question and a hint on which aspect of the question is ambiguous, figure out its interpreta-
tions and answer them one by one.
Question: Where do the philadelphia eagles play their home games?
Hint: This question is ambiguous in terms of which specific location or venue is being referred to.
Answer: In order to figure out its interpretations, we need to consider the different possible locations or venues that could be
considered the home field of the Philadelphia Eagles. These include the city, the sports complex, or the stadium. Therefore,
this question has 3 interpretations: (1) What city do the Philadelphia Eagles play their home games? (2) In what sports
complex do the Philadelphia Eagles play their home games? (3) What stadium do the Philadelphia Eagles play their home
games? The answers to all interpretations are: (1) Philadelphia Eagles play their home games in the city Philadelphia. (2)
Philadelphia Eagles play their home games in the South Philadelphia Sports Complex. (3) Philadelphia Eagles play their
home games in the Lincoln Financial Field stadium.
Given an ambiguous question and a hint on which aspect of the question is ambiguous, figure out its interpreta-
tions and answer them one by one.
Question: When did xbox one come out in australia?
Hint: This question is ambiguous in terms of which specific version of the Xbox One is being referred to.
Answer: In order to figure out its interpretations, we need to consider the different versions of the Xbox One that have been
released. Xbox One has two versions: the Xbox One video game console or the Xbox One X high-end model. Therefore, this
question has 2 interpretations: (1) When did the Xbox One release in Australia? (2) When did the Xbox One X release
in Australia? The answers to all interpretations are: (1) The Xbox One video game console was released in Australia on
November 22, 2013. (2) The Xbox One X video game console was released in Australia on November 7, 2017.
Given an ambiguous question and a hint on which aspect of the question is ambiguous, figure out its interpreta-
tions and answer them one by one.
Question: When does the movie summer of 84 come out?
Hint: This question is ambiguous in terms of which release of the movie is being referred to.
Answer: In order to figure out its interpretations, we need to consider different releases of the movie Summer of ’84. The
movie Summer of ’84 is first released at the Sundance Festival before it’s released throughout the US. Therefore, this question
has 2 interpretations: (1) When did the movie Summer of ’84 first release at the Sundance Festival? (2) When did the movie
Summer of ’84 first release throughout the US? The answers to all interpretations are: (1) Summer of ’84 was released at the
Sundance Festival on January 22, 2018. (2) Summer of ’84 was released throughout the US on August 10, 2018.
7990Prompt D.9: exemplars of ASQA-hint (cont.)
Given an ambiguous question and a hint on which aspect of the question is ambiguous, figure out its interpretations and
answer them one by one.
Question: What was roy orbison’s first number one hit?
Hint: This question is ambiguous in terms of which specific chart or region is being referred to.
Answer: In order to figure out its interpretations, we need to consider the different charts and regions where Roy Orbison’s
music was popular. Roy Orbison is popular in both the US Hot 100 and Canada, and the UK and Ireland. Therefore, this
question has 2 interpretations: (1) What was Roy Orbison’s first number one hit in the US Hot 100 and Canada? (2) What
was Roy Orbison’s first number one hit in the UK and Ireland? The answers to all interpretations are: (1) Running Scared
was the first number one hit for Roy Orbison in the US Hot 100 and Canada. (2) Only the Lonely (Know the Way I Feel) was
the first number one hit for Roy Orbison in the UK and Ireland.
Given an ambiguous question and a hint on which aspect of the question is ambiguous, figure out its interpreta-
tions and answer them one by one.
Question: What is the criminal’s name in the breakfast club?
Hint: This question is ambiguous in terms of which specific name is being referred to - the character’s name or the actor’s
name.
Answer: In order to figure out its interpretations, we need to consider both possibilities: the character’s name or the actor’s
name. Therefore, this question has 2 interpretations: (1) What is the criminal’s character name in The Breakfast Club? (2)
What is the the name of the actor who played the criminal in The Breakfast Club? The answers to all interpretations are: (1)
John Bender was the name of the criminal’s character in The Breakfast Club. (2) Judd Nelson was the actor of the criminal in
The Breakfast Club.
Given an ambiguous question and a hint on which aspect of the question is ambiguous, figure out its interpreta-
tions and answer them one by one.
Question: How many state parks are there in virginia?
Hint: This question is ambiguous in terms of the time frame or period being referred to.
Answer:
7991Prompt D.10: exemplars of WikiAsp
Generate a summary about Aslanhane Mosque including the following aspects: location, history with one aspect per line.
# Location
The mosque is in the old quarter of ankara next to ankara castle. With an altitude of 947 metres (3,107 ft) it overlooks ankara
at 39°56’12"N 32°51’55"E.
# History
The mosque is one of the oldest mosques in Turkey still standing. It was built during the reign of Mesud II of the Anatolian
Seljuks in 1290. Its architect was Ebubekir Mehmet. It was commissioned by two Ahi leaders named Hüsamettin and
Hasaneddin. However, in 1330, it was repaired by another Ahi leader named ¸ Serafettin after whom the mosque was named.
After several minor repairs the mosque was restored by the directorate general of foundations in 2010-2013 term.
Generate a summary about Untold Legends: The Warrior’s Code including the following aspects: reception,
gameplay, development with one aspect per line.
# Reception
The game received "mixed or average reviews" according to video game review aggregator Metacritic.
# Gameplay
The warrior’s code is a hack n’ slash action role-playing game, which concentrates on action-oriented combat.
# Development
As a pre-order bonus, the game was shipped with a small action figure of the Guardian class.
Generate a summary about Raid on St. Augustine including the following aspects: aftermath, background with
one aspect per line.
# Aftermath
Once the English had gone Menéndez and the rest of the Spanish settlers returned to find a smoldering ruins and very little
left. He soon and begged for help from the viceroy of Cuba and the settlement took a while to build itself back up. The
destroyed fort was replaced with the present day Castillo de San Marcos.
# Background
War had already been unofficially declared by Philip II of Spain after the Treaty of Nonsuch in which Elizabeth I had
offered her support to the rebellious Protestant Dutch rebels. The Queen through Francis Walsingham ordered Sir Francis
Drake to lead an expedition to attack the Spanish New World in a kind of preemptive strike. Sailing from Plymouth,
England, he struck first at Santiago in November 1585 then across the Atlantic at the Spanish new world city of Santo
Domingo of which was captured and ransomed on 1 January 1586 and following that successfully attacked the important
city of Cartagena on 19 February. Drake wanted to strike at another Spanish city on the Main before finally visiting and
replenishing Sir Walter Raleigh’s new colony of Roanoke Colony on the American East Coast. Then after this he hoped
to make the Transatlantic crossing back to England. The fleet headed north, and in late April Drake put into the Spanish
Cuban mainland and his men dug wells in search of fresh water and gathered supplies to help counter an outbreak of
dysentery after which he moved on. The fleet traveled north within sight of land on the Florida peninsula sailing past
the West coast. On 27 May 1586 as they approached further north a small fort was spotted on the shore, with a small
inlet close by. This was the location of St Augustine, the most northerly town in Spain’s New World Empire, and the
oldest permanent colonial settlement in North America. Drake knew of the place and was also aware of the fact that
the spanish under Pedro Menéndez de Avilés had ordered all of the French Huguenot colonists that had tried to settle
in the area executed. Drake decided on one final opportunity to raid and plunder, and a chance to avenge his fellow Protestants.
Generate a summary about Lakewood (Livingston, Alabama) including the following aspects: architecture, his-
tory with one aspect per line.
# Architecture
The house has a plan that is relatively rare in early Alabama architecture. The plan features a brick ground floor that is topped
by one-and-a-half-stories of wood-frame construction. The ground floor originally contained domestic spaces, with the
formal rooms on the principle floor and bedrooms on the upper floor. A central hallway is present on all levels. The facade is
five bays wide, with central entrance doors on the ground and principle floors. The bays are divided by two-story Doric
pilasters, with the middle third of the facade occupied by a two-tiered tetrastyle Doric portico. Two curved wrought iron
staircases ascend from ground level to the front center of the upper portico, leading to the formal entrance.
# History
Lakewood was built for Joseph lake, a native of North Carolina, by Hiram W. Bardwell, a master builder. Construction
was completed in 1840. Located adjacent to the University of West Alabama, Julia Strudwick Tutwiler, a Lake relative,
periodically resided in the house from 1881 to 1910 while she served as president of the university. It was then known as
Livingston Normal College. The house was extensively photographed by Alex Bush for the Historic American Buildings
Survey in November and December 1936. Lakewood has continued to be owned by descendants of the Lake family to the
current day. The house and its surviving 10 acres (4.0 ha) of grounds were listed on the Places in Peril in 2012 due to the
immediate threat of its acquisition by developers.
Generate a summary about Carlos Moedas including the following aspects: biography, early life, political career
with one aspect per line.
7992

Published as a conference paper at ICLR 2024
RETRIEVAL MEETS LONG CONTEXT LARGE LANGUAGE
MODELS
Peng Xu†, Wei Ping†, Xianchao Wu, Lawrence McAfee
Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhturina
Mohammad Shoeybi, Bryan Catanzaro
NVIDIA
†{pengx, wping}@nvidia.com
ABSTRACT
Extending the context window of large language models (LLMs) is getting popular
recently, while the solution of augmenting LLMs with retrieval has existed for years.
The natural questions are: i) Retrieval-augmentation versus long context window,
which one is better for downstream tasks? ii) Can both methods be combined to
get the best of both worlds?In this work, we answer these questions by studying
both solutions using two state-of-the-art pretrained LLMs, i.e., a proprietary 43B
GPT and Llama2-70B. Perhaps surprisingly, we find that LLM with 4K context
window using simple retrieval-augmentation at generation can achieve compa-
rable performance to finetuned LLM with 16K context window via positional
interpolation on long context tasks, while taking much less computation. More
importantly, we demonstrate that retrieval can significantly improve the perfor-
mance of LLMs regardless of their extended context window sizes. Our best model,
retrieval-augmented Llama2-70B with 32K context window, outperforms GPT-3.5-
turbo-16k and Davinci003 in terms of average score on nine long context tasks
including question answering, query-based summarization, and in-context few-shot
learning tasks. It also outperforms its non-retrieval Llama2-70B-32k baseline by a
margin, while being much faster at generation. Our study provides general insights
on the choice of retrieval-augmentation versus long context extension of LLM for
practitioners.
1 I NTRODUCTION
The long context large language models (LLM) have recently received a lot of attention in produc-
tion (e.g., Anthropic, 2023; OpenAI, 2023b), research community (e.g., Chen et al., 2023; Liu et al.,
2023; Tworkowski et al., 2023), and open source community (e.g., Kaiokendev, 2023). Although
the approximate attention methods have been studied for years (e.g., Tay et al., 2022a) (due to the
quadratic time and memory complexities of self-attention mechanism in sequence length), the recent
advance for long context LLMs with exact attention is mainly driven by the development of faster
GPU with more memory and memory-efficient exact attention (Dao et al., 2022; Dao, 2023).
An alternative and long-standing solution for handling long context is retrieval. Specifically, the
LLMs only read relevant context retrieved from a standalone retriever (e.g., Karpukhin et al., 2020;
Wang et al., 2022; Lin et al., 2023), which is much easier to scale1 and runs orders of magnitudes
faster than LLMs for selecting relevant context. Conceptually, the retrieval-augmented decoder-only
LLM can be viewed as applying the sparse attention over its long context window, where the sparsity
pattern is not predefined as Child et al. (2019) but determined by the standalone retriever. In other
words, unretrieved context is treated as irrelevant and has zero-valued attention weights.
Given the surge of interest in long context LLM research and much more required computation
at inference 2, it is still unclear for practitioners whether extending the context window of LLM
1The dense embedding retriever can easily retrieve context from billions of tokens using the fast similarity
search library (Johnson et al., 2019).
2For example, the price of GPT-4 with 32k context length is twice the 8k context model.
1Published as a conference paper at ICLR 2024
provides higher accuracy than the retrieval-augmentation for downstream tasks with informative
queries. Moreover, it would be compelling if we could combine the strength of both methods and
achieve even higher accuracies. In this work, we attempt to answer the above questions through a
comprehensive study.
Specifically, we make the following contributions:
1. We perform comprehensive study using two state-of-the-art LLMs, a proprietary 43B pre-
trained GPT and Llama2-70B (Touvron et al., 2023b) on 9 downstream long context tasks,
including single and multi document question answering (QA), query-based summarization,
and in context few-shot learning tasks.
2. We demonstrate that retrieval-augmentation significantly improves the performance of 4K
context LLMs. Perhaps surprisingly, we find this simple retrieval-augmented baseline can
perform comparable to 16K long context LLMs, i.e., average score 29.32 vs. 29.45 by using
GPT-43B, and 36.02 vs. 36.78 by using Llama2-70B, while using much less computation.
3. Furthermore, we demonstrate that the performance of long context LLM (i.e., 16K or 32K)
can still be improved by retrieval, especially for the larger Llama2-70B. As a result, our best
model, retrieval augmented Llama2-70B-32k-ret with 32K context window (avg. score 43.6),
outperforms GPT-3.5-turbo-16k (avg. score 42.8) and Davinci-003 in terms of average score.
It also largely outperforms its non-retrieval Llama2-70B-32k baseline (avg. score 40.9), while
can be much faster at generation (e.g., 4× faster on NarrativeQA).
We organize the rest of the paper as follows. We discuss related work in Section 2, and present the
experimental setup in Section 3. We report results in Section 4 and conclude the paper in Section 5.
2 R ELATED WORK
In this section, we discuss the related work in long context LLM, efficient attention methods, and
retrieval-augmented language models.
2.1 P ARALLEL WORK
When we are preparing this manuscript, we notice that a concurrent work (Bai et al., 2023) (arXived
on 28 Aug 2023) also studies the impact of retrieval on long context LLM, including black-box model
GPT-3.5-Turbo-16k (OpenAI, 2022), white-box model Llama2-7B-chat-4k (Touvron et al., 2023b),
and ChatGLM2-6B-32k (Zeng et al., 2022). Different from our findings, they find that retrieval is
only helpful for Llama2-7B-chat-4k with 4K context window, but not helpful for long context model,
i.e., GPT-3.5-Turbo-16k and ChatGLM2-6B-32k. We hypothesize the major reasons are: i) it is
challenging to do controlled experiments using black-box APIs, ii) the white-box LLMs used in their
study are relatively small, thus they have limited zero-shot capability of incorporating context through
retrieval. Our conclusions are drawn from much larger LLMs. In particular, our best long context
model Llama2-70B-32k performs as well as Davinci003 and GPT-3.5-turbo-16k, while it can still be
further enhanced by retrieval (see Table 3).
2.2 L ONG CONTEXT LARGE LANGUAGE MODELS
Over the past few years, pretraining large language models (LLMs) with long context window
becomes a viable solution thanks to faster GPU with more memory and memory-efficient exact
attention (e.g., Dao et al., 2022). For example, the context window for pretrained LLM have been
increased from 1024 of GPT-2 (Radford et al., 2019), 2048 of GPT-3 (Brown et al., 2020), 4096 of
Llama 2 (Touvron et al., 2023b), to 8192 of GPT-4 (OpenAI, 2023a). However, further extending the
context window in pretraining can be challenging, because,i) pretraining LLM from scratch with long
context (e.g., >16K tokens) is very expensive due to the quadratic time and memory complexities of
exact attention, and ii) most of documents in pretraining corpus (e.g., Common Crawl) are relatively
short.
Most recently, researchers start to extend the context window of LLMs with continued training or
fine-tuning (e.g., Kaiokendev, 2023; Nijkamp et al., 2023; Chen et al., 2023; Tworkowski et al., 2023;
Mohtashami & Jaggi, 2023). Tworkowski et al. (2023) introduced LongLLaMA by fine-tuning the
2Published as a conference paper at ICLR 2024
3B and 7B OpenLLaMA checkpoints with contrastive training on 8K context length. Landmark
attention (Mohtashami & Jaggi, 2023) extends the context length of LLaMA 7B from 4K to 32K
by introducing “landmark tokens” to represent blocks of the context and fine-tuning the attention
to use landmark tokens for selecting relevant blocks. Chen et al. (2023) and Kaiokendev (2023)
introduced positional interpolationto extend the context window sizes of RoPE-based (Su et al.,
2021) pretrained LLMs. In particular, Chen et al. (2023) demonstrates promising results on LLaMA
7B to 65B (Touvron et al., 2023a) with minimal fine-tuning effort (within 1000 steps). ALiBi (Press
et al., 2021) extrapolates context window length by removing the positional embeddings while simply
biasing the key-query attention scores with a linear penalty that is proportional to their distance, so
one does not need finetuning for context window extrapolation. Ratner et al. (2023) chunks long
context into multiple sub-windows and re-use the positional embeddings across these windows,
thus can handle longer context without any further finetuning. In this work, we apply positional
interpolation method to extend the 4K context window of a proprietary 43B pretrained LLM and
Llama2-70B (Touvron et al., 2023b) to 16K and 32K, as they both use rotary position embedding at
pretraining. In terms of evaluation, we focus on downstream task performance (e.g., Shaham et al.,
2023; Bai et al., 2023) after instruction tuning (Wei et al., 2021).
There are other studies showing the interplay between retrieval-augmentation and long context LLM.
Liu et al. (2023) performs the black-box evaluation for the long context capability of existing LLM
products, including ChatGPT 3.5 (OpenAI, 2022), GPT-4 (OpenAI, 2023a), Claude (Anthropic,
2023), in retrieval-augmented setting, and identify the “lost in the middle” phenomenon in these
models.
2.3 E FFICIENT ATTENTION METHODS
In previous study, many approximate attention methods (Tay et al., 2022a) have been introduced for
dealing with the quadratic complexity of self-attention that becomes a computational bottleneck for
long context. They can be grouped into the following categories: i) Sparse attention mechanisms
with predefined sparsity patterns (e.g., Child et al., 2019; Parmar et al., 2018; Ho et al., 2019; Beltagy
et al., 2020; Zaheer et al., 2020; Zhu et al., 2021), ii) recurrence-based method (Dai et al., 2019;
Bulatov et al., 2022), iii) low-rank projection attention (e.g., Wang et al., 2020; Xiong et al., 2021;
Tay et al., 2021; Zhu et al., 2021), iv) memory-based mechanisms (e.g., Rae et al., 2020; Liu et al.,
2018), v) similarity and clustering based methods (e.g., Kitaev et al., 2020; Tay et al., 2020; Roy
et al., 2021). These approximate methods introduce inductive bias (e.g., predefined sparsity) that can
fit well for specific domain, but may reduce model quality in general LLM training.
Most recently, FlashAttention (Dao et al., 2022; Dao, 2023) is introduced to speed up the exact atten-
tion computation by accounting for reads and writes between levels of GPU memory. FlashAttention
is particularly useful for handling longer sequences.
2.4 R ETRIEVAL -AUGMENTED LANGUAGE MODELS
Retrieval has been integrated into language models for years to improve perplexity (Borgeaud et al.,
2022; Wang et al., 2023), factual accuracy (Nakano et al., 2021), downstream task accuracy (Guu
et al., 2020; Izacard & Grave, 2021; Izacard et al., 2022; Lewis et al., 2020), and in-context learning
capability (Huang et al., 2023). Combined with a standalone retriever (Karpukhin et al., 2020;
Wang et al., 2022; Lin et al., 2023), retrieval-augmented LLM is well established for handling
question answering with long document and in open-domain. In previous study, language models
have been augmented with retrieval at inference (Khandelwal et al., 2019; Yogatama et al., 2021),
fine-tuning (Izacard et al., 2022; Lewis et al., 2020; Guu et al., 2020), and pretraining (Borgeaud
et al., 2022; Izacard et al., 2022; Wang et al., 2023). There are also methods that try to integrate LLM
and retriever in a single model and build the end-to-end solution (e.g., Jiang et al., 2022; Shi et al.,
2023). However, most of previous works mainly study retrieval-augmentation for LLMs that have
around 10 billion parameters, except a few recent ones (e.g., Shi et al., 2023).
In this work, we focus on decoder-only LLMs with 43B and 70B parameters trained on trillions of
tokens, because the LLMs at such scale exhibit strong zero-shot capability to incorporate context
after instruction tuning (Wei et al., 2021; 2022).
3Published as a conference paper at ICLR 2024
3 E XPERIMENTAL SETUP
In this section, we present the details of our experimental setup.
3.1 L ARGE LANGUAGE MODELS
We focus on comparing the zero-shot capability of integrating long context information for generative
QA or summarization tasks via retrieval or LLM’s own self-attention mechanism. In contrast to
most existing works that focus on relatively small models (e.g., 3B or 7B) (Kaiokendev, 2023;
Nijkamp et al., 2023; Tworkowski et al., 2023; Mohtashami & Jaggi, 2023), we gather the insights by
exploring model sizes that are larger than 40B after instruction tuning, as previous study suggests that
instruction tuning becomes effective when the decoder-only LLM has around 50B parameters (Wei
et al., 2021; 2022).
Specifically, we experimented with two pretrained GPT models, a proprietary Nemo GPT-43B and
Llama2-70B. GPT-43B is a 43 billion parameter model that is trained with 1.1T tokens with 70%
English corpus and the other 30% for multilingual and code data. For the English pretraining corpus,
GPT-43B used Common Crawl web archive (W ARC), Wikipedia, Reddit, Books, Gutenberg, ArXiv,
StackExchange, PubMed, etc. It contains 48 layers with the hidden dimension of 8,192. It is trained
with a sequence length of 4,096 and RoPE embeddings (Su et al., 2021). The other Llama2-70B is a
public available 70B GPT model trained on 2T tokens using around 90% English data. It contains 80
layers with the hidden dimension of 8,192. It also has the context window size of 4,096 and trained
with RoPE embeddings.
3.2 D ATASETS AND METRICS
In this study, we include seven datasets ranging from single document QA, multi document QA, to
query-based summarization for our zero shot evaluations. Specifically, we include four datasets from
the validation set of the Scroll benchmark (Shaham et al., 2022).
• QMSum (QM) (Zhong et al., 2021) is a query-based summarization dataset, consisting
of meetings’ transcripts and their corresponding summaries from multiple domains such as
academic, industrial product. In this task, a meeting dialogue transcript is given, and a question
to summarize certain topic is raised about the dialogue, such as “what is agreed between them”.
The answer generally contains a few sentences.
• Qasper (QASP)(Dasigi et al., 2021) is a question answering dataset over NLP papers filtered
from the Semantic Scholar Open Research Corpus (S2ORC) (Lo et al., 2020). Qasper contains
abstractive, extractive, and yes/no questions, as well as unanswerable ones. In this task, one
script is provided together with an information seeking question, such as “which multilingual
approaches do they compare with?”. A model needs to give a short answer by reasoning over
the given context.
• NarrativeQA (NQA) (Koˇciský et al., 2018) is an established question answering dataset over
entire books from Project Gutenberg3 and movie scripts from a list of websites. In this task, the
given passage is transcribed from books and is usually noisy. A model is required to generate a
short phrase by reasoning over the long and noisy text.
• QuALITY (QLTY)(Pang et al., 2022) is a question answering dataset over stories and articles
collected from several resources, such as Project Gutenberg and the Open American National
Corpus4. Different from all the other tasks, this is a multi-choices dataset and a model is required
to select one among four given choices.
We take another three datasets from LongBench (Bai et al., 2023).
• HotpotQA (HQA)(Yang et al., 2018) is a Wikipedia-based question-answer dataset. Different
from above single hot datasets, HQA is a multi-hop dataset where multiple supporting documents
are required to be read for answering and reasoning and the questions are diverse and not
constrained to any pre-existing knowledge bases.
3https://www.gutenberg.org/
4https://anc.org/
4Published as a conference paper at ICLR 2024
QM QASP NQA QLTY MSQ HQA MFQA
# of samples 200 1,726 2,000 2,000 200 200 150
avg doc length 14,140 4,912 84,770 6,592 16,198 13,319 7,185
avg top-5 chunks 2,066 2,071 2,549 2,172 2,352 2,322 2,385
avg top-10 chunks 4,137 3,716 5,125 4,018 4,644 4,554 4,305
avg top-20 chunks 8,160 4,658 10,251 5,890 9,133 8,635 6,570
Table 1: Statistics of seven datasets used for zero-shot evaluation. All lengths are counted by the
number of tokens using Llama2-70B tokenizer, and “avg top N chunks" denotes the average number
of tokens from the top N retrieved chunks. Figure 2 gives more details.
• MuSiQue (MSQ)(Trivedi et al., 2022) is another multi-hop question answering dataset. Com-
pared to HQA, MSQ requires connected reasoning by reducing potential reasoning shortcuts,
minimizing train-test leakage, and including harder distractor contexts. Thus, MSQ is much
harder task than HQA and significantly less cheatable.
• MultiFieldQA-en (MFQA)(Bai et al., 2023) was manually curated to better test the model’s
long context understanding ability across diverse fields. The evidences from multiple sources,
including legal documents, government reports, encyclopedias, and academic papers, are fairly
randomly located in the documents to avoid biases that might occur at the beginning or ending
of the documents.
The full details of the dataset can be found in Table 1. We can see that our evaluation datasets have a
wide range of average document length from 4.9k (QASP) to 84k (NQA). Therefore, for the baseline
model without retrieval, we truncate the document accordingly to fit into the input sequence length.
Following the official metrics, we report the geometric mean of ROUGE scores (i.e., ROUGE-
1/2/L) (Lin, 2004) for QM, the exact matching (EM) score for QLTY , and F1 scores for the remaining
five datasets QASP, NQA, MSQ, HQA and MFQA.
3.3 C ONTEXT WINDOW EXTENSION
We extend the context window length with position interpolation method (Chen et al., 2023), as it is
simple and effective for RoPE embeddings. We extend the 4K context window to 16K for GPT-43B.
For Llama2, we extend its 4K context window to 32k for Llama2-7B and both 16K and 32K for
Llama2-70B. We follow Chen et al. (2023) and finetune both LLMs on the Pile dataset (Gao et al.,
2021) with batch size as 128, constant learning rate of 5e-6 to adapt the position embeddings.
3.4 R ETRIEVAL
For the retriever, we experimented with three retrievers: 1) Dragon (Lin et al., 2023) as it achieves
state-of-the-art results on both supervised and zero-shot information retrieval benchmarks (Thakur
et al., 2021). Dragon is a dual encoder model that consists of a query encoder and a context encoder.
2) a widely used Contriever model (Izacard et al., 2021). Following the MoCo technique (He et al.,
2020), Contriever used a simple contrastive learning framework to pre-train models for information
retrieval. It was trained without supervision and achieved competitive results with BM25 for R@100
on the BEIR benchmark (Thakur et al., 2021), and 3)OpenAI embedding5. For the OpenAI embedding
model, we use the latest “text-embedding-ada-002” as recommended by OpenAI. It accepts 8,191
maximum input tokens for one sequence with an output vector of 1,536 dimensions. The cosine
similarities are then computed between the questions and the list of contexts for retrieval ranking.
To use these retrievers, we first chunk each context document with 300 words, and then we encode
both the questions and all chunks independently with corresponding encoders. The most relevant
N chunks, ranked by the dot product of the question embedding and chunk embedding, are then
concatenated together (following the left to right order from the most relevant to least relevant) as the
context of the prompt for generation. Table 1 shows the statistics of the top N retrieved chunks while
Figure 2 in the Appendix gives more details of the token length distribution of all seven datasets. We
5https://platform.openai.com/docs/guides/embeddings
5Published as a conference paper at ICLR 2024
Model Seq len. Avg. QM QASP NQA QLTY MSQ HQA MFQA
GPT-43B 4k 26.44 15.56 23.66 15.64 49.35 11.08 28.91 40.90
+ ret 4k 29.32 16.60 23.45 19.81 51.55 14.95 34.26 44.63
GPT-43B 16k 29.45 16.09 25.75 16.94 50.05 14.74 37.48 45.08
+ ret 16k 29.65 15.69 23.82 21.11 47.90 15.52 36.14 47.39
Llama2-70B 4k 31.61 16.34 27.70 19.07 63.55 15.40 34.64 44.55
+ ret 4k 36.02 17.41 28.74 23.41 70.15 21.39 42.06 48.96
Llama2-70B 16k 36.78 16.72 30.92 22.32 76.10 18.78 43.97 48.63
+ ret 16k 37.23 18.70 29.54 23.12 70.90 23.28 44.81 50.24
Llama2-70B 32k 37.36 15.37 31.88 23.59 73.80 19.07 49.49 48.35
+ ret 32k 39.60 18.34 31.27 24.53 69.55 26.72 53.89 52.91
Llama2-7B 4k 22.65 14.25 22.07 14.38 40.90 8.66 23.13 35.20
+ ret 4k 26.04 16.45 22.97 18.18 43.25 14.68 26.62 40.10
Llama2-7B 32k 28.20 16.09 23.66 19.07 44.50 15.74 31.63 46.71
+ ret 32k 27.63 17.11 23.25 19.12 43.70 15.67 29.55 45.03
Table 2: Comparison of model variants (GPT-43B, Llama2-7B, Llama2-70B) with sequence length
ranging from 4k to 32k under seven datasets. “ret” denotes using the best retriever (Dragon or
Contriever or OpenAI embeddings) and here we used top-5 for the retriever.
can see that top-5 chunks can all fit into 4k sequence length (except few outliers) while top-10 and
top-20 chunks can fit into 16k sequence length.
3.5 I NSTRUCTION TUNING
To train the pretrained LLMs to follow instructions for question answering or text summarization, we
also performed instruction tuning. We first construct a blend of instruction tuning datasets consisting
of 102K training samples from the Soda dataset (Kim et al., 2022), ELI5 dataset (Fan et al., 2019),
FLAN dataset (Wei et al., 2021) , Open Assistatant dataset (Köpf et al., 2023), Dolly (Conover et al.,
2023) and a proprietary sourced conversational dataset, to adapt all foundation models to follow
instructions. In terms of the template, we use "System: {System}\n\nUser: {Question}\n\nAssistant:
{Answer}" as the format to support multi-turn dialogue training. As all of the tasks contain the
context information for reasoning over at inference time, we add the context before the dialogue, i.e.
"System: {System}\n\n{Context}\n\nUser: {Question}\n\nAssistant: {Answer}".
We finetune the LLM by taking the loss only on the {Answer} part with batch size 128 and learning
rate of 5e-6 for 1000 steps. For the rest of the paper, results are all reported using the instruction
tuned chat model on top of the foundational GPT-43B, Llama2-7B, and Llama2-70B.
4 R ESULTS
In this section, we report the results and provide detailed analysis.
4.1 M AIN RESULTS
In Table 2, we compare different model variants with context lengths ranging from 4K to as long
as 32K using GPT-43B and Llama2-70B. First, we find that baseline models without retrieval of
4k sequence length achieve the worst results for both GPT-43B and Llama2-70B. This is because
the minimum average sequence length of all seven tasks exceeds 4096, the context window of
the foundation models and therefore valuable texts get truncated randomly. As a result, retrieval
is especially helpful for 4K LLMs e.g., Llama2-70B-4K is improved from 31.61 to 35.73 while
GPT-43B-4K is improved from 26.44 to 29.32. Second, we observe that HotpotQA (HQA) especially
favors long sequence models as the score improves from 34.64 to 43.97 for Llama2-70B and from
28.91 to 37.48 for GPT-43B when the sequence length increases from 4k to 16k. This is because
Hotpot QA is a multi-hop dataset where the questions are not hard to answer but all intermediate
hops are necessary to get correct answer. Therefore, long context are beneficial to increase the recall
of incorporating all intermediate hops.
6Published as a conference paper at ICLR 2024
0 4 9 14 19
Position
40
42
44
46
48
50
52
54EM
EM scores (Llama2 70b)
Llama2-70b-4k
Llama2-70b-32k
Figure 1: Llama2-70B also displays lost-in-the-middle phenomenon
Model Avg-7 Avg-4* QM* QASP* NQA* QLTY* MSQ HQA MFQA
Davinci003 (175B) 39.2 40.8* 16.9* 52.7* 24.6* 69.0* 22.1 41.2 47.8
GPT-3.5-turbo (4k) 38.4 39.2* 15.6* 49.3* 25.1* 66.6* 21.2 40.9 49.2
+ret 24.4 49.5 49.5
GPT-3.5-turbo-16k 42.8 42.4 17.6 50.5 28.8 72.6 26.9 51.6 52.3
+ret 30.4 46.6 52.8
Llama2-70B-32k 40.9 42.4 15.6 45.9 28.4 79.6 19.1 49.5 48.4
Llama2-70B-32k-ret 43.6 43.0 18.5 46.3 31.5 75.6 26.7 53.9 52.9
Table 3: Comparison of our best retrieval-augmented Llama2-70B-32k-ret with GPT-3.5-turbo-16k
and Davinci-003 (175B parameters). For QMSum (QM), Qasper (QASP), NarrativeQA (NQA),
QuALITY (QLTY), we used the test set from the ZeroSCROLLS leaderboard as the organizers have
prepared the scores of GPT-3.5-turbo (4k) and Davinci-003 (highlighted with *). Avg-7 refers to the
average score of all 7 datasets, and Avg-4* refers to the average of 4 datasets from ZeroSCROLLS.
It is quite interesting that the retrieval-augmented long context LLM (e.g., 16K and 32K) can obtain
better results than retrieval-augmented 4K context LLM, even they are feed with the same top 5
chunks of evidence. We hypothesize this interesting observation is related to the “lost in the middle”
phenomenon (Liu et al., 2023), where the LLMs has such “U-shaped” performance curve. Specifically,
LLMs are better at utilizing relevant information that occurs at the beginning or end of its input
context window. To further verify the hypothesis, we conduct the “lost-in-the-middle” study following
Liu et al. (2023) for Llama2-70B-4k and Llama2-70B-32k. As show in Figure 1, we confirm that the
phenomenon also exists in Llama2-70B with different context lengths. In particular, the comparison
of the curves from Llama2-70B-4k and Llama2-70B-32k suggests that the long context model has
better accuracy for incorporating top-5 retrieved context.
Note that, we have very different observation from the conclusion drawn from LongBench work (Bai
et al., 2023): “Retrieval brings improvement for model with weak ability on long contexts, but the
performance still lags behind models that have strong long context understanding capability”. Here,
we demonstrate retrieval can significantly improve the performance of both GPT-43B and Llama2-70B
regardless their context window size. For example, our best retrieval-augmented Llama2-70B-32k-ret
outperforms its baseline w/o retrieval by a margin, i.e., 39.60 vs. 37.36. We think the major reason for
such different conclusion is that Bai et al. (2023) uses much smaller LLM with 6B and 7B parameters,
which usually has relatively worse zero-shot capability to incorporate the retrieved chunked context.
To further validate the hypothesis, we also report the results using Llama2-7B in Table 5. One can
actually draw similar conclusions to Bai et al. (2023) . We think the underlying reasons are: i) For
Llama2-7B-chat-4k, its short context length is the bottleneck for long context tasks. Thus, retrieval-
augmentation largely improves the results. ii) For Llama2-7B-chat-32 and ChatGLM2-6B-32k, the
context length bottleneck has been mostly removed. However, their retrieval-augmented models have
limited zero-shot capability of incorporating retrieved chunks of context, due to the smaller size. As
a result, retrieval is not helpful for both Llama2-7B-32k and ChatGLM2-6B-32k, which is different
from large LLMs like Llama2-70B-32k in our case.
7Published as a conference paper at ICLR 2024
Seq len Setting Avg. QM QASP NQA QLTY MSQ HQA MFQA
4k baseline (w/o ret) 31.61 16.34 27.70 19.07 63.55 15.40 34.64 44.55
Dragon 35.73 18.14 29.20 23.39 70.30 20.09 41.54 47.45
Contriever 36.02 17.41 28.74 23.41 70.15 21.39 42.06 48.96
OpenAI-embedding 35.79 17.76 28.85 23.57 70.70 19.92 41.76 47.99
32k baseline (w/o ret) 37.36 15.37 31.88 23.59 73.80 19.07 49.49 48.35
Dragon 39.60 18.34 31.27 24.53 69.55 26.72 53.89 52.91
Contriever 38.85 17.60 31.56 23.88 69.00 26.61 49.65 53.66
OpenAI-embedding 39.34 18.24 32.07 24.36 69.45 24.90 51.64 54.75
Table 4: Comparisons of adding top 5 retrieved chunks from different retrievers to the context under
Llama2-70B.
Seq len Setting Avg. QM QASP NQA QLTY MSQ HQA MFQA
4k base 31.61 16.34 27.70 19.07 63.55 15.40 34.64 44.55
top-5 35.73 18.14 29.20 23.39 70.30 20.09 41.54 47.45
top-10 34.62 16.54 28.67 24.38 68.70 19.00 42.18 42.84
top-20 34.61 16.52 28.67 24.38 68.70 19.00 42.18 42.84
16k base 36.78 16.72 30.92 22.32 76.10 18.78 43.97 48.63
top-5 37.23 18.70 29.54 23.12 70.90 23.28 44.81 50.24
top-10 38.31 18.41 30.20 25.53 73.60 22.78 47.72 49.91
top-20 36.61 17.26 29.60 25.81 72.30 22.69 41.36 47.23
32k base 37.36 15.37 31.88 23.59 73.80 19.07 49.49 48.35
top-5 39.60 18.34 31.27 24.53 69.55 26.72 53.89 52.91
top-10 38.98 17.71 30.34 25.94 70.45 22.80 55.73 49.88
top-20 38.38 16.36 30.42 24.42 69.60 24.51 54.67 48.65
Table 5: Comparisons of adding top-5/10/20 retrieved chunks to the context under 4k, 16k, and 32k
input sequence lengths using Llama2-70B. More context does not always give better results.
In contrast, the larger instruction tuned LLMs like Llama2-70B has much stronger zero-shot capability
to incorporate retrieved evidence. This observation is becoming more clear when one compares the
gain of retrieval-augmentation between GPT-43B and Llama2-70B, where Llama2-70B enjoys larger
benefit of incorporating context through retrieval.
4.2 C OMPARING TO OPEN AI MODELS
To further understand how good is our best model, i.e., augmenting Llama2-70B-32k with retrieval, we
also compare it to GPT-3.5-turbo(4k), GPT-3.5-turbo-16k and Davinci-003 on those seven datasets.6
We found that Llama2-70B-32k-ret achieves better results than GPT-3.5-turbo-16k in terms of the
average accuracy over seven datasets, while better than Davinci-003 (w/ 175B parameters) on the
average over 4 tasks. This indicates Llama2-70B-32k with retrieval is a strong model for these long
context tasks, and our conclusion is built on the state-of-the-art results.
We also report the retrieval augmented results for GPT3.5-turbo on MSQ, HQA and MFQA. For
GPT3.5-turbo-4k, retrieval significantly improves the performance (avg from 37.08 to 41.15). For
GPT3.5-turbo-16k, the average scores for retrieval (43.27) and non-retrieval (43.60) scores are close
to each other which are both lower than our Llam2-70B-32k-ret results (44.51). Note that GPT3.5-
turbo-16k is a blackbox API, we don’t know how it is implemented, the model size as well as any
preprocessing steps.
6For QMSum (QM), Qasper (QASP), NarrativeQA (NQA), QuALITY (QLTY), we used the test set from the
ZeroSCROLLS leaderboard as the organizers have prepared the scores of GPT-3.5-turbo (4k) and Davinci-003
there.
8Published as a conference paper at ICLR 2024
Model Trec SAMSum
GPT-3.5-turbo-16k 68 41.7
Llama2-70B 73 46.5
Llama2-70B-ret 76 47.3
Table 6: Comparison of Llama2-70B to GPT-3.5-turbo-16k with two few-shot learning tasks from
LongBench. Retrieval is helpful for few-shot learning as well.
4.3 A BLATION ON DIFFERENT RETRIEVERS
To investigate the impacts of different retrievers on top of Llama2-70B, we compare Dragon, Con-
triever, and OpenAI embeddings on top of Llama2-70B-4k and Llama2-70B-32k. The results in
Table 4 confirms that our finding, i.e., retrieval can boost the performance of both short context and
long context LLMs, is consistent across different retrievers.
4.4 I NCREASING THE NUMBER OF RETRIEVED CHUNKS
To study the impact of adding more retrieved chunks to the context, we increase the number of
retrieved chunks from 5 to 20 using Dragon retriever and the results can be found in Table 5. We
observe that for different sequence lengths, the best averaged results are obtained either from top 5 or
top 10. Even if 20 chunks can still fit into the 16K and 32K context window (as shown in Figure 2),
adding more chunks up to 20 is not helpful and will sometime hurt the performance. We believe this
is related to the “lost in the middle” phenomenon (Liu et al., 2023) or the model is getting distracted
by irrelevant information and therefore needs further research.
4.5 R ETRIEVAL FOR FEW-SHOT TASKS
In addition to the zero-shot tasks of query-based summarization tasks and question answering tasks
mentioned above, we further investigate the benefits of long context models for few-shot tasks using
two additional datasets (Trec and SAMSum) from LongBench. We take the question from each
dataset as the query and use it to search relevant QA pairs provided in the given few-shot examples.
Table 6 shows that our best model Llama2-70B-32k-ret outperforms its non-retrieval Llama2-70B-32k
baseline as well as GPT-3.5-turbo-16k by a large margin. It again confirms the benefits on using
retrieval together with long context models.
5 C ONCLUSION
In this work, we systematically study the retrieval-augmentation versus long context extension
using the state-of-the-art LLMs after instruction tuning for various long context QA and query-
based summarization tasks. After study, we have the following interesting findings: i) Retrieval
largely boosts the performance of both 4K short context LLM and 16K/32K long context LLMs.
ii) The 4K context LLMs with simple retrieval-augmentation can perform comparable to 16K long
context LLMs, while being more efficient at inference. iii) After context window extension and
retrieval-augmentation, the best model Llama2-70B-32k-ret can outperform GPT-3.5-turbo-16k and
Davinci003 in terms of average score on a suit of downstream tasks with informative queries. Our
study shed light on the promising direction of combining retrieval and long context techniques
together to build better LLM.
6 F UTURE DIRECTIONS
There are many potential research directions that can be extended from this work. One direction is
to develop advanced methods (e.g. memory or hierarchical attention) for existing pretrained large
language models e.g. Llama2-70B, which is itself non-trivial. Also, further extending the context
window to 64k and even longer would be a very interesting study for large 70B parameter models
even though pre-training longer sequence requires much more computation. Lastly, how to mitigate
the “lost-in-the-middle” phenomenon is an open research topic and continue pretraining with UL2
loss (Tay et al., 2022b) could be one potential solution.
9Published as a conference paper at ICLR 2024
REFERENCES
Anthropic. Introducing 100k context windows. https://www.anthropic.com/index/
100k-context-windows, 2023.
Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao
Liu, Aohan Zeng, Lei Hou, et al. Longbench: A bilingual, multitask benchmark for long context
understanding. arXiv preprint arXiv:2308.14508, 2023.
Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.
arXiv preprint arXiv:2004.05150, 2020.
Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican,
George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al.
Improving language models by retrieving from trillions of tokens. In ICML, 2022.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. NeurIPS, 2020.
Aydar Bulatov, Yury Kuratov, and Mikhail Burtsev. Recurrent memory transformer.NeurIPS, 2022.
Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of
large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023.
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse
transformers. arXiv preprint arXiv:1904.10509, 2019.
Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick
Wendell, Matei Zaharia, and Reynold Xin. Free dolly: Introducing the world’s first truly open
instruction-tuned llm, 2023. URL https://www.databricks.com/blog/2023/04/
12/dolly-first-open-commercially-viable-instruction-tuned-llm .
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov.
Transformer-XL: Attentive language models beyond a fixed-length context. In ACL, 2019.
Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv
preprint arXiv:2307.08691, 2023.
Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast and memory-
efficient exact attention with io-awareness. NeurIPS, 2022.
Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. A dataset of
information-seeking questions and answers anchored in research papers. In Proceedings of the
2021 Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, pp. 4599–4610, Online, June 2021. Association for Computational
Linguistics. doi: 10.18653/v1/2021.naacl-main.365. URL https://aclanthology.org/
2021.naacl-main.365.
Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. Eli5:
Long form question answering. arXiv preprint arXiv:1907.09190, 2019.
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason
Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile:
An 800gb dataset of diverse text for language modeling. CoRR, abs/2101.00027, 2021. URL
https://arxiv.org/abs/2101.00027.
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. REALM: Retrieval
augmented language model pre-training. In ICML, 2020.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR), June 2020.
10Published as a conference paper at ICLR 2024
Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans. Axial attention in multidi-
mensional transformers. arXiv preprint arXiv:1912.12180, 2019.
Jie Huang, Wei Ping, Peng Xu, Mohammad Shoeybi, Kevin Chen-Chuan Chang, and Bryan Catanzaro.
Raven: In-context learning with retrieval augmented encoder-decoder language models. arXiv
preprint arXiv:2308.07922, 2023.
Gautier Izacard and Édouard Grave. Leveraging passage retrieval with generative models for open
domain question answering. In EACL, 2021.
Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand
Joulin, and Edouard Grave. Unsupervised dense information retrieval with contrastive learning,
2021. URL https://arxiv.org/abs/2112.09118.
Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane
Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Few-shot learning with
retrieval augmented language models. arXiv preprint arXiv:2208.03299, 2022.
Zhengbao Jiang, Luyu Gao, Jun Araki, Haibo Ding, Zhiruo Wang, Jamie Callan, and Graham Neubig.
Retrieval as attention: End-to-end learning of retrieval and reading within a single transformer. In
EMNLP, 2022.
Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with GPUs. IEEE
Transactions on Big Data, 7(3):535–547, 2019.
Kaiokendev. Things I’m learning while training SuperHOT. https://kaiokendev.github.
io/til#extending-context-to-8k , 2023.
Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi
Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In EMNLP,
2020.
Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization
through memorization: Nearest neighbor language models. arXiv preprint arXiv:1911.00172,
2019.
Hyunwoo Kim, Jack Hessel, Liwei Jiang, Ximing Lu, Youngjae Yu, Pei Zhou, Ronan Le Bras, Malihe
Alikhani, Gunhee Kim, Maarten Sap, et al. Soda: Million-scale dialogue distillation with social
commonsense contextualization. arXiv preprint arXiv:2212.10465, 2022.
Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In ICLR,
2020.
Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens,
Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Richárd Nagyfi, et al. Openassistant
conversations–democratizing large language model alignment. arXiv preprint arXiv:2304.07327,
2023.
Tomáš Koˇciský, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis,
and Edward Grefenstette. The NarrativeQA Reading Comprehension Challenge. Transactions
of the Association for Computational Linguistics, 6:317–328, 05 2018. ISSN 2307-387X. doi:
10.1162/tacl_a_00023. URL https://doi.org/10.1162/tacl_a_00023.
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,
Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented genera-
tion for knowledge-intensive nlp tasks. NeurIPS, 2020.
Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization
Branches Out, pp. 74–81, Barcelona, Spain, July 2004. Association for Computational Linguistics.
URL https://aclanthology.org/W04-1013.
Sheng-Chieh Lin, Akari Asai, Minghan Li, Barlas Oguz, Jimmy Lin, Yashar Mehdad, Wen-tau Yih,
and Xilun Chen. How to train your dragon: Diverse augmentation towards generalizable dense
retrieval. arXiv preprint arXiv:2302.07452, 2023.
11Published as a conference paper at ICLR 2024
Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni,
and Percy Liang. Lost in the middle: How language models use long contexts. arXiv preprint
arXiv:2307.03172, 2023.
Peter J Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam
Shazeer. Generating wikipedia by summarizing long sequences. In ICLR, 2018.
Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel Weld. S2ORC: The semantic
scholar open research corpus. In Proceedings of the 58th Annual Meeting of the Association for
Computational Linguistics, pp. 4969–4983, Online, July 2020. Association for Computational
Linguistics. doi: 10.18653/v1/2020.acl-main.447. URL https://aclanthology.org/
2020.acl-main.447.
Amirkeivan Mohtashami and Martin Jaggi. Landmark attention: Random-access infinite context
length for transformers. arXiv preprint arXiv:2305.16300, 2023.
Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher
Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. WebGPT: Browser-assisted
question-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.
Erik Nijkamp, Hiroaki Hayashi, Tian Xie, Congying Xia, Bo Pang, Congying Xia, and et al. Long
sequence modeling with XGen: A 7b LLM trained on 8k input sequence length. https://
blog.salesforceairesearch.com/xgen/, 2023.
OpenAI. Introducing chatgpt. https://openai.com/blog/chatgpt, 2022.
OpenAI. Gpt-4. https://openai.com/research/gpt-4, 2023a.
OpenAI. Function calling and other API updates (longer context). https://openai.com/
blog/function-calling-and-other-api-updates , 2023b.
Richard Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen,
Vishakh Padmakumar, Johnny Ma, Jana Thompson, He He, and Samuel Bowman. QuALITY:
Question answering with long input texts, yes! In Proceedings of the 2022 Conference of the
North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, pp. 5336–5358, Seattle, United States, July 2022. Association for Computational
Linguistics. doi: 10.18653/v1/2022.naacl-main.391. URL https://aclanthology.org/
2022.naacl-main.391.
Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and
Dustin Tran. Image transformer. In ICML, pp. 4055–4064, 2018.
Ofir Press, Noah A Smith, and Mike Lewis. Train short, test long: Attention with linear biases
enables input length extrapolation. In ICLR, 2021.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language
models are unsupervised multitask learners. 2019.
Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, and Timothy P Lillicrap. Compressive
Transformers for long-range sequence modelling. In ICLR, 2020.
Nir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Inbal Magar, Omri Abend, Ehud Karpas,
Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. Parallel context windows for large
language models. In ACL, 2023.
Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse
attention with routing transformers. Transactions of the Association for Computational Linguistics,
2021.
Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong,
Mor Geva, Jonathan Berant, and Omer Levy. SCROLLS: Standardized CompaRison over long
language sequences. In Proceedings of the 2022 Conference on Empirical Methods in Natural
Language Processing, pp. 12007–12021, Abu Dhabi, United Arab Emirates, December 2022.
Association for Computational Linguistics. URL https://aclanthology.org/2022.
emnlp-main.823.
12Published as a conference paper at ICLR 2024
Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and Omer Levy. Zeroscrolls: A zero-shot
benchmark for long text understanding. arXiv preprint arXiv:2305.14196, 2023.
Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettle-
moyer, and Wen-tau Yih. RePlug: Retrieval-augmented black-box language models.arXiv preprint
arXiv:2301.12652, 2023.
Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced
transformer with rotary position embedding. arXiv preprint arXiv:2104.09864, 2021.
Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. Sparse sinkhorn attention. In
ICML, 2020.
Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, and Che Zheng. Synthesizer:
Rethinking self-attention for transformer models. In ICML, 2021.
Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM
Computing Surveys, 2022a.
Yi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won
Chung, Dara Bahri, Tal Schuster, Steven Zheng, et al. Ul2: Unifying language learning paradigms.
In The Eleventh International Conference on Learning Representations, 2022b.
Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava, and Iryna Gurevych. Beir: A
heterogenous benchmark for zero-shot evaluation of information retrieval models. In NeurIPS,
2021.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée
Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and
efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation
and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.
Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. MuSiQue: Multihop
questions via single-hop question composition. Transactions of the Association for Computational
Linguistics, 10:539–554, 2022. doi: 10.1162/tacl_a_00475. URL https://aclanthology.
org/2022.tacl-1.31.
Szymon Tworkowski, Konrad Staniszewski, Mikołaj Pacek, Yuhuai Wu, Henryk Michalewski,
and Piotr Miło´s. Focused transformer: Contrastive training for context scaling. arXiv preprint
arXiv:2307.03170, 2023.
Boxin Wang, Wei Ping, Peng Xu, Lawrence McAfee, Zihan Liu, Mohammad Shoeybi, Yi Dong,
Oleksii Kuchaiev, Bo Li, Chaowei Xiao, et al. Shall we pretrain autoregressive language models
with retrieval? a comprehensive study. arXiv preprint arXiv:2304.06762, 2023.
Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder,
and Furu Wei. Text embeddings by weakly-supervised contrastive pre-training. arXiv preprint
arXiv:2212.03533, 2022.
Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with
linear complexity. arXiv preprint arXiv:2006.04768, 2020.
Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,
Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint
arXiv:2109.01652, 2021.
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama,
Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models.
arXiv preprint arXiv:2206.07682, 2022.
13Published as a conference paper at ICLR 2024
Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and
Vikas Singh. Nyströmformer: A nyström-based algorithm for approximating self-attention. AAAI,
2021.
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and
Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop question answer-
ing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,
pp. 2369–2380, Brussels, Belgium, October-November 2018. Association for Computational Lin-
guistics. doi: 10.18653/v1/D18-1259. URL https://aclanthology.org/D18-1259.
Dani Yogatama, Cyprien de Masson d’Autume, and Lingpeng Kong. Adaptive semiparametric
language models. Transactions of the Association for Computational Linguistics, 9:362–373, 2021.
Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon,
Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big Bird: Transformers for longer
sequences. In NeurIPS, 2020.
Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,
Wendi Zheng, Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv preprint
arXiv:2210.02414, 2022.
Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan Awadallah,
Asli Celikyilmaz, Yang Liu, Xipeng Qiu, and Dragomir Radev. QMSum: A new benchmark
for query-based multi-domain meeting summarization. In Proceedings of the 2021 Conference
of the North American Chapter of the Association for Computational Linguistics: Human Lan-
guage Technologies, pp. 5905–5921, Online, June 2021. Association for Computational Linguis-
tics. doi: 10.18653/v1/2021.naacl-main.472. URL https://aclanthology.org/2021.
naacl-main.472.
Chen Zhu, Wei Ping, Chaowei Xiao, Mohammad Shoeybi, Tom Goldstein, Anima Anandkumar,
and Bryan Catanzaro. Long-short transformer: Efficient transformers for language and vision.
NeurIPS, 2021.
14Published as a conference paper at ICLR 2024
A A PPENDIX
Figure 2: Token length distribution of the full document and the top-5, 10, 20 chunks of the seven
datasets.
A.1 E XAMPLE
We show an example below where the smaller model Llama2-7B fails to incorporate relevant context,
while larger models with retrieval could successfully predict the correct answer.
15Published as a conference paper at ICLR 2024
Chunk 1 On September 18, 2015, the deluxe edition of the album was released
containing live and instrumental tracks from the standard edition album,
in addition to the single "Light" featuring Little Dragon. Critical recep-
tion ... Angelspit has toured with Angel Theory, Ayria, Ikon, KMFDM,
Tankt and The Crüxshadows, and have also shared the stage with bands
such as The Sisters of Mercy, Nitzer Ebb, Skinny Puppy and Front Line
Assembly. They performed with Lords of Acid during a 22-date U.S.
tour in March 2011 and toured the United States with Blood on the
Dance Floor in October 2011. History Karl Learmont (ZooG) and
Amelia Tan (Destroyx) met on an online zine forum. They shared an
interest in zines and started the distro V ox Populis in 2002.
Chunk 2 They then started making zines for themselves which became the lyrical
inspiration for releases to follow. Angelspit was formed in 2003, and the
duo then self-released their debut EP, Nurse Grenade on 3 October 2004.
... A video for the remix of "Sleep Now" was released on 2 October
2010. They released their third remix album, Carbon Beauty on 8 March
2011. This new remix album contains 3 new tracks as well as 10 remixes
of tracks from the Hideous and Perfect album. A video for "Toxic Girl"
was released on 13 April 2011, and a video for "Like It?
Chunk 3 Passage 1: Blood on the Dance Floor (band) Blood on the Dance Floor
was an American electronic music group from Orlando, Florida,
formed in 2006.The group’s longest standing lineup, from 2009 to
2016, consisted of Jesus David Torres also known as Dahvie Vanity (born
1984) and Jayy V on Monroe (born 1991). ... The CD was self-released in
October 2008. Only 300 copies were made.Vanity and Ecstasy recorded
the singles "Siq With a Q" and "Suicide Club" as a duo in 2008, and
released three extended plays over the first half of 2009, I Scream I
Scream, OMFG Sneak Peak, and Extended Play.
Chunk 4 title: , source: Lick It!" was released on 27 July 2011. On 15 April
2011, Angelspit announced the addition of three new members: guitarist
Valerie Gentile (Black Tape for a Blue Girl, The Crüxshadows), drummer
Chris Kling (Hanzel und Gretyl, Mortiis) and videojammer The Liar.
The new line-up of Angelspit released their fourth studio album, Hello
My Name Is on 11 October 2011. Matt James replaced Chris Kling in
early 2012, and former Crüxshadows guitarist George Bikos filled in for
Valerie Gentile on the band’s 2012 tour...
Chunk 5 Vanity denied these allegations in a video.In April 2017, Vanity an-
nounced that there would be a new member, and that Blood on the Dance
Floor would be returning on May 5 of that year without Jayy V on Mon-
roe. ... On January 1, 2021, Vanity released a new version of Blood on
the Dance Floor’s "Bewitched" as "Bewitched Reimagined" featuring
singer Sammy Beare. As of 2016, Jayy V on Monroe has continued to
work as a drag monster under the name "The Dahli" and Vanity has
continued to release music and merchandise by himself under both "The
Most Vivid Nightmares" and "Dark Arts Official".
Question: Angelspit toured the United States with an American electronic music
duo from Orlando, Florida, that formed in what year?
LLaMA2-7b-32k Angelspit toured the United States with Blood on the Dance Floor in
October 2011.
LLaMA2-7b-32k-ret Angelspit toured the United States with Blood on the Dance Floor in
October 2011.
LLaMA2-70b-32k 2011
LLaMA2-70b-32k-ret 2006
Ground truth 2006
Table 7: All models fail to get the correct answer of 2006, except LLaMA2-70b-32k-ret, which shows
how retrieval together with long context can help derive the correct answer.
16

Lift Yourself Up: Retrieval-augmented Text
Generation with Self-Memory
Xin Cheng1 Di Luo2 Xiuying Chen3 Lemao Liu4 Dongyan Zhao1 Rui Yan2
1 Peking University 2 Remin University of China
3 KAUST 4 Tencent AI Lab
chengxin1998@stu.pku.edu.cn
Abstract
With direct access to human-written reference as memory, retrieval-augmented
generation has achieved much progress in a wide range of text generation tasks.
Since better memory would typically prompt better generation (we define this as
primal problem). The traditional approach for memory retrieval involves selecting
memory that exhibits the highest similarity to the input. However, this method
is constrained by the quality of the fixed corpus from which memory is retrieved.
In this paper, by exploring the duality of the primal problem: better generation
also prompts better memory, we propose a novel framework, Selfmem, which
addresses this limitation by iteratively employing a retrieval-augmented generator
to create an unbounded memory pool and using a memory selector to choose one
output as memory for the subsequent generation round. This enables the model
to leverage its own output, referred to as self-memory, for improved generation.
We evaluate the effectiveness ofSelfmem on three distinct text generation tasks:
neural machine translation, abstractive text summarization, and dialogue generation,
under two generation paradigms: fine-tuned small model and few-shot LLM.
Our approach achieves state-of-the-art results in four directions in JRC-Acquis
translation dataset, 50.3 ROUGE-1 in XSum, and 62.9 ROUGE-1 in BigPatent,
demonstrating the potential of self-memory in enhancing retrieval-augmented
generation models. Furthermore, we conduct thorough analyses of each component
in the Selfmem framework to identify current system bottlenecks and provide
insights for future research1.
1 Introduction
In recent years, retrieval-augmented text generation has attracted growing interest across various
fields, including neural machine translation[28, 17, 2], dialogue response generation[81, 6, 46], and
language modeling[36, 77, 19]. This innovative generation paradigm initially equips a fine-tuned
small model or a large language model (LLM) with access to an external database (typically the
training corpus) using information retrieval techniques. Subsequently, the generation process is
conducted based on both the input text and the retrieved memory.
In this paradigm, the guiding principle for memory retrieval is to find the memory that exhibits
the highest similarity to the current input [36, 96, 49]. This aligns with the human intuition that a
more similar demonstration sample typically offers more hints. As demonstrated in Figure 1, for a
retrieval-augmented translation model, the memory similarity alone exhibits a strong correlation with
the final translation quality, regardless of other factors that may influence translation quality (e.g.,
1Code and data available at: https://github.com/Hannibal046/SelfMemory
37th Conference on Neural Information Processing Systems (NeurIPS 2023).polysemy, morphology, and coreference). We define this as the primal problem: better memory
prompts better generation. Consequently, numerous studies have focused on how to retrieve better
memory, ranging from sparse retrieval to dense retrieval [10, 63], from a fixed retriever to a learnable
retriever [41, 8], and from sentence-level memory to more fine-grained token-level memory [36, 35].
0.0 0.2 0.4 0.6 0.8 1.0
Memory Similarity
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9Hypothesis BLEU
Figure 1: Relation between memory and hy-
pothesis on JRC-Acquis En→De dataset.
The hypothesis is generated by a retrieval-
augmented translator whose memory is re-
trieved from the training set. The X-axis
represents the similarity between memory
and the reference.
However, a fundamental limitation exists in all previous
works: the memory is retrieved from a fixed corpus
and is constrained by the corpus’s quality. Due to the
finite retrieval space, bounded memory significantly
restricts the potential of memory-augmented generation
models [97]. In this paper, we explore the duality of the
primal problem, which posits that better generation
also prompts better memory. We propose a novel
framework called Selfmem, which iteratively employs
a retrieval-augmented generator to create an unbounded
memory pool and uses a memory selector to choose one
output as memory for the subsequent generation round.
By combining the primal and dual problem, a retrieval-
augmented generation model can elevate itself using
its own output, referred to as self-memory. The key
insight behind Selfmem is that the text more closely
resembling the data distribution during inference is not
the training data [87], but the model’s own output.
Selfmem consists of two complementary components:
a retrieval-augmented generator and a memory selector. The generator operates under two distinct
paradigms: fine-tuning a small model or few-shot prompting an LLM. For the former, we train the
generator with labeled data and retrieved memory, while for the latter, we employ a fixed black-box
LLM exclusively for inference alongside retrieved in-context learning samples. We then use the
generator’s output to train a memory selector based on a specific performance metric. By simply
replacing the retrieved memory with unbounded generated memory, we achieve higher-quality
generation output (primal problem), which subsequently serves as memory for the next round after
being refined by the memory selector (dual problem).
To evaluate the efficacy of theSelfmem, we carry out comprehensive experiments in three distinct text
generation tasks: neural machine translation, abstractive text summarization, and dialogue generation.
We witness substantial enhancements over robust baselines, attaining state-of-the-art outcomes in
JRC-Acquis (four directions), XSum (50.3 ROUGE-1), and BigPatent (62.9 ROUGE-1). To gain
deeper insights into the Selfmem, we meticulously investigate each crucial component and pinpoint
the existing system bottleneck to guide future research endeavors.
2 Related Work
2.1 Retrieval-augmented Text Generation
Since the world is not a snapshot once the training corpus is collected, we can never expect an
ever-large model to capture everything in its parameters, even for LLMs like GPT-4 [62]. Therefore,
it is crucial to equip these models with an external memory bank to store additional knowledge or
useful demonstration examples for solving various NLP tasks[41, 78, 95].
In the translation domain, retrieval techniques have long been employed by the localization industry
to enhance human translators’ productivity and consistency even before the advent of machine
translation [94]. Early works on machine translation primarily focused on utilizing memory for
statistical machine translation (SMT) systems [ 80, 50]. For neural machine translation (NMT),
[28] were the first to use search engines to retrieve memory from the training set and incorporate
it with an external memory network. Subsequent research explored various aspects of retrieval-
augmented NMT, such as memory encoding methods [92, 93, 31], joint training of retrievers and
generators with monolingual data [ 8], memory granularity [ 35], and memory diversity [ 17]. For
few-shot LLM generation, strategies for in-context example selection have been proposed to improve
translation quality [2]. Furthermore, in-context machine translation has been shown to be effective
for on-the-fly adaptation [79]. For dialogue response generation tasks, employing exemplar/template
2retrieval as an intermediate step has proven advantageous for generating informative responses [89,
91, 6, 7]. In-context learning example retrieval also aids in controllable dialogue [ 46]. Other
applications include abstractive summarization [64, 14, 18, 15], code generation [30], paraphrase
generation [34, 83], language modeling [36, 105], counterfactual data generation [24], open domain
question answering [12, 33] and semantic parsing [99].
2.2 Neural Text Reranking
By alleviating the discrepancy between training and inference (i.e., exposure bias) and directly
optimizing desired metrics, two-stage reranking methods have facilitated significant progress in
various text generation tasks. In machine translation, pioneering works by [75] and [61] introduced
and popularized discriminative reranking for SMT. In the context of NMT, research has focused on
two primary reranking approaches: generative reranking [56, 32, 88] and discriminative reranking [39,
71, 23]. For syntactic parsing, [21] were the first to employ a two-stage reranking method to select
outputs from a base parser, while [11] introduced a maximum entropy reranker. In text summarization,
RefSum [53] proposed a second-stage summarization framework to address train-test distribution
mismatches. SimCLS [ 54] used pairwise Learning To Rank (LTR) to select candidates with the
highest matching scores. SummaReranker [68] adopted a multi-task mixture-of-experts framework
to leverage different metrics capturing various aspects of generated candidates. BRIO [55] reused
the base model for a second round of fine-tuning with both cross-entropy loss and a candidate-level
ranking loss. JGR [76] employed an alternate training paradigm to train the generator and reranker.
A key limitation of these reranking methods is that they only represent a one-way process, wherein the
selected candidates become the system’s final output. In contrast, our framework innovatively utilizes
the chosen candidates as memory for the subsequent generation round of a retrieval-augmented
generator, which can produce better candidates with enhanced memory.
3 Methods
In this section, we begin with a motivating experiment on generation as memory(§ 3.1). Then, we
introduce Selfmem, a framework comprising a retrieval-augmented generator(§ 3.2) and a memory
selector (§ 3.3). The complete framework and algorithm are illustrated in Figure 2 and Algorithm 1.
3.1 Generation as Memory
The primary motivation behind our framework stems from the observation that the memory, which is
more similar in distribution to the data during inference, is not the training data (38.89 BLEU, as
shown in the first row of Table 1). Instead, it is the model’s own output (58.58 BLEU) within the
unbounded generation space. One interesting exploration involves directly utilizing the generated
output as memory in relation to the primal problem: better memory prompts better generation.
Table 1: Experiments on the relation between mem-
ory quality and the final hypothesis quality, measured
by the BLEU score with ground truth translation. The
retrieval-augmented translator keeps fixed while the
memory is obtained from different sources.
Memory Source Memory Quality Hypothesis Quality
Retrieval 38.89 58.58
Beam 58.58 58.43
Reference 100 90.43
Random 1.14 49.08
We conduct experiments on the JRC-Acquis
En→De dataset. The first row in Table 1
represents conventional retrieval-augmented
training with retrieved memory and achieves
a 58.58 BLEU score. However, directly in-
corporating beam output of this trained model
as memory (Beam) back into the generation
model does not yield any improvements (row
2), despite its higher similarity to the reference
compared to the retrieved ones. We hypoth-
esize two potential reasons for this: (1) the
retrieval-augmented generator may not gen-
eralize effectively in this context due to the
memory distribution shift (from 38.89 to 58.58), and (2) the beam memory does not offer any
information gain compared to the retrieved one, even it exhibits more overlap with the references.
To investigate the first hypothesis, we conduct experiments under the oracle and random scenarios by
using the reference as memory (Reference) and randomly sampled sentences as memory (Random).
The result is shown in Table 1 and it illustrates that a retrieval-augmented generator (trained with
3Target Distribution
 Frozen LLM / Trainable LM
NLL Loss
KL Loss
Y N 
Y 1 
... 
Y 
X 
 Y 
X 
candidates 
source 
target training 
memory 
... ... 
... ... 
(a) Retrieval-augmented Generator (b) Memory Selector
Retrieval
Predicted Distribution
M 
Primal
Dual
Figure 2: Overall framework. There are two components in Selfmem, a retrieval-augmented genera-
tor (a) and a memory selector (b). For the primal problem, (a) takes source and memory as input to
generate candidates for (b). For the dual problem, (b) takes as input source and generated candidates
to select memory for (a).
retrieved memory) has already learned to discriminate between different memories in both oracle and
random scenarios, without updating the model weights.
To evaluate the second conjecture, we first define the token sets of the reference, retrieved memory,
and beam memory as R , M , and B , respectively. The overlap token set, denoted by O , is defined
as the tokens that overlap with the references in the beam memory but not in the retrieved memory,
which is represented as R ∩ B − R ∩ M . O is considered as the additional information provided
by the beam memory. Inspired by the confidence analysis of NMT model [58], we compute the set
confidence score, ψ(· ), as follows:
ψ(· ) = 1
| · |
X
yi∈·
p(yi| x, y<i) (1)
where p(yi| x, y<i) is defined by the generation model. ψ(· ) measures the confidence with which the
generation model generates the tokens. The value of ψ(R ) is 0.58, while that of O is 0.76, indicating
that the generator is relatively confident in generating tokens in O , and therefore does not need to
resort to external memory [38]. Beam search ranks generated candidates based on p(y| x), where the
selected memory falls within the confidence region of the generator and consequently provides no
information gain. This observation motivates us to select memory according to metrics other than
p(y| x) in the memory selector (§3.3).
3.2 Retrieval-augmented Generator
Given a text pair (x, y), where x = { x1, ...,x|x|} is the source, y = { y1, ...,y|y|} is the target. They
could be (document, summary) in summarization, (context, response) in dialogue generation or
(source, target) in machine translation. The retrieval-augmented generation would first use x to
retrieve memory m from datastore D. Then the generator Gξ(x, m), parameterized by ξ, would take
both x and m as input to generate the target sentence y. In this paper, following standard practice,
we choose the training set as D = { (xi, yi)} |D|
i=1. For LLM as Gξ, we use the standard in-context
learning format to give (x, y) as demonstration example. For tunable generator Gξ, we only keep the
target side of top-1 retrieval results as memory and we consider two commonly used architectures:
Joint-Encoder [29, 87, 41] and Dual-Encoder [92, 8, 17].
Joint-Encoder This architecture is the standard encoder-decoder-based model [3, 84]. The input is
the concatenation of x and m. The encoder would first map the input into the hidden states H:
H = Encoder(x [SEP] m) (2)
4And the decoder would incorporate H by attention mechanism and generate tokens in an auto-
regressive manner:
hi = Decoder(CrossAttn(H), y<i) PGξ (·|x, y<i) = Softmax(hi) (3)
Dual-Encoder Instead of treating x and m as a long sequence, this architecture has two encoders,
one for x and the other for m. Their outputs are sequentially attended by the decoder with dual cross
attention as in [17]:
Hx = SourceEncoder(x) Hm = MemoryEncoder(m) (4)
hi = Decoder(CrossAttn(Hx, Hm), y<i) (5)
We use Transformer [84] as the building block for both architectures and optimize Gξ with NLL loss:
Lnll = −
|y|X
t=1
log PGξ (yt|x, m, y<t) (6)
3.3 Memory Selector
The role of memory selector Sθ(x, c), parameterized by θ, is to select one candidate c from the
candidate pool C generated by Gξ based on a specific metric ∆(·, ·). The chosen candidate c is
then utilized as memory m for the subsequent generation round of Gξ. As discussed in §3.1, using
pGξ (y|x) as the metric ∆(·, ·) would result in falling into the confidence region of Gξ, leading to
no information gain. Moreover, a larger value of pGξ (y|x) does not necessarily guarantee improved
generation quality [ 59]. Consequently, we define ∆(·, ·) as model-free metrics that are widely
employed for assessing generation quality, such as BLEU for Neural Machine Translation (NMT)
and ROUGE for Summarization. Our memory selector takes the concatenation of the source x and
candidate ci as input, and produces a multinomial distribution pSθ (·|x) over C.
In this paper, we focus on the role of the memory selector, Sθ(x, c), which is parameterized by θ.
The objective of this selector is to choose a single candidate c from the candidate pool C, generated
by Gξ, based on a specific metric, ∆(·, ·).
pSθ (ci|x) = exp(Sθ(x [SEP] ci))
P|C|
j=1 exp(Sθ(x [SEP] cj))
(7)
In accordance with [39], the training goal for Sθ is to minimize the discrepancy between the Sθ’s
predictions and the scores determined by ∆(·, ·). This divergence is quantified using the Kullback-
Leibler (KL) divergence.
Lkl = −
|C|X
i=1
pM (ci)logpSθ (ci|x) where pM (ci) = exp(∆(ci, y)/τ)
P|C|
j=1 exp(∆(cj, y)/τ)
(8)
τ is the temperature to control the smoothness of the distribution. At inference, the output of the Sθ
is arg max
ci∈C
pSθ (ci|x).
3.4 Combine Generator and Selector
We define two generation modes forGξ. The first mode, referred to as the hypothesis mode, generates
a single output for each input, which is utilized for system evaluation. The second mode, known as
the candidate mode, produces N outputs for a given input, and is employed for training Sθ as well as
memory selection. By integrating two modes together, we present the complete framework of our
proposed model, Selfmem, as illustrated in Algorithm 1.
4 Experimental Setup
4.1 Dataset
We assess the performance of Selfmem on three generation tasks, utilizing a total of seven datasets.
Translation. We evaluate our framework on JRC-Acquis datasets [82], a collection of parallel
5Algorithm 1Selfmem Framework
Require: a dataset D, a retriever R, a memory selection metric ∆(·, ·), a retrieval-augmented
generator Gξ, and a memory selector Sθ
1: retrieve memory M in D with R
2: train Gξ with D and M (if not LLM)
3: use Gξ to generate candidate pool C with M in candidate mode
4: train Sθ on C with ∆(·, ·)
5: while not converged in the validation set do
6: Sθ selects memory from C as M
7: Gξ generates candidate pool C with M in candidate mode
8: end while
9: Gξ generates the final hypothesis with M in hypothesis mode
legislative text of European Union Law. It is the benchmark dataset used in translation memory-
augmented NMT task [28, 92, 8, 17]. We choose 4 translation directions, namely, Spanish↔English
(Es↔En), German↔English (De↔En). Summarization. We evaluate on 2 summarization datasets:
1) XSum [60], extreme summarization, a single-document summarization dataset with highly abstrac-
tive articles from British Broadcasting Corporation. 2) BigPatent [73], consisting of 1.3 million
records of U.S. patent documents along with human-written abstractive summaries. Dialogue. We
experiment on DailyDialog [44], which contains multi-turn dialogs on daily life topics and is used
by [13, 4, 103]. The detailed statistics for these datasets can be found in the Appendix A.
4.2 Implementation Details
We utilize the BM25 algorithm [70] for retrieval purposes. For all tasks, the candidate generation
method consists of beam search with a beam width of 50. The number of iterations is determined
by the performance on the validation set. For translation, we follow the approach of [93, 8, 17],
employing a randomly initialized Transformerbase architecture as Gξ for trainable small model and
XGLM [48] for LLM in-context learning. Evaluation metrics include BLEU, TER, and chrF++
obtained from SACRE BLEU [66]. The memory selector Sθ utilizes an XLM-R base[22] as back-
bone, with BLEU serving as ∆(·, ·). For summarization, we initialize Gξ with BARTbase[40]
for BigPatent and employ BRIO [ 55] for XSum. The evaluation metric comprises ROUGE (R-
1/2/L) [47]. For dialogue generation, BARTbase serves as the backbone for Gξ. Our dialogue system
is evaluated using BLEU (B-1/2) and Distinct (D-1/2) scores [43]. For both dialogue and summariza-
tion tasks, we adhere to the methods of [54, 26], adopting RoBERTabase [52] as the backbone for Sθ.
The linear combination of B-1/2 is chosen as ∆(·, ·) for Dialogue Generation, while R-1/2/L is used
for Summarization, following [76]. For further implementation details, please refer to the Appendix
B and Appendix C for evaluation metrics.
5 Experimental Results
5.1 Machine Translation
We select four translation directions and experiment with two generation paradigms: trainable small
models and few-shot prompted LLMs [85, 20]. For trainable models, we explore two architectures
(joint and dual, as detailed in §3.2). The baselines comprise two types of translation systems: one
being the vanilla sequence-to-sequence model [3, 84] without memory augmentation, and the other
consisting of retrieval-augmented translation models focusing on memory encoding [28, 92], memory
construction [101], memory retrieval [8], and memory diversity [17]. Based on the experimental
results2 shown in Table 2, Selfmem significantly enhances the performance of Gξ across four
translation datasets and two different architectures. This is noteworthy, given that the parameters of
the Gξ remain fixed, with the only variable being the input memory. This finding is consistent with
the primal problemwhich posits that improved memory typically leads to better generation results.
2As higher BLEU scores in this range do not necessarily guarantee a superior translation system [9], we also
evaluate our system using TER and chrF++. The results can be found in the Appendix D.
6Table 2: Results of translation task on JRC-Acquis measured by BLEU. Models denoted by the
same symbol (⋆ and †) have the same parameters and only differ in memory as input. The bolded
numbers show the SOTA performance and the underlined numbers show the second-best result. ∗
denotes the system is significantly better than baselines with p-value < 0.05 tested by [37].
System Es→En En→Es De→En En→De
Dev Test Dev Test Dev Test Dev Test
None Memory
RNNsearch [3] 55.02 59.34 50.54 50.48 50.20 49.74 44.94 43.98
Transformer [84] 64.08 64.63 62.02 61.80 60.18 60.16 54.65 55.43
Retrieval Memory
SEG-NMT [28] 60.28 59.34 57.62 57.27 55.63 55.33 49.26 48.80
NMT-pieces [101] 63.97 64.30 61.50 61.56 60.10 60.26 55.54 55.14
G-TFM [92] 66.37 66.21 62.50 62.76 61.85 61.72 57.43 56.88
MonoNMT [8] 67.73 67.42 64.18 63.86 64.48 64.62 58.77 58.42
CMM [17] 67.48 67.76 63.84 64.04 64.22 64.33 58.94 58.69
Transformerdual⋆ 66.87 67.12 63.14 63.54 64.09 63.36 58.69 58.06
Transformeruni† 67.74 67.32 63.93 64.12 64.50 64.40 58.16 58.58
Self-Memory
Transformerdual⋆ 68.63∗ 69.20∗ 64.12∗ 64.67∗ 65.06∗ 64.98∗ 59.26∗ 59.49∗
Transformeruni† 68.26∗ 68.80∗ 66.07∗ 65.94∗ 65.32∗ 65.65∗ 59.88∗ 60.11∗
Table 3: Comparison between retrieval memory and self-memory. The quality of memory and
hypothesis is measured by the n-gram overlap with reference (BLEU). All experiments are conducted
with Transformerjoint on JRC-Acquis.
Retrieval Self
memory hypothesis memory hypothesis
En-De → 38.89 58.58 57.92 60.11
← 42.56 64.40 64.32 65.65
En-Es → 40.67 64.12 63.57 65.94
← 43.05 67.32 67.78 68.80
The dual problemis revealed in Table 3. Self-memory, which essentially represents the model’s
own output, exhibits greater similarity with the ground truth and serves as a more effective memory
for generating the final output. This observation highlights a key distinction between Selfmem and
previous reranking works [39, 68]. Reranking aims to select candidates of higher quality than the
beam output, whereas inSelfmem, the chosen candidates serve as memory for the retrieval-augmented
generator and do not necessarily need to surpass the quality of the beam hypotheses.
Table 4: Evaluation results of in-context learning with self-memory.
XGLM-1.7B XGLM-4.5B XGLM-7.5B
Random kNN Self Random kNN Self Random kNN Self
En-De → 11.51 37.87 40.94 17.51 37.60 38.25 18.48 47.82 48.32
← 27.42 51.00 51.88 30.62 48.12 48.36 33.03 55.65 55.12
En-Es → 23.87 46.20 48.56 31.83 48.37 49.17 29.97 53.86 54.32
← 25.29 51.55 53.13 32.16 48.55 49.22 35.22 57.25 57.56
In Table 4, we present the results of LLM with self-memory. We employ XGLM [48] as our backbone
generator, with three different sizes ranging from 1.7B to 7.5B. We utilize the recommended prompt
as described in [ 48]. We select three in-context learning examples and report the average scores
from three separate runs, taking into account the sensitivity of example selection in ICL [49]. From
the table, we first observe a general trend where few-shot translation performance improves as the
7size of the model increases. Furthermore, we find that more similar translation demonstrations
significantly enhance performance across all model sizes (from random, kNN to Self). This suggests
that demonstration examples in in-context learning not only act as triggers for model ability but also
adhere to the primal problem, where better demonstration example leads to better generation. Also,
by comparing the results in Table 2 and Table 4, we can conclude that the cross-lingual LLM with
designed examples still falls short of the supervised baselines in this task.
5.2 Summarization
In this paper, we compare the performance of our trainable model with those of REINA [ 87],
PEGASUS [100], and BART [40]. The results are presented in Table5. Initially, it can be observed
that memory has varying impacts on different datasets. The enhancement brought by memory in the
BigPatent dataset is significantly larger than that in the XSum dataset. This can be attributed to the
inherent characteristics of the BigPatent dataset, which consists of official patent documents that
exhibit considerable similarity. Consequently, this greatly improves the summarization quality in
accordance with the primal problem. Furthermore, we discovered that self-memory substantially
enhances the performance of both BRIO (+1.2 R1) and BART (+18.5 R1), achieving state-of-the-art
results on both datasets. We selected these baselines for a fair comparison, as they share the same
base generator. Due to space constraints, additional comparisons and the confidence region of the
SOTA model can be found in the Appendix E.
Table 5: Results of summarization task on XSum and BigPatent measured by ROUGE.
System Memory R-1 R-2 R-L
XSum
PEGASUS None 47.2 24.6 39.3
BRIO None 49.1 25.6 40.4
REINA (PG) Retrieval 48.2 26.0 40.2
REINA (B) Retrieval 43.2 21.0 35.5
REINA (L) Retrieval 46.5 24.1 38.6
BRIOdual⋆ Retrieval 48.6 26.1 40.6
BRIOjoint† Retrieval 49.5 26.5 41.2
BRIOdual⋆ Self 49.2 26.2 40.8
BRIOjoint† Self 50.3 26.7 41.6
System Memory R-1 R-2 R-L
BigPatent
PEGASUS None 53.6 33.2 43.2
BART None 44.4 21.3 31.0
REINA (B) Retrieval 59.5 42.6 50.6
REINA (L) Retrieval 60.7 43.3 51.3
REINA (PG) Retrieval 44.6 21.5 33.3
BARTdual⋆ Retrieval 57.4 43.3 49.7
BARTjoint† Retrieval 59.6 43.4 51.0
BARTdual⋆ Self 61.2 44.6 52.3
BARTjoint† Self 62.9 48.1 59.6
5.3 Dialogue Generation
As demonstrated in Table 6, the self-memory significantly enhances the performance of the retrieval-
augmented generator for dialogue generation tasks. By optimizing memory using BLEU as ∆(·, ·),
the self-memory improves the B-1,2 score over retrieved memory by 3.08 B-1 and 0.6 B-2 on
BARTjoint. Intriguingly, although Selfmem surpasses the baselines in terms of B-1/2, it falls behind in
D-1 and D-2, which can be attributed to the trade-off between BLEU score and Distinct score when
evaluating a dialogue system [104]. To address this issue, we opt for D-1,2 as∆(·, ·) when optimizing
Sθ, denoted as BARTjoint†(D). The results in Table 6 highlight the remarkable flexibility of Selfmem
by directly optimizing memory to achieve the desired attributes for diverse and informative dialogue.
6 Further Analysis
To gain a deeper insight intoSelfmem, we first examine the impact of each key component, namelyGξ
and Sθ. Subsequently, we perform a detailed token-level analysis of the generated output concerning
their frequency in the training set. Experiments are conducted on the JRC-Acquis En→De dataset.
We also include latency analysis and human evaluation on Appendix F and G.
Tuning Sθ We explored various Sθ by direct selection from the candidate pool based on gold
rankings. As shown in Figure 3a, both architectures with enhanced Sθ significantly outperform
the current SOTA performance (60.11 BLEU). Moreover, we assessed the candidate pool quality
during this iterative process using an oracle Sθ, as displayed in Figure 3b. A clear pattern emerges
8Table 6: Results of dialogue generation task on DailyDialog measured by B-1/2 and D-1/2.
BARTjoint (D) denotes the metric ∆(·, ·) for Sθ is the average of D-1 and D-2.
System Memory B-1 B-2 D-1 D-2
NCM [86] None 33.60 26.80 3.00 12.80
iV AE [25] None 30.90 24.90 2.90 25.00
PLATO-2 [5] None 34.80 25.12 3.54 25.11
DialoFlow [45] None 36.17 27.67 4.56 27.12
BART None 20.72 11.36 3.92 19.44
BARTdual⋆ Retrieval 29.50 21.89 4.74 26.01
BARTjoint† Retrieval 36.72 31.55 6.13 35.65
BARTdual⋆ Self 33.43 22.85 4.66 26.16
BARTjoint† Self 39.80 32.15 5.84 32.16
BARTjoint† (D) Self 36.92 32.09 9.12 37.05
1 2 3 4
Iteration
60
62
64
66Hypothesis BLEU
Transformer-joint
rank1
rank2
rank3
rank4
rank5
rank6
1 2 3 4
Iteration
60
62
64
66
Transformer-dual
rank1
rank2
rank3
rank4
rank5
rank6
(a) Hypothesis
1 2 3 4 5
Iteration
45
50
55
60
65
70Candidates BLEU (b) Candidates
Figure 3: (a) shows generation quality in the iteration process with different Sθ in both trainable
generator architectures. (b) shows candidates quality in the iteration process with an oracle Sθ.
in this boxplot, revealing improvements in the oracle, quartile, average, and minimum scores of
the candidate pool. These two experiments jointly clarify the Selfmem’s underlying intuition: a
retrieval-augmented generator profits from superior memory, which can be chosen from its own
unbounded output, and subsequently, the generator with improved memory produces a higher-quality
candidate pool for the next selection round. Consequently, the model lift itself up.
The Least 20% 40% 60% 80% The Most
Frequency Rank
0.65
0.70
0.75
0.80
0.85
0.90
0.95
1.00F1
Self
Retrieval
None
Figure 4: 1-gram F1 score sorted by
training corpus frequency.
Tuning Gξ As discussed in §3.1, we demonstrated that a
trained retrieval-augmented generator, with fixed parameters,
possesses the ability to distinguish between "good" and "bad"
memory. This observation not only justifies our decision to
maintain a fixed generator within our framework but also im-
plies that the Gξ is not the current bottleneck of the Selfmem.
Frequency Analysis We conduct a comprehensive token-
level analysis by computing the 1-gram F1 scores for generated
translations and subsequently categorizing the tokens based
on their frequency in the training set. The results are depicted
in Figure 4. A noticeable pattern emerges, suggesting that the
more frequently a model encounters a token during training,
the higher the accuracy of the generated output [102]. Moreover, our findings indicate that retrieval-
augmented models, particularly those incorporating self-memory augmentation, exhibit superior
performance in handling long-tail inputs which are challenges for parametric models [67, 57].
97 Conclusion
For the first time, we investigate the fundamental limitation of bounded memory in the current
retrieval-augmented literature. We combine the primal and dual problemstogether and propose
Selfmem, a general framework for retrieval-augmented text generation by uplifting generation model
with its own output. We conduct comprehensive experiments across various text generation tasks
and different generation paradigms, including trainable small model and few-shot prompted LLM.
We surpass strong baselines and improve the state-of-the-art performance in serval datasets. We also
meticulously investigate each crucial component and pinpoint the existing system bottleneck to guide
future research endeavors.
Limitations
We discuss the limitations of our framework as follows:
(1) Although Selfmem greatly improves the generation quality compared with other retrieval-
augmented generation models, it requires more computational resources with respect to the memory
selection process. For large dataset with long context (e.g., BigPatent), it would become a more
crucial problem considering the quadratic time complexity of transformer architecture.
(2) This paper proposes a general idea for the retrieval-augmented generation. But we only experiment
with transformer-based architecture for both generator and memory selector and the architecture
of generator and memory selector keeps the same across all text generation tasks. We believe the
task-specific design for the model architecture, training objective and generation methods in different
text generation scenarios would further improve the performance.
Acknowledgement
This work was supported by the National Key Research and Development Program of China
(No.2021YFC3340304) and National Natural Science Foundation of China (NSFC Grant
No.62122089). We appreciate the anonymous reviewers for their helpful comments. Dongyan
Zhao and Rui Yan are the corresponding authors.
References
[1] Armen Aghajanyan, Anchit Gupta, Akshat Shrivastava, Xilun Chen, Luke Zettlemoyer, and
Sonal Gupta. Muppet: Massive multi-task representations with pre-finetuning. In Proc. of
EMNLP, 2021.
[2] Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Zettlemoyer, and Marjan Ghazvininejad.
In-context examples selection for machine translation. CoRR, 2022.
[3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by
jointly learning to align and translate. In Proc. of ICLR, 2015.
[4] Siqi Bao, Huang He, Fan Wang, Hua Wu, and Haifeng Wang. PLATO: pre-trained dialogue
generation model with discrete latent variable. In Proc. of ACL, 2020.
[5] Siqi Bao, Huang He, Fan Wang, Hua Wu, Haifeng Wang, Wenquan Wu, Zhen Guo, Zhibin
Liu, and Xinchao Xu. PLATO-2: Towards building an open-domain chatbot via curriculum
learning. In Proc. of ACL Findings, 2021.
[6] Deng Cai, Yan Wang, Wei Bi, Zhaopeng Tu, Xiaojiang Liu, Wai Lam, and Shuming Shi.
Skeleton-to-response: Dialogue generation guided by retrieval memory. In Proc. of NAACL,
2019.
[7] Deng Cai, Yan Wang, Wei Bi, Zhaopeng Tu, Xiaojiang Liu, and Shuming Shi. Retrieval-guided
dialogue response generation via a matching-to-generation framework. In Proc. of EMNLP,
2019.
10[8] Deng Cai, Yan Wang, Huayang Li, Wai Lam, and Lemao Liu. Neural machine translation with
monolingual translation memory. In Proc. of ACL, 2021.
[9] Chris Callison-Burch, Miles Osborne, and Philipp Koehn. Re-evaluating the role of Bleu in
machine translation research. In Proc. of EACL, 2006.
[10] Qian Cao and Deyi Xiong. Encoding gated translation memory into neural machine translation.
In Proc. of EMNLP, 2018.
[11] Eugene Charniak and Mark Johnson. Coarse-to-fine n-best parsing and maxent discriminative
reranking. In Proc. of ACL, 2005.
[12] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading wikipedia to answer
open-domain questions. In Proceedings of the 55th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), pages 1870–1879, 2017.
[13] Wei Chen, Yeyun Gong, Song Wang, Bolun Yao, Weizhen Qi, Zhongyu Wei, Xiaowu Hu,
Bartuer Zhou, Yi Mao, Weizhu Chen, Biao Cheng, and Nan Duan. Dialogved: A pre-trained
latent variable encoder-decoder model for dialog response generation. In Proc. of ACL, 2022.
[14] Xiuying Chen, Hind Alamro, Mingzhe Li, Shen Gao, Rui Yan, Xin Gao, and Xiangliang
Zhang. Target-aware abstractive related work generation with contrastive learning. In Proc. of
SIGIR, 2022.
[15] Xiuying Chen, Mingzhe Li, Shen Gao, Xin Cheng, Qiang Yang, Qishen Zhang, Xin Gao,
and Xiangliang Zhang. A topic-aware summarization framework with different modal side
information. Proc. of SIGIR, 2023.
[16] Xiuying Chen, Mingzhe Li, Xin Gao, and Xiangliang Zhang. Towards improving faithfulness
in abstractive summarization. In Proc. of NeurIPS, 2022.
[17] Xin Cheng, Shen Gao, Lemao Liu, Dongyan Zhao, and Rui Yan. Neural machine translation
with contrastive translation memories. In Proc. of EMNLP, 2022.
[18] Xin Cheng, Shen Gao, Yuchi Zhang, Yongliang Wang, Xiuying Chen, Mingzhe Li, Dongyan
Zhao, and Rui Yan. Towards personalized review summarization by modeling historical
reviews from customer and product separately. arXiv preprint arXiv:2301.11682, 2023.
[19] Xin Cheng, Yankai Lin, Xiuying Chen, Dongyan Zhao, and Rui Yan. Decouple knowledge
from paramters for plug-and-play language modeling. In Findings of the Association for
Computational Linguistics: ACL 2023, pages 14288–14308, Toronto, Canada, July 2023.
Association for Computational Linguistics.
[20] Xin Cheng, Xun Wang, Tao Ge, Si-Qing Chen, Furu Wei, Dongyan Zhao, and Rui Yan. Scale:
Synergized collaboration of asymmetric language translation engines, 2023.
[21] Michael Collins and Terry Koo. Discriminative reranking for natural language parsing.Comput.
Linguistics, 2005.
[22] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek,
Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov.
Unsupervised cross-lingual representation learning at scale. In Proc. of ACL, 2020.
[23] Yuntian Deng, Anton Bakhtin, Myle Ott, Arthur Szlam, and Marc’Aurelio Ranzato. Residual
energy-based models for text generation. In Proc. of ICLR, 2020.
[24] Tanay Dixit, Bhargavi Paranjape, Hannaneh Hajishirzi, and Luke Zettlemoyer. CORE: A
retrieve-then-edit framework for counterfactual data generation. In Proc. of EMNLP Findings,
2022.
[25] Le Fang, Chunyuan Li, Jianfeng Gao, Wen Dong, and Changyou Chen. Implicit deep latent
variable models for text generation. In Proc. of EMNLP, 2019.
11[26] Jiazhan Feng, Chongyang Tao, Zhen Li, Chang Liu, Tao Shen, and Dongyan Zhao. Reciprocal
learning of knowledge retriever and response ranker for knowledge-grounded conversations.
In Proc. of COLING, 2022.
[27] Tingchen Fu, Xueliang Zhao, Chongyang Tao, Ji-Rong Wen, and Rui Yan. There are a
thousand hamlets in a thousand people’s eyes: Enhancing knowledge-grounded dialogue with
personal memory. In Proc. of ACL, 2022.
[28] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O. K. Li. Search engine guided neural
machine translation. In Proc. of AAAI, 2018.
[29] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Retrieval
augmented language model pre-training. In Proc. of ICML, 2020.
[30] Tatsunori B. Hashimoto, Kelvin Guu, Yonatan Oren, and Percy Liang. A retrieve-and-edit
framework for predicting structured outputs. In Proc. of NeurIPS, 2018.
[31] Qiuxiang He, Guoping Huang, Qu Cui, Li Li, and Lemao Liu. Fast and accurate neural
machine translation with translation memory. In Proc. of ACL, 2021.
[32] Kenji Imamura and Eiichiro Sumita. Ensemble and reranking: Using multiple models in the
NICT-2 neural machine translation system at W AT2017. InProceedings of the 4th Workshop
on Asian Translation, WAT@IJCNLP 2017, Taipei, Taiwan, November 27- December 1, 2017,
2017.
[33] Gautier Izacard and Edouard Grave. Leveraging passage retrieval with generative models for
open domain question answering. In Proceedings of the 16th Conference of the European
Chapter of the Association for Computational Linguistics: Main Volume, pages 874–880,
Online, April 2021. Association for Computational Linguistics.
[34] Amirhossein Kazemnejad, Mohammadreza Salehi, and Mahdieh Soleymani Baghshah. Para-
phrase generation by learning how to edit from samples. In Proc. of ACL, 2020.
[35] Urvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Nearest
neighbor machine translation. In Proc. of ICLR, 2021.
[36] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Gen-
eralization through memorization: Nearest neighbor language models. In Proc. of ICLR,
2020.
[37] Philipp Koehn. Statistical significance tests for machine translation evaluation. In Proc. of
EMNLP, 2004.
[38] Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani,
Victor Zhong, Romain Paulus, and Richard Socher. Ask me anything: Dynamic memory
networks for natural language processing. In Proc. of ICML, 2016.
[39] Ann Lee, Michael Auli, and Marc’Aurelio Ranzato. Discriminative reranking for neural
machine translation. In Proc. of ACL, 2021.
[40] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,
Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: denoising sequence-to-sequence
pre-training for natural language generation, translation, and comprehension. In Proc. of ACL,
2020.
[41] Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,
Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel,
and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive NLP tasks. In
Proc. of NeurIPS, 2020.
[42] Jinpeng Li, Yingce Xia, Rui Yan, Hongda Sun, Dongyan Zhao, and Tie-Yan Liu. Stylized
dialogue generation with multi-pass dual learning. In Proc. of NeurIPS, 2021.
[43] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting
objective function for neural conversation models. In Proc. of NAACL, 2016.
12[44] Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, and Shuzi Niu. Dailydialog: A
manually labelled multi-turn dialogue dataset. In Proceedings of the Eighth International Joint
Conference on Natural Language Processing, IJCNLP 2017, Taipei, Taiwan, November 27 -
December 1, 2017 - Volume 1: Long Papers, 2017.
[45] Zekang Li, Jinchao Zhang, Zhengcong Fei, Yang Feng, and Jie Zhou. Conversations are not
flat: Modeling the dynamic information flow across dialogue utterances. In Proc. of ACL,
2021.
[46] Zekun Li, Wenhu Chen, Shiyang Li, Hong Wang, Jing Qian, and Xifeng Yan. Controllable
dialogue simulation with in-context learning. In Proc. of EMNLP Findings, 2022.
[47] Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summa-
rization Branches Out, 2004.
[48] Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig,
Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer,
Punit Singh Koura, Vishrav Chaudhary, Brian O’Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa
Kozareva, Mona T. Diab, Veselin Stoyanov, and Xian Li. Few-shot learning with multilingual
generative language models. In Proc. of EMNLP, 2022.
[49] Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen.
What makes good in-context examples for gpt-3? In Proceedings of Deep Learning Inside Out:
The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures,
DeeLIO@ACL 2022, Dublin, Ireland and Online, May 27, 2022, 2022.
[50] Lemao Liu, Hailong Cao, Taro Watanabe, Tiejun Zhao, Mo Yu, and Conghui Zhu. Locally
training the log-linear model for SMT. In Proc. of EMNLP, 2012.
[51] Yang Liu and Mirella Lapata. Text summarization with pretrained encoders. In Proc. of
EMNLP, 2019.
[52] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,
Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT
pretraining approach. CoRR, 2019.
[53] Yixin Liu, Zi-Yi Dou, and Pengfei Liu. Refsum: Refactoring neural summarization. In Proc.
of NAACL, 2021.
[54] Yixin Liu and Pengfei Liu. Simcls: A simple framework for contrastive learning of abstractive
summarization. In Proc. of ACL, 2021.
[55] Yixin Liu, Pengfei Liu, Dragomir R. Radev, and Graham Neubig. BRIO: bringing order to
abstractive summarization. In Proc. of ACL, 2022.
[56] Yuchen Liu, Long Zhou, Yining Wang, Yang Zhao, Jiajun Zhang, and Chengqing Zong. A
comparable study on model averaging, ensembling and reranking in NMT. In Proc. of NLPCC,
2018.
[57] Alexander Long, Wei Yin, Thalaiyasingam Ajanthan, Vu Nguyen, Pulak Purkait, Ravi Garg,
Alan Blair, Chunhua Shen, and Anton van den Hengel. Retrieval augmented classification for
long-tail visual recognition. In Proc. of CVPR, 2022.
[58] Yu Lu, Jiali Zeng, Jiajun Zhang, Shuangzhi Wu, and Mu Li. Learning confidence for
transformer-based neural machine translation. In Proc. of ACL, 2022.
[59] Clara Meister, Ryan Cotterell, and Tim Vieira. If beam search is the answer, what was the
question? In Proc. of EMNLP, 2020.
[60] Shashi Narayan, Shay B. Cohen, and Mirella Lapata. Don’t give me the details, just the
summary! topic-aware convolutional neural networks for extreme summarization. In Proc. of
EMNLP, 2018.
13[61] Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Alexan-
der M. Fraser, Shankar Kumar, Libin Shen, David Smith, Katherine Eng, Viren Jain, Zhen Jin,
and Dragomir R. Radev. A smorgasbord of features for statistical machine translation. In Proc.
of NAACL, 2004.
[62] OpenAI. GPT-4 technical report. CoRR, 2023.
[63] Md. Rizwan Parvez, Wasi Uddin Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei
Chang. Retrieval augmented code generation and summarization. InProc. of EMNLP Findings,
2021.
[64] Hao Peng, Ankur P. Parikh, Manaal Faruqui, Bhuwan Dhingra, and Dipanjan Das. Text
generation with exemplar-based adaptive decoding. In Proc. of NAACL, 2019.
[65] Jonathan Pilault, Raymond Li, Sandeep Subramanian, and Chris Pal. On extractive and
abstractive neural document summarization with transformer language models. In Proc. of
EMNLP, 2020.
[66] Matt Post. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference
on Machine Translation: Research Papers, WMT 2018, Belgium, Brussels, October 31 -
November 1, 2018, 2018.
[67] Vikas Raunak, Siddharth Dalmia, Vivek Gupta, and Florian Metze. On long-tailed phenomena
in neural machine translation. In Proc. of EMNLP Findings, 2020.
[68] Mathieu Ravaut, Shafiq R. Joty, and Nancy F. Chen. Summareranker: A multi-task mixture-of-
experts re-ranking framework for abstractive summarization. In Proc. of ACL, 2022.
[69] Mathieu Ravaut, Shafiq R. Joty, and Nancy F. Chen. Towards summary candidates fusion.
CoRR, 2022.
[70] Stephen E. Robertson and Hugo Zaragoza. The probabilistic relevance framework: BM25 and
beyond. Found. Trends Inf. Retr., 2009.
[71] Julian Salazar, Davis Liang, Toan Q. Nguyen, and Katrin Kirchhoff. Masked language model
scoring. In Proc. of ACL, 2020.
[72] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words
with subword units. In Proc. of ACL, 2016.
[73] Eva Sharma, Chen Li, and Lu Wang. BIGPATENT: A large-scale dataset for abstractive and
coherent summarization. In Proc. of ACL, 2019.
[74] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory
cost. In Proc. of ICML, 2018.
[75] Libin Shen, Anoop Sarkar, and Franz Josef Och. Discriminative reranking for machine
translation. In Proc. of NAACL, 2004.
[76] Weizhou Shen, Yeyun Gong, Yelong Shen, Song Wang, Xiaojun Quan, Nan Duan, and Weizhu
Chen. Joint generator-ranker learning for natural language generation. CoRR, 2022.
[77] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke
Zettlemoyer, and Wen tau Yih. Replug: Retrieval-augmented black-box language models,
2023.
[78] Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke
Zettlemoyer, and Wen-tau Yih. REPLUG: retrieval-augmented black-box language models.
CoRR, 2023.
[79] Suzanna Sia and Kevin Duh. In-context learning as maintaining coherency: A study of
on-the-fly machine translation using large language models. CoRR, 2023.
14[80] Michel Simard and Pierre Isabelle. Phrase-based machine translation in a computer-assisted
translation environment. In Proceedings of Machine Translation Summit XII: Papers, MTSum-
mit 2009, Ottawa, Canada, August 26-30, 2009, 2009.
[81] Yiping Song, Rui Yan, Xiang Li, Dongyan Zhao, and Ming Zhang. Two are better than one:
An ensemble of retrieval- and generation-based dialog systems. CoRR, 2016.
[82] Ralf Steinberger, Bruno Pouliquen, Anna Widiger, Camelia Ignat, Tomaz Erjavec, Dan Tufis,
and Dániel Varga. The jrc-acquis: A multilingual aligned parallel corpus with 20+ languages.
In Proc. of LREC, 2006.
[83] Yixuan Su, David Vandyke, Simon Baker, Yan Wang, and Nigel Collier. Keep the primary,
rewrite the secondary: A two-stage approach for paraphrase generation. In Proc. of ACL
Findings, 2021.
[84] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proc. of NeurIPS, 2017.
[85] David Vilar, Markus Freitag, Colin Cherry, Jiaming Luo, Viresh Ratnakar, and George Foster.
Prompting palm for translation: Assessing strategies and performance, 2023.
[86] Oriol Vinyals and Quoc V . Le. A neural conversational model. CoRR, 2015.
[87] Shuohang Wang, Yichong Xu, Yuwei Fang, Yang Liu, Siqi Sun, Ruochen Xu, Chenguang
Zhu, and Michael Zeng. Training data is more valuable than you think: A simple and effective
method by retrieving from training data. In Proc. of ACL, 2022.
[88] Yuguang Wang, Shanbo Cheng, Liyang Jiang, Jiajun Yang, Wei Chen, Muze Li, Lin Shi,
Yanfeng Wang, and Hongtao Yang. Sogou neural machine translation systems for WMT17.
In Proceedings of the Second Conference on Machine Translation, WMT 2017, Copenhagen,
Denmark, September 7-8, 2017, 2017.
[89] Jason Weston, Emily Dinan, and Alexander H. Miller. Retrieve and refine: Improved sequence
generation models for dialogue. In Proceedings of the 2nd International Workshop on Search-
Oriented Conversational AI, SCAI@EMNLP 2018, Brussels, Belgium, October 31, 2018,
2018.
[90] Wenhao Wu, Wei Li, Xinyan Xiao, Jiachen Liu, Ziqiang Cao, Sujian Li, Hua Wu, and Haifeng
Wang. BASS: boosting abstractive summarization with unified semantic graph. In Proc. of
ACL, 2021.
[91] Yu Wu, Furu Wei, Shaohan Huang, Yunli Wang, Zhoujun Li, and Ming Zhou. Response
generation by context-aware prototype editing. In Proc. of AAAI, 2019.
[92] Mengzhou Xia, Guoping Huang, Lemao Liu, and Shuming Shi. Graph based translation
memory for neural machine translation. In Proc. of AAAI, 2019.
[93] Jitao Xu, Josep Maria Crego, and Jean Senellart. Boosting neural machine translation with
similar translations. In Proc. of ACL, 2020.
[94] Masaru Yamada. The effect of translation memory databases on productivity. Translation
research projects, 2011.
[95] Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec, Percy Liang,
Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. Retrieval-augmented multimodal language
modeling. CoRR, 2022.
[96] Dani Yogatama, Cyprien de Masson d’Autume, and Lingpeng Kong. Adaptive semiparametric
language models. Trans. Assoc. Comput. Linguistics, 2021.
[97] Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang
Zhu, Michael Zeng, and Meng Jiang. Generate rather than retrieve: Large language models
are strong context generators, 2023.
15[98] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti,
Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. Big
bird: Transformers for longer sequences. In Proc. of NeurIPS, 2020.
[99] Yury Zemlyanskiy, Michiel de Jong, Joshua Ainslie, Panupong Pasupat, Peter Shaw, Linlu Qiu,
Sumit Sanghai, and Fei Sha. Generate-and-retrieve: Use your predictions to improve retrieval
for semantic parsing. In Proceedings of the 29th International Conference on Computational
Linguistics, pages 4946–4951, Gyeongju, Republic of Korea, October 2022. International
Committee on Computational Linguistics.
[100] Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J. Liu. PEGASUS: pre-training with
extracted gap-sentences for abstractive summarization. In Proc. of ICML, 2020.
[101] Jingyi Zhang, Masao Utiyama, Eiichiro Sumita, Graham Neubig, and Satoshi Nakamura.
Guiding neural machine translation with retrieved translation pieces. In Proc. of NAACL, 2018.
[102] Tong Zhang, Wei Ye, Baosong Yang, Long Zhang, Xingzhang Ren, Dayiheng Liu, Jinan Sun,
Shikun Zhang, Haibo Zhang, and Wen Zhao. Frequency-aware contrastive learning for neural
machine translation. In Proc. of AAAI, 2022.
[103] Xueliang Zhao, Lemao Liu, Tingchen Fu, Shuming Shi, Dongyan Zhao, and Rui Yan. Towards
efficient dialogue pre-training with transferable and interpretable latent structure. CoRR, 2022.
[104] Yinhe Zheng, Zikai Chen, Rongsheng Zhang, Shilei Huang, Xiaoxi Mao, and Minlie Huang.
Stylized dialogue response generation using stylized unpaired texts. In Proc. of AAAI, 2021.
[105] Zexuan Zhong, Tao Lei, and Danqi Chen. Training language models with memory augmenta-
tion. CoRR, 2022.
16A Dataset Details
Table 7: Dataset statistics for three tasks.
Task Dataset #Train #Dev #Test
Translation JRC (en ↔ de) 663,487 2,454 2,483
JRC (en ↔ es) 653,127 2,533 2,596
Summarization BigPatent 1,207,222 67,068 67,072
XSum 204,045 11,332 11,334
Dialogue DailyDialog 87,170 8,069 7,740
B Self Memory Details
For machine translation tasks, following [ 93, 8, 17] we use randomly initialize Transformer base
architecture [84] as Gξ. We use the joint-bpe algorithm [72] and share the parameters between the
memory encoder and source encoder for dual encoder architecture. The hyper-parameter setting
follows [17] with dropout 0.1, label smoothing 0.1, gradient clipping 1.0, Adafactor [74], warm-up
steps 4000, maximum learning rate 4.4e-2 and training epochs 30 for total. The evaluation metrics
are BLEU, TER and chrF++ from SACRE BLEU [66]. The backbone of memory selector Sθ is
XLM-Rbase [22] with BLEU as ∆(·, ·). The hyper-parameter setting for Sθ follows [39] with τ 0.5,
minmax normalization for candidates ranking, Adam optimizer with max learning rate 5e-5 and
polynomial decay scheduler, and classifier dropout 0.2.
For Summarization, we init the Gξ with BARTbase [40] for BigPatent following [87] and state-of-the-
art BRIO [55] for XSum. Optimization is based on Adafactor with a maximum learning rate of 5e-3,
warm-up steps 10000 and gradient clipping value 1.0. The maximum input length is 512 for XSum
and 1024 for BigPatent. The evaluation metric is Rouge (R-1/2/L) [47].
For Dialogue Generation, we use BARTbase as the backbone for Gξ on DailyDialog. We tune the
hyper-parameters from learning rate {5e-3,1e-3,4e-4} and set dropout 0.1, batch size 64, label
smoothing factor 0.1, maximum input length 120 for DailyDialog. Following [4, 13], we evaluate
our dialogue system with BLEU (B-1/2) and Distinct (D-1,2) [ 43]. For both Summarization and
Dialogue Generation task, we follow [ 54, 26] and adopt RoBERTabase [52] as the backbone for
Sθ. We choose the linear combination of B-1/2 as ∆(·, ·) for Dialogue Generation and R-1/2/L for
Summarization following [76]. We tune the hyper-parameters τ from {0.08,0.2,0.5,0.8}, learning
rate from {5e-5,7e-5,2e-4}. The maximum input length for Sθ is 512 and we truncate tokens from
the longer input of source and candidate.
C Evaluation Details
Machine Translation We evaluate our MT system with BLEU, TER and chrF++ from SACRE -
BLEU 3 [66]. The signatures for BLEU, TER and chrF++ are shown in Table 8.
Table 8: Signature from SACRE BLEU.
[c]Signature
nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.0.0
nrefs:1|case:lc|tok:tercom|norm:no|punct:yes|asian:no|version:2.0.0
nrefs:1|case:mixed|eff:yes|nc:6|nw:2|space:no|version:2.0.0
3https://github.com/mjpost/sacrebleu.git
17Summarization We evaluate our Summarization system with standard ROUGE [47] Perl package4
for evaluation. Following [ 55], we use PTB tokenizer 5 for tokenization. And the parameters for
ROUGE are "-c 95 -r 1000 -n 2 -m".
Dialogue Generation Following [27], we evaluate our dialogue system with NLTK BLEU6 with
space as tokenizer and smoothing method1. The Distinction score is from [42].
D More results on translation tasks
Table 9: Evaluation results on JRC-Acquis En→De measured by BLEU, TER and chrF++.
System Memory BLEU ↑ chrF++ ↑ TER↓
Transformer None 55.43 70.31 36.35
Transformerdual Retrieval 58.06 71.58 35.41
Transformerjoint Retrieval 58.58 72.22 34.39
Transformerdual Self 59.49 72.62 34.04
Transformerjoint Self 60.11 73.25 32.62
E More Summarization Baselines
In this Table 10, we include more baselines on the benchmark datasetXSum and BigPatent. We also
report the confidence region of SOTA model for XSum and BigPatent as shown in Table 11.
Table 10: More baselines on XSum and BigPatent.
System R-1 R-2 R-L
XSum
[51] 38.8 16.5 31.3
[40] 45.1 22.3 37.3
[100] 47.2 24.6 39.3
[54] 47.6 24.6 39.4
[55] 49.1 25.6 40.4
[87](PG) 48.2 26.0 40.2
[87](B) 43.1 21.0 35.5
[87](L) 46.5 24.1 38.6
[68] 48.1 25.0 40.0
[69] 47.1 24.1 38.8
[16] 47.8 25.0 39.7
Selfmem 50.3 26.7 41.6
System R-1 R-2 R-L
BigPatent
[100] 53.6 33.1 42.3
[40] 44.4 21.3 31.0
[98] 60.6 42.5 50.0
[65] 38.7 12.3 34.1
[90] 45.0 20.3 39.2
[1] 52.3 33.5 42.8
[87] (B) 59.5 42.6 50.6
[87] (L) 60.7 43.3 51.3
[87] (PG) 44.6 21.5 33.3
Selfmem 62.9 48.1 59.6
F Empirical analysis of latency
In Table 12, we present empirical results of Selfmem latency, measured in seconds. We compare
Selfmem with a retrieval-augmented baseline model across various datasets and computational
platforms, including CPU and CUDA. The number of iterations for Selfmem is set to one. All
experiments are conducted on the same device, equipped with one NVIDIA A100 GPU and one
AMD EPYC 7V13 64-Core Processor.
4https://github.com/summanlp/evaluation/tree/master/ROUGE-RELEASE-1.5.5
5https://nlp.stanford.edu/nlp/javadoc/javanlp/edu/stanford/nlp/process/
PTBTokenizer.html
6https://www.nltk.org/_modules/nltk/translate/bleu_score.html
18Table 11: Confidence region for SOTA model in XSum and BigPatent.
System ROUGE-1/2/L 95%-conf.int
XSum
BRIOjoint
50.3 0.49986 - 0.50602
26.7 0.26300 - 0.26989
41.6 0.41231 - 0.41900
BigPatent
BARTjoint
62.9 0.62664 - 0.63080
48.1 0.47783 - 0.48333
59.6 0.59401 - 0.59847
Table 12: Generation Latency analysis.
NMT XSum BigPatent DailyDialog
Average Input Length 87 512 1024 71
Average Output Length 44 75 127 16
CPU
Retrieval-augmented Baseline 0.97 1.79 3.16 0.32
Selfmem
Candidate Generation 3.20 7.50 15.00 1.02
Memory Selection 0.50 0.52 0.95 0.14
Hypothesis Generation 0.97 1.79 3.00 0.32
×4.80 ×5.47 ×6.04 ×4.63
CUDA
Retrieval-augmented Baseline 0.29 0.44 0.75 0.10
Selfmem
Candidate Generation 0.51 1.00 1.72 0.18
Memory Selection 0.01 0.01 0.01 0.01
Hypothesis Generation 0.29 0.44 0.75 0.10
×2.76 ×2.99 ×3.35 ×2.91
19G Human and GPT-4 Evaluation
We employ both human annotators and GPT-4 (gpt-4-0314) annotators to perform pairwise ranking
of the output generated by Selfmem and baseline systems. For GPT-4 annotators, we utilize the
prompt from Alpaca Eval 7. We randomly select 50 samples for translation tasks and 20 samples for
summarization and dialogue tasks. The win rate of Selfmem versus retrieval-augmented baselines is
depicted in Figure 1.
En-De En-Es XSum DailyDialog
60
70
80
90Win Rate
76.0
80.0
75.0
60.0
88.0
82.0
65.0 65.0
Human and GPT-4 Evaluation Results
GPT-4 Human Annotator
Figure 5: Human and GPT-4 evaluation results.
7https://github.com/tatsu-lab/alpaca_eval/blob/main/src/alpaca_eval/evaluators_
configs/alpaca_eval_gpt4/alpaca_eval.txt
20

1
Retrieval-Augmented Generation for
AI-Generated Content: A Survey
Penghao Zhao∗, Hailin Zhang ∗, Qinhan Yu, Zhengren Wang, Yunteng Geng,
Fangcheng Fu†, Ling Yang, Wentao Zhang †, Jie Jiang, Bin Cui †
Abstract—Advancements in model algorithms, the growth of
foundational models, and access to high-quality datasets have
propelled the evolution of Artificial Intelligence Generated Con-
tent (AIGC). Despite its notable successes, AIGC still faces
hurdles such as updating knowledge, handling long-tail data,
mitigating data leakage, and managing high training and infer-
ence costs. Retrieval-Augmented Generation (RAG) has recently
emerged as a paradigm to address such challenges. In partic-
ular, RAG introduces the information retrieval process, which
enhances the generation process by retrieving relevant objects
from available data stores, leading to higher accuracy and better
robustness. In this paper, we comprehensively review existing
efforts that integrate RAG techniques into AIGC scenarios. We
first classify RAG foundations according to how the retriever
augments the generator, distilling the fundamental abstrac-
tions of the augmentation methodologies for various retrievers
and generators. This unified perspective encompasses all RAG
scenarios, illuminating advancements and pivotal technologies
that help with potential future progress. We also summarize
additional enhancements methods for RAG, facilitating effective
engineering and implementation of RAG systems. Then from
another view, we survey on practical applications of RAG across
different modalities and tasks, offering valuable references for
researchers and practitioners. Furthermore, we introduce the
benchmarks for RAG, discuss the limitations of current RAG
systems, and suggest potential directions for future research.
Github: https://github.com/PKU-DAIR/RAG-Survey.
Index Terms —Retrieval-augmented generation, AI-generated
content, generative models, information retrieval.
I. INTRODUCTION
A. Background
Recent years have witnessed the surge in interests surround-
ing Artificial Intelligence Generated Content (AIGC). Various
content generation tools have been meticulously crafted to
produce diverse outputs across various modalities, such as
Large Language Models (LLMs) including the GPT series [1]–
[3] and the LLAMA series [4]–[6] for texts and codes, DALL-
E [7]–[9] and Stable Diffusion [10] for images, and Sora [11]
for videos. The word “AIGC” emphasizes that the contents are
produced by advanced generative models other than human
beings or rule-based approaches. These generative models
have achieved remarkable performance due to the utilization of
∗ Both authors contributed equally to this research.
† Corresponding authors.
• Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng
Geng, Fangcheng Fu, Ling Yang, Wentao Zhang and Bin Cui are with
Peking University (e-mail: penghao.zhao@stu.pku.edu.cn, z.hl@pku.edu.cn,
yuqinhan@stu.pku.edu.cn, wzr@stu.pku.edu.cn, 1800012997@pku.edu.cn,
ccchengff@pku.edu.cn, yangling0818@163.com, wentao.zhang@pku.edu.cn,
bin.cui@pku.edu.cn).
• Jie Jiang is with Tencent Inc. (email: zeus@tencent.com)
novel model algorithms, explosive scale of foundation models,
and massive high-quality datasets. Specifically, sequence-to-
sequence tasks have transitioned from utilizing Long Short-
Term Memory (LSTM) networks [12] to Transformer-based
models [13], and image-generation tasks have shifted from
Generative Adversarial Networks (GANs) [14] to Latent Dif-
fusion Models (LDMs) [10] as well. Notably, the architecture
of foundation models, initially constituted by millions of
parameters [15], [16], has now grown to billions or even
trillions of parameters [1], [4], [17]. These advancements
are further bolstered by the availability of rich, high-quality
datasets [1], [18], which provide ample training samples to
fully optimize model parameters.
Information retrieval is another pivotal application within
the field of computer science. Different from generation,
retrieval aims to locate relevant existing objects from a vast
pool of resources. The most prevalent application of retrieval
lies in web search engines, which primarily focus on the task
of document retrieval [19], [20]. In the present era, efficient
information retrieval systems can handle document collections
on the order of billions [21], [22]. Besides documents, retrieval
has also been applied for many other modalities [23]–[26].
Despite significant advancements in generative models,
AIGC still grapples with challenges like outdated knowledge,
lack of long-tail knowledge [27], and risks of leaking private
training data [28]. Retrieval-Augmented Generation (RAG)
aims to mitigate these issues with its flexible data reposi-
tory [29]. The retrievable knowledge acts as non-parametric
memory, which is easily updatable, accommodates extensive
long-tail knowledge, and can encode confidential data. More-
over, retrieval can lower generation costs. RAG can reduce
the size of large models [30], support long contexts [31], and
eliminate certain generation steps [32].
A typical RAG process is depicted in Fig. 1. Given an input
query, the retriever identifies relevant data sources, and the re-
trieved information interacts with the generator to improve the
generation process. There are several foundational paradigms
(foundations in short) according to how the retrieved results
augment the generation: they can serve as augmented input
to the generator [33], [34]; they can join at the middle
stage of generation as latent representations [35], [36]; they
can contribute to the final generation results in the form of
logits [37], [38]; they can even influence or omit certain
generation steps [32], [39]. Additionally, researchers have pro-
posed various enhancements to improve the foundational RAG
process. These methods encompass specific optimizations for
individual components as well as holistic enhancements aimed
arXiv:2402.19473v6  [cs.CV]  21 Jun 20242
Fig. 1: A generic RAG architecture. The user queries, spanning different modalities, serve as input to both the retriever and
the generator. The retriever extracts relevant information from data sources. The generator interacts with the retrieval results
and ultimately produces outcomes of various modalities.
at the entire pipeline.
In addition, while the concept of RAG initially emerged
in text-to-text generation [34], this technique has also found
applications across various domains, including codes [40]–
[42], audios [43], [44], images [45]–[47], videos [48], [49],
3D [50], [51], knowledge [52]–[54], and AI for science [55],
[56]. In particular, the essential idea and process of RAG are
largely consistent across modalities. However, it necessitates
minor adjustments in augmentation techniques, and the se-
lection of retrievers and generators varies depending on the
specific modalities and applications.
Despite the rapid growth in recent research on RAG and
the booming applications, a systematic review encompassing
all foundations, enhancements, and applications is notably
absent, hindering the development of this field. For one thing,
the absence of discussion on RAG foundations significantly
undermines the practical value of the research in this do-
main, leaving the potential of RAG not fully explored. While
the majority of research interest, particularly among LLM
researchers, centers on query-based RAG in text-generation
tasks, it is essential to acknowledge that other RAG foun-
dations are also effective and with significant potential for
usage and further development. For another, the lack of an
overview on RAG applications causes researchers and practi-
tioners to overlook RAG’s progress across multiple modalities
and remain unaware of how RAG can be effectively applied.
Although text generation is typically considered as the main
application of RAG, we emphasize that the development of
RAG in other modalities has also begun to catch on and
has yielded promising advancements. Certain modalities have
a rich historical connection to retrieval techniques, infusing
RAG with distinctive characteristics. Inspired by this, in this
paper, our objective is to present a comprehensive survey to
provide a systematic overview of RAG.
B. Contribution
This survey offers a comprehensive overview of RAG, cov-
ering foundations, enhancements, applications, benchmarks,
limitations, and potential future directions. Despite variations
in retrievers and generators across modalities and tasks, we
distill the core principles of RAG foundations, viewing appli-
cations as adaptations of these principles. We aim to offer
references and guidelines to researchers and practitioners,
providing valuable insights for advancing RAG methodologies
and related applications. In summary, we list our contributions
as follows:
• We conduct a comprehensive review of RAG, and distill
the abstractions of RAG foundations for various retrievers
and generators.
• We investigate the enhancements in the literature of
RAG, elaborating the techniques leveraged to enable
more effective RAG systems.
• For various modalities and tasks, we survey existing
AIGC methods that incorporate RAG techniques, exhibit-
ing how RAG contributes to current generative models.
• We discuss the limitations and promising research di-
rections of RAG, shedding light on its potential future
development.
C. Related Work
As the field of RAG advances, several surveys have
emerged; yet they address only specific facets of the area. In3
particular, they either exclusively focus on a single RAG foun-
dation or provide only a brief overview of RAG augmentation
methodologies for limited scenarios.
Most of the existing works focus on text-related RAG tasks
that are facilitated by LLMs, without in-depth investigation
in other modalities. The survey by Li et al. [57] offers a
basic overview of RAG and discusses specific applications
within the scope of text generation tasks. In a similar vein,
the tutorial crafted by Asai et al. [58] centers on retrieval-
based language models, detailing their structures and training
strategies. Meanwhile, a recent survey by Gao et al. [59]
explores RAG in the context of LLMs, with a particular
emphasis on enhancement approaches for query-based RAG.
Recognizing that RAG has extended beyond the text domain,
our work broadens its reach to the entire AIGC landscape,
facilitating a more comprehensive coverage of RAG research.
In addition, another survey proposed by Zhao et al. [60] in-
troduces RAG applications across multiple modalities, but ig-
noring the discussion on RAG foundations. Another work [61]
covers only part works of other modalities. While existing
research has explored various aspects of RAG, there remains
a need for a comprehensive overview that covers RAG foun-
dations, enhancements, and its applicability across different
domains. In this paper, we aim to address the gap by presenting
a systematic survey of RAG.
D. Roadmap
The rest of the paper is organized as follows. Section II elab-
orates on the preliminary of RAG, introducing retrievers and
generators. Section III presents RAG foundations and further
enhancements on RAG. Section IV reviews existing research
on RAG across various applications. Section V investigates the
benchmark frameworks for RAG. Section VI discusses current
limitations of RAG and potential future directions. Finally,
Section VII concludes this paper.
II. PRELIMINARY
In this section, we provide an overview of the general RAG
architecture and explore the generators and the retrievers in
today’s RAG-based AIGC.
A. Overview
As shown in Fig. 1, the entire RAG system consists of
two core modules: the retriever and the generator, where the
retriever searches for relevant information from the data store
and the generator produces the required contents. The RAG
process unfolds as follows: (i) the retriever initially receives
the input query and searches for relevant information; (ii) then,
the original query and the retrieval results are fed into the
generator through a specific augmentation methodology; (iii)
finally, the generator produces the desired outcomes.
B. Generator
The remarkable performance of generative AI across di-
verse tasks has ushered in the era of AIGC. The generation
module plays a crucial role within the RAG system. Different
generative models are applied for different scenarios, such
as transformer models for text-to-text tasks, VisualGPT [62]
for image-to-text tasks, Stable Diffusion [10] for text-to-
image tasks, Codex [2] for text-to-code tasks, etc. Here we
introduce 4 typical generators that are frequently used in RAG:
transformer model, LSTM, diffusion model, and GAN.
1) Transformer Model: Transformer models are one of the
best performing models in the field of Natural Language Pro-
cessing (NLP), consisting of self-attention mechanisms, feed-
forward networks, layer normalization modules, and residual
networks [63]. As illustrated in Figure 2, the final output
sequence is produced by executing vocabulary classification
at each generative step on a sequence of latent representations
derived from tokenization and embedding.
2) LSTM: As shown in Fig. 2, Long Short-Term Memory
(LSTM) [64] is a special form of Recurrent Neural Network
(RNN) model. It tackles the issues of exploding/vanishing
gradients in long-term dependency processing by incorporating
cell states and gating mechanisms. The model comprises
three gates (Input, Forget, and Output) that filter information,
and a central Cell State module that retains and manages
information. It uses the same vocabulary classification method
as transformer models to autoregressively generate outputs.
3) Diffusion Model: Diffusion models are a family of deep
generative models that can create realistic and diverse samples
of data (including images, texts, videos, molecules, etc.) [65].
As shown in Fig. 2, diffusion models work by gradually adding
noise to data until it becomes random, then reversing the
process to generate new data from noise. This process is based
on probabilistic modeling and neural networks.
4) GAN: Generative Adversarial Networks (GANs) [14]
are highly anticipated deep learning models which can simu-
late and generate realistic images, audio, and other data [66].
As shown in Fig. 2, a typical GAN consists of two main
components: a generator and a discriminator. These two parts
compete with each other through adversarial learning, allowing
the generator to continuously improve its ability to generate re-
alistic samples, while the discriminator continuously improves
its ability to distinguish between true and false samples.
C. Retriever
Retrieval is to identify and obtain relevant information given
an information need. Specifically, let’s consider information
resources that can be conceptualized as a key-value store,
where each key corresponds to a value (keys and values
can be identical). Given a query, the objective is to search
for the top- k most similar keys using a similarity function,
and obtain the paired values. Based on different similarity
functions, existing retrieval methods can be categorized into
sparse retrieval, dense retrieval, and others. In widely used
sparse and dense retrieval, the entire process can be divided
into two distinct phases: (i) each object is first encoded into
a specific representation; and then (ii) an index is constructed
to organize the data source for efficient search.
1) Sparse Retriever : Sparse retrieval methods are com-
monly used in document retrieval, where the keys/values rep-
resent the documents to be searched. These methods leverage4
Fig. 2: General architectures of several generators.
term matching metrics such as TF-IDF [67], query likeli-
hood [68], and BM25 [19], which analyze word statistics from
texts and construct inverted indices for efficient searching.
Essentially, BM25 is a strong baseline in large-scale web
search, integrating inverse document frequency weights, query
token occurrences, and other pertinent metrics.
To enable efficient search, sparse retrieval typically lever-
ages an inverted index to organize documents. Concretely, each
term from the query performs a lookup to obtain a list of
candidate documents, which are subsequently ranked based
on their statistical scores.
2) Dense Retriever: Unlike sparse retrieval, dense retrieval
methods represent queries and keys using dense embedding
vectors, and build Approximate Nearest Neighbor (ANN)
index to speed up the search. This can be applied to all
modalities. For text data, recent advancements in pre-trained
models (such as BERT [15]) have been employed encode
queries and keys individually [20]. This approach is often
referred to as Dense Passage Retrieval (DPR). Similar to text,
models have been proposed to encode code data [25], audio
data [69], image data [24], video data [70], etc. The similarity
score between dense representations are usually computed
with metrics such as cosine, inner product, L2-distance.
During training, dense retrieval uses contrastive learning to
increase the similarity of positive samples and decrease that
of negative ones. Several hard negative techniques [71] have
been proposed to further enhance model quality. For efficient
searching during inference, ANN methods are employed.
Various indices are developed to serve ANN search, including
tree [72], [73], locality sensitive hashing [74], neighbor graph
indices (e.g., HNSW [75], DiskANN [76]), and combined
graph and inverted indices (e.g., SPANN [22]).
3) Others: In addition to sparse retrieval and dense re-
trieval, there are alternative methods for retrieving relevant
objects [77], [78]. Instead of calculating representations, some
research works directly use the edit distance between natural
language texts [79] or abstract syntax trees (AST) of code snip-
pets [80], [81]. In knowledge graphs, entities are connected
by relations, serving as a pre-built index for retrieval. Thus,
RAG methods utilizing knowledge graphs can employ k-hop
neighbor searches for retrieval [82], [83]. Another retrieval
method is Named Entity Recognition (NER) [84], where the
query is the input and the entities act as keys.
III. METHODOLOGIES
In this section, we first introduce foundational paradigms
of RAG, and then outline enhancement methods that further
improve the effectiveness.
A. RAG Foundations
Based on how the retriever augments the generator, we
categorize RAG foundations into 4 classes, as shown in Fig. 3.
1) Query-based RAG: Stemming from the idea of prompt
augmentation, query-based RAG seamlessly integrates the
user’s query with insights from retrieved information, feeding
it directly into the initial stage of the generator’s input. This
method is prevalent in RAG applications. Post-retrieval, the
obtained content is merged with the user’s original query
to form a composite input, which is then processed by the
generator to create a response. Query-based RAG is widely
employed across various modalities.
For text generation, REALM [33] employs a dual-BERT
framework to streamline knowledge retrieval and integra-
tion, marrying pre-trained models with knowledge extractors.
Lewis et al. [34] leveraged DPR for information retrieval
and employs BART as the generator to effectively enhance
the generation. SELF-RAG [85] utilizes a critique module to
determine whether the retrieval is required. In addition to being
compatible with local generators, query-based RAG is also
applicable to scenarios that use LLM through API calls. RE-
PLUG [86] follows this methodology by treating the language5
Fig. 3: Taxonomy of RAG foundations.
model as a “black box”, and effectively integrates relevant
external documents into the query. In-Context RALM [87]
uses BM25 for document retrieval and trains a predictive
reranker to reorder and integrate the top-ranked documents.
In the field of code, several works [42], [88]–[91] have
utilized the query-based paradigm to incorporate contextual
information from text or code into the prompt, resulting in
improved effectiveness of downstream tasks.
Recent researches in Knowledge Base Question Answering
(KBQA) has also shown significant effects of combining
retrieval and language models. For instance, Uni-Parser [92],
RNG-KBQA [82], and ECBRF [93] effectively improve the
performance and accuracy of QA systems by merging queries
and retrieved information into prompts.
In the AI-for-Science field, Chat-Orthopedist [94] aids
shared decision-making for adolescents with idiopathic scolio-
sis, improving LLMs’ effectiveness and information precision
by incorporating retrieved data into model prompts.
In the image generation task, RetrieveGAN [45] boosts the
relevance and precision of generated images by incorporat-
ing retrieved data, such as selected image patches and their
bounding boxes, into the generator’s input stage. IC-GAN [95]
modulates the specific conditions and details of the generated
images by concatenating noise vectors with instance features.
For 3D generation, RetDream [50] initially utilizes
CLIP [24] to retrieve relevant 3D assets, then merges the
retrieved contents with the user input during the input phase.
Query-based RAG, often paired with LLM generators,
offers modular flexibility, allowing swift integration of pre-
trained components for quick deployment. Prompt design is
crucial for utilizing retrieved data within this setup.
2) Latent Representation-based RAG : In latent
representation-based RAG framework, retrieved objects are
incorporated into generative models as latent representations.
This enhances the model’s comprehension abilities and
improves the quality of the generated content.
In the text field, FiD [35] and RETRO [36] are two classic
structures of latent representation-based RAG, with many
subsequent works conducting modifications based on them.
FiD [35] processes each retrieved paragraph and its title along-
side the query through distinct encoders, then amalgamates
the resulting latent representations for decoding by a single
decoder to produce the final output. RETRO [36] retrieves rel-
evant information for each segmented sub-query, then applies
a novel module termed Chunked Cross-Attention (CCA) to
integrate the retrieved contents with each sub-query tokens. In
addition, there are other noteworthy novel structures within the
scope of latent representation-based RAG. Several studies [31],
[96] have integrated k Nearest Neighbor (kNN) search within
transformer blocks, allowing for input chunking and, in theory,
addressing the long-criticized context length constraints of
Transformer models. Kuratov et al. [97] integrated Trans-
former with RNN, utilizing the model’s intermediate output
as the content for retrieval.
In the realms of code and science, FiD has gained
widespread adoption, with applications spanning various code-
related fields [98]–[102], and AI-for-Science [55].
In the image domain, several studies [103]–[106] employ
cross-attention mechanisms to fuse retrieval results by inte-
grating their latent representations. Conversely, Li et al. [107]
implement a text-image Affine Combination Module (ACM)
that directly concatenates hidden features.
Within the knowledge domain, several studies [108]–[112]
have adopted FiD and its derivatives for downstream tasks.
EaE [113] enhances the generator’s understanding through
entity-specific parameterization, while TOME [114] pivots to
a nuanced encoding of mentions, prioritizing the granularity
of mentions over entity representations alone.
In the field of 3D generation, ReMoDiffuse [51] introduces
a semantics-modulated attention mechanism which enhances
the accuracy of generating corresponding 3D motions based on
textual descriptions. AMD [115] achieves efficient conversion
from text to 3D motion by fusing the original diffusion process
with the reference diffusion process.
In the audio domain, Koizumi et al. [43] utilized an
LLM, incorporating encoded dense features in the atten-
tion module to guide the generation of audio captions. Re-
AudioLDM [116] utilizes distinct encoders to extract deep
features from text and audio, which are then integrated into the
attention mechanism of its Latent Diffusion Model (LDM).
For video captioning, R-ConvED [48] uses a convolutional
encoder-decoder network to process retrieved video-sentence
pairs with an attention mechanism, generating hidden states to
produce captions. CARE [117] introduces a concept detector
to produce concept probabilities, and incorporates concept
representations into a hybrid attention mechanism.
EgoInstructor [49] uses gated-cross attention to merge text
and video features, improving the relevance and coherence
of captions for egocentric videos. Latent representation-based
RAG, adaptable across modalities and tasks, blends retriever
and generator hidden states but requires additional training
for aligning latent spaces. It enables the development of
sophisticated algorithms that seamlessly incorporate retrieved
information.
3) Logit-based RAG: In logit-based RAG, generative mod-
els integrate retrieval information through logits during the6
decoding process. Typically, the logits are combined through
simple summation or models to compute the probabilities for
step-wise generation.
In the text domain, kNN-LM [37] and its variant [38] blend
language model probabilities with those from retrieval dis-
tances of similar prefixes at each decoding step. TRIME [118]
and NPM [119] are radical evolutions of traditional kNN-
LM approaches, using closely aligned tokens from a local
database as output, particularly boosting performance in long-
tail distribution scenarios.
Beyond text, other modalities, such as code and image, also
leverage logit-based RAG.
In the domain of code, several studies [80], [120] have
also adopted the concept kNN to enhance final output control,
thereby achieving superior performance. Furthermore, EDIT-
SUM [98] improves the quality of code summarization by
integrating prototype summaries at the logit level. For image
captioning, MA [121] directly applies the kNN-LM framework
to address the image caption problem, achieving favorable
results. In summary, logit-based RAG utilizes historical data to
deduce current states and merges information at the logit level,
ideal for sequence generation. It focuses on generator training
and allows for novel methods that capitalize on probability
distributions for future tasks.
4) Speculative RAG: Speculative RAG seeks opportunities
to use retrieval instead of pure generation, aiming to save
resources and accelerate response speed. REST [32] replaces
the small models in speculative decoding [122] with retrieval,
enabling the generation of drafts. GPTCache [39] addresses the
issue of high latency when using the LLM APIs by building
a semantic cache for storing LLM responses. COG [123]
decomposes the text generation process into a series of copy-
and-paste operations, retrieving words or phrases from the
documents instead of generation. Cao et al. [124] proposed a
new paradigm to eliminate the dependence of the final result
on the quality of the first-stage retrieved content, replacing
generation with directly retrieved phrase level content.
In conclusion, speculative RAG is currently primarily ap-
plicable to sequential data. It decouples the generator and
the retriever, enabling the direct use of pre-trained models as
components. Within this paradigm, we can explore a wider
range of strategies to effectively utilize the retrieved content.
B. RAG Enhancements
In this section, we introduce methods which enhance the
performance of a constructed RAG system. We categorize
existing methods into 5 groups based on their enhancement tar-
gets: input, retriever, generator, result, and the entire pipeline.
1) Input Enhancement: The input, initially fed into the re-
triever, significantly impacts the final outcome of the retrieval
stage. In this section, we introduce two methods for input
enhancement: query transformation and data augmentation.
Query Transformation: Query transformation can enhance
the result of retrieval by modifying the input query.
Query2doc [125] and HyDE [126] use the original query to
generate a pseudo document, which is later used as the query
for retrieval. The pseudo document contains richer relevant
information, which helps to retrieve more accurate results.
TOC [127] leverages retrieved contents to decompose the
ambiguous query into multiple clear sub-queries, which are
sent to the generator and aggregated to produce the final result.
For complex or ambiguous queries, RQ-RAG [128] breaks
them down into clear subqueries for fine-grained retrieval and
synthesizes the responses to deliver a cohesive answer to the
original query. Tayal et al. [129] refined the initial query using
dynamic few-shot examples and context retrieval, enhancing
the generator’s grasp of user intent.
Data Augmentation: Data augmentation improves data before
retrieval, including techniques such as removing irrelevant
information, eliminating ambiguity, updating outdated docu-
ments, synthesize new data, etc.
Make-An-Audio [44] uses captioning and audio-text re-
trieval to generate captions for language-free audio to mitigate
data sparsity, and adds random concept audio to improve the
original audio. LESS [130] optimizes dataset selection for
downstream tasks by analyzing gradient information, aiming
to enhance model performance in response to instructional
prompts. ReACC [91] employs data augmentation (including
renaming and dead code insertion) to pre-train the code re-
trieval model. Telco-RAG [131] enhances the retrieve accuracy
by appling a “V ocabulary for 3GPP Specifications”, and match
them to user queries with a router module.
2) Retriever Enhancement : In RAG systems, the quality
of retrieved content determines the information fed into the
generators. Lower content quality increases the risk of model
hallucinations or other degradation. In this section, we intro-
duce efficient ways to enhance retrieval effectiveness.
Recursive Retrieval: Recursive retrieval is to perform multi-
ple searches to retrieve richer and higher-quality contents.
ReACT [132] uses Chain-of-Thought (CoT) [133] to break
queries down for recursive retrieval and provide richer infor-
mation. RATP [134] uses the Monte-Carlo Tree Search for
simulations to select optimal retrieval content, which is then
templated and forwarded to the generator for output.
Chunk Optimization: Chunk optimization refers to adjusting
chunk size for improved retrieval results.
LlamaIndex [135] incorporates a series of chunk optimiza-
tion methods, one of which operates on a ‘small to big’ princi-
ple. The core concept here is to pinpoint finer-grained content
but return richer information. For instance, Sentence-window
retrieval fetches small text chunks and returns a window of
relevant sentences surrounding the retrieved segment. In auto-
merge retrieval, documents are arranged in a tree structure.
The process retrieves the parent node, which encapsulates the
content of its child nodes, by fetching the child node first. To
address the lack of contextual information, RAPTOR [136]
employs recursive embedding, clustering, and summarization
of text chunks until further clustering becomes infeasible,
thereby constructing a multi-level tree structure. Prompt-
RAG [137] enhances retrieval accuracy by pre-generating a
table of contents, enabling the model to autonomously select
relevant chapters based on the query. Raina et al. [138] break
text chunks into finer atomic statements to achieve higher
recall and improved results.7
Fig. 4: Taxonomy of RAG Enhancements.
Retriever Finetuning: The retriever, central to the RAG
system, relies on a proficient embedding model [139]–[142]
to represent related content and feed the generator, enhancing
system performance.
Additionally, embedding models with strong expressive
power can be fine-tuned with domain-specific or task-related
data to boost performance in targeted areas. REPLUG [86]
treats LM as a black box and update the retriever model
based on the final results. APICoder [88] finetunes the re-
triever with python files and api names, signature, descrip-
tion. EDITSUM [98] finetunes the retriever to decrease the
jaccard distance between summaries after retrieval. SYN-
CHROMESH [81] adds tree distance os ASTs in the loss and
uses Target Similarity Tuning (TST) to finetune the retriever.
R-ConvED [48] finetunes the retriever with the same data
as generator. Kulkarni et al. [143] applied infoNCE loss to
finetune the retriever.
Hybrid Retrieval: Hybrid retrieve denotes the concurrent
employment of a diverse array of retrieval methodologies or
the extraction of information from multiple distinct sources.
RAP-Gen [144], BlendedRAG [145]and ReACC [91] use
both dense retriever and sparse retriever to improve the quality
of retrieval. Rencos [80] uses sparse retriever to retrieve similar
code snippets on syntactic-level and uses dense retriever to
retrieve similar code snippets on semantic-level. BASHEX-
PLAINER [99] first uses dense retriever to capture semantic
information and then uses sparse retriever to acquire lexical
information. RetDream [50] first retrieves with text and then
retrieves with the image embedding. CRAG [146] features a
retrieval evaluator that gauges document relevance to queries,
prompting three retrieval responses based on confidence: direct
use of results for Knowledge Refinement if accurate, Web
Search if incorrect, and a hybrid approach for ambiguous
cases. Huang et al. [147] improved question-answering by
introducing DKS (Dense Knowledge Similarity) and RAC
(Retriever as Answer Classifier) in the retrieval phase, evalu-
ating answer relevance and knowledge applicability. UniMS-
RAG [148] introduces a novel kind of token, termed as the
“acting token”, which determines the source from which to
retrieve information. Koley et al. [149] enhance image retrieval
by integrating sketch and text for fine-grained retrieval, yield-
ing improved results.
Re-ranking: The Rerank technique refers to reordering the
retrieved content in order to achieve greater diversity and better
results.
Re2G [150] applies a re-ranker [151] model after the tradi-
tional retriever to reduce the impact of information loss caused
by compressing text into vectors. AceCoder [152] reranks
the retrieved programs with a selector to reduce redundant
programs and obtain diverse retrieved programs. XRICL [153]
uses a distillation-based exemplar reranker after retrieval.
Rangan [154] employs the Quantized Influence Measure, as-
sessing statistical biases between a query and a reference to
evaluate the similarity of data subsets and rerank retrieval re-
sults. UDAPDR [155] uses LLMs to cost-effectively generate
synthetic queries that train domain-specific rerankers, which
then apply multi-teacher knowledge distillation to develop a
cohesive retriever. LLM-R [156] refines its retriever iteratively
by employing a static LLM for document ranking and reward
model training, complemented by knowledge distillation. Each
training cycle incrementally improves the retriever, enabling
progressive optimization. Finardi et al. [157] integrated recip-
rocal rank into the retrieval process for enhanced text chunk
relevance, and utilized monoT5 as a reranker to optimize the
result quality. Li et al. [158] integrate a reranking module into
their end-to-end RAG system, enhancing the retrieval quality
and factual accuracy of LLMs.
Retrieval Transformation: Retrieval Transformation involves
rephrasing retrieved content to better activate the generator’s
potential, resulting in improved output.
FILCO [159] efficiently purges extraneous material from
retrieved text, isolating only the pertinent supporting content to
streamline the generator’s task and facilitate accurate answer
prediction. FiD-Light [160] initially employs an encoder to
convert the retrieved content into a vector, which it then
compresses, resulting in a substantial reduction of latency
time. RRR [161] integrates the current query with the top-k
document in each round through a template, and subsequently
restructures it via a pre-trained LLMs (GPT-3.5-Turbo etc.).
Others: In addition to the above optimization methods, there
are also some other optimization methods for the retrieve
process.
For example, meta-data filtering [162] is a method to help
processing retrieved documents which uses metadata (such8
as time, purpose, etc.) to filter the retrieved documents for
better results. GENREAD [163] and GRG [164] introduce
a novel approach where the retrieval process is supplanted
or improved by prompting a LLM to generate documents in
response to a given question. Multi-Head-RAG [165] employs
multiple embedding models to project the same text chunk into
various vector spaces and utilizes a multi-head attention layer
to capture different informational aspects, thereby increasing
the accuracy of the retrieval process.
3) Generator Enhancement: In RAG systems, the quality
of the generator often determines the quality of the final output
results. Therefore, the ability of the generator determines the
upper limit of the entire RAG system’s effectiveness.
Prompt Engineering: Technologies in prompt engineer-
ing [166] that focus on improving the quality of LLMs’ output,
such as prompt compression, Stepback Prompt [167], Active
Prompt [168], Chain of Thought Prompt [133], etc., are all
applicable to LLM generators in RAG systems.
LLMLingua [169] applies a small model to compresses
the overall length of the query to accelerate model inference,
relieving the negative impact of irrelevant information on the
model and alleviating the phenomenon of “Lost in the Middle”
[170]. ReMoDiffuse [51] decomposes complex descriptions
into anatomical text scripts by using ChatGPT. ASAP [171] in-
corporates exemplar tuples, consisting of input code, function
definitions, analysis results, and corresponding comments, into
prompts to yield better results. CEDAR [89] uses a designed
prompt template to organize code demonstration, query, and
natural language instructions into a prompt. XRICL [153]
utilizes COT technology to add translation pairs as an interme-
diate step in cross linguistic semantic parsing and inference.
ACTIVERAG [172] employs the Cognition Nexus mechanism
to calibrate the intrinsic cognition of LLMs and applies COT
prompt in answer generation. Make-An-Audio [44] is able to
use other modalities as input which can provide much richer
information for the following process.
Decoding Tuning: Decoding tuning involves enhancing gen-
erator control by fine-tuning hyperparameters for increased
diversity and constraining the output vocabulary, among other
adjustments.
InferFix [90] balances the diversity and quality of results by
adjusting the temperature in decoder. SYNCHROMESH [81]
limits the output vocabulary of the decoder by implementing
a completion engine to eliminate implementation errors.
Generator Finetuning: The finetuning of the generator can
enhance the model’s ability to have more precise domain
knowledge or better fit with the retriever.
RETRO [36] fixes the parameters of the retriever and uses
the chunked cross attention mechanism in the generator to
combine the content of the query and retriever. APICoder [88]
finetunes the generator CODEGEN-MONO 350M [173] with
a shuffled new file combined with API information and code
blocks. CARE [117] trains encoders with image, audio, and
video-text pairs, then fine-tunes the decoder (generator) to
simultaneously reduce caption and concept detection loss,
while keeping the encoders and retriever fixed. Animate-A-
Story [174] optimizes the video generator with image data, and
then finetunes a LoRA [175] adapter to capture the appearance
details of the given character. RetDream [50] finetunes a LoRA
adapter [175] with the rendered images.
4) Result Enhancement : In many scenarios, the result of
RAG may not achieve the expected effect, and some tech-
niques of Result Enhancement can help alleviate this problem.
Output Rewrite: Output Rewrite refers to rewriting the con-
tent generated by the generator in certain scenarios to meet the
needs of downstream tasks. SARGAM [176] refines outputs in
code-related tasks by employing a special Transformer along-
side Deletion, Placeholder, and Insertion Classifiers to better
align with the real-world code context. Ring [177] obtains
diversity results by reranking candidates based on the average
of per token log probabilities produced by the generator. CBR-
KBQA [54] revises the result by aligning generated relations
with those presented in the local neighborhood of the query
entity in knowledge graph.
5) RAG Pipeline Enhancement : RAG pipeline enhance-
ment refers to optimizing the overall process of RAG in order
to achieve better performance results.
Adaptive Retrieval: Some studies on RAG suggest that re-
trieval doesn’t always enhance the final results. Over-retrieval
can lead to resource wastage and potential confusion when
the model’s inherent parameterized knowledge suffices for
answering relevant questions. Consequently, this chapter will
delve into two methods for determining retrieval necessity:
rule-based and model-based approaches.
Rule-based: FLARE [178] actively decides whether and
when to search through the probability in the generation pro-
cess. Efficient-KNNLM [38] combines the generation proba-
bility of KNN-LM [37] and NPM [119] with a hyperparameter
λ to determine the proportion of generation and retrieval.
Mallen et al. [179] used statistical analysis on questions to
enable direct answers for high-frequency ones and applied
RAG for low-frequency ones. Jiang et al. [180] evaluated
model confidence based on Model Uncertainty, Input Uncer-
tainty, and Input Statistics to guide retrieval decisions. Kandpal
et al. [181] studied the correlation between the number of
relevant documents and the model’s knowledge mastery to
assess the need for retrieval.
Model-based: Self-RAG [85] uses a trained generator to
determine whether to perform a retrieval based on the retrieve
token under different user querys. Ren et al. [182] used “Judg-
ment Prompting” to determine whether LLMs can answer
relevant questions and whether their answers are correct or not,
thereby assisting in determining the necessity of a retrieval.
SKR [183] uses the ability of LLMs themselves to judge in
advance whether they can answer the question, and if they
can answer, no retrieval is performed. Rowen [184] translates
a question into multiple languages and checks for answer
consistency across these languages, using the results to deter-
mine the need for information retrieval. AdaptiveRAG [185]
dynamically decides whether to retrieve based on the query
complexity by a classifier, which is a smaller LM.
Iterative RAG: Iterative RAG progressively refines results
by repeatedly cycling through retrieval and generation phases,
rather than a single round.
RepoCoder [186] uses an iterative retrieval-generation ap-
proach for code completion, refining queries with previously9
TABLE I: Taxonomy of RAG applications across various modalities.
RAG for Text
Question Answering Human-Machine ConversationNeural Machine Translation Summarization Others
REALM‡§ TKEGEN§ RIAG‡
Fid‡§ RETRO§ NPM‡§
SKR§¶ Self-RAG§¶ TOG‡
ConceptFlow‡§ Skeleton-to-Response‡§
CREA-ICL†‡ Internet-Augmented-DG‡§
BlenderBot3‡§ CEG‡∥
NMT-with-Monolingual-TM†‡§
KNN-MT‡§ COG‡ TRIME‡§
RAMKG‡§ Unlimiformer§
RPRR‡ RIGHT‡§
CONCRETE‡§ Atlas‡§
KG-BART‡§ R-GQA‡§
RAG for Code
Code GenerationCode Summary Code CompletionAutomatic Program RepairText-to-SQL andCode-based Semantic ParsingOthers
SKCODER§ RRGCode‡
ARKS†¶ KNN-TRANX∥
RECODEToolcoder§∥
RACE† BASHEXPLAINER‡
READSUM∥ Rencos‡
CoRec‡ Tram§
 EDITSUM‡
ReACC†‡ RepoCoder†§¶
De-Hallucinator¶ REPOFUSE§
RepoFusion§ EDITAS§
RING∥ CEDAR§
RAP-Gen‡§ InferFix§
SARGAM§ RTLFixer‡§
XRICL‡§ SYNCHROMESH‡§
RESDSQL§ REFSQL‡§
CodeICL§ MURRE∥¶
StackSpotAI‡§ E&V
Code4UIE§ De-fine‡∥
ImputBlaster¶
RAG for Knowledge
Knowledge Base QA Knowledge-augmented Open-domain QA Table for QA Others
CBR-KBQA ‡§∥ TIARA †‡§ Keqing †‡§
RNG-KBQA ‡∥ ReTraCk § SKP †‡§
UniK-QA †‡ KG-FiD ‡ GRAPE ‡
SKURG †‡ KnowledGPT ‡ EFSUM §
EfficientQA ‡ CORE § Convinse †‡
RINK ‡§ T-RAG ‡§ StructGPT ‡
GRetriever § SURGE §
K-LaMP RHO ∥
RAG for 3D
Text-to-3D
ReMoDiffuse †‡
AMD †
RAG for Image
Image Generation Image Captioning Others
RetrieveGAN‡ IC-GAN§ Re-imagen§
RDM Retrieve&Fuse§ KNN-Diffusion
MA∥ REVEAL‡ SMALLCAP†
CRSR† RA-Transformer
PICa∥ Maira‡
KIF‡ RA-VQA‡
RAG for Video
Video Captioning Video QA&Dialogue Others
KaVD‡§ R-ConvED‡§
CARE§
 EgoInstructor†‡§
MA-DRNN†‡ R2A‡
Tvqa+§ VGNMN‡
VidIL†‡ RAG-Driver‡
Animate-A-Story†§
RAG for Science
Drug Discovery Biomedical Informatics Enhancement Math Applications
RetMol†§ PromptDiff†‡ PoET‡ Chat-Orthopedist†§ BIOREADER† MedWriter‡ QARAG†‡ LeanDojo‡ RAG-for-math-QA†‡
RAG for Audio
Audio Generation Audio Captioning
Re-AudioLDM§ Make-An-Audio†§ RECAP‡§
Query-based Latent-based Logit-based
Speculative
 Query+Latent
 Latent+Logit
† Input ‡ Retriever § Generator
∥ Output ¶ Pipeline
generated code to better utilize dispersed information and
improve outcomes. ITER-RETGEN [187] iteratively enhances
content quality by using the generator’s output to pinpoint
knowledge gaps, retrieving necessary information, and in-
forming future generation cycles. SelfMemory [188] utilizes a
retrieval-augmented generator iteratively to form an expansive
memory pool, from which a memory selector picks an output
to inform the next generation cycle. RAT [189] initially
generates content by an LLM with a zero-shot CoT prompt,
then revises each thought step by retrieving knowledge from
external knowledge base.
IV. APPLICATIONS
In this section, we focus on RAG applications spanning
various modalities. To echo with the taxonomy of RAG
foundations and enhancements, we also demonstrate their
utilization across different tasks in Table I.
A. RAG for Text
To begin with, text generation is among the most important
and widely deployed applications for RAG. Here we introduce
popular works for seven tasks, respectively.
1) Question Answering : Question answering involves the
process of providing responses to posed questions by drawing
from a vast and comprehensive collection of textual sources.
FiD [35] and REALM [33] identify the top-k most pertinent
article snippets based on the query and forward each snippet
along with the question to LLMs to generate k responses.
These responses are then synthesized into a final answer.
Toutanova et al. [190] substituted the text corpus in REALM
with subgraphs from a knowledge graph, yielding impressive
results. As shown in Fig. 5, RETRO [36] employs attention
mechanisms to integrate the question with relevant retrieved
documents within the model to produce the final answer.
SKR [183] observes that using RAG does not invariably
benefit question answering and thus explored guiding the
model to evaluate its grasp of pertinent knowledge, subse-
quently adapting its use of external resources for retrieval
enhancement. TOG [191] introduces an innovative knowledge
graph-augmented LLM framework, which excels by fostering
interactions between LLMs and the knowledge graph and
by expanding the inference path space with beam search.
NPM [119] pioneers the use of nonparametric data distribu-
tions in lieu of the softmax layer, enabling models with fewer
parameters to perform effectively. CL-ReLKT [192] employs
a language-generalized encoder to bridge the gap between
question-document pairs across languages, thus better leverag-
ing multilingual data. CORE [193] mitigates language resource
disparities by introducing a novel dense passage retrieval
algorithm and a multilingual autoregressive generation model.
Lastly, EAE [113] enhances answer quality by retrieving
entity embeddings for query entities and integrating these with
hidden states for further processing. UR-QA [194] proposes to
CCA FFW
Transformer 
EncoderRetrieval
dataset
Frozen kNN Retriever
K V
RETRO block (x L) 
Neighbours
Input 
tokens
Chunked cross-attention (CCA)
BERT
BERT
Condition
Attending chunks
Encoded neighbours
CA
CA
ATTN QEMB READ
Attend
Encoded neighbours
C1
C2
C3
H1
H2
H3
H
H1
+
H2
+
E1
 E2
E1
E2
CA(H1
+, E1)
CA(H2
+, E2)
CCA(H, E)
X
Fig. 5: Architecture of RETRO [36] model.
simultaneously retrieve QA pairs and text chunks, selecting the
final answer by comparing their calibrated confidences. DISC-10
LawLLM [195] constructs a supervised fine-tuning dataset
through a legal syllogism prompting strategy, enabling the
model to receive support from the latest legal information.
RAG-end2end [196] conducts simultaneous training of the
retriever (DPR) and the generator (BART) to optimize per-
formance for the end-to-end question-answering task and to
facilitate domain adaptation. MultiHop-RAG [197] extracts
and aggregates information from distinct documents, providing
the generator with the necessary context for definitive query
answers.
2) Fact Verification: Fact verification typically refers to
determining whether a given natural language text and a
related claim or assertion match the facts in the text.
CONCRETE [198] leverages cross-lingual retrieval mecha-
nisms to tap into a wealth of multilingual evidence, effectively
bridging the gap in resources for languages that are underrep-
resented in fact-checking datasets. Atlas [30] shows that using
RAG to support LLMs in knowledge-intensive tasks markedly
improves their few-shot learning performance. Hagstr ¨om et
al. [199] proved on LLaMA [4] and Atlas [30] that search
augmentation is more beneficial for solving inconsistency
problems than increasing model size. Stochastic RAG [200]
employs stochastic sampling without replacement to address
the non-differentiable topk selection process in RAG retrieval,
enabling end-to-end optimization and achieving excellent re-
sults in fact verification scenarios.
3) Commonsense Reasoning: Commonsense reasoning en-
tails the capability of machines to infer or make decisions on
problems or tasks in a human-like manner, drawing upon their
acquired external knowledge and its application.
KG-BART [201] expands the conceptual landscape by
incorporating intricate interrelations among diverse concepts
within a knowledge graph. It employs graph attention mech-
anisms to aid LLMs in crafting more nuanced and logi-
cally coherent sentences. Wan et al. [202] constructed the
CONFLICTINGQA dataset with contentious questions and
conflicting answers to study how textual features affect LMs’
handling of controversial issues.
4) Human-Machine Conversation : Human-machine con-
versation encompasses the ability of machines to comprehend
natural language and adeptly employ this skill to engage with
humans seamlessly.
ConceptFlow [203] leverages a commonsense knowledge
graph to structure conversations, directing the flow of dialogue
based on attention scores, and propelling the conversation
forward. Cai et al. [204] reimagined the text generation
task as a cloze test by retrieving and distilling the essence
of past conversational history, leading to notable outcomes.
Komeili et al. [205] augmented dialogue generation qual-
ity by harnessing advanced search engine technologies to
source pertinent content from the internet. BlenderBot3 [206]
broadens its search horizon, not only mining relevant internet
content but also local dialogue history, and employs entity
extraction among other techniques to refine the quality of the
resulting dialogue. Kim et al. [207], PARC [208], and CREA-
ICL [209] improve the caliber of non-English conversations by
incorporating cross-lingual knowledge, effectively addressing
the scarcity of non-English datasets and enhancing the quality
of the generated dialogue. CEG [210] addresses hallucination
issues through a post-processing mechanism, verifying LLM-
generated answers through retrieval.
5) Neural Machine Translation: Neural Machine Transla-
tion (NMT) is the automated process of translating text from a
source language to a target language [118], [211], [212]. It is a
pivotal task in the domain of NLP and represents a significant
objective in the pursuit of AI, boasting considerable scientific
and practical significance.
Cai et al. [211] proposed an innovative approach that
utilizes monolingual corpora alongside multilingual learning
techniques, challenging the traditional dependency on bilin-
gual corpora in Neural Machine Translation. kNN-MT [212]
executes translation tasks at the token level by computing
vector space distances. TRIME [118] effectively minimizes the
discrepancy between training and inference phases by jointly
training the retrieval system and the generation model, thereby
enhancing the precision of translations.
6) Event Extraction : Event extraction is a process in
NLP that involves identifying and categorizing specific events
within a text and associating them with relevant entities. These
events are usually represented by verbs and the entities are
the participants involved in the event. R-GQA [213] enhances
the context of a given issue by identifying and utilizing the
most closely aligned Question-Answer pair from a repository,
thereby enriching the information available for processing the
current query.
7) Summarization: Summarization is a task aimed at dis-
tilling the essential information from lengthy texts and produc-
ing a concise, coherent summary that encapsulates the primary
themes. There are two main approaches to summarization:
extractive and abstractive.
Extractive summarization involves the automatic selection
and compilation of key phrases directly from the source
text, which refrains from creating new sentences, instead
repurposing segments from the original text.
Abstractive summarization, on the other hand, entails com-
prehending the original text’s meaning and reformulating it
into new sentences [96], [214]–[216], which can convey the
source’s intent more fluidly but poses greater challenges in
terms of implementation due to its complexity. RAMKG [214]
effectively leverages a comprehensive English corpus to bol-
ster the performance of keyphrase generation in non-English
contexts. Unlimiformer [96] addresses the issue of input length
constraints in transformer-based models by retrieving and uti-
lizing the top-k most relevant hidden states, thereby extending
the model’s capacity to handle longer inputs. RPRR [215]
employs a Retrieve-Plan-Retrieve-Read approach to overcome
the limited context window constraints faced by LLMs, utiliz-
ing retrieved information to generate high-quality Wikipedia
documents for emerging events. RIGHT [216] chooses to use
different types of retrievers in different datasets to enhance the
generator. M-RAG [217] significantly enhances text summa-
rization by segmenting documents into various databases and
incorporating multi-agent reinforcement learning techniques.11
B. RAG for Code
Separate retrieval and generation approaches have histor-
ically been employed for code-related tasks. For retrieval,
similar code snippets can be identified using Abstract Syntax
Trees (AST) or text edit distance. For generation, sequence-
to-sequence models are employed to generate code or natural
language. Recent RAG research combines both retrieval and
generation techniques to enhance the overall performance.
1) Code Generation: Code generation aims to convert Nat-
ural Language (NL) descriptions into code implementations.
Query-based RAG is a common method for code gen-
eration. It builds prompts for transformer-based generative
models with retrieved information, including similar exam-
ples [40], [152], [218]–[221], relevant API details [88],
[222], documentations [42], imports [223], and global func-
tions [224]. SKCODER [225] retrieves relevant code snip-
pets to produce sketch template for final code generation.
RRGCode [226] employs a cross-encoder to rank the re-
trieval results. CODEAGENT [227] designs agents for web
search, documentation retrieval, program generation, and cor-
rectness testing. ARKS [228] incorporates iterative RAG to
re-formulate queries and update retrieval sources.
Logit-based RAG is also applicable for code generation.
RECODE [79] retrieves NL descriptions and paired codes
using edit distance, then extracts n-gram action subtrees from
ASTs. During LSTM-based generation, the processed subtrees
are leveraged through logits at each decoding step. kNN-
TRANX [120] uses a seq2tree model to convert NL to code
AST. During each decoding step, hidden states are searched
in the AST prefix datastore to create new probabilities, later
merged with the seq2tree model’s output via a confidence
network.
ToolCoder [229] generates codes containing special tokens.
When it encounters these tokens, ToolCoder performs online
search or offline retrievals to fill in the blanks with API calls,
which is a specialized form of speculative RAG.
2) Code Summarization: Code summarization tasks in turn
convert the code into NL descriptions.
Many research works process retrieval results using ad-
ditional encoders and then combine them for subsequent
decoder, which is similar to the Fusion-in-Decoder [35].
Source Code Repository
Extract
Java Methods
Comments
CommentCode
CommentCode
CommentCode
Training Set
Test Set
Validation Set
Divided by project
Retrieval Corpus / Training SetRetrieve Module
Refine Module
Attention Mechanism
Input Code RepresentationSimilar Code RepresentationExemplar Representation
Data Preprocessing Training and Test
EncodersEncodersEncoders
Decoder
Fig. 6: Architecture of Re2Com [101] model.
Re2Com [101] and EditSum [98] retrieve similar codes
using BM25 and generate summary using LSTM. They sep-
arately encode the input, the retrieved code, and the corre-
sponding summary, then combine the hidden states or logits
in the decoder. HGNN [230] instead uses code edit distance for
retrieval, and substitutes the code encoder with hybrid GNN
on their Code Property Graphs (CPG) [231]. RACE [102]
employs separate encoders for the input code difference,
the retrieved code differences through dense retrieval, and
corresponding commit message to generate the final commit
messages. BASHEXPLAINER [99] applies dense retrieval,
and fuses the embeddings for subsequent transformer-based
decoder. READSUM [232] uses Levenshtein distance for
retrieval, and employs a fusion network to combine the repre-
sentations of retrieved codes and summaries.
Query-based RAG is prevalent for code summary genera-
tion. REDCODER [40], ASAP [171], and SCCLLM [233] all
form prompts with retrieved contents for summarization. They
employ dense retrieval, sparse retrieval, and hybrid retrieval
(including semantic, syntactic, and lexical-based retrieval),
respectively. The paradigm is also leveraged for pseudocode
generation [234] and log statement generation [235].
Logit-based RAG also prevails in code summarization.
Rencos [80] and CoRec [236] retrieve similar code snippets or
code differences through AST or dense representations. They
both adopt multiple LSTMs for the input and the retrieved
results, and the probabilities are combined for final generation.
kNN-Transformer [237] uses a transformer-based generator to
obtain context vectors of input codes, then combines three
parts of logits from vector search, the generator, and the
copy mechanism for rare tokens in the input. Tram [238] also
combines three sets of logits from the original generator, the
generator for sentence-level retrieved results, and the search
logits of the token-level vectors (which represent the source
codes and their ASTs). CMR-Sum [239] incorporates the
cross-attention probabilities between the retrieved summary
and the generated summary, to the original generation logits.
3) Code Completion: Code completion is akin to the code
version of the “next sentence prediction” task.
Query-based RAG is the mainstream paradigm for code
completion. Drain et al. [240] retrieved template functions
for function completion. ReACC [91] uses both sparse and
dense retrieval. RepoCoder [186] performs iterative RAG
by augmenting the retrieval input with previously generated
code. De-Hallucinator [241] retrieves API references using
first-time generated contents, then conducts query-based RAG
for improved code completion. REPOFUSE [242] includes
rationale context and retrieved codes to form prompt, and
ranks the contexts to fit in the length limit.
Many works leverage latent representation-based RAG.
Retrieve-and-edit [100], RepoFusion [243], and EDITAS [244]
employ multiple encoders for retrieved contents or edit se-
quences, then fuse the information for subsequent decoder.
CoCoMic [245] retrieves codes on the project context graph of
the whole code project. It jointly processes the representations
of source codes and retrieved contexts in the generator.
kNM-LM [246] performs logit-based RAG, combining the
logits of retrieval and generation using bayes inference.
4) Automatic Program Repair: Query-based RAG is often
used in automatic program repair to help generative models fix
buggy codes. RING [177], CEDAR [89], and RAP-Gen [144]
all use hybrid retrieval (including both sparse and dense
retrieval) for similar error messages, buggy codes, or fixes
to build prompts. InferFix [90] includes the bug type, the12
location, relevant syntax hierarchies, and similar fixes into the
prompt. SARGAM [176] utilizes prompts with similar buggy
codes to generate patches; then another model is employed to
refine the final result. RTLFixer [247] leverages ReAct [132] to
implement an agent fixing errors in Verilog codes. It iteratively
retrieves errors and paired solutions, and combines reasoning
and action planning into prompts for LLMs.
5) Text-to-SQL and Code-based Semantic Parsing : Se-
mantic parsing converts NL into clear, structured represen-
tations, like SQL or other domain-specific languages, often
with the assistance of codes. All related works that employ
RAG specifically utilize its query-based variant. XRICL [153]
searches and reranks English utterance using non-English
ones, then builds prompt to generate SQL queries. SYN-
CHROMESH [81] retrieves similar NL and SQL to build
prompts, then conducts constrained semantic decoding to
enforce rich syntactic and semantic constraints during SQL
generation. CodeICL [248] uses Python for semantic parsing,
leveraging BM25 to incorporate similar training examples
into prompts. RESDSQL [249] includes ranked schemas into
prompts to generate SQL skeleton and SQL query. ReF-
SQL [250] uses a structure-enhanced retriever with schema
linking and Mahalanobis contrastive learning, which helps to
make better text-to-SQL generation. To build prompts for SQL
generation, ODIS [251] retrieves both in-domain and out-
of-domain demonstrations, while Nan et al. [252] retrieved
both similar and diverse demonstrations. MURRE [253] con-
ducts multi-hop retrieve-rewrite on tables to generate tabu-
larized question, then ranks the results for prompt construc-
tion. CodeS [254] retrieves relevant information from table
databases in a coarse-to-fine manner to generate SQL.
6) Others: There are several other code-related tasks that
adopt query-based RAG paradigm, incorporating similar ex-
amples to construct prompts. Jie et al. [255] used programs
as the intermediate step in numerical reasoning. De-fine [256]
uses programs to solve complex tasks. It refines the answer
generated by query-based RAG, then adds the refined pro-
grams back to the retrieval source. For program static analysis,
E&V [257] leverages an LLM agent to form intermediate
results with AST-based source code retrieval, pseudo-code ex-
ecution, execution specifications verification, and other tools.
Code4UIE [258] performs information extraction through code
representation. StackSpotAI [259] builds an AI coding assis-
tant with an RAG component. InputBlaster [260] generates
unusual text input that could cause mobile app crash.
C. RAG for Knowledge
Structured knowledge, including KGs (Knowledge Graph)
and tables, is widely used in language-related tasks. It usually
serves as the retrieval source to augment generation. In addi-
tion to regular sparse and dense retrieval, NER (Named-Entity
Recognition) technique and graph-aware neighbor retrieval are
applied to identify and extract relevant entities and relations.
1) Knowledge Base Question Answering : KBQA (knowl-
edge base question answering) typically utilizes a knowledge
base to determine the correct answer to a question. Many
semantic parsing methods have been proposed, generating
logical forms (e.g. SPARQL) based on the question.
Query-based RAG is the mainstream approach. Unseen
Entity Handling [53] uses FreeBase [261] to retrieve topic
entities, which are combined with query to generate SPARQL
output. CBR-KBQA [54] combines the query and the retrieved
(query, logical form) pairs for generation. It also revises the
final result to align with the relations present in the knowledge
graph. GMT-KBQA [52] re-ranks the retrieved entities and
relations, and conducts relation classification and entity disam-
biguation before generation. RNG-KBQA [82], TIARA [83],
BLLM augmentation [262], and Shu et al. [263] re-rank the
candidate logical forms or entities from the knowledge graph
for prompt construction. Uni-Parser [92] includes entities from
mention detection, 2-hop paths extraction, and tables from
databases into generator input. ECBRF [93] follows the case-
based reasoning paradigm [264], retrieving similar triplet to
build prompt input. FC-KBQA [265] extracts relevant classes,
relations, and entities from BM25 or mention detection, Struct-
GPT [266] extracts relevant triplets and nearest entities, and
KAPING [267] extracts relevant facts through entity match-
ing. Sen et al. [268] replaced the retrieval with a relation
distribution generation model for weighted triplets. Retrieve-
Rewrite-Answer [269] retrieves subgraphs into prompts using
hop prediction, relation path prediction, and triplet sampling.
Keqing [270] decomposes a complex question into simple
sub-questions through LLM, then retrieves sub-question tem-
plates and extract candidate entities from knowledge graph,
and finally generates the answer through ChatGPT. Liu et
al. [271] leveraged retrieved pairs to explore the capability
of formal language understanding and generation. Interactive-
KBQA [272] employs the LLM as an agent, which conducts
entity-linking on KG and generates current thought and action
until obtaining the final answer.
Latent representation-based RAG is also employed for
KBQA. ReTraCk [273] retrieves entities and schemas through
mention detection and dense retrieval. It generates logical
forms using LSTM, using retrieved items through knowledge-
specific rules. SKP [110], DECAF [109], and KD-CoT [111]
all retrieve triplets and conduct fusion-in-decoder [35] RAG.
KD-CoT also follows a chain-of-thought paradigm, iteratively
performing retrieval, generation, and verification.
2) Knowledge-augmented Open-domain Question An-
swering: Structured knowledge is often leveraged to augment
ODQA (open-domain question answering).
Latent representation-based RAG, especially the fusion-in-
decoder [35] technique, is prevalent for knowledge-augmented
ODQA. UniK-QA [108], KG-FiD [274], GRAPE [275] all ap-
ply the fusion-in-decoder technique. They incorporate triplet-
based documents, re-ranked documents through KG, and bi-
partite graph for pairs of question and passage, respectively.
OREOLM [276] empowers LLM with knowledge reasoning
paths, integrating the entity value memory derived from con-
textualized random walk paths on KG into the hidden states
of the LLM. SKURG [277] performs iterative retrieval and
generation, using cross-attention to incorporate data sources
into the input embedding. It uses a gate score to determine
whether to re-start retrieval or to generate the real answer.
With the rapid development of LLMs, query-based RAG
is emerging as a new standard. DIVKNOWQA [278] re-13
Encoder
L
1
 
Layers 
Text 
Knowledge 
Source
DPR 
Retriever
P1
P3
P5
P6
P2
P4
P7
P8
KG
Decoder
Input 
Question
Encoder
L
1
 
Layers 
Encoder
L-L
1
 
Layers 
Encoder
L
1
 
Layers 
Encoder
L
1
 
Layers 
Concatenation
Output
Answer
P1
P3
P5
P2
P7
Question 
+ 
P1
Question 
+ 
P2
Question 
+ 
P3
Question 
+ 
P5
Encoder
L
1
 
Layers 
Question 
+ 
P7
Encoder
L-L
1
 
Layers 
Retrieved
Passages 
& 
Embeddings
Stage-1 
Reranking
Reading 
Module
When 
did 
the 
Yankees 
move 
to 
New 
York?
1903
New 
York 
Yankees
Yankee 
Stadium
Staten 
Island 
Yankees
New 
York 
Yankees
Operator
Yankee 
Stadium
New 
York 
Yankees
Parent
Club
Staten 
Island 
Yankees
......
N
0
 
Passages
N
1
 
Passages
N
2
 
Passages
Stage-2 
Reranking
Fig. 7: Architecture of KG-FiD [274] model.
trieves from multiple sources using different techniques. It
iteratively retrieves and re-ranks the data before generating
the final answer. KnowledGPT [279] uses generated code
to retrieve from both public and personal knowledge bases.
EFSUM [280] optimizes the evidence-focused summary after
facts-augmented generation, so as to align the QA-specific
preference for helpfulness and faithfulness. GenTKGQA [281]
employs GNN (graph neural network) to integrate structural
and temporal information from subgraph retrieval into virtual
token representations. KnowledgeNavigator [282] performs
retrieval on KG through iterative filtering of relations with
respect to core entities, so as to obtain relevant triplets.
GNN-RAG [283] fuses LLMs’ language understanding with
GNN’s reasoning prowess and employs a retrieval augmenta-
tion strategy to enhance KGQA performance.
3) Table for Question Answering: Tables, as another form
of structured knowledge, also facilitates question answering.
Fusion-in-decoder [35] style RAG is often used for table
QA. EfficientQA [284], a competition held in NeurIPS 2020,
witnessed the proposal of numerous retrieval-reader systems
that rely on textual and tabular data. Dual Reader-Parser [285]
and CORE [286] both re-rank the retrieved textual and tabular
data for generation. Convinse [287] retrieves information from
knowledge bases, tables, and texts after question understand-
ing. RINK [288] designs a set-level reader-inherited re-ranker
to get the relevance score of table segments. TAG-QA [289]
retrieves tables and texts through GNN (after table-to-graph
conversion) and BM25, respectively.
Tables can be integrated into prompts for query-based
RAG. Both T-RAG [290] and OmniTab [291] concatenates
the retrieved tables with the query to generate the answer.
CARP [292] extracts hybrid chain of retrieved tables and
passages for prompt construction. StructGPT [266] retrieves
from multiple sources including KGs, tables, and databases.
cTBLS [293] forms prompts with ranked tables after retrieval.
Min et al. [294] integrated tabular data through table-to-
text techniques, then experiments on both finetuning and
RAG. ERATTA [295] generates SQL code to extract table
information, integrating it into the prompt to minimize model
hallucination.
4) Others: Prototype-KRG [296] integrates retrieved
knowledge facts and dialogue prototypes into a GRU model
through both hidden states and logits. SURGE [297] combines
relevant subgraphs into the input for dialogue generation.
RHO [298] fuses KG embedding of relevant entities and
relations into textual embeddings during dialogue generation.
K-LaMP [299] retrieves entities in history queries to construct
prompt for query suggestion. ReSKGC [112] retrieves rele-
vant triplets to complete triplet using Fid. G-Retriever [300]
retrieves nodes and edges from textual graphs to construct
subgraph and perform graph prompt tuning for QA. Hussien et
al. [301] fuse the reasoning power of KG with the expressive-
ness of LLMs through RAG techniques. HippoRAG [302] ex-
cels in multi-hop question answering by emulating mammalian
brain knowledge storage with KG triples and employing a
personalized PageRank algorithm for retrieval.
D. RAG for Image
1) Image Generation: Image generation refers to the pro-
cess of creating new images, typically using algorithms in the
field of artificial intelligence and machine learning.
The retrieval process can not only help yield high-quality
images even for rare or unseen subjects, but also reduces the
parameter count and computational expense [45], [95], [103]–
[107], [303]. For GAN-based model, RetrieveGAN [45] uses
a differentiable retriever for image patch selection, facilitating
end-to-end training. IC-GAN [95] models data as conditional
distributions around each training instance, conditioning both
the generator and discriminator on these instances.
Recently, diffusion models beat GANs on image generation
[304]. KNN-Diffusion [104] and RDM [105] train diffusion
models conditioned on CLIP embeddings and image neigh-
bors, enabling post-hoc conditioning on labels, prompts, and
zero-shot stylization [106]. Beyond only images, Re-imagen
[103] extends retrieval to image-text pairs for text-to-image
generation, with interleaved guidance to balance the align-
ment between prompts and retrieval conditions. Retrieve&Fuse
[303] prevents information loss of CLIP embeddings by con-
catenating retrieved and noised images before each U-Net at-
tention block, allowing fully interaction via self-attention. RPG
[305] retrieves representative images to construct in-context
examples, and utilizes chain-of-thought reasoning [306] to
plan out complementary subregions for compositional text-to-
image diffusion.
2) Image Captioning : Image captioning is the process of
generating a textual description of an image.
Retrieval-augmented image captioning typically synthesises
description with a collection of retrieved captions. MA [121]
augments via a memory bank, built with historical context
and target word of image-text training set, and queried with
inference context. In adversarial training, RAMP [307] takes
retrieved captions as discriminator reference, and employs
memory-augmented attention and copying mechanisms for
better utilization of retrieved captions. The RA-Transformer
[46] and EXTRA [308], both retrieval-augmented transformer-
based captioning models, utilize cross-attention over encoded
retrieved captions. Beyond caption retrieval, REVEAL [309]
uniformly encodes and retrieves multi-modal world knowl-
edge, integrated with retrieval score-aware attention. Directly,14
a man slope
SEP
...
... a
a
BOS
EOS
skier
skier heads
mountains
mountains
the
...
...
...
...
v1 w1 wMv2 vN-1 vN CLS
...
"a man riding skis
down a snow covered slope"
Autoregressive
Language Decoder
Vision-and-Language
Encoder
Cross-Attention
Current Image Retrieved Caption
"a man riding skis
down a snow covered slope"
.........
"a couple of people with
ski 's standing in the snow"
DISTANCES
INPUT
IMAGE CAPTION
Datastore
2
15
...
hv1 hv2 hvN-1 hvN hw1 hw2 hwM-1 hwM
...
Fig. 8: Architecture of EXTRA [308] model.
SMALLCAP [47] employs a CLIP vision encoder and a LLM
decoder, with retrieved captions serving as input-specific in-
context examples. For remote sensing images, CRSR [310]
refines retrieved captions, filtering out misleading details and
emphasizing visually salient content.
3) Others: There also exist many retrieval augmented
works for other image-related tasks. For Visual Question
Answering (VQA), PICa [311] converts images into textual
descriptions, prompts GPT-3 and ensembles multi-query re-
sults. RA-VQA [312] enables an end-to-end training with
differentiable retrieval for answer generation. For visually
grounded dialogue, KIF [313] and Maria [314] enhances
dialog generation with external knowledge like visual experi-
ences. In multi-modal machine translation, [315] incorporates
visual information at the phrase level to improve NMT with
multi-modal information.
E. RAG for Video
1) Video Captioning: Video captioning translates the visual
content into descriptive utterances. KaVD [316] generates
news video caption with background knowledge in related
documents like named entities and events. R-ConvED [48] re-
trieves relevant sentences and videos via Dual Encoding [70],
and predicts the target word with a convolutional encoder-
decoder network. CARE [117] combines three modalities data,
i.e. frame, audio, and retrieved texts, to provide both global and
local semantic guidance as augmentation. EgoInstructor [49]
focuses on first-person videos, retrieves relevant exocentric
videos and texts, and generates captions through LLM via
cross-attention with encoded videos
2) Video QA&Dialogue : Video QA&Dialogue generates
single or multiple-round responses in alignment with video
content. For VideoQA, MA-DRNN [317] stores and retrieves
useful information in queries and videos with external mem-
ory, therefore models the long-term visual-textual dependence.
R2A [318] retrieves semantically similar texts by CLIP, and
prompts LLM with both the query and the retrieved texts. For
video dialogue, [319] proposes TVQA+ dataset to enable rel-
evant moments and visual concepts retrieval, and designs cor-
responding spatio-temporal-aware generator. VGNMN [320]
extracts visual cues from videos, while the retrieval process is
parameterized by entities and actions in previous dialogues.
3) Others: RAG also works for other video-related tasks.
VidIL [321] converts video content into temporal-aware LLM
prompts for tasks like video captioning, question answer-
ing, and future event prediction. For trustworthy autonomous
Plot 1
Motion structure retrieval
Storyboard description
Structure-guidedtext-to-video synthesis
Text prompts
Video database
Text queriesPlot i Plot n⋯ ⋯
⋯ ⋯
Story script
Fig. 9: Architecture of Animate-A-Story [174] model.
driving, RAG-Driver [322] grounds the MLLM in retrieved
expert demonstrations, to produce driving action explanations.
Animate-A-Story [174] simplifies text-to-video generation by
dividing it into plot-based video augmentation and video-
diffusion generation conditioned on text and video inputs.
F .RAG for Audio
1) Audio Generation : Audio generation usually synthe-
sises audio with natural language prompt. Given input prompt,
“A bottle of champagne is popped and then poured into a glass”Inputprompt
OutputWaveform
CLAPEncoder
Database
Audio FeatureLanguageFeature“Some water pure into the glass”
“Water pure into the glass”
“A champagne is popped while a man talks”
VAEDecoderHiFi-GAN
Retrieval
AudioMAE
T5
Audio & Language FeatureLDMCrossAttention
Fig. 10: Architecture of Re-AudioLDM [116] model.
Re-AudioLDM [116] retrieves relevant caption-audio pairs
with dense retriever CLAP [26] for generation. Make-An-
Audio [44] retrieves audios given text prompt, then constructs
pseudo prompts for text-to-audio diffusion model training.15
2) Audio Captioning : Audio captioning, basically a
sequence-to-sequence task, generates natural language data
for audio data. RECAP [323] and [43] leverages dense re-
trievers, CLAP [26] and VGGish [69] respectively, to retrieve
related captions given audio data. For RECAP, captions are
included into LLM prompts, while [43] uses both audio and
retrieved captions in attention module. Other research studies
align audio modality with text to leverage advancements in
LLMs [324]–[326] for various downstream text generation.
G. RAG for 3D
1) Text-to-3D: Retrieval can be applied to augment 3D
asset generation.
...
A man is pretending to 
be a chicken, constantly 
pecking at the ground 
and waving his arms like a 
chicken.
1) A man lowers his head     
towards the ground.
2) ... opens and closes 
his mouth rapidly.
3) ... moves his head up and 
down, mimicking a 
pecking motion.
4) ... flaps his arms up 
and down, imitating a 
chicken's wings.
𝑡 MLP
Cross
Attention
Linear
Fusion Block
Transformer  Encoder
CLIP
Text
CLIP
Text
Linear𝑥𝑡
1 𝑥𝑡
2 𝑥𝑡
3 𝑥𝑡
𝑁
𝑚1 𝑚2𝑧𝑡𝑘
𝑠
𝑧𝑡𝑘
𝑙
𝑚3 𝑚𝑅⊕
ො𝑥0
1
ො𝑥0
2
ො𝑥0
3
ො𝑥0
𝑁
⋯
⋯
⋯
⋯
⋯
⋯
Linear
Linear
Transformer  Encoder
Fusion Block
⊕
Origin  Motion Diffusion
Feature Fusion
Text Decomposition
fine-tuned
Self 
Attention
Dropout
Layer Norm
Linear
GELU
Layer Norm
Dropout
Linear
Fusion Blockoptional
ℱ𝑙
2
ℱ𝑠
ℱ𝑡
𝒞𝑙
𝒞𝑠
Hybrid Retrieval
ℰ𝑠
ℰ𝑙
ℱ𝑙
1
𝑝𝜃1
Search with Anatomical Text Best Match
Dataset
𝑚1:𝑅 Reference Action 
Tokens
Random Select
Complex and Decomposed
Text Features
Diffused Motion and 
Reference Action Features 
𝑥𝑡
1:𝑁 Diffused Motion Data
𝑘,𝑣
𝑞
Reference Motion Diffusion
𝑝𝜃2
Fig. 11: Architecture of AMD [115] model.
ReMoDiffuse [51] retrieves relevant motion entities
and generates motions using diffusion models, with the
semantic-modulated attention and condition mixture guidance.
AMD [115] designs and fuses two motion diffusion models.
One branch conditions on the original prompt, while the other
decomposes the prompt into anatomical scripts and retrieves
similar motions. RetDream [50] retrieves 3D assets to augment
the variational score distillation [327] of 2D diffusion models.
These assets offer geometric and adapted 2D priors, which not
only impose additional velocity on particles for initialization
but also help optimize 2D diffusion models by LoRA.
H. RAG for Science
RAG has also emerged as a promising research direction
for many interdisciplinary applications, such as molecular
generation, medical tasks and computational research.
1) Drug Discovery : The goal of drug discovery is to
generate molecules that concurrently fulfill diverse properties.
-4.9 kcal/mol
Retrieval 
database
Retriever
Information 
fusion DecoderEncoder
-8.4 kcal/mol -10.3 kcal/mol -10.9 kcal/mol
Encoder
Shared  
weights
Input molecule
Retrieved exemplar molecules
Input embedding
Retrieved embeddings
Fused embedding
-8.4 kcal/mol
Output molecule
Retrieval module
Pre-trained module
Fig. 12: Architecture of RetMol [55] model.
RetMol [55] integrates a lightweight retrieval mechanism
and molecular strings into a pre-trained encoder-decoder gen-
erative model to retrieve and fuse exemplar molecules with
the input. PromptDiff [328] introduces an interaction-based,
retrieval-augmented 3D molecular diffusion model that re-
trieves a curated set of ligand references to guide the synthesis
of ligands meeting specific design criteria.
2) Biomedical Informatics Enhancement : Several recent
studies have improved the expressiveness of LLM by retriev-
ing information from biomedical domain-specific databases,
thereby augmenting the model’s capabilities to provide valu-
able guidance for tasks in the medical field.
PoET [329] is an autoregressive model using a transformer
variant with a retrieval mechanism for prompt augmenta-
tion, speeding up the prediction of protein variant fitness
properties. Chat-Orthopedist [94] enhances ChatGPT with a
retrieval-augmented mechanism focused on adolescent idio-
pathic scoliosis (AIS), utilizing an external knowledge base
for precise responses. BIOREADER [330] is the first retrieval-
enhanced text-to-text transformer-based model for biomedical
natural language processing, incorporating the retrieved liter-
ature evidence into the model using a chunked-cross attention
mechanism. MedWriter [331] employs a hierarchical retrieval-
augmented generation method that combines report-level and
sentence-level templates to produce coherent and clinically
accurate medical reports from images. QA-RAG [332] em-
ploys a dual-track RAG strategy to enhance pharmaceutical
compliance by effectively retrieving and integrating regu-
latory guidelines based on language model responses and
user queries. RAG-RLRC-LaySum [333] leverages biomedical
text knowledge for llms, employing reinforcement learning
and re-ranking techniques to enhance content relevance and
readability of the output.
3) Math Applications : Retrieval-augmented generation
technology in mathematics streamlines problem-solving,
boosts research innovation, and refines educational strategies.
LeanDojo [334] boosts theorem proving by using retrieval-
augmented methods to choose relevant premises from exten-
sive mathematical libraries, improving automation and theo-
rem generalization. RAG-for-math-QA [335] improves math
question-answering by integrating a high-quality math text-
book with RAG, enhancing LLM-generated responses for
middle-school algebra and geometry.
V. B ENCHMARK
Given the increasing research interests and applications of
RAG, there have also been several benchmarks assessing RAG
from certain aspects.
Chen et al. [336] proposed an RAG benchmark that eval-
uates across four dimensions: (1) Noise Robustness, testing
if LLMs can extract necessary information from noisy docu-
ments; (2) Negative Rejection, assessing if LLMs can reject
to respond when retrieved content is insufficient; (3) Infor-
mation Integration,checking if LLMs can acquire knowledge
and respond by integrating multiple retrieved contents; (4)
Counterfactual Robustness, determining if LLMs can identify
counterfactual errors in retrieved content.
Three other benchmarks, RAGAS [337], ARES [338], and
TruLens [339], evaluate three aspects using a separate evalu-
ator LLM: (1) Faithfulness, assessing factual accuracy based16
on retrieved content; (2) Answer Relevance, determining if
results address the queries; (3) Context Relevance, evaluating
the relevance of retrieved content and its conciseness.
CRUD-RAG [340] divides RAG tasks into four types:
Create, Read, Update, and Delete, assessing them through
text continuation, question answering, hallucination correction,
and open-domain multi-document summary. MIRAGE [341]
assesses RAG in the medical domain, focusing on the perfor-
mance of medical question-answering systems. KILT [342]
aligns Wikipedia snapshots to verify information accuracy,
using BLEU scores to pinpoint relevant texts and filtering to
uphold quality, thus providing diverse retrieval systems for
evidence-backed predictions or citations.
VI. D ISCUSSION
A. Limitations
Despite the widespread adoption of RAG, it suffers from
several limitations by nature.
1) Noises in Retrieval Results : Information retrieval is
inherently flawed due to information loss in item represen-
tations and ANN search. The inevitable noise, manifesting as
irrelevant content or misleading information, can create failure
points in RAG systems [343]. However, although improv-
ing retrieval accuracy seems intuitive for RAG effectiveness,
recent research surprisingly finds that noisy retrieval results
might enhance generation quality [344]. A possible expla-
nation is that diverse retrieval outcomes could contribute to
prompt construction [345]. Thus, the impact of retrieval noise
remains unclear, leading to confusion about metric selection
and retriever-generator interaction in practical uses.
2) Extra Overhead: While retrieval can reduce generation
costs in certain cases [30]–[32], it incurs non-negligible over-
head in most cases. In other words, the retrieval and interaction
processes increase latency inevitably. This is amplified when
RAG is combined with complex enhancement methods, such
as recursive retrieval [346] and iterative RAG [186]. Further-
more, as the scale of retrieval sources expands, the storage
and access complexity will also increase [347]. Such overhead
hampers the practicality of RAG in real-time services that are
sensitive to latency.
3) The Gap between Retrievers and Generators : Since the
objectives of retrievers and generators may not align, and their
latent spaces might differ, designing their interaction requires
meticulous design and optimization. Current approaches either
disentangle retrieval and generation or integrate them at an
intermediate stage. While the former is more modular, the
latter could benefit from joint training but hamper generality.
Selecting a cost-effective interaction method to bridge the gap
poses a challenge and necessities deliberation in practice.
4) Increased System Complexity : The introduction of
retrieval unavoidably increases the system complexity and
the number of hyper-parameters to tune. For instance, a
recent study found that using top- k rather than a single
retrieval improves attribution but harms fluency in query-based
RAG [348], while other aspects such as metric selection are
still under explored. Thus, it requires more expertise to tune
the generation service when RAG is involved.
5) Lengthy Context : One of the primary shortcomings of
RAG, in particular the query-based RAG, is that it lengthens
the context tremendously, making it infeasible for generators
with limited context length. In addition, the lengthened context
also slows down the generation process generally. The research
advancements in prompt compression [169] and long-context
support [349] have partially mitigated these challenges, albeit
with a slight trade-off in accuracy or costs.
B. Potential Future Directions
Lastly, we wish to outline several potential directions for
future RAG research and applications.
1) Novel Design of Augmentation Methodologies : Exist-
ing research has explored various interaction patterns between
retrievers and generators. However, due to distinct objectives
in these two components, the practical augmentation process
has a significant impact on the final generation results. Inves-
tigation of more advanced foundations for augmentation holds
promise for fully unleashing the potential of RAG.
2) Flexible RAG Pipelines : RAG systems are progres-
sively embracing flexible pipelines, such as recursive, adap-
tive, and iterative RAG. With precise tuning and meticulous
engineering, the unique blend of retrieval sources, retrievers,
generators, and RAG subsystems promises to tackle complex
tasks and boost overall performance. We eagerly anticipate
pioneering exploration that will drive the evolution of even
more innovative RAG systems.
3) Broader Applications : RAG is a general technique
applied in various applications. However, some generative
tasks have not yet explored RAG, and in many domains, RAG
is applied naively without considering the domain’s unique
characteristics. We believe designing domain-specific RAG
techniques will significantly benefit broader applications.
4) Efficient Deployment and Processing : There exist
several deployment solutions for query-based RAG with
LLMs, such as LangChain [350], LLAMA-Index [135], and
PipeRAG [351]. However, for other RAG foundations and/or
generation tasks, there lacks a plug-and-play solution. Besides,
due to retrieval overhead and increasing complexities in re-
trievers and generators, achieving efficient RAG is still chal-
lenging and necessitates further system-level optimizations.
5) Incorporating Long-tail and Real-time Knowledge :
While a key motivation of RAG is to harness real-time and
long-tail knowledge, few studies have explored the pipeline
for knowledge updating and expansion. Many existing works
use merely the generators’ training data as retrieval sources,
neglecting the dynamic and flexible information that retrieval
could offer. As a consequence, there is a growing research on
designing RAG systems with continuously updated knowledge
and flexible sources. We also expect RAG to step further,
adapting to personalized information in today’s web service.
6) Combined with Other Techniques : RAG is orthogonal
to other techniques that also aim to improve AIGC effec-
tiveness, such as fine-tuning, reinforcement learning, chain-
of-thought, and agent-based generation. The combining of
these methods [352] is still in its early stages, calling for
further research to fully exploit their potential through novel17
algorithm designs. It is worthy to note that a recent notion
appears “long-context models like Gemini 1.5 will replace
RAG”. Nevertheless, this assertion overlooks RAG’s flexibility
in managing dynamic information, encompassing both up-to-
date and long-tail knowledge [353]. We expect RAG to benefit
from long context generation, rather than being replaced by it.
VII. C ONCLUSION
In this paper, we conducted a thorough and comprehensive
survey on RAG within the context of AIGC, with a particular
focus on augmentation foundations, enhancements, and ap-
plications. We first systematically organized and summarized
the foundation paradigms in RAG, providing insights into
the interaction between retrievers and generators. Then, we
reviewed the enhancements that further improve the effective-
ness of RAG, including the enhancements on each component
or the entire pipeline. To facilitate researchers across diverse
domains, we showcased practical applications of RAG in a
range of modalities and tasks. Finally, we also presented
existing benchmarks for RAG, discussed current limitations
of RAG, and shed light on promising future directions.
REFERENCES
[1] T. B. Brown, B. Mann et al., “Language models are few-shot learners,”
in NeurIPS, 2020.
[2] M. Chen, J. Tworek et al., “Evaluating large language models trained
on code,” arXiv:2107.03374, 2021.
[3] OpenAI, “GPT-4 technical report,” arXiv:2303.08774, 2023.
[4] H. Touvron, T. Lavril et al., “Llama: Open and efficient foundation
language models,” arXiv:2302.13971, 2023.
[5] H. Touvron, L. Martin et al., “Llama 2: Open foundation and fine-tuned
chat models,” arXiv:2307.09288, 2023.
[6] B. Rozi `ere, J. Gehring et al., “Code llama: Open foundation models
for code,” arXiv:2308.12950, 2023.
[7] A. Ramesh, M. Pavlov, G. Goh et al., “Zero-shot text-to-image gener-
ation,” in ICML, 2021.
[8] A. Ramesh, P. Dhariwal, A. Nichol et al., “Hierarchical text-conditional
image generation with CLIP latents,” arXiv:2204.06125, 2022.
[9] J. Betker, G. Goh, L. Jing et al., “Improving image generation with
better captions,” Computer Science, vol. 2, no. 3, p. 8, 2023.
[10] R. Rombach, A. Blattmann, D. Lorenz et al., “High-resolution image
synthesis with latent diffusion models,” in IEEE/CVF, 2022.
[11] OpenAI, “Video generation models as world simulators,” https://openai.
com/research/video-generation-models-as-world-simulators, 2024.
[12] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural
Comput., vol. 9, no. 8, pp. 1735–1780, 1997.
[13] A. Vaswani, N. Shazeer, N. Parmar et al., “Attention is all you need,”
in NeurIPS, 2017.
[14] I. Goodfellow, J. Pouget-Abadie, M. Mirza et al., “Generative adver-
sarial networks,” CACM, vol. 63, no. 11, pp. 139–144, 2020.
[15] J. Devlin, M. Chang et al., “BERT: pre-training of deep bidirectional
transformers for language understanding,” in NAACL-HLT, 2019.
[16] C. Raffel, N. Shazeer, A. Roberts et al., “Exploring the limits of transfer
learning with a unified text-to-text transformer,” JMLR, vol. 21, pp.
140:1–140:67, 2020.
[17] W. Fedus, B. Zoph, and N. Shazeer, “Switch transformers: Scaling to
trillion parameter models with simple and efficient sparsity,” JMLR,
vol. 23, no. 120, pp. 1–39, 2022.
[18] J. Kaplan, S. McCandlish, T. Henighan et al., “Scaling laws for neural
language models,” 2020.
[19] S. E. Robertson and H. Zaragoza, “The probabilistic relevance frame-
work: BM25 and beyond,” FTIR, vol. 3, no. 4, pp. 333–389, 2009.
[20] V . Karpukhin, B. Oguz, S. Min et al., “Dense passage retrieval for
open-domain question answering,” in EMNLP, 2020.
[21] J. Johnson, M. Douze, and H. J ´egou, “Billion-scale similarity search
with gpus,” IEEE Trans. Big Data, vol. 7, no. 3, pp. 535–547, 2021.
[22] Q. Chen, B. Zhao, H. Wang et al., “SPANN: highly-efficient billion-
scale approximate nearest neighborhood search,” in NeurIPS, 2021.
[23] R. Datta, D. Joshi, J. Li et al., “Image retrieval: Ideas, influences, and
trends of the new age,” CSUR, vol. 40, no. 2, pp. 5:1–5:60, 2008.
[24] A. Radford, J. W. Kim, C. Hallacy et al., “Learning transferable visual
models from natural language supervision,” in ICML, 2021.
[25] Z. Feng, D. Guo et al., “Codebert: A pre-trained model for program-
ming and natural languages,” in EMNLP Findings, 2020.
[26] Y . Wu, K. Chen, T. Zhang et al., “Large-scale contrastive language-
audio pretraining with feature fusion and keyword-to-caption augmen-
tation,” in ICASSP, 2023.
[27] A. Mallen, A. Asai, V . Zhong et al., “When not to trust language
models: Investigating effectiveness of parametric and non-parametric
memories,” in ACL, 2023.
[28] N. Carlini, F. Tram `er et al., “Extracting training data from large
language models,” in USENIX, 2021.
[29] M. Kang, N. M. G ¨urel et al., “C-RAG: certified generation risks for
retrieval-augmented language models,” arXiv:2402.03181, 2024.
[30] G. Izacard, P. Lewis, M. Lomeli et al., “Atlas: Few-shot learning with
retrieval augmented language models,” arXiv:2208.03299, 2022.
[31] Y . Wu, M. N. Rabe, D. Hutchins, and C. Szegedy, “Memorizing
transformers,” in ICLR, 2022.
[32] Z. He, Z. Zhong, T. Cai et al., “REST: retrieval-based speculative
decoding,” arxiv:2311.08252, 2023.
[33] K. Guu, K. Lee, Z. Tung et al., “REALM: retrieval-augmented language
model pre-training,” ICML, 2020.
[34] P. S. H. Lewis, E. Perez, A. Piktus et al., “Retrieval-augmented
generation for knowledge-intensive NLP tasks,” in NeurIPS, 2020.
[35] G. Izacard and E. Grave, “Leveraging passage retrieval with generative
models for open domain question answering,” in EACL, 2021.
[36] S. Borgeaud, A. Mensch et al., “Improving language models by
retrieving from trillions of tokens,” in ICML, 2022.
[37] U. Khandelwal, O. Levy, D. Jurafsky et al., “Generalization through
memorization: Nearest neighbor language models,” in ICLR, 2020.
[38] J. He, G. Neubig, and T. Berg-Kirkpatrick, “Efficient nearest neighbor
language models,” in EMNLP, 2021.
[39] zilliztech. (2023) Gptcache. [Online]. Available: https://github.com/
zilliztech/GPTCache
[40] M. R. Parvez, W. U. Ahmad et al., “Retrieval augmented code gener-
ation and summarization,” in EMNLP Findings, 2021.
[41] W. U. Ahmad, S. Chakraborty, B. Ray et al., “Unified pre-training for
program understanding and generation,” in NAACL-HLT, 2021.
[42] S. Zhou, U. Alon, F. F. Xu et al., “Docprompting: Generating code by
retrieving the docs,” in ICLR, 2023.
[43] Y . Koizumi, Y . Ohishiet al., “Audio captioning using pre-trained large-
scale language model guided by audio-based similar caption retrieval,”
arXiv:2012.07331, 2020.
[44] R. Huang, J. Huang, D. Yang et al., “Make-an-audio: Text-to-audio
generation with prompt-enhanced diffusion models,” in ICML, 2023.
[45] H.-Y . Tseng, H.-Y . Lee et al., “Retrievegan: Image synthesis via
differentiable patch retrieval,” in ECCV, 2020.
[46] S. Sarto, M. Cornia, L. Baraldi, and R. Cucchiara, “Retrieval-
augmented transformer for image captioning,” in CBMI, 2022.
[47] R. Ramos, B. Martins et al., “Smallcap: lightweight image captioning
prompted with retrieval augmentation,” in CVPR, 2023.
[48] J. Chen, Y . Pan, Y . Li et al., “Retrieval augmented convolutional
encoder-decoder networks for video captioning,” TOMCCAP, vol. 19,
no. 1s, pp. 48:1–48:24, 2023.
[49] J. Xu, Y . Huang, J. Hou et al., “Retrieval-augmented egocentric video
captioning,” arXiv:2401.00789, 2024.
[50] J. Seo, S. Hong et al., “Retrieval-augmented score distillation for text-
to-3d generation,” arXiv:2402.02972, 2024.
[51] M. Zhang, X. Guo, L. Pan et al., “Remodiffuse: Retrieval-augmented
motion diffusion model,” in ICCV, 2023.
[52] X. Hu, X. Wu, Y . Shu, and Y . Qu, “Logical form generation via multi-
task learning for complex question answering over knowledge bases,”
in COLING, 2022.
[53] X. Huang, J. Kim, and B. Zou, “Unseen entity handling in complex
question answering over knowledge base via language generation,” in
EMNLP Findings, 2021.
[54] R. Das, M. Zaheer, D. Thai et al., “Case-based reasoning for natural
language queries over knowledge bases,” in EMNLP, 2021.
[55] Z. Wang, W. Nie, Z. Qiao et al., “Retrieval-based controllable molecule
generation,” in ICLR, 2022.
[56] Q. Jin, Y . Yang, Q. Chen, and Z. Lu, “Genegpt: Augmenting large
language models with domain tools for improved access to biomedical
information,” Bioinformatics, vol. 40, no. 2, p. btae075, 2024.
[57] H. Li, Y . Su, D. Cai et al., “A survey on retrieval-augmented text
generation,” arxiv:2202.01110, 2022.18
[58] A. Asai, S. Min, Z. Zhong, and D. Chen, “Acl 2023 tutorial: Retrieval-
based language models and applications,” ACL 2023, 2023.
[59] Y . Gao, Y . Xiong et al., “Retrieval-augmented generation for large
language models: A survey,” arxiv:2312.10997, 2023.
[60] R. Zhao, H. Chen et al., “Retrieving multimodal information for
augmented generation: A survey,” in EMNLP, 2023.
[61] Y . Ding, W. Fan et al., “A survey on rag meets llms: Towards retrieval-
augmented large language models,” arXiv:2405.06211, 2024.
[62] J. Chen, H. Guo, K. Yi et al., “Visualgpt: Data-efficient adaptation of
pretrained language models for image captioning,” in CVPR, 2022.
[63] Y . Tay, M. Dehghani, D. Bahri, and D. Metzler, “Efficient transformers:
A survey,” CSUR, vol. 55, no. 6, pp. 109:1–109:28, 2023.
[64] G. V . Houdt et al., “A review on the long short-term memory model,”
Artif. Intell. Rev., vol. 53, no. 8, pp. 5929–5955, 2020.
[65] L. Yang, Z. Zhang et al., “Diffusion models: A comprehensive survey
of methods and applications,” CSUR, vol. 56, no. 4, pp. 1–39, 2023.
[66] J. Gui, Z. Sun, Y . Wen et al., “A review on generative adversarial
networks: Algorithms, theory, and applications,” TKDE, vol. 35, no. 4,
pp. 3313–3332, 2023.
[67] S. E. Robertson and S. Walker, “On relevance weights with little
relevance information,” in SIGIR, 1997.
[68] J. D. Lafferty and C. Zhai, “Document language models, query models,
and risk minimization for information retrieval,” in SIGIR, 2001.
[69] S. Hershey, S. Chaudhuri et al., “CNN architectures for large-scale
audio classification,” in ICASSP, 2017.
[70] J. Dong, X. Li, C. Xu et al., “Dual encoding for zero-example video
retrieval,” in CVPR, 2019.
[71] L. Xiong, C. Xiong, Y . Li et al., “Approximate nearest neighbor
negative contrastive learning for dense text retrieval,” in ICLR, 2021.
[72] J. L. Bentley, “Multidimensional binary search trees used for associa-
tive searching,” CACM, vol. 18, no. 9, pp. 509–517, 1975.
[73] W. Li, C. Feng, D. Lian et al., “Learning balanced tree indexes for
large-scale vector retrieval,” in SIGKDDg, 2023.
[74] M. Datar, N. Immorlica, P. Indyk et al., “Locality-sensitive hashing
scheme based on p-stable distributions,” in SCG, 2004.
[75] Y . A. Malkov and D. A. Yashunin, “Efficient and robust approxi-
mate nearest neighbor search using hierarchical navigable small world
graphs,” TPAMI, vol. 42, no. 4, pp. 824–836, 2018.
[76] S. Jayaram Subramanya, F. Devvrit et al., “Diskann: Fast accurate
billion-point nearest neighbor search on a single node,” NeurIPS, 2019.
[77] Y . Wang, Y . Hou, H. Wang et al., “A neural corpus indexer for
document retrieval,” in NeurIPS, 2022.
[78] H. Zhang, Y . Wang, Q. Chen et al., “Model-enhanced vector index,”
in NeurIPS, 2023.
[79] S. A. Hayati, R. Olivier, P. Avvaru et al., “Retrieval-based neural code
generation,” in EMNLP, 2018.
[80] J. Zhang, X. Wang, H. Zhang et al., “Retrieval-based neural source
code summarization,” in ICSE, 2020.
[81] G. Poesia, A. Polozov, V . Le et al., “Synchromesh: Reliable code
generation from pre-trained language models,” in ICLR, 2022.
[82] X. Ye, S. Yavuz et al., “RNG-KBQA: generation augmented iterative
ranking for knowledge base question answering,” in ACL, 2022.
[83] Y . Shu et al., “TIARA: multi-grained retrieval for robust question
answering over large knowledge bases,” arXiv:2210.12925, 2022.
[84] X. V . Lin, R. Socher et al., “Bridging textual and tabular data for
cross-domain text-to-sql semantic parsing,” arXiv:2012.12627, 2020.
[85] A. Asai, Z. Wu, Y . Wanget al., “Self-rag: Learning to retrieve, generate,
and critique through self-reflection,” arxiv:2310.11511, 2023.
[86] W. Shi, S. Min, M. Yasunaga et al., “Replug: Retrieval-augmented
black-box language models,” arXiv:2301.12652, 2023.
[87] O. Ram, Y . Levine, I. Dalmedigos et al., “In-context retrieval-
augmented language models,” arXiv:2302.00083, 2023.
[88] D. Zan, B. Chen, Z. Lin et al., “When language model meets private
library,” in EMNLP Findings, 2022.
[89] N. Nashid, M. Sintaha, and A. Mesbah, “Retrieval-based prompt
selection for code-related few-shot learning,” in ICSE, 2023.
[90] M. Jin, S. Shahriar, M. Tufano et al., “Inferfix: End-to-end program
repair with llms,” in ESEC/FSE, 2023.
[91] S. Lu, N. Duan, H. Han et al., “Reacc: A retrieval-augmented code
completion framework,” in ACL, 2022.
[92] Y . Liu et al., “Uni-parser: Unified semantic parser for question answer-
ing on knowledge base and database,” in EMNLP, 2022.
[93] Z. Yang, X. Du, E. Cambria et al., “End-to-end case-based reasoning
for commonsense knowledge base completion,” in EACL, 2023.
[94] W. Shi, Y . Zhuang, Y . Zhu et al., “Retrieval-augmented large language
models for adolescent idiopathic scoliosis patients in shared decision-
making,” in ACM-BCB, 2023.
[95] A. Casanova, M. Careil, J. Verbeek et al., “Instance-conditioned gan,”
in NeurIPS, 2021.
[96] A. Bertsch, U. Alon, G. Neubig, and M. R. Gormley, “Unlimiformer:
Long-range transformers with unlimited length input,” 2023.
[97] Y . Kuratov, A. Bulatov et al., “In search of needles in a 10m haystack:
Recurrent memory finds what llms miss,” arXiv:2402.10790, 2024.
[98] J. Li, Y . Li, G. Li et al., “Editsum: A retrieve-and-edit framework for
source code summarization,” in ASE, 2021.
[99] C. Yu, G. Yang, X. Chen et al., “Bashexplainer: Retrieval-augmented
bash code comment generation based on fine-tuned codebert,” in
ICSME, 2022.
[100] T. B. Hashimoto, K. Guu, Y . Oren, and P. Liang, “A retrieve-and-edit
framework for predicting structured outputs,” in NeurIPS, 2018.
[101] B. Wei, Y . Li, G. Li et al., “Retrieve and refine: Exemplar-based neural
comment generation,” in ASE, 2020.
[102] E. Shi, Y . Wang, W. Tao et al., “RACE: retrieval-augmented commit
message generation,” in EMNLP, 2022.
[103] W. Chen, H. Hu, C. Saharia, and W. W. Cohen, “Re-imagen: Retrieval-
augmented text-to-image generator,” in ICLR, 2023.
[104] S. Sheynin, O. Ashual, A. Polyak et al., “Knn-diffusion: Image gener-
ation via large-scale retrieval,” in ICLR, 2023.
[105] A. Blattmann, R. Rombach, K. Oktay et al., “Retrieval-augmented
diffusion models,” in NeurIPS, 2022.
[106] R. Rombach, A. Blattmann, and B. Ommer, “Text-guided synthe-
sis of artistic images with retrieval-augmented diffusion models,”
arXiv:2207.13038, 2022.
[107] B. Li, P. H. Torr, and T. Lukasiewicz, “Memory-driven text-to-image
generation,” arXiv:2208.07022, 2022.
[108] B. Oguz, X. Chen, V . Karpukhin et al., “Unik-qa: Unified repre-
sentations of structured and unstructured knowledge for open-domain
question answering,” in NAACL Findings, 2022.
[109] D. Yu, S. Zhang et al., “Decaf: Joint decoding of answers and logical
forms for question answering over knowledge bases,” in ICLR, 2023.
[110] G. Dong, R. Li, S. Wang et al., “Bridging the kb-text gap: Leveraging
structured knowledge-aware pre-training for KBQA,” in CIKM, 2023.
[111] K. Wang, F. Duan, S. Wang et al., “Knowledge-driven cot: Exploring
faithful reasoning in llms for knowledge-intensive question answering,”
arXiv:2308.13259, 2023.
[112] D. Yu and Y . Yang, “Retrieval-enhanced generative model for large-
scale knowledge graph completion,” in SIGIR, 2023.
[113] T. F ´evry, L. B. Soares et al., “Entities as experts: Sparse memory access
with entity supervision,” in EMNLP, 2020.
[114] M. de Jong, Y . Zemlyanskiy, N. FitzGerald et al., “Mention memory:
incorporating textual knowledge into transformers through entity men-
tion attention,” in ICLR, 2021.
[115] B. Jing, Y . Zhang, Z. Song et al., “Amd: Anatomical motion diffusion
with interpretable motion decomposition and fusion,” in AAAI, 2024.
[116] Y . Yuan, H. Liu, X. Liu et al., “Retrieval-augmented text-to-audio
generation,” in ICASSP, 2024.
[117] B. Yang, M. Cao, and Y . Zou, “Concept-aware video captioning:
Describing videos with effective prior information,” TIP, vol. 32, pp.
5366–5378, 2023.
[118] Z. Zhong, T. Lei, and D. Chen, “Training language models with
memory augmentation,” in EMNLP, 2022.
[119] S. Min, W. Shi, M. Lewis et al., “Nonparametric masked language
modeling,” in ACL Findings, 2023.
[120] X. Zhang, Y . Zhou, G. Yang, and T. Chen, “Syntax-aware retrieval
augmented code generation,” in EMNLP Findings, 2023.
[121] Z. Fei, “Memory-augmented image captioning,” in AAAI, 2021.
[122] Y . Leviathan, M. Kalman, and Y . Matias, “Fast inference from trans-
formers via speculative decoding,” in ICML, 2023.
[123] T. Lan, D. Cai, Y . Wang et al., “Copy is all you need,” in ICLR, 2023.
[124] B. Cao, D. Cai, L. Cui et al., “Retrieval is accurate generation,”
arXiv:2402.17532, 2024.
[125] L. Wang, N. Yang, and F. Wei, “Query2doc: Query expansion with
large language models,” in EMNLP, 2023.
[126] L. Gao, X. Ma, J. Lin, and J. Callan, “Precise zero-shot dense retrieval
without relevance labels,” in ACL, 2023.
[127] G. Kim, S. Kim, B. Jeon et al., “Tree of clarifications: Answering
ambiguous questions with retrieval-augmented large language models,”
in EMNLP, 2023.
[128] C.-M. Chan, C. Xu et al., “Rq-rag: Learning to refine queries for
retrieval augmented generation,” arXiv:2404.00610, 2024.
[129] A. Tayal and A. Tyagi, “Dynamic contexts for generating sugges-
tion questions in rag based conversational systems,” in WWW’24
Companion, 2024.19
[130] M. Xia, S. Malladi, S. Gururangan et al., “LESS: selecting influential
data for targeted instruction tuning,” arXiv:2402.04333, 2024.
[131] A.-L. Bornea, F. Ayed et al., “Telco-rag: Navigating the challenges
of retrieval-augmented language models for telecommunications,”
arXiv:2404.15939, 2024.
[132] S. Yao, J. Zhao, D. Yu et al., “React: Synergizing reasoning and acting
in language models,” in ICLR, 2023.
[133] J. Wei, X. Wang, D. Schuurmans et al., “Chain-of-thought prompting
elicits reasoning in large language models,” in NeurIPS, 2022.
[134] T. Pouplin, H. Sun, S. Holt, and M. Van der Schaar, “Retrieval-
augmented thought process as sequential decision making,”
arXiv:2402.07812, 2024.
[135] J. Liu, “LlamaIndex,” 11 2022. [Online]. Available: https://github.
com/jerryjliu/llama index
[136] P. Sarthi, S. Abdullah, A. Tuli et al., “Raptor: Recursive abstractive
processing for tree-organized retrieval,” in ICLR, 2023.
[137] B. Kang, J. Kim et al., “Prompt-rag: Pioneering vector embedding-
free retrieval-augmented generation in niche domains, exemplified by
korean medicine,” arXiv:2401.11246, 2024.
[138] V . Raina et al., “Question-based retrieval using atomic units for
enterprise rag,” arXiv:2405.12363, 2024.
[139] S. Xiao, Z. Liu, P. Zhang et al., “C-pack: Packaged resources to advance
general chinese embedding,” arxiv:2309.07597, 2023.
[140] J. Chen, S. Xiao, P. Zhang et al., “Bge m3-embedding: Multi-lingual,
multi-functionality, multi-granularity text embeddings through self-
knowledge distillation,” arxiv:2309.07597, 2023.
[141] S. Xiao, Z. Liu, P. Zhang, and X. Xing, “Lm-cocktail: Resilient tuning
of language models via model merging,” arxiv:2311.13534, 2023.
[142] P. Zhang, S. Xiao, Z. Liu, Z. Dou, and J.-Y . Nie, “Retrieve anything
to augment large language models,” arxiv:2310.07554, 2023.
[143] M. Kulkarni, P. Tangarajan, K. Kim et al., “Reinforcement learning for
optimizing RAG for domain chatbots,” arXiv:2401.06800, 2024.
[144] W. Wang, Y . Wang et al., “Rap-gen: Retrieval-augmented patch gener-
ation with codet5 for automatic program repair,” in ESEC/FSE, 2023.
[145] K. Sawarkar, A. Mangal et al., “Blended rag: Improving rag (retriever-
augmented generation) accuracy with semantic search and hybrid
query-based retrievers,” arXiv:2404.07220, 2024.
[146] S.-Q. Yan, J.-C. Gu, Y . Zhu, and Z.-H. Ling, “Corrective retrieval
augmented generation,” arXiv:2401.15884, 2024.
[147] W. Huang, M. Lapata, P. V ougiouklis et al., “Retrieval augmented
generation with rich answer encoding,” in IJCNLP-AACL, 2023.
[148] H. Wang, W. Huang, Y . Denget al., “Unims-rag: A unified multi-source
retrieval-augmented generation for personalized dialogue systems,”
arXiv:2401.13256, 2024.
[149] S. Koley, A. K. Bhunia et al., “You’ll never walk alone: A sketch and
text duet for fine-grained image retrieval,” in CVPR, 2024.
[150] M. R. Glass, G. Rossiello, M. F. M. Chowdhury et al., “Re2g: Retrieve,
rerank, generate,” in NAACL, 2022.
[151] R. F. Nogueira and K. Cho, “Passage re-ranking with BERT,”
arxiv:1901.04085, 2019.
[152] J. Li, Y . Zhao, Y . Li et al., “Acecoder: Utilizing existing code to
enhance code generation,” arXiv:2303.17780, 2023.
[153] P. Shi, R. Zhang, H. Bai, and J. Lin, “XRICL: cross-lingual retrieval-
augmented in-context learning for cross-lingual text-to-sql semantic
parsing,” in EMNLP Findings, 2022.
[154] K. Rangan and Y . Yin, “A fine-tuning enhanced rag system with
quantized influence measure as ai judge,” arXiv:2402.17081, 2024.
[155] J. Saad-Falcon, O. Khattab, K. Santhanam et al., “Udapdr: Unsu-
pervised domain adaptation via llm prompting and distillation of
rerankers,” in EMNLP, 2023.
[156] L. Wang, N. Yang, and F. Wei, “Learning to retrieve in-context
examples for large language models,” arXiv:2307.07164, 2023.
[157] P. Finardi, L. Avila et al., “The chronicles of rag: The retriever, the
chunk and the generator,” arXiv:2401.07883, 2024.
[158] J. Li, Y . Yuan, and Z. Zhang, “Enhancing llm factual accuracy with
rag to counter hallucinations: A case study on domain-specific queries
in private knowledge-bases,” arXiv:2403.10446, 2024.
[159] Z. Wang, J. Araki, Z. Jiang et al., “Learning to filter context for
retrieval-augmented generation,” arxiv:2311.08377, 2023.
[160] S. Hofst ¨atter, J. Chen, K. Raman, and H. Zamani, “Fid-light: Efficient
and effective retrieval-augmented text generation,” in SIGIR, 2023.
[161] D. Arora, A. Kini, S. R. Chowdhury et al., “Gar-meets-rag paradigm
for zero-shot information retrieval,” arXiv:2310.20158, 2023.
[162] https://www.pinecone.io.
[163] W. Yu, D. Iter et al., “Generate rather than retrieve: Large language
models are strong context generators,” arXiv:2209.10063, 2022.
[164] A. Abdallah and A. Jatowt, “Generator-retriever-generator: A novel ap-
proach to open-domain question answering,” arXiv:2307.11278, 2023.
[165] M. Besta, A. Kubicek et al., “Multi-head rag: Solving multi-aspect
problems with llms,” arXiv:2406.05085, 2024.
[166] E. Saravia, “Prompt Engineering Guide,”
https://github.com/dair-ai/Prompt-Engineering-Guide, 12 2022.
[167] H. S. Zheng, S. Mishra et al., “Take a step back: Evoking reasoning
via abstraction in large language models,” arxiv:2310.06117, 2023.
[168] S. Diao, P. Wang, Y . Lin, and T. Zhang, “Active prompting with chain-
of-thought for large language models,” arxiv:2302.12246, 2023.
[169] H. Jiang, Q. Wu, C. Lin et al., “Llmlingua: Compressing prompts for
accelerated inference of large language models,” in EMNLP, 2023.
[170] N. F. Liu, K. Lin, J. Hewitt et al., “Lost in the middle: How language
models use long contexts,” arxiv:2307.03172, 2023.
[171] T. Ahmed, K. S. Pai, P. Devanbu, and E. T. Barr, “Automatic semantic
augmentation of language model prompts (for code summarization),”
arXiv:2304.06815, 2024.
[172] Z. Xu, Z. Liu, Y . Liu et al., “Activerag: Revealing the treasures of
knowledge via active learning,” arXiv:2402.13547, 2024.
[173] E. Nijkamp, B. Pang, H. Hayashi et al., “A conversational paradigm
for program synthesis,” arxiv:2203.13474, 2022.
[174] Y . He, M. Xia, H. Chen et al., “Animate-a-story: Storytelling with
retrieval-augmented video generation,” arXiv:2307.06940, 2023.
[175] E. J. Hu, Y . Shen, P. Wallis et al., “Lora: Low-rank adaptation of large
language models,” in ICLR, 2022.
[176] C. Liu, P. C ¸ etin, Y . Patodiaet al., “Automated code editing with search-
generate-modify,” arXiv:2306.06490, 2023.
[177] H. Joshi, J. P. C. S ´anchez, S. Gulwani et al., “Repair is nearly
generation: Multilingual program repair with llms,” in AAAI, 2023.
[178] Z. Jiang, F. F. Xu, L. Gao et al., “Active retrieval augmented genera-
tion,” arXiv:2305.06983, 2023.
[179] A. Mallen, A. Asai, V . Zhong et al., “When not to trust language
models: Investigating effectiveness of parametric and non-parametric
memories,” in ACL, 2023.
[180] Z. Jiang, J. Araki, H. Ding, and G. Neubig, “How can we know When
language models know? on the calibration of language models for
question answering,” TACL, 2021.
[181] N. Kandpal, H. Deng, A. Roberts et al., “Large language models
struggle to learn long-tail knowledge,” in ICML, 2023.
[182] R. Ren, Y . Wang, Y . Qu et al., “Investigating the factual knowledge
boundary of large language models with retrieval augmentation,”
arxiv:2307.11019, 2023.
[183] Y . Wang, P. Li, M. Sun, and Y . Liu, “Self-knowledge guided retrieval
augmentation for large language models,” in EMNLP Findings, 2023.
[184] H. Ding, L. Pang, Z. Wei et al., “Retrieve only when it needs: Adaptive
retrieval augmentation for hallucination mitigation in large language
models,” arXiv:2402.10612, 2024.
[185] S. Jeong, J. Baek, S. Cho et al., “Adaptive-rag: Learning to adapt
retrieval-augmented large language models through question complex-
ity,” arXiv:2403.14403, 2024.
[186] F. Zhang, B. Chen et al., “Repocoder: Repository-level code completion
through iterative retrieval and generation,” in EMNLP, 2023.
[187] Z. Shao, Y . Gong, Y . Shen et al., “Enhancing retrieval-augmented
large language models with iterative retrieval-generation synergy,” in
EMNLP Findings, 2023.
[188] X. Cheng, D. Luo, X. Chen et al., “Lift yourself up: Retrieval-
augmented text generation with self-memory,” in NeurIPS, 2023.
[189] Z. Wang, A. Liu, H. Lin et al., “Rat: Retrieval augmented
thoughts elicit context-aware reasoning in long-horizon generation,”
arXiv:2403.05313, 2024.
[190] O. Agarwal, H. Ge, S. Shakeri, and R. Al-Rfou, “Knowledge graph
based synthetic corpus generation for knowledge-enhanced language
model pre-training,” in NAACL-HLT, 2021.
[191] J. Sun, C. Xu, L. Tang et al., “Think-on-graph: Deep and respon-
sible reasoning of large language model with knowledge graph,”
arXiv:2307.07697, 2023.
[192] P. Limkonchotiwat, W. Ponwitayarat, C. Udomcharoenchaikit et al.,
“Cl-relkt: Cross-lingual language knowledge transfer for multilingual
retrieval question answering,” in NAACL Findings, 2022.
[193] A. Asai, X. Yu, J. Kasai, and H. Hajishirzi, “One question answering
model for many languages with cross-lingual dense passage retrieval,”
in NeurIPS, 2021.
[194] K. Lee, S. Han et al., “When to read documents or QA history: On
unified and selective open-domain QA,” in ACL Findings, 2023.
[195] S. Yue, W. Chen et al., “Disc-lawllm: Fine-tuning large language
models for intelligent legal services,” arXiv:2309.11325, 2023.20
[196] S. Siriwardhana, R. Weerasekera, T. Kaluarachchi et al., “Improving
the domain adaptation of retrieval augmented generation (RAG) models
for open domain question answering,” TACL, vol. 11, pp. 1–17, 2023.
[197] Y . Tang and Y . Yang, “Multihop-rag: Benchmarking retrieval-
augmented generation for multi-hop queries,” arXiv:2401.15391, 2024.
[198] K. Huang, C. Zhai, and H. Ji, “CONCRETE: improving cross-lingual
fact-checking with cross-lingual retrieval,” in COLING, 2022.
[199] L. Hagstr ¨om, D. Saynova, T. Norlund et al., “The effect of scaling,
retrieval augmentation and form on the factual consistency of language
models,” arXiv:2311.01307, 2023.
[200] H. Zamani and M. Bendersky, “Stochastic rag: End-to-end retrieval-
augmented generation through expected utility maximization,”
arXiv:2405.02816, 2024.
[201] Y . Liu, Y . Wanet al., “KG-BART: knowledge graph-augmented BART
for generative commonsense reasoning,” in AAAI, 2021.
[202] A. Wan, E. Wallace, and D. Klein, “What evidence do language models
find convincing?” arXiv:2402.11782, 2024.
[203] H. Zhang, Z. Liu et al., “Grounded conversation generation as guided
traverses in commonsense knowledge graphs,” in ACL, 2020.
[204] D. Cai, Y . Wang, W. Bi et al., “Skeleton-to-response: Dialogue gener-
ation guided by retrieval memory,” in NAACL-HLT, 2019.
[205] M. Komeili, K. Shuster, and J. Weston, “Internet-augmented dialogue
generation,” in ACL, 2022.
[206] K. Shuster, J. Xu et al., “Blenderbot 3: a deployed conversational agent
that continually learns to responsibly engage,” arXiv:2208.03188, 2022.
[207] S. Kim, J. Y . Jang, M. Jung, and S. Shin, “A model of cross-lingual
knowledge-grounded response generation for open-domain dialogue
systems,” in EMNLP Findings, 2021.
[208] E. Nie, S. Liang, H. Schmid, and H. Sch ¨utze, “Cross-lingual retrieval
augmented prompt for low-resource languages,” in ACL, 2023.
[209] X. Li, E. Nie, and S. Liang, “From classification to generation: Insights
into crosslingual retrieval augmented icl,” in NeurIPS, 2023.
[210] W. Li, J. Li, W. Ma, and Y . Liu, “Citation-enhanced generation for
llm-based chatbot,” arXiv:2402.16063, 2024.
[211] D. Cai, Y . Wang, H. Li et al., “Neural machine translation with
monolingual translation memory,” in ACL/IJCNLP, 2021.
[212] U. Khandelwal, A. Fan, D. Jurafsky et al., “Nearest neighbor machine
translation,” in ICLR, 2021.
[213] X. Du and H. Ji, “Retrieval-augmented generative question answering
for event argument extraction,” in EMNLP, 2022.
[214] Y . Gao, Q. Yin, Z. Li et al., “Retrieval-augmented multilingual
keyphrase generation with retriever-generator iterative training,” in
NAACL Findings, 2022.
[215] J. Zhang, E. J. Yu, Q. Chen et al., “Retrieval-based full-length wikipedia
generation for emergent events,” arXiv:2402.18264, 2024.
[216] R. Fan, Y . Fan, J. Chen et al., “RIGHT: retrieval-augmented generation
for mainstream hashtag recommendation,” arxiv:2312.10466, 2023.
[217] Z. Wang, S. X. Teo et al., “M-rag: Reinforcing large language model
performance through retrieval-augmented generation with multiple par-
titions,” arXiv:2405.16420, 2024.
[218] Y . Wang, H. Le, A. D. Gotmare et al., “Codet5mix: A pretrained
mixture of encoder-decoder transformers for code understanding and
generation,” 2022.
[219] A. Madaan, S. Zhou, U. Alon et al., “Language models of code are
few-shot commonsense learners,” in EMNLP, 2022.
[220] Y . Wang, H. Le, A. Gotmareet al., “Codet5+: Open code large language
models for code understanding and generation,” in EMNLP, 2023.
[221] J. Chen, X. Hu, Z. Li et al., “Code search is all you need? improving
code suggestions with code search,” in ICSE, 2024.
[222] D. Zan, B. Chen, Y . Gong et al., “Private-library-oriented code gener-
ation with large language models,” arXiv:2307.15370, 2023.
[223] M. Liu, T. Yang, Y . Lou et al., “Codegen4libs: A two-stage approach
for library-oriented code generation,” in ASE, 2023.
[224] D. Liao, S. Pan, Q. Huang et al., “Context-aware code generation
framework for code repositories: Local, global, and third-party library
awareness,” arXiv:2312.05772, 2023.
[225] J. Li, Y . Li, G. Li et al., “Skcoder: A sketch-based approach for
automatic code generation,” in ICSE, 2023.
[226] Q. Gou, Y . Dong, Y . Wu, and Q. Ke, “Rrgcode: Deep hierarchical
search-based code generation,” Journal of Systems and Software, vol.
211, p. 111982, 2024.
[227] K. Zhang, J. Li, G. Li et al., “Codeagent: Enhancing code generation
with tool-integrated agent systems for real-world repo-level coding
challenges,” arXiv:2401.07339, 2024.
[228] H. Su, S. Jiang, Y . Lai et al., “Arks: Active retrieval in knowledge soup
for code generation,” arXiv:2402.12317, 2024.
[229] K. Zhang, G. Li, J. Li et al., “Toolcoder: Teach code generation models
to use API search tools,” arXiv:2305.04032, 2023.
[230] S. Liu, Y . Chen, X. Xie et al., “Retrieval-augmented generation for
code summarization via hybrid GNN,” in ICLR, 2021.
[231] F. Yamaguchi, N. Golde, D. Arp, and K. Rieck, “Modeling and
discovering vulnerabilities with code property graphs,” in S&P, 2014.
[232] Y . Choi, C. Na et al., “Readsum: Retrieval-augmented adaptive trans-
former for source code summarization,” IEEE Access, 2023.
[233] J. Zhao, X. Chen, G. Yang, and Y . Shen, “Automatic smart contract
comment generation via large language models and in-context learn-
ing,” IST, vol. 168, p. 107405, 2024.
[234] A. Alokla, W. Gad, W. Nazih et al., “Retrieval-based transformer
pseudocode generation,” Mathematics, vol. 10, no. 4, p. 604, 2022.
[235] J. Xu, Z. Cui et al., “Unilog: Automatic logging via LLM and in-
context learning,” in ICSE, 2024.
[236] H. Wang, X. Xia et al., “Context-aware retrieval-based deep commit
message generation,” TOSEM, vol. 30, no. 4, pp. 56:1–56:30, 2021.
[237] X. Zhu, C. Sha, and J. Niu, “A simple retrieval-based method for code
comment generation,” in SANER, 2022.
[238] T. Ye, L. Wu, T. Ma et al., “Tram: A token-level retrieval-augmented
mechanism for source code summarization,” arXiv:2305.11074, 2023.
[239] L. Li, B. Liang, L. Chen, and X. Zhang, “Cross-modal retrieval-
enhanced code summarization based on joint learning for retrieval and
generation,” Available at SSRN 4724884.
[240] D. Drain, C. Hu, C. Wu et al., “Generating code with the
help of retrieved template functions and stack overflow answers,”
arXiv:2104.05310, 2021.
[241] A. Eghbali and M. Pradel, “De-hallucinator: Iterative grounding for
llm-based code completion,” arXiv:2401.01701, 2024.
[242] M. Liang, X. Xie, G. Zhang et al., “Repofuse: Repository-level code
completion with fused dual context,” arXiv:2402.14323, 2024.
[243] D. Shrivastava, D. Kocetkov et al., “Repofusion: Training code models
to understand your repository,” arXiv:2306.10998, 2023.
[244] W. Sun, H. Li, M. Yan et al., “Revisiting and improving retrieval-
augmented deep assertion generation,” in ASE, 2023.
[245] Y . Ding, Z. Wang et al., “Cocomic: Code completion by jointly
modeling in-file and cross-file context,” arXiv:2212.10007, 2022.
[246] Z. Tang, J. Ge, S. Liu et al., “Domain adaptive code completion via
language models and decoupled domain databases,” in ASE, 2023.
[247] Y . Tsai, M. Liu, and H. Ren, “Rtlfixer: Automatically fixing RTL syntax
errors with large language models,” arXiv:2311.16543, 2023.
[248] B. Bogin, S. Gupta, P. Clark et al., “Leveraging code to improve in-
context learning for semantic parsing,” arXiv:2311.09519, 2023.
[249] H. Li, J. Zhang, C. Li, and H. Chen, “Resdsql: Decoupling schema
linking and skeleton parsing for text-to-sql,” in AAAI, 2023.
[250] K. Zhang, X. Lin, Y . Wang et al., “Refsql: A retrieval-augmentation
framework for text-to-sql generation,” in EMNLP Findings, 2023.
[251] S. Chang and E. Fosler-Lussier, “Selective demonstrations for cross-
domain text-to-sql,” arXiv:2310.06302, 2023.
[252] L. Nan, Y . Zhao, W. Zou et al., “Enhancing text-to-sql capabilities
of large language models: A study on prompt design strategies,” in
EMNLP Findings, 2023.
[253] X. Zhang, D. Wang, L. Dou et al., “Multi-hop table retrieval for open-
domain text-to-sql,” arXiv:2402.10666, 2024.
[254] H. Li, J. Zhang, H. Liu et al., “Codes: Towards building open-source
language models for text-to-sql,” arXiv:2402.16347, 2024.
[255] Z. Jie and W. Lu, “Leveraging training data in few-shot prompting for
numerical reasoning,” arXiv:2305.18170, 2023.
[256] M. Gao, J. Li, H. Fei et al., “De-fine: Decomposing and refining visual
programs with auto-feedback,” arXiv:2311.12890, 2023.
[257] Y . Hao, W. Chen, Z. Zhou, and W. Cui, “E&v: Prompting large
language models to perform static analysis by pseudo-code execution
and verification,” arXiv:2312.08477, 2023.
[258] Y . Guo, Z. Li et al., “Retrieval-augmented code generation for universal
information extraction,” arXiv:2311.02962, 2023.
[259] G. Pinto, C. de Souza et al., “Lessons from building stackspot ai: A
contextualized ai coding assistant,” arXiv:2311.18450, 2024.
[260] Z. Liu, C. Chen, J. Wang et al., “Testing the limits: Unusual text inputs
generation for mobile app crash detection with large language model,”
arXiv:2310.15657, 2023.
[261] K. D. Bollacker, C. Evans et al., “Freebase: a collaboratively created
graph database for structuring human knowledge,” in SIGMOD, 2008.
[262] M. Patidar, A. K. Singh, R. Sawhney et al., “Combining transfer
learning with in-context learning using blackbox llms for zero-shot
knowledge base question answering,” arXiv:2311.08894, 2023.
[263] Y . Shu and Z. Yu, “Data distribution bottlenecks in grounding language
models to knowledge bases,” arXiv:2309.08345, 2023.21
[264] D. Leake and D. J. Crandall, “On bringing case-based reasoning
methodology to deep learning,” in ICCBR, 2020.
[265] L. Zhang, J. Zhang et al., “FC-KBQA: A fine-to-coarse composition
framework for knowledge base question answering,” in ACL, 2023.
[266] J. Jiang, K. Zhou et al., “Structgpt: A general framework for large
language model to reason over structured data,” in EMNLP, 2023.
[267] J. Baek, A. F. Aji, and A. Saffari, “Knowledge-augmented language
model prompting for zero-shot knowledge graph question answering,”
arXiv:2306.04136, 2023.
[268] P. Sen, S. Mavadia, and A. Saffari, “Knowledge graph-augmented
language models for complex question answering,” in NLRSE, 2023.
[269] Y . Wu, N. Hu, S. Bi et al., “Retrieve-rewrite-answer: A kg-to-text
enhanced llms framework for knowledge graph question answering,”
arXiv:2309.11206, 2023.
[270] C. Wang, Y . Xu, Z. Peng et al., “keqing: knowledge-based ques-
tion answering is a nature chain-of-thought mentor of LLM,”
arXiv:2401.00426, 2024.
[271] J. Liu, S. Cao, J. Shi et al., “Probing structured semantics under-
standing and generation of language models via question answering,”
arXiv:2401.05777, 2024.
[272] G. Xiong, J. Bao, and W. Zhao, “Interactive-kbqa: Multi-turn inter-
actions for knowledge base question answering with large language
models,” arXiv:2402.15131, 2024.
[273] S. Chen, Q. Liu, Z. Yu et al., “Retrack: A flexible and efficient
framework for knowledge base question answering,” in ACL, 2021.
[274] D. Yu, C. Zhu, Y . Fang et al., “Kg-fid: Infusing knowledge graph in
fusion-in-decoder for open-domain question answering,” in ACL, 2022.
[275] M. Ju, W. Yu, T. Zhao et al., “Grape: Knowledge graph enhanced
passage reader for open-domain question answering,” in EMNLP
Findings, 2022.
[276] Z. Hu, Y . Xu, W. Yu et al., “Empowering language models with
knowledge graph reasoning for open-domain question answering,” in
EMNLP, 2022.
[277] Q. Yang, Q. Chen, W. Wang et al., “Enhancing multi-modal multi-
hop question answering via structured knowledge and unified retrieval-
generation,” in MM, 2023.
[278] W. Zhao, Y . Liu, T. Niuet al., “DIVKNOWQA: assessing the reasoning
ability of llms via open-domain question answering over knowledge
base and text,” arXiv:2310.20170, 2023.
[279] X. Wang, Q. Yang, Y . Qiu et al., “Knowledgpt: Enhancing large
language models with retrieval and storage access on knowledge bases,”
arXiv:2308.11761, 2023.
[280] S. Ko, H. Cho, H. Chae et al., “Evidence-focused fact summa-
rization for knowledge-augmented zero-shot question answering,”
arXiv:2403.02966, 2024.
[281] Y . Gao, L. Qiao, Z. Kan et al., “Two-stage generative question
answering on temporal knowledge graph using large language models,”
arXiv:2402.16568, 2024.
[282] T. Guo, Q. Yang, C. Wang et al., “Knowledgenavigator: Leveraging
large language models for enhanced reasoning over knowledge graph,”
arXiv:2312.15880, 2023.
[283] C. Mavromatis and G. Karypis, “Gnn-rag: Graph neural retrieval for
large language model reasoning,” arXiv:2405.20139, 2024.
[284] S. Min, J. Boyd-Graber, C. Alberti et al., “Neurips 2020 efficientqa
competition: Systems, analyses and lessons learned,” in NeurIPS 2020
Competition and Demonstration Track, 2021.
[285] A. H. Li, P. Ng, P. Xu et al., “Dual reader-parser on hybrid tex-
tual and tabular evidence for open domain question answering,” in
ACL/IJCNLP, 2021.
[286] K. Ma, H. Cheng, X. Liu et al., “Open-domain question answering
via chain of reasoning over heterogeneous knowledge,” in EMNLP
Findings, 2022.
[287] P. Christmann, R. S. Roy, and G. Weikum, “Conversational question
answering on heterogeneous sources,” in SIGIR, 2022.
[288] E. Park, S.-M. Lee et al., “Rink: reader-inherited evidence reranker for
table-and-text open domain question answering,” in AAAI, 2023.
[289] W. Zhao, Y . Liu, Y . Wan et al., “Localize, retrieve and fuse: A
generalized framework for free-form question answering over tables,”
arXiv:2309.11049, 2023.
[290] F. Pan, M. Canim et al., “End-to-end table question answering via
retrieval-augmented generation,” arXiv:2203.16714, 2022.
[291] Z. Jiang, Y . Mao, P. He et al., “Omnitab: Pretraining with natural
and synthetic data for few-shot table-based question answering,” in
NAACL, 2022.
[292] W. Zhong, J. Huang, Q. Liu et al., “Reasoning over hybrid chain for
table-and-text open domain question answering,” in IJCAI, 2022.
[293] A. S. Sundar and L. Heck, “ctbl: Augmenting large language models
for conversational tables,” arXiv:2303.12024, 2023.
[294] D. Min, N. Hu, R. Jin et al., “Exploring the impact of table-to-text
methods on augmenting llm-based question answering with domain
hybrid data,” arXiv:2402.12869, 2024.
[295] S. Roychowdhury, M. Krema et al., “Eratta: Extreme rag for table to
answers with large language models,” arXiv:2405.03963, 2024.
[296] S. Wu, Y . Li, D. Zhang, and Z. Wu, “Improving knowledge-aware
dialogue response generation by using human-written prototype dia-
logues,” in EMNLP Findings, 2020.
[297] M. Kang, J. M. Kwak et al., “Knowledge-consistent dialogue genera-
tion with knowledge graphs,” in ICML Workshop, 2022.
[298] Z. Ji, Z. Liu, N. Lee et al., “RHO: reducing hallucination in open-
domain dialogues with knowledge grounding,” in ACL Findings, 2023.
[299] J. Baek, N. Chandrasekaran, S. Cucerzan et al., “Knowledge-
augmented large language models for personalized contextual query
suggestion,” arXiv:2311.06318, 2023.
[300] X. He, Y . Tian, Y . Sun et al., “G-retriever: Retrieval-augmented
generation for textual graph understanding and question answering,”
arXiv:2402.07630, 2024.
[301] M. M. Hussien, A. N. Melo et al., “Rag-based explainable prediction
of road users behaviors for automated driving using knowledge graphs
and large language models,” arXiv:2405.00449, 2024.
[302] B. J. Guti ´errez, Y . Shu et al., “Hipporag: Neurobiologically inspired
long-term memory for large language models,” arXiv:2405.14831,
2024.
[303] Y . Kirstain, O. Levy, and A. Polyak, “X&fuse: Fusing visual informa-
tion in text-to-image generation,” arXiv:2303.01000, 2023.
[304] P. Dhariwal and A. Nichol, “Diffusion models beat gans on image
synthesis,” NeurIPS, 2021.
[305] L. Yang, Z. Yu, C. Meng et al., “Mastering text-to-image diffu-
sion: Recaptioning, planning, and generating with multimodal llms,”
arXiv:2401.11708, 2024.
[306] Z. Zhang, A. Zhang, M. Li et al., “Multimodal chain-of-thought
reasoning in language models,” arXiv:2302.00923, 2023.
[307] C. Xu, M. Yang, X. Ao et al., “Retrieval-enhanced adversarial train-
ing with dynamic memory-augmented attention for image paragraph
captioning,” Knowledge-Based Systems, vol. 214, p. 106730, 2021.
[308] R. Ramos, D. Elliott, and B. Martins, “Retrieval-augmented image
captioning,” in EACL, 2023.
[309] Z. Hu, A. Iscen, C. Sun et al., “Reveal: Retrieval-augmented visual-
language pre-training with multi-source multimodal knowledge mem-
ory,” in CVPR, 2023.
[310] Z. Li, W. Zhao, X. Du et al., “Cross-modal retrieval and semantic
refinement for remote sensing image captioning,” Remote Sensing,
vol. 16, no. 1, p. 196, 2024.
[311] Z. Yang, Z. Gan, J. Wang et al., “An empirical study of gpt-3 for
few-shot knowledge-based vqa,” in AAAI, 2022.
[312] W. Lin and B. Byrne, “Retrieval augmented visual question answering
with outside knowledge,” in EMNLP, 2022.
[313] A. Fan, C. Gardent, C. Braud, and A. Bordes, “Augmenting transform-
ers with knn-based composite memory for dialog,” TACL, vol. 9, pp.
82–99, 2021.
[314] Z. Liang, H. Hu, C. Xu et al., “Maria: A visual experience powered
conversational agent,” in ACL-IJCNLP, 2021.
[315] Q. Fang and Y . Feng, “Neural machine translation with phrase-level
universal visual representations,” in ACL, 2022.
[316] S. Whitehead, H. Ji, M. Bansal et al., “Incorporating background
knowledge into video description generation,” in EMNLP, 2018.
[317] C. Yin, J. Tang, Z. Xu, and Y . Wang, “Memory augmented deep
recurrent neural network for video question answering,” TNNLS,
vol. 31, no. 9, pp. 3159–3167, 2019.
[318] J. Pan, Z. Lin, Y . Ge et al., “Retrieving-to-answer: Zero-shot video
question answering with frozen large language models,” inICCV, 2023.
[319] J. Lei, L. Yu, T. L. Berg, and M. Bansal, “Tvqa+: Spatio-temporal
grounding for video question answering,” in ACL, 2020.
[320] H. Le, N. Chen, and S. Hoi, “Vgnmn: Video-grounded neural module
networks for video-grounded dialogue systems,” in NAACL, 2022.
[321] Z. Wang, M. Li, R. Xu et al., “Language models with image descriptors
are strong few-shot video-language learners,” in NeurIPS, 2022.
[322] J. Yuan, S. Sun, D. Omeiza et al., “Rag-driver: Generalisable driving
explanations with retrieval-augmented in-context learning in multi-
modal large language model,” arXiv:2402.10828, 2024.
[323] S. Ghosh, S. Kumar, C. K. R. Evuru et al., “Recap: retrieval-augmented
audio captioning,” in ICASSP, 2024.
[324] B. Elizalde, S. Deshmukh, and H. Wang, “Natural language supervision
for general-purpose audio representations,” in ICASSP, 2024.22
[325] T. Kouzelis and V . Katsouros, “Weakly-supervised automated audio
captioning via text only training,” in DCASE Workshop, 2023.
[326] S. Deshmukh, B. Elizalde, D. Emmanouilidou et al., “Training audio
captioning models without audio,” in ICASSP, 2024.
[327] Z. Wang, C. Lu, Y . Wang et al., “Prolificdreamer: High-fidelity and
diverse text-to-3d generation with variational score distillation,” in
NeurIPS, 2024.
[328] L. Yang, Z. Huang, X. Zhou et al., “Prompt-based 3d molecular
diffusion models for structure-based drug design,” 2023.
[329] T. Truong Jr and T. Bepler, “Poet: A generative model of protein
families as sequences-of-sequences,” NeurIPS, 2024.
[330] G. Frisoni, M. Mizutani, G. Moro, and L. Valgimigli, “Bioreader: a
retrieval-enhanced text-to-text transformer for biomedical literature,”
in EMNLP, 2022.
[331] X. Yang, M. Ye, Q. You et al., “Writing by memorizing: Hierarchical
retrieval-based medical report generation,” arXiv:2106.06471, 2021.
[332] J. Kim and M. Min, “From rag to qa-rag: Integrating generative ai
for pharmaceutical regulatory compliance process,” arXiv:2402.01717,
2024.
[333] Y . Ji, Z. Li et al., “Rag-rlrc-laysum at biolaysumm: Integrating retrieval-
augmented generation and readability control for layman summariza-
tion of biomedical texts,” arXiv:2405.13179, 2024.
[334] K. Yang, A. Swope et al., “Leandojo: Theorem proving with retrieval-
augmented language models,” in NeurIPS, 2024.
[335] Z. Levonian, C. Li, W. Zhu et al., “Retrieval-augmented generation to
improve math question-answering: Trade-offs between groundedness
and human preference,” arXiv:2310.03184, 2023.
[336] J. Chen, H. Lin, X. Han, and L. Sun, “Benchmarking large language
models in retrieval-augmented generation,” arxiv:2309.01431, 2023.
[337] S. ES, J. James, L. E. Anke, and S. Schockaert, “RAGAS: automated
evaluation of retrieval augmented generation,” arxiv:2309.15217, 2023.
[338] J. Saad-Falcon, O. Khattab, C. Potts et al., “ARES: an automated
evaluation framework for retrieval-augmented generation systems,”
arxiv:2311.09476, 2023.
[339] https://github.com/truera/trulens.
[340] Y . Lyu, Z. Li, S. Niu et al., “CRUD-RAG: A comprehensive chinese
benchmark for retrieval-augmented generation of large language mod-
els,” arxiv:2401.17043, 2024.
[341] G. Xiong, Q. Jin, Z. Lu, and A. Zhang, “Benchmarking retrieval-
augmented generation for medicine,” arXiv:2402.13178, 2024.
[342] F. Petroni, A. Piktus et al., “Kilt: a benchmark for knowledge intensive
language tasks,” in NAACL-HLT, 2021.
[343] S. Barnett, S. Kurniawan, S. Thudumu et al., “Seven failure
points when engineering a retrieval augmented generation system,”
arXiv:2401.05856, 2024.
[344] F. Cuconasu, G. Trappolini, F. Siciliano et al., “The power of noise:
Redefining retrieval for RAG systems,” arXiv:2401.14887, 2024.
[345] L. Qiu, P. Shaw, P. Pasupat et al., “Evaluating the impact of
model scale for compositional generalization in semantic parsing,”
arXiv:2205.12253, 2022.
[346] R. Jagerman, H. Zhuang, Z. Qin et al., “Query expansion by prompting
large language models,” arxiv:2305.03653, 2023.
[347] H. Zhang, P. Zhao, X. Miao et al., “Experimental analysis of large-scale
learnable vector storage compression,” VLDB, 2023.
[348] R. Aksitov, C. Chang, D. Reitter et al., “Characterizing attribution
and fluency tradeoffs for retrieval-augmented large language models,”
arXiv:2302.05578, 2023.
[349] C. Han, Q. Wang, W. Xiong et al., “Lm-infinite: Simple on-the-fly
length generalization for large language models,” arXiv:2308.16137,
2023.
[350] H. Chase, “Langchain,” https://github.com/langchain-ai/langchain,
2022.
[351] W. Jiang, S. Zhang, B. Han et al., “Piperag: Fast retrieval-augmented
generation via algorithm-system co-design,” arXiv:2403.05676, 2024.
[352] K. Meduri et al., “Efficient rag framework for large-scale knowledge
bases,” 2024.
[353] S. Jindal, “Did google gemini 1.5 really kill rag?” https://
analyticsindiamag.com/did-google-gemini-1-5-really-kill-rag/, 2024.

The Power of Noise: Redefining Retrieval for RAG Systems
Florin Cuconasu∗
cuconasu@diag.uniroma1.it
Sapienza University of Rome
Rome, Italy
Giovanni Trappolini∗
trappolini@diag.uniroma1.it
Sapienza University of Rome
Rome, Italy
Federico Siciliano
siciliano@diag.uniroma1.it
Sapienza University of Rome
Rome, Italy
Simone Filice
filice.simone@gmail.com
Technology Innovation Institute
Haifa, Israel
Cesare Campagnano
campagnano@di.uniroma1.it
Sapienza University of Rome
Rome, Italy
Yoelle Maarek
yoelle@yahoo.com
Technology Innovation Institute
Haifa, Israel
Nicola Tonellotto
nicola.tonellotto@unipi.it
University of Pisa
Pisa, Italy
Fabrizio Silvestri
fsilvestri@diag.uniroma1.it
Sapienza University of Rome
Rome, Italy
ABSTRACT
Retrieval-Augmented Generation (RAG) has recently emerged as
a method to extend beyond the pre-trained knowledge of Large
Language Models by augmenting the original prompt with relevant
passages or documents retrieved by an Information Retrieval (IR)
system. RAG has become increasingly important for Generative
AI solutions, especially in enterprise settings or in any domain in
which knowledge is constantly refreshed and cannot be memorized
in the LLM. We argue here that the retrieval component of RAG
systems, be it dense or sparse, deserves increased attention from
the research community, and accordingly, we conduct the first com-
prehensive and systematic examination of the retrieval strategy
of RAG systems. We focus, in particular, on the type of passages
IR systems within a RAG solution should retrieve. Our analysis
considers multiple factors, such as the relevance of the passages in-
cluded in the prompt context, their position, and their number. One
counter-intuitive finding of this work is that the retriever’s highest-
scoring documents that are not directly relevant to the query (e.g.,
do not contain the answer) negatively impact the effectiveness of
the LLM. Even more surprising, we discovered that adding random
documents in the prompt improves the LLM accuracy by up to
35%. These results highlight the need to investigate the appropriate
strategies when integrating retrieval with LLMs, thereby laying the
groundwork for future research in this area.1
CCS CONCEPTS
• Information systems →Novelty in information retrieval.
1The code and data are available at github.com/florin-git/The-Power-of-Noise
*These authors contributed equally to this work.
This work is licensed under a Creative Commons Attribution
International 4.0 License.
SIGIR ’24, July 14–18, 2024, Washington, DC, USA
© 2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0431-4/24/07
https://doi.org/10.1145/3626772.3657834
KEYWORDS
RAG, LLM, Information Retrieval
ACM Reference Format:
Florin Cuconasu∗, Giovanni Trappolini ∗, Federico Siciliano, Simone Fil-
ice, Cesare Campagnano, Yoelle Maarek, Nicola Tonellotto, and Fabrizio
Silvestri. 2024. The Power of Noise: Redefining Retrieval for RAG Sys-
tems. InProceedings of the 47th International ACM SIGIR Conference on
Research and Development in Information Retrieval (SIGIR ’24), July 14–18,
2024, Washington, DC, USA. ACM, New York, NY, USA, 11 pages. https:
//doi.org/10.1145/3626772.3657834
1 INTRODUCTION
Large Language Models (LLMs) [ 9] have demonstrated unprece-
dented proficiency in various tasks, ranging from text generation
and complex question answering [6], to information retrieval (IR)
tasks [22, 57]. However, LLMs have limitations in the handling of
long contexts [52], a constraint that leads to an increased reliance
on their pre-trained knowledge. This limitation not only confines
their ability to effectively manage extended discourse, such as in
books or long conversations, but also increases the probability of
generating hallucinations, instances for which the model produces
factually incorrect or nonsensical information [41]. To improve the
accuracy of responses generated by LLMs, Retrieval-Augmented
Generation (RAG) has emerged as a promising solution [28]. RAG
is primarily designed to improve factual accuracy by providing
the model access to auxiliary information, thereby augmenting the
original prompt with information not necessarily memorized in
the LLM. A key benefit of this approach is that it helps ground the
prompt with relevant information that might help the LLM gener-
ate more accurate answers at inference time. At their core, RAG
systems consist of two fundamental components: a retriever and a
generator. The retriever is responsible for invoking an external IR
system (dense and/or sparse) and feeding the selected results to a
generator component.
This study focuses on the IR aspect of RAG, posing the following
research question: “What characteristics are desirable in a retriever
to optimize prompt construction for RAG systems? Are current re-
trievers ideal?". We focus on the three main types of documents
719
SIGIR ’24, July 14–18, 2024, Washington, DC, USA Florin Cuconasu et al.
(or passages2) that a retriever can return: relevant, distracting, and
random. Relevant documents contain pertinent information that
either directly answers or might inform the query. Distracting doc-
uments, while not directly answering the query, are semantically
or contextually linked to the topic. For instance, if one asks for
the color of Napoléon’s horse, a passage describing the color of
Joséphine de Beauharnais’ (Napoléon’s first wife) horse, while not
containing the right information, would be highly related. Random
documents have no relation whatsoever to the query and can be
seen as a kind of informational noise within the retrieval process.
One of the key goals of our study is to determine the role of each
type of document and the relative value they bring to the LLM
effectiveness. In particular, we verify whether there is a need to
revisit some of the commonly accepted assumptions in IR systems
when used in the context of LLMs. The main contributions of our
work are the following:
(1) We conduct the first comprehensive study examining the
impact of the type of retrieved documents in RAG on the
LLM effectiveness.
(2) We propose retrieval RAG heuristics that leverage the unex-
pected results of this study.
(3) We release all associated code and data to the community to
encourage further research.
2 RELATED WORKS
2.1 Generative Language Models
The inception of the modern LLM era can be traced back to the
seminal paper titled “Attention Is All You Need" [52]. This work in-
troduced the transformer architecture, a framework that adopts an
attention mechanism instead of recurrent layers, enabling the model
to capture global dependencies within the data. The following year,
BERT (Bidirectional Encoder Representations from Transformers)
[22] offered a significant improvement over the state-of-the-art via
a novel bidirectional, unsupervised language representation. The
evolution of transformer-based models continued with the devel-
opment of the Generative Pre-trained Transformer (GPT) [37]. Its
successor, GPT-2 [38], expanded upon this foundation with a larger
scale model and demonstrated improved performance across a vari-
ety of language tasks without task-specific training. The subsequent
iteration, GPT-3 [9], represented a further enhancement in model
scale and capabilities, particularly in the realm of few-shot learning.
Finally, recent times have seen a surge in the production of large,
publicly available language models. Several actors have released
their models, most notably, Llama [49, 50], Falcon [1], Mosaic MPT
[47], and Phi [16, 29]. There are also versions of these models that
have been fine-tuned on specific languages [5, 10, 12, 17, 43]. The
proliferation and quality of these models are expanding the range
of tasks and the vision they address [48, 54, 56].
2.2 Information Retrieval
Foundational information retrieval methodologies, such as the Vec-
tor Space Model and the TF-IDF scoring [ 42] introduced in the
2We interchangeably use here the terms “passage" or “document" to represent the
indexing/retrieval unit of the IR system.
1980s are the basis for quantifying textual similarity. These re-
trieval methods are characterized by their use of high-dimensional
and sparse feature vectors and have been essential in developing
a full generation of IR systems. BM25 represents the most famous
current iteration [ 40]. A significant evolution in IR is the intro-
duction of dense retrievers, which emerged from advancements
in deep learning; they utilize low-dimensional dense vectors for
textual representation, and allow to capture semantic relationships.
This is in contrast to traditional IR methods (referred to as sparse
in opposition to dense), which typically rely on lexical match and
struggle with semantic match [ 32]. In the last few years, dense
methods such as DPR [19] and others [15, 24] have demonstrated
that they can compete with sparse methods.
2.3 Retrieve and Generate
RAG introduces a new approach in AI, combining the strengths of
both retrieval-based and generative models. The concept of RAG
was coined and popularized in [28], which introduced a model that
combines a dense passage retriever with a sequence-to-sequence
model, demonstrating substantial improvements in knowledge-
intensive tasks. Similar methods/variations have also been pro-
posed concurrently or soon after, such as [2, 4, 8, 13, 21]; see [33]
for a survey on augmented language models. Researchers and prac-
titioners have recently started to explore these RAG systems’ in-
ner workings. Notably, [44, 51] analyzed the impact of different
types of documents on cascading IR/NLP systems. Other works
have tried to study how attentive transformers are to their input
[23, 30, 31, 39, 46]. [7] studied the effect of the retriever’s similarity
metric, which was found to be insufficient for reasoning. In [25, 55],
authors analyzed LLM’s receptiveness to external evidence against
internal memory. In [60], they test the model’s (in)ability to ground
references.
In this paper, we want to provide the first comprehensive analysis
of the implications of using a retriever module in a RAG system,
studying the impact of several key factors, like the type, number,
and position of documents that should augment the prompt to the
LLM.
3 RAG
In this paper, we explore the application of RAG in the context of
Question Answering, arguably its most popular application.
3.1 Open-Domain Question Answering
Open-domain Question Answering (OpenQA) refers to the task
of developing systems capable of providing accurate and contex-
tually relevant answers to a broad range of questions posed in
natural language without limitations to specific domains or prede-
fined datasets. In general, we want to find an answer Ato a query
𝑞. To do so, we draw information from a corpus of documents
D= {𝑑1,𝑑2,...,𝑑 𝑛}, which is usually assumed to be large in size.
A prevalent approach for this task involves a two-step architecture,
typically comprising a retriever and a reasoner (typically a gen-
erator). This methodology addresses the inherent complexities of
OpenQA by dividing the process into distinct phases: first finding
the appropriate set of documents that can potentially address the
720The Power of Noise: Redefining Retrieval for RAG Systems SIGIR ’24, July 14–18, 2024, Washington, DC, USA
query and then synthesizing an answer, which can be consumed
by the user of the QA system.
3.2 Retriever
The retriever plays a critical role in the OpenQA task. Its goal
is to find a sufficiently small subset of documents D𝑟 to allow
the reasoner to answer the query correctly. Among the various
retrieval methodologies, the use of a dense retriever has gained
prominence due to its effectiveness in handling semantic matches.
Dense retrieval requires transforming textual data into vector rep-
resentations, which is typically achieved with a neural network,
often a transformer-based encoder, like BERT [
22]. The dense re-
triever processes both the query 𝑞and potential source documents
to generate corresponding embeddings ®𝑞for the query and ®𝑑𝑖 for
each document 𝑑𝑖 ∈D. The embedding process can be represented
as:
®𝑞= 𝐸𝑛𝑐𝑜𝑑𝑒𝑟𝑞 (𝑞); ®𝑑𝑖 = 𝐸𝑛𝑐𝑜𝑑𝑒𝑟𝑑 (𝑑𝑖)
where 𝐸𝑛𝑐𝑜𝑑𝑒𝑟𝑞 and 𝐸𝑛𝑐𝑜𝑑𝑒𝑟𝑑 are neural network-based encoders,
potentially sharing weights or architecture, designed to map the
textual data into a vector space. Once the embeddings are generated,
the retrieval process involves computing the similarity between
the query embedding and each document embedding. The most
common approach is to use dot product [20], defined as: 𝑠(𝑞,𝑑𝑖)=
®𝑞 ·®𝑑𝑖. This score quantifies the relevance of each document to
the query by measuring their similarity in the embedded vector
space, with higher scores indicating greater relevance. According
to these scores, the top-ranked documents are selected for further
processing in the generator component.
3.3 Reasoner
The second step involves a generator component in charge of syn-
thesizing an answer, typically implemented via an LLM. Generative
language models operate by predicting the probability distribution
of the next token, given the previous tokens. For a given sequence
of words 𝑤1,𝑤2,...,𝑤 𝑛, a generative language model aims to max-
imize the likelihood of this sequence, expressed using the chain
rule of probability:
𝑃(𝑤1,𝑤2,...,𝑤 𝑛)=
𝑁Ö
𝑖=1
𝑃(𝑤𝑖 |𝑤1,𝑤2,...,𝑤 𝑖−1)
where 𝑃(𝑤𝑖 |𝑤1,𝑤2,...,𝑤 𝑖−1)is the conditional probability of the
word 𝑤𝑖 given the preceding sequence of words 𝑤1,𝑤2,...,𝑤 𝑖−1.
In RAG, the generative language model takes a query 𝑞 and the
retrieved documents D𝑟 as input and generates a response by se-
quentially predicting the next token in the sequence. More formally,
𝑃𝑟𝑎𝑔 (𝑦|𝑞)≈
𝑁Ö
𝑖
∑︁
𝑑∈D𝑟
𝑝𝜂 (𝑑|𝑞)𝑝𝜃 (𝑦𝑖 |𝑞,𝑑,𝑦 1:𝑖−1),
where 𝑝𝜂 (𝑑|𝑞)is the retrieval component that provides a (trun-
cated) probability distribution for the top-scoring documents, and
𝑝𝜃 (𝑦𝑖 |𝑞,𝑑,𝑦 1:𝑖−1)is a probability distribution parameterized by 𝜃
that generates a current token based on the previously generated
tokens, the query, and the retrieved document; this role is filled by
the LLM. In the case of dense retrieval, the probability distribution
for the top-scoring documents may assume a functional form of
the kind 𝑝𝜂 (𝑑|𝑞)∝ exp(®𝑞·®𝑑). Given our formalization of the RAG
task, we notice how the generative component 𝑝𝜃 depends on a
given text, that is the query, and a dynamic text, that is the set of
retrieved documents. We study in the next two sections the impact
of changing the set of retrieved documents on the generator and,
consequently, the whole end-to-end system. In particular, we aim
to find the best set of documents D𝑟 that a retriever should feed
the generator to maximize the system’s effectiveness.
4 EXPERIMENTAL METHODOLOGY
In this section, we detail the experimental framework. We start by
describing the data used in the experiments and then discuss the
type of documents that a retriever can return and pass to the LLM.
4.1 Natural Question Dataset
The Natural Questions (NQ) dataset [26] is a large-scale collection
of real-world queries derived from Google search data. Each en-
try in the dataset consists of a user query and the corresponding
Wikipedia page containing the answer. The NQ-open dataset [27], a
subset of the NQ dataset, differs by removing the restriction of link-
ing answers to specific Wikipedia passages, thereby mimicking a
more general information retrieval scenario similar to web searches.
This open-domain nature significantly impacts our experimental de-
sign, particularly in the selection and categorization of documents.
Following the methodology of Lee et al. [27], our primary source
for answering queries is the English Wikipedia dump as of 20 De-
cember 2018. Consistently with the Dense Passage Retrieval (DPR)
approach [20], each Wikipedia article in this dump was segmented
into non-overlapping passages of100 words. A significant challenge
in open-domain question answering is the potential temporal mis-
match between the Wikipedia dump and the question-answer pairs
in the dataset, which can lead to missing answers in the dataset,
as highlighted in the AmbigQA study [
34]. To mitigate this, we
integrated the gold documents from the original NQ dataset into
our Wikipedia document set. Given the open-domain nature of our
task, there may be additional documents relevant to the query, i.e.,
containing the answer, but we will not consider them as gold. The
final dataset comprises 21,035,236 documents, with 72,209 queries
in the train set and 2,889 in the test set.
4.2 Types of Documents
In our study, we categorize documents into four distinct types,
each represented by a unique symbol, based on their relevance and
relationship to the queries:
⋆Gold Document. The gold document, identified by ⋆, refers
to the original context in the NQ dataset, specifically the passage of
a Wikipedia page containing the answer and contextually relevant
to a given query.
ὑ7Relevant Documents. Denoted by ὑ7, relevant documents are
passages that, akin to the gold document, contain the correct answer
and are contextually useful for answering the query. They provide
additional sources of information that are correct and pertinent to
the query. Notably, the gold document is a relevant document.
/u♀kDistracting Documents. Symbolized by /u♀k, distracting docu-
ments are semantically similar to the query but do not contain the
721SIGIR ’24, July 14–18, 2024, Washington, DC, USA Florin Cuconasu et al.
correct answer. They serve a crucial role in evaluating the genera-
tor’s proficiency in discerning between relevant and non-relevant
information. In practice, these are the top-scoring retrieved docu-
ments that are not relevant.
Random Documents. Indicated by , random documents are
neither related to the query nor contain the answer. They are in-
strumental in assessing the model’s ability to handle completely
unrelated information. In practice, in our tests, we will randomly
sample these documents from the corpus.
In our analysis, the entire set of documents fetched by the retriever
is represented by the symbol . This possibly encompasses all doc-
ument types — gold, relevant, distracting, or random — and serves
to discuss the retrieval output in a generalized manner without
specifying individual document categories.
4.3 Document Retrieval
Our methodology utilizes a two-step approach in line with a typical
RAG setting, as explained in Section 3.2. As the first component,
our experiments use Contriever [15], a BERT-based dense retriever,
as the default retriever. It is trained without supervision using a
contrastive loss. To enhance the efficiency of similarity searches
within our corpus, comprising about 21 million documents, we also
employ the FAISS IndexFlatIP indexing system [11]. The embedding
of each document and query is obtained by averaging the hidden
state of the last layer of the model.
4.4 LLM Input
Upon receiving a query, the retriever selects the top-𝑘 documents
from the corpus according to a given similarity measure. These
documents, in conjunction with the task instruction and the query,
constitute the input for the LLM to generate a response. The NQ-
open dataset was structured to include only those queries whose
answers consist of no more than five tokens [ 27]. Consequently,
the LLM is tasked with extracting a query response, confined to a
maximum of five tokens, from the provided documents. The input is
encoded into a prompt, whose template is shown in Figure 1, begin-
ning with the task instruction, presented in italics for clarity. This
is followed by the context, which comprises the selected documents
followed by the query string. This prompt design aligns with the
methodological approach outlined in [30]. While the composition
of the context will vary according to the single experiment, the
instruction will always be placed at the beginning of the prompt
and the query always at the end.
4.5 LLMs Tested
We consider several LLMs in our experiments. Consistently across
all models, we adopt a greedy generation approach with a maxi-
mum response length of 15 tokens. Acknowledging the constraints
imposed by memory and computational resources, we have im-
plemented a model quantization strategy, reducing all models to a
4-bit representation. Besides the above prompt, the models are not
provided with additional exemplars for few-shot learning, which,
while of interest, is outside the scope of this paper. We conduct tests
on both thebase and the instruct versions of the LLMs. However, we
LLM Input - Only Gold ⋆
You are given a question and you MUST respond by EX-
TRACTING the answer (max 5 tokens) from one of the pro-
vided documents. If none of the documents contain the answer,
respond with NO-RES.
Documents:
Document [3](Title: Millennium Falcon) Han Solo won
the Millennium Falcon from Lando Calrissian in the card
game sabacc...
Question: who owned the millennium falcon be-
fore han solo
Answer: Han Solo
Figure 1: Example LLM input with an erroneous output, high-
lighted in red. The input consists of an italicized task instruc-
tion, followed by the context (documents), and the query. The
LLM’s response is marked under ‘Answer’. The gold color
highlights both the gold document and the correct answer,
“Lando Calrissian”, indicating the expected source and con-
tent of the accurate response.
only report on the latter, as while the behavior is consistent across
both, the instruct versions demonstrate superior performance.
•Llama2. The 7B parameters version of the Llama2 family [50]
shows state-of-the-art performance on most downstream
tasks compared to models of the same size. It was trained
with a 4096 tokens context window and uses multi-query
attention [45].
•Falcon. Falcon 7B, the smallest model of the Falcon series, [1]
was trained on the RefinedWeb dataset [35], a large, filtered,
and deduplicated corpus. Similarly to Llama2, it uses multi-
query attention, with a context length of 2048 tokens.
•Phi-2. This is the smallest model used in this work (2.7B
parameters). Despite its modest size, it achieves performance
comparable to the other models [16, 29], thanks to its pre-
training on “textbook-quality” data. It has a context window
of 2048 tokens.
•MPT. This 7B parameters model uses ALiBi attention [36, 47]
for a virtually unlimited context length. In our experiments,
to leverage the model’s full potential, we set the limit to 2048
tokens, i.e., the same used for the model’s pre-training.
4.6 Accuracy
The NQ-open dataset allows a range of potential answers for each
query. Frequently, these answers are different variants of the same
concept (e.g., “President D. Roosevelt” or “President Roosevelt”),
while in some cases, a single query may accept multiple distinct
correct answers. To evaluate the accuracy of responses generated
by LLMs, we use an assessment technique in line with [18, 30].
This methodology examines whether at least one of the predefined
correct answers is contained within the response produced by the
LLM. We measure the correctness of the LLM’s responses as either
722The Power of Noise: Redefining Retrieval for RAG Systems SIGIR ’24, July 14–18, 2024, Washington, DC, USA
accurate or inaccurate based on the presence of the answer in a
binary fashion. Nevertheless, this evaluation strategy is not without
challenges. A principal issue arises in determining response cor-
rectness, particularly in instances involving date representations
or varying phrasings conveying identical meanings. For example,
if the LLM generates “Roosevelt” in response to a query where the
established correct answer is “President Roosevelt”, the response
would be deemed incorrect under our current evaluation schema.
Recognizing this limitation, we acknowledge the necessity for a
more advanced analysis of answer variations, which we leave to
future research.
5 RESULTS
Studying the characteristics of optimal prompts for RAG systems
corresponds to answering our research question (RQ): "What char-
acteristics are desirable in a retriever to optimize prompt construction
for RAG systems in order to increase the LLM effectiveness?". More
specifically, we focus on three essential elements of the configura-
tion: type, number, and positioning of the documents, and for each,
we test various prompt combinations. To facilitate the understand-
ing of our experimental setup, we employ a streamlined schema for
representing the composition of prompts via the following symbols:
[I, ⋆, ὑ7, /u♀k, , Q]. The task instruction (I) and the query (Q) are
consistently positioned at the beginning and end, respectively. The
middle section varies and represents different contextual elements
- in this instance, these are gold, relevant, distracting, and random,
appearing in that specific sequence. Additionally, the number of
contextual documents is a variable in its own right and will be
reported in the results tables below.
5.1 Impact of Distracting Documents
LLM
Input - Distracting /u♀kand Gold ⋆
T
ask Instruction...
Documents:
Document [1](Title: Han Solo) Before the events of the
film, he and Chewbacca had lost the “Millennium Falcon”
to thieves, but they reclaim the ship after it...
Document [2](Title: Millennium Falcon) The “Falcon” has
been depicted many times in the franchise, and ownership
has changed several times...
Document [3](Title: Millennium Falcon) Han Solo won
the Millennium Falcon from Lando Calrissian in the card
game sabacc...
Question: who owned the millennium falcon be-
fore han solo
Answer: Han Solo
Figure 2: Example LLM input with an erroneous output, high-
lighted in red. The context of the prompt is composed of
distracting documents and the gold near the query. The task
instruction is as in Figure 1.
In our first set of experiments, we use a selection of 10K queries
from the training set of the NQ-open dataset and assume an oracle
setup in which the gold document for the query is known. To this
effect, we add to the gold document a set of distracting documents,
i.e., documents with high retrieval scores but not containing the
answer, in order to measure their impact on the system; schemat-
ically [I, /u♀k, ⋆, Q]. Figure 2 shows an example of this setup’s
visualization. Results of this experiment are shown in Table 1 (far,
mid, and near relate to the distance between the gold document
and the query; more details in the following sub-section). A crit-
ical observation emerging from this analysis is a clear pattern of
progressive accuracy degradation as the number of distracting doc-
uments included in the context increases. This was observed across
all LLMs, with accuracy deteriorating by more than 0.38 (−67%)
in some cases. Even more importantly, adding just one distracting
document causes a sharp reduction in accuracy, with peaks of 0.24
(−25%), as can be seen by comparing the row with 0 distracting
documents (only gold scenario, as seen in Figure 1) with that of 1
distracting document. This experiment highlights a critical issue
for RAG systems, particularly in real-world IR settings where re-
lated but non-answer-containing documents are commonplace. Our
empirical analysis suggests that introducing semantically aligned
yet non-relevant documents adds a layer of complexity, potentially
misguiding LLMs away from the correct response. A visual ex-
planation can be seen in Figure 3, which illustrates the attention
scores within the prompt’s context for a specific example in which
the LLM incorrectly answers. This figure highlights the model’s
disproportionate focus on a distracting document (leftmost) at the
expense of the gold document (rightmost), likely contributing to
the erroneous response. Note that for consistency of results across
LLMs, we need to account for their various input token capabilities:
Llama2 can process up to 4096 tokens, but other models are lim-
ited to 2048 tokens. This led to the exclusion of evaluations with a
higher number of distracting documents (namely greater than 10)
as reflected by the empty values in the tables.
In addition, we wanted to verify that our results were not overly
dependent on the type of dense retrieval system we used. We
wanted, in particular, to check whether another dense retriever
specifically trained on “hard negatives" would better distinguish
between directly relevant and distracting documents, potentially
leading to different results. To explore this hypothesis, we used
ADORE [59], a state-of-the-art retriever trained with “dynamic
hard negatives”, to select the distracting documents. In scenarios
with 1, 2, and 4 distracting documents in the [I, /u♀k, ⋆, Q] setting
with Llama2, we obtain an accuracy of 0.4068, 0.3815, and 0.3626,
respectively. This is significantly lower than the baseline accuracy
of 0.5642, where no distracting documents were included, and than
the results obtained with Contriever in the same settings. We con-
clude from this that distinguishing between relevant and distracting
information is a hard problem that cannot be mitigated simply by
changing the dense retrieval method at this stage.
5.2 Impact of Gold Positioning
We conduct here another experiment where we systematically shift
the position of the gold document within the context to study its
723SIGIR ’24, July 14–18, 2024, Washington, DC, USA Florin Cuconasu et al.
Table 1: Accuracy results of the LLMs when evaluated with prompts composed of the gold document ⋆and a varying number
of distracting /u♀kdocuments. The table illustrates how the inclusion of an increasing number of distracting documents affects
LLM’s performance. Scenarios where the prompt exceeded the model’s input limit, leading to potential data truncation, are not
included ( - ). All values not marked with an asterisk * denote statistically significant changes from the gold-only document
scenario [I, ⋆, Q] (first row), as determined by a Wilcoxon test (p-value < 0.01). Additionally, the closed-book accuracy scores
for the models are as follows: Llama2 (0.1123), MPT (0.1205), Phi-2 (0.0488), Falcon (0.1083).
Far - [I, ⋆, /u♀k, Q] Mid - [I, /u♀k,⋆, /u♀k, Q] Near - [I, /u♀k,⋆, Q]
# /u♀kLlama2 MPT Phi-2 Falcon Llama2 MPT Phi-2 Falcon Llama2 MPT Phi-2 Falcon
0 0.5642 0.2148 0.4438 0.4330 0.5642 0.2148 0.4438 0.4330 0.5642 0.2148 0.4438 0.4330
1 0.4586 0.1976 0.3585 0.3469 no-mid no-mid no-mid no-mid 0.4283 0.1791 0.4227 0.3602
2 0.3455 0.1913 0.3430 0.3246 0.3322 0.1802 0.3375 0.2823 0.3974 0.2002 0.3975 0.3111
4 0.2745 0.2209* 0.3019 0.2670 0.2857 0.1775 0.2885 0.2378 0.3795 0.2059* 0.3701 0.2736
6 0.2898 0.2171* 0.2943 0.2392 0.2698 0.1424 0.2625 0.2103 0.3880 0.1892 0.3623 0.2656
8 0.2643 0.2077* 0.2513 0.1878 0.2268 0.1002 0.2360 0.1745 0.3748 0.1944 0.3423 0.2424
10 0.2537 - - - 0.2180 - - - 0.3716 - - -
12 0.2688 - - - 0.2382 - - - 0.3991 - - -
14 0.2583 - - - 0.2280 - - - 0.4118 - - -
16 0.2413 - - - 0.2024 - - - 0.3889 - - -
18 0.2348 - - - 0.1795 - - - 0.3781 - - -
Table 2: Accuracy results of the LLMs when evaluated with prompts composed of the gold document ⋆and a varying number
of random documents. Surprisingly, increasing the number of random documents in the Near setting improves LLM’s
performance. Scenarios where the prompt exceeded the model’s input limit, leading to potential data truncation, are not
included ( - ). All values not marked with an asterisk * denote statistically significant changes from the gold-only document
scenario [I, ⋆, Q] (first row), as determined by a Wilcoxon test (p-value < 0.01). Additionally, the closed-book accuracy scores
for the models are as follows: Llama2 (0.1123), MPT (0.1205), Phi-2 (0.0488), Falcon (0.1083).
Far - [I, ⋆, , Q] Mid - [I, , ⋆, , Q] Near - [I, , ⋆, Q]
# Llama2 MPT Phi-2 Falcon Llama2 MPT Phi-2 Falcon Llama2 MPT Phi-2 Falcon
0 0.5642 0.2148 0.4438 0.4330 0.5642 0.2148 0.4438 0.4330 0.5642 0.2148 0.4438 0.4330
1 0.4733 0.2447 0.4329 0.4035 no-mid no-mid no-mid no-mid 0.4862 0.2125* 0.4587 0.4091
2 0.3776 0.2639 0.4249 0.3805 0.3928 0.2584 0.4293 0.3612 0.5032 0.2660 0.4614 0.3912
4 0.3109 0.2933 0.4091 0.3468 0.3998 0.2577 0.3985 0.3462 0.5221 0.2930 0.4311 0.3949
6 0.3547 0.3036 0.4130 0.3250 0.4138 0.2265 0.3891 0.3196 0.5681* 0.2890 0.4388 0.3908
8 0.3106 0.3039 0.3812 0.2543 0.3734 0.1566 0.3596 0.2767 0.5609* 0.2911 0.4258 0.3704
10 0.3390 - - - 0.3675 - - - 0.5579* - - -
12 0.3736 - - - 0.3641 - - - 0.5836 - - -
14 0.3527 - - - 0.3372 - - - 0.5859 - - -
16 0.3401 - - - 0.3159 - - - 0.5722 - - -
18 0.3466 - - - 0.2982 - - - 0.5588* - - -
impact on the model’s effectiveness. We define the positions of the
gold document as follows:
•Near: placed adjacent to the query in the prompt [I, /u♀k, ⋆,
Q] (as in Figure 2)
•Mid: inserted in the middle of the context [I, /u♀k,⋆, /u♀k, Q]
•Far: positioned as far as possible from the query in the con-
text [I, ⋆, /u♀k, Q]
Results in these settings partially corroborate evidence from [30].
The accuracy is higher when the gold document is near the query,
lower when the gold document is furthest from it, and lowest when
the gold document is placed in the middle of the context. For in-
stance, Llama2, with 18 distracting documents, reaches an accuracy
of 0.37, 0.23, and 0.17, respectively. These results are consistent
across all models tested in the setting with distracting documents.
5.3 Impact of Noise
We devise an additional experimental setting aimed at evaluating
the robustness of the RAG system against noise. To this effect, we
take the gold document and add to it a certain number of docu-
ments picked at random from the corpus; see an example in Figure
4. Against our expectations, the performance does not deteriorate
724The Power of Noise: Redefining Retrieval for RAG Systems SIGIR ’24, July 14–18, 2024, Washington, DC, USA
Table 3: Accuracy of Llama2-7b in configurations involving random Wikipedia documents and retrieved documents [I, , , Q].
Rows denote the number of random documents added, and columns show the quantity of retrieved documents . The left
section reports results using Contriever, and the right section using BM25. Scenarios where the prompt exceeded the model’s
input limit, leading to potential data truncation, are not included ( - ). Each value not marked with an asterisk * represents a
statistically significant change from the base case of retrieved documents only [I, , Q] (first row), as determined by a Wilcoxon
test (p-value < 0.01).
Contrie
ver BM25
#
# 1
2 3 4 5 8 10 1
2 3 4 5 8 10
0 0.1620
0.1866 0.1876 0.1866 0.1921 0.2198 0.2108 0.2008
0.2208 0.2084 0.2028 0.2243 0.2492 0.2447
1 0.1308
0.1616 0.1717 0.1893* 0.1987* 0.2153* 0.2146* 0.1568
0.1963 0.1921 0.2115 0.2295* 0.2475* 0.2506*
2 0.1315
0.1644 0.1859* 0.2008 0.2174 0.2156* 0.2368 0.1644
0.1973 0.2080* 0.2281 0.2558 0.2495* 0.2596
3 0.1301
0.1727 0.2008 0.2316 0.2201 0.2198 0.2409 0.1568
0.2063 0.2160 0.2520 0.2579 0.2644 0.2707
5 0.1464
0.2056 0.2233 0.2240 0.2150 0.2451 0.2482 0.1772
0.2402 0.2437 0.2520 0.2554 0.2804 0.2866
8 0.1734
0.2066 0.2336 0.2375 0.2454 0.2416 0.2364 0.1994
0.2451 0.2579 0.2769 0.2817 0.2859 0.2777
10 0.1796
0.2174 0.2450 0.2502 0.2499 0.2420 - 0.2108
0.2589 0.2734 0.2835 0.2935 0.2853 -
15 0.2018
0.2354 0.2551 0.2530 - - - 0.2243
0.2686 0.2790 0.2928 - - -
16 0.2032 0.2471
0.2558 - - - - 0.2323
0.2662 0.2838 - - - -
17 0.2039
0.2426 - - - - - 0.2326
0.2693 - - - - -
18 0.2073 -
- - - - - 0.2309
- - - - - -
Figure 3: This heatmap depicts the attention distribution
across the context documents from the example shown in
Figure 2, relative to the answer generated by Llama2-7b in
a prompt structured as [I,
/u♀k, ⋆, Q]. Cell (i, j) denotes the
mean attention that tokens in the generated answer allocate
to the tokens of the i-th document within the j-th attention
layer. This mean attention for each document is calculated
by averaging the attention scores across all its constituent
tokens.
in the presence of noise, as can be seen in Table 2. Instead, we ob-
serve an improvement in performance under the best-performing
setting (near [I, , ⋆, Q]), with an improvement of 0.08 (+36%) in
LLM
Input - Random and
Gold ⋆
T
ask instruction...
Documents:
Document [140](Title: Richard Yates (novelist)) For much
of his life, Yates’s work met almost universal critical ac-
claim, yet not one of his books sold over 12,000 copies in...
Document [242](Title: Android version history) Code
name Version number Initial release date API level Security
patches (No codename ) 1.0 September 23...
Document [3](Title: Millennium Falcon) Han Solo won
the Millennium Falcon from Lando Calrissian in the card
game sabacc...
Question: who owned the millennium falcon be-
fore han solo
Answer: Lando Calrissian
Figure 4: Example LLM input with a correct output, high-
lighted in green. The context of the prompt is composed of
random documents and the gold near the query. The task
instruction is as in Figure 1.
the case of MPT. Furthermore, we observe that different models
exhibit distinct behaviors. Both Llama2 and Phi-2 showed improve-
ments in this setting when the noise is introduced furthest from
the query. However, when the noise is positioned in the far [I, ⋆,
, Q] and mid [I, , ⋆, , Q] settings, these models exhibit a
decline in performance. Notably, this performance degradation is
much less accentuated when compared to the earlier setting with
distracting documents. This suggests that while Llama2 and Phi-2
can effectively handle noise far from the query, their ability to sift
725SIGIR ’24, July 14–18, 2024, Washington, DC, USA Florin Cuconasu et al.
through irrelevant information diminishes as the noise is placed
closer to it. The MPT model presented a unique response; it showed
an improvement in performance under all settings. Standing out
from the rest, the Falcon model did not exhibit an improvement in
performance as observed in other models with the introduction of
noise. Peculiarly enough, Falcon and Llama2 do not consistently ex-
hibit a “lost in the middle” phenomenon, having in some instances
better accuracy in the mid than far setting, for instance, in the case
with 8 noisy documents added.
5.4 RAG in Practice
To address our primary Research Question (RQ) about the char-
acteristics of an effective RAG retriever, and following the results
reported above, we now consider a more realistic scenario than an
oracle setup. Namely, given a query, we retrieve a set of documents
that can be either relevant or distracting. We then add random doc-
uments to this set of retrieved ones, schematically:[I,
, , Q]. For
this second set of experiments, we use the test set of the NQ-open
dataset. Results for this experiment, using Llama2, can be seen on
the left side of Table 3. These results show that, regardless of the
number of retrieved documents, adding random documents up until
the context length is filled is almost always beneficial, with gains
in terms of accuracy up to 0.07 ( +35%) in the case of 4 retrieved
documents.
5.4.1 Testing Sparse Retrievers. In an effort to validate our initial
observations, we replicate our experiment using a sparse retrieval
approach, specifically BM25. The corresponding results are outlined
in the right section of Table 3. Consistent with earlier findings, we
observe that including random documents leads to an improvement
in the effectiveness of the LLM. Notably, the use of BM25 yields
an average increase in accuracy of 3-4 percentage points. This im-
provement is attributed to the quality of documents retrieved by
BM25. We quantitatively evaluate the effectiveness of the retrieval
methods by computing the top-𝑘 accuracy for varying numbers of
retrieved documents. Note that this heuristic, while indicative, does
not capture the full spectrum of relevance. Our evaluation, based on
the presence of correct answers within documents, might overlook
the context-specific relevance due to potential lexical matches of
the answer string in documents. Despite this limitation, this method
aligns with established computational practices in literature [
15, 19].
In our analysis, BM25 demonstrated higher relative top-𝑘 accuracy
(0.2966, 0.4105, 0.5237, 0.6663 for 𝑘 = 1,2,4,10) compared to those
of Contriever (0.2502, 0.3569, 0.4784, 0.6085 for the same 𝑘), under-
scoring its effectiveness in retrieving more relevant documents in
our experimental setup.
5.4.2 Increasing The Randomness. While our previous experiments
show the benefits of adding random documents, one might argue
that these documents are not totally random as they originate from
the same corpus (Wikipedia) and that they might help the LLM
answer in a fashion that is consistent with the corpus. For this rea-
son, we carry out another experiment in which random documents
are drawn from a drastically different corpus in terms of tone and
style, namely Reddit Webis-TLDR-17 dataset [53]. The results are
outlined on the left of Table 4. The inclusion of documents from the
Reddit corpus not only maintains the observed increase in accuracy
but even enhances it, with an improvement of 0.023 (
+9% accuracy)
when compared to the previous best score. Pushing the randomness
even further, we carry out another test where we consider nonsen-
sical sentences made up of random words as random documents.
Remarkably, even in this scenario, we observe a performance im-
provement when compared to the base case of Wikipedia random
documents, as shown in the right side of Table 4.
5.4.3 Falcon. As shown in Table 2, Falcon does not reach the same
performance increase when random documents are added to the
gold document[I, , ⋆, Q]. Accordingly, we want to verify whether
it behaves differently when adding retrieved rather than gold docu-
ments. We find that the addition of random documents on top of
retrieved documents [I, , , Q] does improve the effectiveness of
Falcon; see detailed results in Table 5. These results are in contrast
with the ones obtained in the oracle setting, where Falcon was
robust to noise. This new finding further validates our experimen-
tal evidence, namely that, outside the oracle setting, all the tested
models show an improvement when a certain amount of noise is
added.
5.5 Retriever Trade-Off
The experimental evidence detailed above not only contradicts the
common perception that semantically close documents are help-
ful for LLMs but also highlights the need for a delicate balance
between relevant and random documents. When arranged as de-
scribed, random documents seem to exert a positive influence on
LLM accuracy. However, for the LLM to generate accurate answers,
some degree of relevant information must exist in the context. On
the other hand, an overabundance of retrieved documents increases
the likelihood of including distracting and non-relevant informa-
tion, leading to a sharp decline in performance. While establishing
a formal or comprehensive theory behind these findings remains an
open research challenge, we can still infer that there seems to be a
trade-off between the number of relevant and totally irrelevant doc-
uments. More specifically, we observed that the best effectiveness is
achieved when a minimal set of documents is initially retrieved and
then supplemented with random documents until the context limit
is reached. For the queries examined in this study, retrieving be-
tween 3 and 5 documents is the most effective choice. Adding more
increases the risk of including too many distracting, thus coun-
terproductive, documents. We argue here that there is a pressing
need for further research towards investigating how these initial
findings can be exploited. More importantly, it is evident that we
have yet to refine our understanding of the retriever’s role within
a RAG system.
On The Unreasonable Effectiveness Of Random Documents. We can-
not close this paper without attempting to explain the results shown
up to this point. We refer back to our RAG formulation, particularly
the conditioned function 𝑝𝜃 (𝑦|·,𝑑). In hindsight, we can now state
that by adding random documents to the context, we are better
conditioning this function, inducing enhanced accuracy. Previous
research [3, 14], particularly [58], hints that there might be cases
in which a pathologically low attention entropy causes the LLM to
generate degenerate outputs with a sharp decrease in performance.
These episodes are named entropy collapse. Following this line of
726The Power of Noise: Redefining Retrieval for RAG Systems SIGIR ’24, July 14–18, 2024, Washington, DC, USA
Table 4: Accuracy of Llama2-7b in configurations involving random documents and retrieved documents by Contriever [I, ,
, Q]. Rows denote the number of random documents added, and columns show the quantity of retrieved documents .
The left section reports results with random documents from Reddit and the right section with nonsensical sentences made
up of random words. Scenarios where the prompt exceeded the model’s input limit, leading to potential data truncation, are
not included ( - ). Each value not marked with an asterisk * represents a statistically significant change from the base case of
retrieved documents only [I, , Q] (first row), as determined by a Wilcoxon test (p-value < 0.01).
Random
from Reddit Random
Words
#
# 1
2 3 4 5 8 10 1
2 3 4 5 8 10
0 0.1620
0.1866 0.1876 0.1866 0.1921 0.2198 0.2108 0.1620
0.1866 0.1876 0.1866 0.1921 0.2198 0.2108
1 0.1693*
0.1931 0.1845* 0.1907 0.2008 0.2084 0.2084 0.1744
0.1924* 0.1969 0.2077 0.2091 0.2139* 0.2073*
2 0.1886
0.2018 0.2101 0.2143 0.2160 0.2222* 0.2219 0.1765
0.1855* 0.2094 0.2122 0.2181 0.2045 0.2084*
3 0.1897
0.2108 0.2212 0.2340 0.2371 0.2326 0.2319 0.1755
0.1990 0.2166 0.2201 0.2288 0.2032 0.2156*
5 0.1897
0.2215 0.2388 0.2468 0.2409 0.2769 0.2451 0.1862
0.2139 0.2319 0.2367 0.2232 0.2184* 0.2278
8 0.2011
0.2326 0.2354 0.2489 0.2440 0.2568 0.2364 0.1973
0.2274 0.2319 0.2316 0.2305 0.2357 0.2412
10 0.2053
0.2326 0.2451 0.2534 0.2551 0.2658 - 0.2053
0.2271 0.2340 0.2385 0.2406 0.2499 -
15 0.2240
0.2489 0.2689 0.2786 - - - 0.2215
0.2416 0.2589 0.2634 - - -
16 0.2240
0.2561 0.2676 - - - - 0.2219 0.2437
0.2568 - - - -
17 0.2243
0.2565 - - - - - 0.2201 0.2450 -
- - - -
18 0.2240
- - - - - - 0.2177
- - - - - -
Table 5: Accuracy of Falcon-7b on Reddit data in the random
+ retrieved setting [I, , , Q]. Rows denote the number of
random documents added, and columns show the quan-
tity of retrieved documents . Scenarios where the prompt
exceeded the model’s input limit, leading to potential data
truncation, are not included ( - ). Each value not marked with
an asterisk * represents a statistically significant change from
the base case (first row), as determined by a Wilcoxon test
(p-value < 0.05).
#
# 1
2 3 4 5 9
0 0.1568
0.1717 0.1855 0.1938 0.1942 0.1998
1 0.1551*
0.1793* 0.1897* 0.1924* 0.1976* -
2 0.1529*
0.1762* 0.1938* 0.2011* 0.1976* -
3 0.1599*
0.1727* 0.1911* 0.2021* 0.2118 -
4 0.1606*
0.1758* 0.1959 0.2073 0.2108 -
5 0.1627*
0.1762* 0.2000 0.2108 - -
6 0.1651*
0.1848 0.2004 - - -
7 0.1675 0.1848 -
- - -
8 0.1682 -
- - - -
research, we measure the entropy of the attention scores in the
case where only the gold document is supplied [I, ⋆, Q] against
the case in which random documents are added [I, , ⋆, Q]. We
find that when we introduce random documents, the entropy of
the systems has a 3X increase. Although these experiments show a
pattern, we cannot yet answer this question in a definitive manner.
While out of the scope of this work, which focuses on the retriever
component of RAG systems, we believe it is highly important to
investigate the reasons for which the LLM shows this behavior.
Future studies should aim to elucidate why this noisy state is more
advantageous and identify the characteristics that contribute to its
effectiveness.
6 CONCLUSIONS
In this paper, we conducted the first comprehensive study focus-
ing on the impact of retrieved documents on the RAG framework,
aiming to understand the traits required in a retriever to optimize
prompt construction for a RAG system. This study led to several im-
portant findings, including two unexpected ones. First, the position
of relevant information should be placed near the query; otherwise,
the model seriously struggles to attend to it. Second, in contrast to
common perception, top-scoring retrieved documents that do not
contain the answer, when added to a prompt, negatively impact
the LLM effectiveness. Finally, and even more surprisingly, random,
noisy documents are actually helpful in increasing the accuracy of
these systems when correctly positioned within a prompt. While
we have proposed heuristics to exploit these findings, further re-
search is needed both to uncover the inner mechanisms behind
this behavior and to develop a new generation of information re-
trieval techniques that are specifically designed to interact with the
generative component.
ACKNOWLEDGMENTS
This work is supported by the Spoke “FutureHPC & BigData” of
the ICSC – Centro Nazionale di Ricerca in High-Performance Com-
puting, Big Data and Quantum Computing, the Spoke “Human-
centered AI” of the M4C2 - Investimento 1.3, Partenariato Esteso
PE00000013 - "FAIR - Future Artificial Intelligence Research", SER-
ICS (PE00000014), IR0000013 - SoBigData.it, funded by European
Union – NextGenerationEU, the FoReLab project (Departments of
Excellence), and the NEREO PRIN project funded by the Italian Min-
istry of Education and Research Grant no. 2022AEFHAZ. This work
was carried out while Florin Cuconasu was enrolled in the Italian
National Doctorate on Artificial Intelligence run by the Sapienza
University of Rome.
727SIGIR ’24, July 14–18, 2024, Washington, DC, USA Florin Cuconasu et al.
REFERENCES
[1] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli,
Ruxandra Cojocaru, Mérouane Debbah, Étienne Goffinet, Daniel Hesslow, Julien
Launay, Quentin Malartic, Daniele Mazzotta, Badreddine Noune, Baptiste Pannier,
and Guilherme Penedo. 2023. The Falcon Series of Open Language Models.
arXiv:2311.16867 [cs.CL]
[2] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023.
Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection.
arXiv:2310.11511 [cs.CL]
[3] Giuseppe Attanasio, Debora Nozza, Dirk Hovy, and Elena Baralis. 2022. Entropy-
based attention regularization frees unintended bias mitigation from lists.
[4] Andrea Bacciu, Florin Cuconasu, Federico Siciliano, Fabrizio Silvestri, Nicola
Tonellotto, and Giovanni Trappolini. 2023. RRAML: Reinforced Retrieval Aug-
mented Machine Learning. In Proceedings of the Discussion Papers - 22nd Interna-
tional Conference of the Italian Association for Artificial Intelligence (AIxIA 2023 DP)
co-located with 22nd International Conference of the Italian Association for Artificial
Intelligence (AIxIA 2023), Rome, Italy, November 6-9, 2023 (CEUR Workshop Proceed-
ings, Vol. 3537), Roberto Basili, Domenico Lembo, Carla Limongelli, and Andrea
Orlandini (Eds.). CEUR-WS.org, 29–37. https://ceur-ws.org/Vol-3537/paper4.pdf
[5] Andrea Bacciu, Giovanni Trappolini, Andrea Santilli, Emanuele Rodolà, and
Fabrizio Silvestri. 2023. Fauno: The Italian Large Language Model that will leave
you senza parole!. InProceedings of the 13th Italian Information Retrieval Workshop
(IIR 2023), Pisa, Italy, June 8-9, 2023 (CEUR Workshop Proceedings, Vol. 3448),
Franco Maria Nardini, Nicola Tonellotto, Guglielmo Faggioli, and Antonio Ferrara
(Eds.). CEUR-WS.org, 9–17. https://ceur-ws.org/Vol-3448/paper-24.pdf
[6] Edward Beeching, Clémentine Fourrier, Nathan Habib, Sheon Han, Nathan Lam-
bert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas Wolf. 2023.
Open LLM Leaderboard. https://huggingface.co/spaces/HuggingFaceH4/open_
llm_leaderboard.
[7] Parishad BehnamGhader, Santiago Miret, and Siva Reddy. 2023. Can Retriever-
Augmented Language Models Reason? The Blame Game Between the Retriever
and the Language Model. In Findings of the Association for Computational
Linguistics: EMNLP 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.).
Association for Computational Linguistics, Singapore, 15492–15509. https:
//doi.org/10.18653/v1/2023.findings-emnlp.1036
[8] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Ruther-
ford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bog-
dan Damoc, Aidan Clark, et al. 2022. Improving language models by retrieving
from trillions of tokens. In International conference on machine learning . PMLR,
Baltimora, 2206–2240.
[9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot learners. Advances in neural
information processing systems 33 (2020), 1877–1901.
[10] Yiming Cui, Ziqing Yang, and Xin Yao. 2023. Efficient and Effective Text Encoding
for Chinese LLaMA and Alpaca. arXiv preprint arXiv:2304.08177 (2023). https:
//arxiv.org/abs/2304.08177
[11] Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy,
Pierre-Emmanuel Mazaré, Maria Lomeli, Lucas Hosseini, and Hervé Jégou. 2024.
The Faiss library. (2024). arXiv:2401.08281 [cs.LG]
[12] Garrachonr. 2023. LlamaDos. https://github.com/Garrachonr/LlamaDos.
[13]
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. 2020.
Retrieval augmented language model pre-training. In International conference on
machine learning. PMLR, Vienna, 3929–3938.
[14] David T Hoffmann, Simon Schrodi, Nadine Behrmann, Volker Fischer, and
Thomas Brox. 2023. Eureka-Moments in Transformers: Multi-Step Tasks Reveal
Softmax Induced Optimization Problems.
[15] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bo-
janowski, Armand Joulin, and Edouard Grave. 2021. Unsupervised dense infor-
mation retrieval with contrastive learning.
[16] Mojan Javaheripi, Sébastien Bubeck, Marah Abdin, Jyoti Aneja, Sebastien Bubeck,
Caio César Teodoro Mendes, Weizhu Chen, Allie Del Giorno, Ronen Eldan,
Sivakanth Gopi, et al
. 2023. Phi-2: The surprising power of small language
models.
[17] jphme. 2023. Llama-2-13b-chat-german. https://huggingface.co/jphme/Llama-2-
13b-chat-german.
[18] Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel.
2023. Large language models struggle to learn long-tail knowledge. InProceedings
of the 40th International Conference on Machine Learning (ICML’23). JMLR.org,
Honolulu, Hawaii, USA, Article 641, 12 pages.
[19] Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey
Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-
domain question answering.
[20] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey
Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for Open-
Domain Question Answering. In Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP), Bonnie Webber, Trevor Cohn,
Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics, Online,
6769–6781. https://doi.org/10.18653/v1/2020.emnlp-main.550
[21] Zixuan Ke, Weize Kong, Cheng Li, Mingyang Zhang, Qiaozhu Mei, and Michael
Bendersky. 2024. Bridging the Preference Gap between Retrievers and LLMs.
arXiv preprint arXiv:2401.06954 (2024).
[22] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. 2019. BERT:
Pre-training of deep bidirectional transformers for language understanding. In
Proceedings of naacL-HLT, Vol. 1. Association for Computational Linguistic, Min-
neapolis, 2.
[23] Urvashi Khandelwal, He He, Peng Qi, and Dan Jurafsky. 2018. Sharp nearby,
fuzzy far away: How neural language models use context.
[24] Omar Khattab and Matei Zaharia. 2020. Colbert: Efficient and effective passage
search via contextualized late interaction over bert. In Proceedings of the 43rd
International ACM SIGIR conference on research and development in Information
Retrieval. ACM, Xi’an, 39–48.
[25] Bevan Koopman and Guido Zuccon. 2023. Dr ChatGPT tell me what I want to
hear: How different prompts impact health answer correctness. InProceedings
of the 2023 Conference on Empirical Methods in Natural Language Processing,
Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational
Linguistics, Singapore, 15012–15022. https://doi.org/10.18653/v1/2023.emnlp-
main.928
[26] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur
Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee,
Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M.
Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural Questions: A
Benchmark for Question Answering Research. Transactions of the Association for
Computational Linguistics 7 (2019), 452–466. https://doi.org/10.1162/tacl_a_00276
[27] Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent Retrieval for
Weakly Supervised Open Domain Question Answering. InProceedings of the 57th
Conference of the Association for Computational Linguistics, ACL 2019, Florence,
Italy, July 28- August 2, 2019, Volume 1: Long Papers, Anna Korhonen, David R.
Traum, and Lluís Màrquez (Eds.). Association for Computational Linguistics,
Florence, 6086–6096. https://doi.org/10.18653/V1/P19-1612
[28] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,
Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel,
et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks.
Advances in Neural Information Processing Systems 33 (2020), 9459–9474.
[29] Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar,
and Yin Tat Lee. 2023. Textbooks are all you need ii: phi-1.5 technical report.
[30] Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua,
Fabio Petroni, and Percy Liang. 2023. Lost in the middle: How language models
use long contexts.
[31] Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022.
Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot
Prompt Order Sensitivity. In Proceedings of the 60th Annual Meeting of the Associ-
ation for Computational Linguistics (Volume 1: Long Papers), Smaranda Muresan,
Preslav Nakov, and Aline Villavicencio (Eds.). Association for Computational
Linguistics, Dublin, Ireland, 8086–8098. https://doi.org/10.18653/v1/2022.acl-
long.556
[32] C Manning, P Raghavan, and H Schutze. 2008. Term weighting, and the vector
space model. Cambridge University Press Cambridge, Cambridge. 109–133 pages.
[33] Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis, Ram
Pasunuru, Roberta Raileanu, Baptiste Rozière, Timo Schick, Jane Dwivedi-Yu,
Asli Celikyilmaz, et al. 2023. Augmented language models: a survey.
[34] Sewon Min, Julian Michael, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2020.
AmbigQA: Answering Ambiguous Open-domain Questions. In Proceedings of the
2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),
Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for
Computational Linguistics, Online, 5783–5797. https://doi.org/10.18653/v1/2020.
emnlp-main.466
[35] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru,
Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei,
and Julien Launay. 2023. The RefinedWeb Dataset for Falcon LLM: Outperforming
Curated Corpora with Web Data, and Web Data Only. arXiv:2306.01116 [cs.CL]
[36] Ofir Press, Noah Smith, and Mike Lewis. 2022. Train Short, Test Long: Attention
with Linear Biases Enables Input Length Extrapolation. https://openreview.net/
forum?id=R8sQPpGCv0
[37] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al . 2018.
Improving language understanding by generative pre-training.
[38] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever,
et al. 2019. Language models are unsupervised multitask learners. OpenAI blog
1, 8 (2019), 9.
[39] Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin
Leyton-Brown, and Yoav Shoham. 2023. In-context retrieval-augmented language
models.
[40] Stephen Robertson, Hugo Zaragoza, et al . 2009. The probabilistic relevance
framework: BM25 and beyond. Foundations and Trends® in Information Retrieval
3, 4 (2009), 333–389.
728The Power of Noise: Redefining Retrieval for RAG Systems SIGIR ’24, July 14–18, 2024, Washington, DC, USA
[41] Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan
Liu, Jing Xu, Myle Ott, Eric Michael Smith, Y-Lan Boureau, and Jason Weston.
2021. Recipes for Building an Open-Domain Chatbot. InProceedings of the
16th Conference of the European Chapter of the Association for Computational
Linguistics: Main Volume, Paola Merlo, Jorg Tiedemann, and Reut Tsarfaty (Eds.).
Association for Computational Linguistics, Online, 300–325. https://doi.org/10.
18653/v1/2021.eacl-main.24
[42] Gerard Salton and Michael J. McGill. 1983. Introduction to modern information
retrieval. McGraw-Hill (1983).
[43] Andrea Santilli and Emanuele Rodolà. 2023. Camoscio: an Italian Instruction-
tuned LLaMA. arXiv:2307.16456 [cs.CL]
[44] Artsiom Sauchuk, James Thorne, Alon Halevy, Nicola Tonellotto, and Fabrizio
Silvestri. 2022. On the Role of Relevance in Natural Language Processing Tasks.
In Proceedings of the 45th International ACM SIGIR Conference on Research and
Development in Information Retrieval. ACM, Madrid, 1785–1789.
[45] Noam Shazeer. 2019. Fast Transformer Decoding: One Write-Head is All You
Need. arXiv:1911.02150 [cs.NE]
[46] Simeng Sun, Kalpesh Krishna, Andrew Mattarella-Micke, and Mohit Iyyer. 2021.
Do long-range language models actually use long-range context?
[47] MosaicML NLP Team et al. 2023. Introducing mpt-7b: A new standard for open-
source, ly usable llms.
[48] Gabriele Tolomei, Cesare Campagnano, Fabrizio Silvestri, and Giovanni Trap-
polini. 2023. Prompt-to-OS (P2OS): Revolutionizing Operating Systems and
Human-Computer Interaction with Integrated AI Generative Models. In5th
IEEE International Conference on Cognitive Machine Intelligence, CogMI 2023, At-
lanta, GA, USA, November 1-4, 2023. IEEE, 128–134. https://doi.org/10.1109/
COGMI58952.2023.00027
[49] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
Azhar, et al. 2023. Llama: Open and efficient foundation language models.
[50] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yas-
mine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-
ale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models.
[51] Giovanni Trappolini, Andrea Santilli, Emanuele Rodolà, Alon Y. Halevy, and
Fabrizio Silvestri. 2023. Multimodal Neural Databases. In Proceedings of the 46th
International ACM SIGIR Conference on Research and Development in Information
Retrieval, SIGIR 2023, Taipei, Taiwan, July 23-27, 2023, Hsin-Hsi Chen, Wei-Jou (Ed-
ward) Duh, Hen-Hsen Huang, Makoto P. Kato, Josiane Mothe, and Barbara Poblete
(Eds.). ACM, 2619–2628. https://doi.org/10.1145/3539618.3591930
[52] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All
you Need. In Advances in Neural Information Processing Systems, I. Guyon, U. Von
Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.),
Vol. 30. Curran Associates, Inc., Long Beach. https://proceedings.neurips.cc/
paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf
[53] Michael Völske, Martin Potthast, Shahbaz Syed, and Benno Stein. 2017. TL;DR:
Mining Reddit to Learn Automatic Summarization. InProceedings of the Workshop
on New Frontiers in Summarization, Lu Wang, Jackie Chi Kit Cheung, Giuseppe
Carenini, and Fei Liu (Eds.). Association for Computational Linguistics, Copen-
hagen, Denmark, 59–63. https://doi.org/10.18653/v1/W17-4508
[54] Shuai Wang, Liang Ding, Li Shen, Yong Luo, Bo Du, and Dacheng Tao. 2024.
OOP: Object-Oriented Programming Evaluation Benchmark for Large Language
Models. arXiv preprint arXiv:2401.06628 (2024).
[55] Jian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, and Yu Su. 2023. Adaptive
chameleon or stubborn sloth: Revealing the behavior of large language mod-
els in knowledge conflicts. In The Twelfth International Conference on Learning
Representations.
[56] Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng
Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, Yitao Liu, Yiheng
Xu, Shuyan Zhou, Silvio Savarese, Caiming Xiong, Victor Zhong, and Tao Yu.
2024. OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in
Real Computer Environments. arXiv:2404.07972 [cs.AI]
[57] Andrew Yates, Rodrigo Nogueira, and Jimmy Lin. 2021. Pretrained Transformers
for Text Ranking: BERT and Beyond. In Proceedings of the 2021 Conference of the
North American Chapter of the Association for Computational Linguistics: Human
Language Technologies: Tutorials, Greg Kondrak, Kalina Bontcheva, and Dan
Gillick (Eds.). Association for Computational Linguistics, Online, 1–4. https:
//doi.org/10.18653/v1/2021.naacl-tutorials.1
[58] Shuangfei Zhai, Tatiana Likhomanenko, Etai Littwin, Dan Busbridge, Jason Rama-
puram, Yizhe Zhang, Jiatao Gu, and Joshua M Susskind. 2023. Stabilizing trans-
former training by preventing attention entropy collapse. In International Con-
ference on Machine Learning. PMLR, PMLR, Hawaii, 40770–40803.
[59] Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Jiafeng Guo, Min Zhang, and Shaoping
Ma. 2021. Optimizing Dense Retrieval Model Training with Hard Negatives.
In Proceedings of the 44th International ACM SIGIR Conference on Research and
Development in Information Retrieval (SIGIR ’21). New York, NY, USA, 1503–1512.
[60] Guido Zuccon, Bevan Koopman, and Razia Shaik. 2023. ChatGPT Hallucinates
when Attributing Answers. In Proceedings of the Annual International ACM SIGIR
Conference on Research and Development in Information Retrieval in the Asia Pacific
Region (Beijing, China) (SIGIR-AP ’23). Association for Computing Machinery,
New York, NY, USA, 46–51. https://doi.org/10.1145/3624918.3625329
729

LeanDojo: Theorem Proving with
Retrieval-Augmented Language Models
Kaiyu Yang1, Aidan M. Swope2, Alex Gu3, Rahul Chalamala1, Peiyang Song4,
Shixing Yu5, Saad Godil∗, Ryan Prenger2, Anima Anandkumar1,2
1Caltech, 2NVIDIA, 3MIT, 4UC Santa Barbara, 5UT Austin
https://leandojo.org
Abstract
Large language models (LLMs) have shown promise in proving formal theorems
using proof assistants such as Lean. However, existing methods are difficult to
reproduce or build on, due to private code, data, and large compute requirements.
This has created substantial barriers to research on machine learning methods for
theorem proving. This paper removes these barriers by introducing LeanDojo:
an open-source Lean playground consisting of toolkits, data, models, and bench-
marks. LeanDojo extracts data from Lean and enables interaction with the proof
environment programmatically. It contains fine-grained annotations of premises in
proofs, providing valuable data for premise selection—a key bottleneck in theorem
proving. Using this data, we develop ReProver (Retrieval-Augmented Prover): an
LLM-based prover augmented with retrieval for selecting premises from a vast
math library. It is inexpensive and needs only one GPU week of training. Our
retriever leverages LeanDojo’s program analysis capability to identify accessible
premises and hard negative examples, which makes retrieval much more effec-
tive. Furthermore, we construct a new benchmark consisting of 98,734 theorems
and proofs extracted from Lean’s math library. It features challenging data split
requiring the prover to generalize to theorems relying on novel premises that are
never used in training. We use this benchmark for training and evaluation, and
experimental results demonstrate the effectiveness of ReProver over non-retrieval
baselines and GPT-4. We thus provide the first set of open-source LLM-based
theorem provers without any proprietary datasets and release it under a permissive
MIT license to facilitate further research.
1 Introduction
Reasoning is a cornerstone of human intelligence and a fundamental goal of AI [3]. One prominent
task is automated theorem proving (ATP): automatically generating proofs for theorems expressed
in formal logic. ATP is useful for formal mathematics, producing mathematical proofs that can be
checked rigorously [4]. Furthermore, it underpins formal verification, which is essential for proving
the correctness and safety of high-stakes applications [5, 6].
ATP is challenging since the search space is prohibitively large. In many applications, it is impractical
to generate proofs fully automatically. Therefore, interactive theorem proving (ITP) has emerged as
an alternative paradigm. In ITP, proofs are constructed by human experts interacting with software
tools called proof assistants, such as Coq [ 7], Isabelle [ 8], and Lean [ 1]. Machine learning can
automate such interactive theorem proving, opening up a new avenue for theorem proving [9]. The
model can learn to interact with proof assistants, given data containing human-written proofs.
∗Research conducted while Saad Godil was at NVIDIA.
37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks.prelude
import init.data.nat.lemmas init.meta.well_founded_tactics
namespace nat
/-
t h e o r e m m o d _ s e l f ( n : n a t ) : n % n = 0 : =
b e g i n
r w [ m o d _ e q _ s u b _ m o d ( l e _ r e f l _ ) , n a t . s u b _ s e l f , z e r o _ m o d ]
e n d
-/
def gcd : nat → nat → nat
| 0 y := y
| ( x + 1 ) y := have y % ( x + 1 ) < x + 1 , from mod_lt _ $ succ_pos _ ,
gcd ( y % ( x + 1 ) ) ( x + 1 )
theorem gcd_zero_left ( x : nat ) : gcd 0 x = x := begin simp [ gcd ] end
theorem gcd_self ( n : nat ) : gcd n n = n :=
begin
cases n ,
{ unfold gcd } ,
unfold gcd ,
rewrite mod_self ,
apply gcd_zero_left
end
end nat
1
prelude
import init.data.nat.lemmas init.meta.well_founded_tactics
namespace nat
theorem mod_self ( n : nat ) : n % n = 0 :=
begin
rw [ mod_eq_sub_mod ( le_refl _ ) , nat.sub_self , zero_mod ]
end
def gcd : nat → nat → nat
| 0 y := y
| ( x + 1 ) y := have y % ( x + 1 ) < x + 1 , from mod_lt _ $ succ_pos _ ,
gcd ( y % ( x + 1 ) ) ( x + 1 )
theorem gcd_zero_left ( x : nat ) : gcd 0 x = x := begin simp [ gcd ] end
theorem gcd_self ( n : nat ) : gcd n n = n :=
begin
cases n ,
{ unfold gcd } ,
unfold gcd ,
rw mod_self ,
apply gcd_zero_left
end
end nat
1
Lean 
LeanDojoBenchmark•98,734 theorems and proofs•217,776 tactics•129,243 premises
k : ℕ⊢gcd((k + 1) % (k + 1)) (k + 1) = k + 1All accessible premises in the math library
Maximum cosine similarity
Encoder theorem mod_lt ( x : nat ) { y : nat } ( h : 0 < y ) : x % y < y
theorem mod_self ( n : nat ) : n % n = 0
theorem mod_eq_of_lt { ab : nat } ( h : a < b ) : a % b = a
theorem zero_mod ( b : nat ) : 0 % b = 0
1
rewrite mod_self
... Encoder...Encoder
EncoderState ConcatEncoder-decoderTactic
Retrieved premises
Machine learning model
Data extractionTraining
Prove theorems by Interactionn : ℕ⊢gcdn n = n
⊢gcd0 0 = 0k : ℕ⊢gcd(k + 1) (k + 1) = k + 1
k : ℕ⊢gcd((k + 1) % (k + 1)) (k + 1) = k + 1
k : ℕ⊢gcd0 (k + 1) = k + 1
Tacticcases n
unfold gcdunfold gcd
rewrite mod_self
apply gcd_zero_left
Local context⊢GoalProof tree
prelude
import init.data.nat.lemmas init.meta.well_founded_tactics
namespace nat
/-
t h e o r e m m o d _ s e l f ( n : n a t ) : n % n = 0 : =
b e g i n
r w [ m o d _ e q _ s u b _ m o d ( l e _ r e f l _ ) , n a t . s u b _ s e l f , z e r o _ m o d ]
e n d
-/
def gcd : nat → nat → nat -- gcd z y
| 0 y := y -- Case 1: z == 0
| ( x + 1 ) y := gcd ( y % ( x + 1 ) ) ( x + 1 ) -- Case 2: z > 0
theorem gcd_zero_left ( x : nat ) : gcd 0 x = x := begin simp [ gcd ] end
theorem gcd_self ( n : nat ) : gcd n n = n :=
begin
cases n ,
{ unfold gcd } ,
unfold gcd ,
rewrite mod_self ,
apply gcd_zero_left
end
end nat
1
... 33K on average…
Figure 1: Top right: LeanDojo extracts proofs in Lean [ 1] into datasets for training machine
learning models. It also enables the trained model to prove theorems by interacting with Lean’s proof
environment. Top left: The proof tree of a Lean theorem ∀n ∈ N, gcd n n = n, where gcdis the
greatest common divisor (details in Sec. 3). When proving the theorem, we start from the original
theorem as the initial state (the root) and repeatedly apply tactics (the edges) to decompose states
into simpler sub-states, until all states are solved (the leaf nodes). Tactics may rely on premises such
as mod_self and gcd_zero_left defined in a large math library. E.g., mod_self is an existing
theorem ∀n ∈ N, n % n = 0used in the proof to simplify the goal. Bottom: Our ReProver model
(Sec. 5). Given a state, it retrieves premises from the math library, which are concatenated with the
state and fed into an encoder-decoder Transformer [2] to generate the next tactic.
Formal theorem proving serves as an important challenge for machine learning. From a computer
science perspective, formal proofs can be treated as programs [10]. But unlike conventional programs
in C++ or Python, the correctness of proofs can be verified using proof assistants. Therefore, theorem
proving may be considered a special form of code generation, with rigorous evaluation and no room
for the model to hallucinate. This can be consequential to current large language models (LLMs), as
they have demonstrated exceptional capability in code generation [11] but have flaws in factuality
and hallucination [12]. In addition, augmenting LLMs with external tools, such as proof assistants,
has shown promise in improving their various capabilities, including multi-step reasoning [13].
Current research on LLMs for theorem proving is facing many barriers. To our knowledge, none
of the existing LLM-based provers are open-source [14–21]. They all use private pretraining data,
and the compute requirements can reach thousands of GPU days [ 17]. Furthermore, some rely
on tailored infrastructure for distributed training and interaction with the proof assistant—both are
not possible to fully reproduce without open-source code [ 17, 19]. We change the status quo by
introducing LeanDojo: open-source toolkits, models, and benchmarks that give researchers access to
state-of-the-art LLM-based provers with modest computational costs.
Tools for Data Extraction and Interaction. We focus on Lean, a proof assistant popular among
mathematicians.2 Our framework LeanDojo provides two essential functions for learning-based
theorem proving (Fig. 1): extracting data and enabling models to interact with Lean programmatically.
For data extraction, LeanDojo extracts training data not directly visible in the raw Lean code (Fig. 2),
e.g., proof trees consisting of intermediate states between proof steps (Fig. 1 Top left). In addition,
LeanDojo is the first tool to locate premises in Lean proofs, enabling training machine learning
models for premise selection. For interaction, LeanDojo turns Lean into a gym-like interactive
environment [22]. Using LeanDojo, the model can observe proof states, change the state by executing
2“Lean” in our paper refers to Lean 3 by default. Lean 4 is not backward-compatible but is also supported by
LeanDojo. Our Lean 4 results are in Appendix D.
2proof steps (referred to as “tactics” in proof assistants), and receive feedback from Lean. LeanDojo
is the first tool capable of interacting with Lean reliably, reducing proof-checking errors in existing
tools [19] (correct proofs misjudged as incorrect) from 21.1% to 1.4%.
Retrieval-Augmented LLMs for Theorem Proving. LeanDojo addresses a key bottleneck in
theorem proving: premise selection [23, 24]. Existing LLM-based provers generate the next proof
step (tactic), taking only the current state as input. However, proving theorems depends critically on
the premises, such as lemmas and definitions, from a math library.
For example, Fig. 1 (Top left) illustrates the proof of “∀n ∈ N, gcd n n = n”, where gcd stands for
greatest common divisor. The proof starts from the original theorem as the initial state and repeatedly
applies tactics to decompose states into simpler sub-states, until all states are solved. Tactics may rely
on premises such as mod_selfand gcd_zero_leftdefined in a large math library. E.g., mod_self
is an existing theorem “∀n ∈ N, n % n = 0” useful for simplifying the goal.
Incorporating all possible premises is too large to fit into LLMs’ input, given the limited context
window. Existing methods must learn to memorize the association between the proof state and the
name mod_self. It works if the premise has been used in the training data to solve similar goals, but
does not generalize to truly novel scenarios, e.g., theorems requiring lemmas unseen in training.
One potential solution is to complement memorization with explicit premise selection. LeanDojo
extracts premise data from Lean, including where they are defined and used. It enables us to tackle
premise selection by augmenting LLMs with retrieval. We introduceReProver (Retrieval-Augmented
Prover) (Fig. 1 Bottom): Given the current state, it generates a tactic conditioning on a small number
of premises retrieved from Lean’s math library,mathlib[25].
We need to limit retrieval to a small number of premises for it to be effective, and ideally, they should
contain the ground truth premise. Our retriever builds upon Dense Passage Retriever (DPR) [ 26]
but incorporates two algorithmic innovations: First, not all premises are accessible when proving
a theorem (Sec. 3). LeanDojo can perform program analysis on Lean code to determine accessible
premises. On our data, that reduces the average number of premises from 128K to 33K, significantly
simplifying the retriever’s task. Second, DPR needs negative examples in training and benefits from
hard negatives, i.e., irrelevant premises that are hard to distinguish from ground truth ones. We
propose in-file negatives: a simple mechanism to find hard negatives in premise selection, which
samples negative premises defined in the same Lean source file as the ground truth premise.
LeanDojo Benchmark. Using LeanDojo, we construct a benchmark containing 98,734 theorem-
s/proofs extracted from mathlib. Our benchmark is one of the largest math-focused theorem-proving
datasets. We find that the common practice of splitting theorems randomly into training/testing has
led to an overestimated performance in the previous papers. LLMs can prove seemingly difficult
theorems simply by memorizing the proofs of similar theorems during training. In LeanDojo Bench-
mark, we mitigate this issue by designing challenging data split requiring the model to generalize to
theorems relying on novel premises that are never used in training.
We use LeanDojo Benchmark to train and evaluate ReProver. Training takes only five days on a single
GPU. In evaluation, ReProver can prove 51.2% theorems, outperforming a baseline that generates
tactics directly without retrieval (47.6%) and another baseline using GPT-4 [27] to generate tactics
in a zero-shot manner (29.0%). We also test ReProver on two existing datasets, MiniF2F [28] and
ProofNet [29]. It can prove 26.5% theorems in MiniF2F and 13.8% in ProofNet, which is competitive
with state-of-the-art methods without reinforcement learning [ 19], even though trained using far
fewer resources. Moreover, it can prove 65 theorems that currently do not have proofs in Lean. Thus,
our tool can also serve as an effective tool for augmenting existing math libraries in Lean.
Contributions. In summary, we make four main contributions: First, we introduce tools for
extracting data from and interacting with Lean. Second, we develop ReProver, the first retrieval-
augmented language model for theorem proving. Third, we construct a challenging benchmark for
learning-based theorem proving and use it to validate the effectiveness of ReProver. Finally, we
facilitate open research on LLMs for theorem proving by releasing our data, model, and code. Our
method does not rely on private datasets and can be trained on a single GPU within a week. We
believe this will significantly lower the barriers to academic research in this area and establish the first
accessible baselines for future work to build upon. Further, our method can be used to automatically
generate new Lean proofs without requiring human effort.
32 Related Work
Theorem Proving. Classical provers express theorems in first-order logic and search for proofs
automatically in a large space [30, 31]. Even with data-driven search heuristics [32, 33], they fail to
scale to large formalization projects. Therefore, recent work on learning-based theorem proving has
focused on an alternative paradigm: automating the interaction with proof assistants.
The architecture of learning-based provers progressed from classical machine learning algorithms such
as KNN [34], to graph neural networks explicitly encoding the syntax of formal expressions [9, 35],
and now Transformer-based LLMs treating expressions as plain strings [ 14]. Besides the model
architecture, researchers have explored several complementary dimensions: proof search algorithms
for assembling model-generated steps into complete proofs [ 17, 21]; overcoming data scarcity
through reinforcement learning (RL) [17, 19, 36, 37] or synthetic/auxiliary data [16, 38–40]; as well
as outsourcing some proof goals to classical provers [18, 41–43]. Our base model without retrieval is
a combination of straightforward design choices. It generates tactics by finetuning an encoder-decoder
Transformer, ByT5 [44], via supervised learning without RL or auxiliary data. Then it searches for
proofs using best-first search. Our model’s algorithmic novelty lies in the retrieval.
Premise Selection. Selecting useful premises is recognized as a key challenge in theorem prov-
ing [23, 24, 45, 46]. Machine learning methods for premise selection have also progressed from
classical models [41, 47, 48], recurrent neural networks [24], graph neural networks [38], to Trans-
formers [49, 50]. However, existing methods either tackle premise selection in isolation without
theorem proving [24, 38, 48] or feed the premises to a symbolic prover [41, 47, 49]. To our knowl-
edge, we are the first to augment a learning-based formal theorem prover with retrieved premises so
that the prover can learn how to use them effectively. For example, it can decide whether to use an
explicitly retrieved premise or an implicitly memorized one.
Data and Tools for Theorem Proving. Tools for data extraction and interacting with proof
assistants have been crucial drivers of learning-based theorem proving. Existing tools and datasets
can be divided by proof assistants: Coq has GamePad [51], CoqGym [9], and PRISM [52]; Isabelle
has IsarStep [ 53] and PISA [ 15]; HOL Light has HOList [ 54] and HoLStep [ 55], and Lean has
LeanStep [16] and lean-gym[19]. MiniF2F [28] is the only cross-system dataset, with 488 theorems
for evaluation. However, it does not have training theorems and is restricted to the domain of math
olympiads.
Among available tools extracting data from proof assistants, LeanDojo is the only one that can extract
premises for retrieval-augmented theorem proving. A few existing datasets also have premises [49,
54], but their data extraction tools are not public, making it difficult to construct new datasets. In
addition, LeanDojo is the only tool that can interact with Lean robustly (Sec. 4) and can extract data
from Lean 4. See Appendix A.3 for a detailed comparison between LeanDojo and alternatives.
Mathematical Reasoning in Natural Language. We focus on proving theorems expressed in
formal logic, whereas researchers have also produced a plethora of work on mathematical reasoning
in natural language [56–63]. A particularly relevant task is autoformalization, translating natural
language texts into formal theorems and proofs [29, 64–72].
Retrieval-Augmented Language Models. Our ReProver is the first retrieval-augmented language
model for formal theorem proving, though similar architectures have been studied extensively in
NLP [73–81]. In addition, there have been many retrieval-augmented methods for code generation [82–
88]. Most of them retrieve from a corpus not directly related to the current file, e.g., GitHub or Stack
Overflow. In contrast, our retrieval corpus consists of premises accessible to the current file, which is
determined by program analysis using LeanDojo. This is similar to what CoCoMIC [ 88] does for
Python. However, their retrieval is based on heuristics, whereas ours is learned.
3 Background: Theorem Proving in Lean
At a high level, Lean is a programming language that allows you to write not only conventional
programs but also theorems and proofs. To that end, it provides two pieces of machinery: First,
it provides a unified language for defining programs, mathematical objects, theorems, and proofs,
based on functional programming with dependent types [89]. Second, it provides a tactic system for
constructing machine-checkable proofs semi-automatically.
4data/nat/gcd.lean
data/nat/lemmas.lean
prelude
import init.data.nat.lemmas init.meta.well_founded_tactics
namespace nat
theorem mod_self ( n : nat ) : n % n = 0 :=
begin
rw [ mod_eq_sub_mod ( le_refl _ ) , nat.sub_self , zero_mod ]
end
def gcd : nat → nat → nat
| 0 y := y
| ( x + 1 ) y := have y % ( x + 1 ) < x + 1 , from mod_lt _ $ succ_pos _ ,
gcd ( y % ( x + 1 ) ) ( x + 1 )
theorem gcd_zero_left ( x : nat ) : gcd 0 x = x := begin simp [ gcd ] end
theorem gcd_self ( n : nat ) : gcd n n = n :=
begin
cases n ,
{ unfold gcd } ,
unfold gcd ,
rw mod_self ,
apply gcd_zero_left
end
end nat
1
Math library
prelude
import init.data.nat.lemmas init.meta.well_founded_tactics
namespace nat
/-
t h e o r e m m o d _ s e l f ( n : n a t ) : n % n = 0 : =
b e g i n
r w [ m o d _ e q _ s u b _ m o d ( l e _ r e f l _ ) , n a t . s u b _ s e l f , z e r o _ m o d ]
e n d
-/
def gcd : nat → nat → nat
| 0 y := y
| ( x + 1 ) y := have y % ( x + 1 ) < x + 1 , from mod_lt _ $ succ_pos _ ,
gcd ( y % ( x + 1 ) ) ( x + 1 )
theorem gcd_zero_left ( x : nat ) : gcd 0 x = x := begin simp [ gcd ] end
theorem gcd_self ( n : nat ) : gcd n n = n :=
begin
cases n ,
{ unfold gcd } ,
unfold gcd ,
rewrite mod_self ,
apply gcd_zero_left
end
end nat
1
prelude
import init.data.nat.lemmas init.meta.well_founded_tactics
namespace nat
/-
t h e o r e m m o d _ s e l f ( n : n a t ) : n % n = 0 : =
b e g i n
r w [ m o d _ e q _ s u b _ m o d ( l e _ r e f l _ ) , n a t . s u b _ s e l f , z e r o _ m o d ]
e n d
-/
def gcd : nat → nat → nat -- gcd z y
| 0 y := y -- Case 1: z == 0
| ( x + 1 ) y := gcd ( y % ( x + 1 ) ) ( x + 1 ) -- Case 2: z > 0
theorem gcd_zero_left ( x : nat ) : gcd 0 x = x := begin simp [ gcd ] end
theorem gcd_self ( n : nat ) : gcd n n = n :=
begin
cases n ,
{ unfold gcd } ,
unfold gcd ,
rewrite mod_self ,
apply gcd_zero_left
end
end nat
1
Import
Figure 2: Definition of greatest common divisor (gcd) in Lean and two related theorems. The proof
of gcd_self(between “begin” and “end”) relies on a premise mod_selfimported from another
file in the math library. Lean can run this proof to produce the proof tree in Fig.1 (Top left).
We use a simple example in Fig. 2 to illustrate how theorems are formalized and proved in Lean.3
Here we want to formalize the greatest common divisor (gcd) of two natural numbers. First, we define
gcdas a recursive function, taking two natural numbers as parameters and returning their gcdvia the
Euclidean algorithm. Then, we state a lemma named gcd_zero_leftthat ∀x ∈ N, gcd 0 x = x,
which can be proved simply by the definition of gcd. Finally, we state our main theorem gcd_self
that ∀n ∈ N, gcd n n = n, followed by its proof consisting of five tactics. In theorem proving, we
are only concerned with generating the proof, i.e., the part between “begin” and “end”; everything
before “begin” is known, including other files imported.
The syntax of tactics is quite expressive. They can take arguments and can be combined into
compound tactics. You can think of tactics as programs in a domain-specific language (DSL). Users
can extend the DSL by defining new tactics. This discrete, combinatorial, and unbounded action
space makes theorem proving challenging for machine learning.
Another challenge is premise selection. Premises are existing lemmas or definitions useful for proving
a theorem. They are used as arguments in tactics. For example, in Fig. 2 and Fig. 1 ( Top left), the
tactic “rewrite mod_self” rewrites the goal using the premise mod_self, which is defined in
another file imported by the current file. Proofs cannot use premises that haven’t been defined. For
example, gcd_selfcannot be used to prove gcd_zero_left. In addition, they cannot use premises
not imported to the current file. Still, premises come from a large math library containing hundreds
of thousands of existing definitions and theorems, making it hard, for humans and machines alike, to
select the right premises when generating a tactic. This is a key bottleneck in theorem proving and is
what we aim to address through retrieval-augmented LLMs.
4 LeanDojo: Toolkit and Benchmark
LeanDojo serves two essential needs of learning-based theorem proving in Lean. First, it extracts
training data from Lean, and we use this capability to construct a challenging theorem proving
benchmark. Second, it enables the model to interact with Lean programmatically.
Data Extraction. Lean repos (e.g., mathlib or lean-liquid) contain source code of human-
written theorems/proofs. However, the raw code is unsuitable for training the prover. It lacks runtime
information that humans can access when using Lean, such as intermediate states between proof
steps. Therefore, LeanDojo extracts the following information not directly visible in the code:
3The process is similar in many other proof assistants, though they may have different logical foundations.
5• File dependencies and abstract syntax trees (ASTs): LeanDojo processes the repo to produce
a directed acyclic graph whose nodes are files and edges are import relations between files. In
addition, LeanDojo produces the AST of each file. File dependencies and ASTs are useful for
program analysis, e.g., collecting theorems defined in a file or premises accessible to a theorem.
• States and tactics: LeanDojo extracts all tactics in proofs. For each tactic, it also extracts the states
before/after the tactic, which allows us to reconstruct the proof tree in Fig. 1 (Top left).
• Premises: For each premise, such as mod_self in Fig. 2, LeanDojo records where it is defined
(location in data/nat/lemma.lean) and where it is used (locations across many files). In addition,
premises have unique fully qualified names (e.g.,nat.mod_self) but are often used by ambiguous
short names (mod_self), relying on Lean to perform name resolution. LeanDojo is capable of
recording their full names.
Lean has basic support for exporting dependencies, ASTs, states, and tactics. However, it cannot
resolve the premises’ full names and locate their definitions. Therefore, we modify Lean to record
this information (details in Appendix A.1). The modified Lean is used only for data extraction but
not for evaluation, so we do not risk accidentally breaking Lean’s logical soundness.
LeanDojo Benchmark. We construct a benchmark for premise selection and theorem proving,
named LeanDojo Benchmark. The data is extracted from mathlib,4 Lean’s centralized math library
covering diverse topics such as analysis, algebra, and geometry.5 LeanDojo Benchmark is one of
the largest math-focused theorem proving datasets, consisting of 98,734 theorems from 3,384 Lean
files. Unlike existing datasets in Lean [ 16], LeanDojo Benchmark also contains the definitions of
130,262 premises, including not only theorems but also other definitions that can be used as premises
(e.g., gcd in Fig. 2. Furthermore, the dataset has 217,776 tactics, 129,243 of them with at least one
premise. The average number of premises is 2.13 among tactics with premises. Appendix B contains
additional information on data format, datasheet [90], hosting, and licensing.
lemma conj_mul : ( a * b ) . conj = b.conj * a.conj := begin
ext ; simp ; ring_exp
end
lemma conj_conj_mul : ( a.conj * b ) . conj = b.conj * a := begin
rw [ conj_mul , conj_conj ]
end
lemma conj_mul_conj : ( a * b.conj ) . conj = b * a.conj := begin
rw [ conj_mul , conj_conj ]
end
1
src/algebra/quaternion.lean
Figure 3: Similar theorems/proofs are common. If splitting them randomly into training/testing, the
model can prove testing theorems by memorization.
LeanDojo Benchmark has 94,734/2,000/2,000 theorems for training/validation/testing. It features
a challenging data split for testing the prover’s generalization in more realistic scenarios. Splitting
theorems randomly can overestimate the prover’s performance, by allowing it to prove many theorems
through memorization. In human-written Lean code, a common idiom is to have a block of similar
theorems/proofs for slightly different properties of the same math concept. For example, in Fig. 3,
the last two theorems not only look similar but have identical proofs. If one of them is in training,
the model can easily prove the other one by memorization. This shortcut enables the model to prove
seemingly nontrivial theorems, including those requiring premises to prove.
To mitigate this issue, besides the random split, we create a challenging data split named
novel_premises. It requires testing proofs to use at least one premise that has never been used in
training. For example, the last two theorems in Fig. 3 both use the premise conj_mul. If one theorem
is in the training set of the novel_premisessplit, the other one must also be in training.
4We use the commit 19c869efa56bbb8b500f2724c0b77261edbfa28c released on October 11, 2023.
5More details, statistics, and visualizations ofmathlib can be found athttps://leanprover-community.
github.io/mathlib_stats.html.
6Interacting with Lean. Another important function of LeanDojo is to interact with Lean program-
matically. It turns Lean into a gym-like environment [22], in which the prover can observe the proof
state, run tactics to change the state, and receive feedback on errors or on proof completion. This
environment is indispensable for evaluating/deploying the prover or training it through RL.
Below is LeanDojo’s main interface for interacting with Lean through tactics. Lean also supports
other proof styles not based on tactics. Although we only support tactic-style proofs, they are
sufficiently general since any proof can be converted to a tactic-style proof.6
• initialize(theorem): Given the theorem to prove, LeanDojo returns the initial state. A valid
state is a string representing current proof goals and local contexts (see the nodes in Fig. 1 Top left).
When there are multiple goals, their strings are concatenated.
• run_tac(state, tactic): Run a tactic on a given state and return the next state. The returned
state will be an error state if the tactic execution is not successful, e.g., due to timeout or inapplicable
tactic. If the input state is an error, the result can only be an error.
Building this environment is technically challenging, as Lean is designed for human users, not
machines. LeanDojo is the first tool that can interact with Lean reliably. Existing tool [19] is limited:
21.1% of the ground truth proofs are misjudged as incorrect, due to issues with how they construct
the proof environment, which distorts the reported performance and produces unreliable feedback
when used in reinforcement learning. In contrast, LeanDojo reduces the number of misjudgments to
1.4%. Details are in Appendix A.2.
5 ReProver: Retrieval-Augmented Theorem Prover
We develop the ReProver model that uses retrieval to select premises explicitly. At its core is a
retrieval-augmented tactic generator (Fig. 1 Bottom). Given the current proof state, it retrieves a
handful of potentially useful premises and generates a tactic conditioning on the concatenation of the
state and retrieved premises. When proving theorems, the model generates multiple tactic candidates
at each step, which are used in a standard best-first search algorithm to find proofs [16, 18, 19, 28].
Premise Retrieval. Our retriever is based on Dense Passage Retriever [ 26]. Given a state s as
the query and a library of candidate premises P = {pi}N
i=1, it retrieves a ranked list of m premises
{p′
i}m
i=1 from P. In DPR, s and pi are both raw texts but are embedded in a vector space, and we
retrieve the top m premises maximizing the cosine similarity between the state and the premise.
More formally, we have a function f parameterized by θ for embedding both the state and the
premises into a h-dimensional vector space: f(s, θ), f(pi, θ) ∈ Rh. We retrieve premises maximizing
f(s, θ)T f(pi, θ)/(∥f(s, θ)∥2∥f(pi, θ)∥2). We choose f to be a Transformer encoder [2] followed
by average pooling: f(·, θ) = AvgPool(Enc(·, θ)).
The retrieval is efficient. The premise embeddings f(pi, θ) can be pre-computed, and we only
need one forward pass to compute f(s, θ). We do not rerank the retrieved premises as in Mag-
nushammer [49], which is more costly since it requires a separate forward pass for each retrieved
premise.
Similar to DPR, we train the retriever by minimizing a contrastive loss between positive premises
and in-batch negative premises. Specifically, suppose we have a batch of b states. For each state, we
sample a positive premise from the ground truth and n negative premises from P.7 They are called
“in-batch” negatives because they are shared by all states in the batch—Every state is associated with
all b · (n + 1) premises; at least 1 of them is positive. Let lij ∈ {0, 1} denote whether a state-premise
pair (si, pj) is positive. We minimize the mean squared loss:
L(θ) =
bX
i=1
b·(n+1)X
j=1
lij − f(si, θ)T f(pj, θ)
∥f(si, θ)∥2∥f(pj, θ)∥2

2
. (1)
6Another common type of proofs is “term-style proofs”. Any term-style proof “X” can always be converted
into an equivalent tactic-style proof “exact X”, though such conversion may lead to unidiomatic proofs.
7When training the retriever, we ignore proof states followed by tactics without using any premise.
7Retrieving from Accessible Premises. We incorporate into DPR two insights tailored to premise
selection. First, instead of retrieving from all premises in the math library, we restrict to premises
accessible to the current theorem. They include premises defined in the same file before the theorem,
as well as those imported from other files. We compute accessible premises for each theorem, relying
on LeanDojo’s capability in program analysis (Sec. 4). Focusing on accessible premises makes P
much smaller. LeanDojo Benchmark contains 130,262 premises in total, but the average number of
accessible premises is only 33,160.
In-file Negative Examples. DPR’s performance depends critically on the quality of negative
examples [91, 92]. In early experiments, we sampled all n negative premises randomly, and the
model often mistakenly retrieved other premises from the same file as the positive one. Therefore, we
propose a scheme that samples k in-file negatives and n − k random negatives for training.
Tactic Generation. As in Fig. 1 ( Bottom), retrieved premises are concatenated with the state. 8
Then an encoder-decoder Transformer, ByT5 [44], takes them as input and generates the tactic. The
model is trained to minimize the cross entropy loss w.r.t. human-written tactics.
Training ReProver takes substantially less compute than prior methods (120 GPU hours vs. more
than 1000 hours [16, 17]). All existing LLM-based provers pretrain on datasets specific to math and
coding [14–20]. The pretraining is computationally expensive, and the datasets are kept private. In
contrast, we choose to avoid domain-specific pretraining and build upon google/byt5-small—a
model checkpoint that is generic, publicly available, and relatively small (299M parameters vs.
837M [16] or 600M [ 17]). We could see further benefits from domain-specific pretraining, as in
Minerva [57], or stronger LLMs like LLaMA [93] or StarCoder [94], but that is beyond our scope. In
addition, our model is finetuned on human-written tactics only, without auxiliary data [16] or data
collected through online interaction with Lean [17, 19]. These orthogonal directions are valuable but
will significantly increase the method’s complexity and compute requirements.
6 Experiments
We evaluate ReProver on LeanDojo Benchmark. It outperforms baselines on premise selection and
theorem proving, demonstrating the promise of theorem proving with retrieval-augmented language
models. Experimental details and hyperparameters are in Appendix C.1.
Premise Selection. For premise selection, we only use tactics in LeanDojo Benchmark that have at
least one premise. The model, based on a ByT5 encoder, uses the state before a tactic as the query to
retrieve 100 premises. Then, we calculate standard metrics in information retrieval: R@k (recall for
the top k retrieved premises) and MRR (mean reciprocal rank).
Our first baseline is a classical BM25 retriever [ 95] without machine learning. Results in Table 1
show that our method outperforms BM25 significantly across the board. However, it exhibits a large
performance degradation on the challenging data split (comparing novel_premises to random).
This is consistent with the general observation that machine learning can be brittle in the presence
of distribution shifts. In addition, we compare with two ablations: one retrieving from all premises
(instead of accessible premises only) and the other without in-file negatives. They perform worse
than our method, demonstrating the effectiveness of our two improvements upon DPR.
Theorem Proving Experimental Setup. Then we evaluate ReProver on theorem proving. The
training has two stages: First, we train the retriever and use it to retrieve 100 premises for all
proof states in LeanDojo Benchmark. Second, we train the tactic generator, taking as input the
concatenation of the state and retrieved premises (truncated to a length limit). During evaluation, the
tactic generator is combined with best-first search to prove theorems. We evaluate thePass@1 metric:
The prover is given only one attempt and must find the proof within a wall time limit of 10 minutes.
Training takes five days on a single NVIDIA A100 GPU with 80GB memory, and evaluation takes
two days on eight V100 GPUs. Please see Appendix C.1 for details.
Baselines. Following prior work [16, 28], we include tidyas a baseline. It is a tactic in mathlib
that tries to complete the proof using heuristics (without machine learning). We apply tidydirectly
8We retrieve 100 premises, concatenate them with the state, and truncate the concatenation to a fixed length.
8Table 1: Premise selection testing performance. For each method, we train and evaluate two models
independently using different data splits (random and novel_premises; see Sec. 4). R@k is the
recall for the top k retrieved premises, and MRR is the mean reciprocal rank metric (higher is better).
Our retriever outperforms BM25 and ablations. Results for Lean 4 are in Appendix D.
Method random novel_premises
R@1 R@10 MRR R@1 R@10 MRR
BM25 6.7 17.2 0.15 5.9 15.5 0.14
w/ all premises 1.9 11.9 0.08 2.1 12.4 0.08
Ours 13.5 38.4 0.31 9.1 27.6 0.24
w/ all premises 11.7 36.2 0.27 7.1 23.1 0.20
w/o in-file negatives 10.8 33.1 0.25 7.9 25.7 0.22
to the original theorem and see if it can succeed within the wall time limit. Another baseline uses
GPT-4 as the tactic generator. Given a state, it queries GPT-4 to generate 35 tactics in zero-shot. After
removing invalid ones, the remaining tactics are combined with best-first search to find proofs. Data
contamination is possible: Many proofs had been publicly available on GitHub before GPT-4’s data
cutoff date (September 2021). See Appendix C.2 for details.
Unfortunately, it is not feasible to compare with existing LLM-based provers in Lean [16, 17, 19].
None of them are open-source or can be reproduced with reasonable effort. Furthermore, we cannot
compare directly with the numbers reported in their papers, due to differences in data, infrastructure,
and training procedures (details in Appendix C.3). Many difficulties are due to the private nature
of existing methods. By releasing our code and models, we hope to create accessible baselines for
future work to build upon.
Table 2: Theorem proving Pass@1 (%) on the testing data of LeanDojo Benchmark. Our ReProver
model outperforms tidy, GPT-4, and a baseline that generates tactics directly without retrieval.
Results for Lean 4 are in Appendix D.
Method random novel_premises
tidy 23.8 5.3
GPT-4 29.0 7.4
ReProver (ours) 51.2 26.3
w/o retrieval 47.6 23.2
Results. Table 2 shows the results on the testing data of LeanDojo Benchmark. ReProver outper-
forms all baselines on two different data splits, demonstrating the effectiveness of retrieval-augmented
theorem proving. GPT-4 performs substantially worse than our method, even though it may have
seen the ground truth proofs due to data contamination. The task cannot be solved out of the box by
state-of-the-art LLMs, calling for algorithmic innovations to make further progress.
Testing theorems in novel_premisesare indeed much more challenging. All methods in Table 2
perform substantially worse on novel_premisesthan the randomsplit. We argue that performance
on challenging splits is more indicative of the prover’s capability and should be emphasized in the
future development of theorem proving.
Evaluation on MiniF2F and ProofNet. We run ReProver to prove theorems in MiniF2F [28] and
ProofNet [29]. These two datasets are for testing only and do not have training theorems, which makes
them challenging since the distribution of theorems is quite different from mathlib used to train
ReProver. MiniF2F focuses on math olympiads, and ProofNet focuses on exercises in undergraduate
math textbooks. On MiniF2F’s test set in Lean, ReProver achieves a Pass@1 of 26.5%, which is
competitive with state-of-the-art methods without RL (25.9% in Polu et al. [19]). On ProofNet, our
Pass@1 is 13.8%, which is the first reported theorem proving result on this dataset. Further, many
theorems do not have ground truth proofs in Lean. Our prover discovers 33 proofs in MiniF2F and
39 proofs in ProofNet that currently do not have Lean proofs. Please see Appendix C.4 for details,
examples, and caveats.
97 Conclusion
We have introduced LeanDojo: an open-source playground for learning-based theorem proving in
Lean, consisting of toolkits, models, and benchmarks. It extracts data from Lean and enables the
model to interact with Lean programmatically. We have developed ReProver, the first retrieval-
augmented LLM for theorem proving. Limitations and future work are discussed in Appendix F.
We have released our code, data, models, and documentation to facilitate future research:
• LeanDojo’s codebase for data extraction and interaction with Lean: https://github.
com/lean-dojo/LeanDojo
• LeanDojo’s documentation: https://leandojo.readthedocs.io
• Datasets: (1) LeanDojo Benchmark: https://doi.org/10.5281/zenodo.8016385
with DOI 10.5281/zenodo.8016385. (2) LeanDojo Benchmark 4 (Appendix D): https:
//doi.org/10.5281/zenodo.8040109with DOI 10.5281/zenodo.8040109.
• ReProver’s code and models: https://github.com/lean-dojo/ReProver
• ChatGPT plugin (Appendix E): https://github.com/lean-dojo/LeanDojoChatGPT
• LeanDojo Website: https://leandojo.org
Acknowledgments and Disclosure of Funding
This work is partially supported by Caltech’s Center for Autonomous Systems and Technologies.
Kaiyu Yang is supported by the Computing, Data, and Society Postdoctoral Fellowship at Caltech.
Alex Gu is supported by the National Science Foundation (NSF) Graduate Research Fellowship. Rahul
Chalamala and Peiyang Song are supported by the Summer Undergraduate Research Fellowships
(SURF) program at Caltech. Anima Anandkumar is partially supported by the Bren endowed chair.
We appreciate the valuable feedback from Logan Murphy and members of the Anima AI+Science
Lab on an initial version of this paper. We thank Junyan Xu for manually inspecting the proofs
generated by our model on ProofNet. We also thank Jeremy Avigad and Mario Carneiro for insightful
discussions on supporting Lean 4 in LeanDojo.
10References
[1] Leonardo de Moura, Soonho Kong, Jeremy Avigad, Floris Van Doorn, and Jakob von Raumer. The Lean
theorem prover (system description). In International Conference on Automated Deduction (CADE) ,
2015. 1, 2, 22
[2] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Neural Information Processing Systems
(NeurIPS), 2017. 2, 7
[3] Allen Newell and Herbert Simon. The logic theory machine–a complex information processing system.
IRE Transactions on information theory, 2(3):61–79, 1956. 1
[4] Kevin Buzzard. The future of mathematics. CRNS-Imperial Lecture, 2019. 1
[5] Xavier Leroy, Sandrine Blazy, Daniel Kästner, Bernhard Schommer, Markus Pister, and Christian
Ferdinand. CompCert—a formally verified optimizing compiler. In Embedded Real Time Software and
Systems, 2016. 1
[6] Talia Ringer, Karl Palmskog, Ilya Sergey, Milos Gligoric, Zachary Tatlock, et al. QED at large: A survey
of engineering of formally verified software. Foundations and Trends® in Programming Languages,
2019. 1
[7] Bruno Barras, Samuel Boutin, Cristina Cornes, Judicaël Courant, Jean-Christophe Filliatre, Eduardo
Gimenez, Hugo Herbelin, Gerard Huet, Cesar Munoz, Chetan Murthy, et al. The Coq proof assistant
reference manual: Version 6.1. PhD thesis, Inria, 1997. 1
[8] Tobias Nipkow, Markus Wenzel, and Lawrence C Paulson.Isabelle/HOL: a proof assistant for higher-
order logic. 2002. 1
[9] Kaiyu Yang and Jia Deng. Learning to prove theorems via interacting with proof assistants. In Interna-
tional Conference on Machine Learning (ICML), 2019. 1, 4, 35
[10] William A Howard. The formulae-as-types notion of construction. To HB Curry: Essays on Combinatory
Logic, Lambda Calculus and Formalism, 1980. 2
[11] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan,
Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models
trained on code. arXiv preprint arXiv:2107.03374, 2021. 2
[12] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea
Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM Computing
Surveys, 55(12):1–38, 2023. 2
[13] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer,
Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools.
arXiv preprint arXiv:2302.04761, 2023. 2
[14] Stanislas Polu and Ilya Sutskever. Generative language modeling for automated theorem proving. arXiv
preprint arXiv:2009.03393, 2020. 2, 4, 8
[15] Albert Qiaochu Jiang, Wenda Li, Jesse Michael Han, and Yuhuai Wu. LISA: Language models of
ISAbelle proofs. In Conference on Artificial Intelligence and Theorem Proving (AITP), 2021. 4
[16] Jesse Michael Han, Jason Rute, Yuhuai Wu, Edward Ayers, and Stanislas Polu. Proof artifact co-training
for theorem proving with language models. In International Conference on Learning Representations
(ICLR), 2022. 4, 6, 7, 8, 9, 19, 20, 26, 36
[17] Guillaume Lample, Timothee Lacroix, Marie-Anne Lachaux, Aurelien Rodriguez, Amaury Hayat, Thibaut
Lavril, Gabriel Ebner, and Xavier Martinet. HyperTree proof search for neural theorem proving. In
Neural Information Processing Systems (NeurIPS), 2022. 2, 4, 8, 9, 26, 36
[18] Albert Qiaochu Jiang, Wenda Li, Szymon Tworkowski, Konrad Czechowski, Tomasz Odrzygó´ zd´ z, Piotr
Miło´s, Yuhuai Wu, and Mateja Jamnik. Thor: Wielding hammers to integrate language models and
automated theorem provers. In Neural Information Processing Systems (NeurIPS), 2022. 4, 7, 26
[19] Stanislas Polu, Jesse Michael Han, Kunhao Zheng, Mantas Baksys, Igor Babuschkin, and Ilya Sutskever.
Formal mathematics statement curriculum learning. In International Conference on Learning Representa-
tions (ICLR), 2023. 2, 3, 4, 7, 8, 9, 18, 19, 20, 26, 36
11[20] Emily First, Markus N Rabe, Talia Ringer, and Yuriy Brun. Baldur: Whole-proof generation and repair
with large language models. arXiv preprint arXiv:2303.04910, 2023. 8, 35
[21] Haiming Wang, Ye Yuan, Zhengying Liu, Jianhao Shen, Yichun Yin, Jing Xiong, Enze Xie, Han Shi,
Yujun Li, Lin Li, et al. DT-Solver: Automated theorem proving with dynamic-tree sampling guided by
proof-level value function. In Annual Meeting of the Association for Computational Linguistics (ACL),
2023. 2, 4
[22] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. OpenAI Gym. arXiv preprint arXiv:1606.01540, 2016. 2, 7
[23] Josef Urban. MPTP—motivation, implementation, first experiments. Journal of Automated Reasoning,
33:319–339, 2004. 3, 4
[24] Geoffrey Irving, Christian Szegedy, Alexander A Alemi, Niklas Eén, François Chollet, and Josef Urban.
DeepMath—deep sequence models for premise selection. In Neural Information Processing Systems
(NeurIPS), 2016. 3, 4
[25] The mathlib Community. The Lean mathematical library. In Proceedings of the 9th ACM SIGPLAN
International Conference on Certified Programs and Proofs, CPP 2020, pages 367–381, New York, NY ,
USA, 2020. Association for Computing Machinery. ISBN 9781450370974. doi: 10.1145/3372885.
3373824. URL https://doi.org/10.1145/3372885.3373824. 3
[26] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen,
and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In Conference on
Empirical Methods in Natural Language Processing (EMNLP), 2020. 3, 7, 33
[27] OpenAI. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 3, 24, 32
[28] Kunhao Zheng, Jesse Michael Han, and Stanislas Polu. MiniF2F: a cross-system benchmark for formal
olympiad-level mathematics. In International Conference on Learning Representations (ICLR), 2022. 3,
4, 7, 8, 9, 26, 27
[29] Zhangir Azerbayev, Bartosz Piotrowski, Hailey Schoelkopf, Edward W Ayers, Dragomir Radev, and
Jeremy Avigad. ProofNet: Autoformalizing and formally proving undergraduate-level mathematics.arXiv
preprint arXiv:2302.12433, 2023. 3, 4, 9, 26, 28, 29
[30] Alan JA Robinson and Andrei V oronkov. Handbook of automated reasoning, volume 1. 2001. 4
[31] Laura Kovács and Andrei V oronkov. First-order theorem proving and vampire. InInternational Conference
on Computer Aided Verification (CAV), 2013. 4
[32] Sarah Loos, Geoffrey Irving, Christian Szegedy, and Cezary Kaliszyk. Deep network guided proof search.
arXiv preprint arXiv:1701.06972, 2017. 4
[33] James P Bridge, Sean B Holden, and Lawrence C Paulson. Machine learning for first-order theorem
proving: learning to select a good heuristic. Journal of Automated Reasoning, 53:141–172, 2014. 4
[34] Thibault Gauthier, Cezary Kaliszyk, Josef Urban, Ramana Kumar, and Michael Norrish. TacticToe:
learning to prove with tactics. Journal of Automated Reasoning, 65:257–286, 2021. 4
[35] Aditya Paliwal, Sarah Loos, Markus Rabe, Kshitij Bansal, and Christian Szegedy. Graph representations
for higher-order logic and theorem proving. In AAAI Conference on Artificial Intelligence, 2020. 4
[36] Kshitij Bansal, Christian Szegedy, Markus N Rabe, Sarah M Loos, and Viktor Toman. Learning to reason
in large theories without imitation. arXiv preprint arXiv:1905.10501, 2019. 4
[37] Minchao Wu, Michael Norrish, Christian Walder, and Amir Dezfouli. TacticZero: Learning to prove
theorems from scratch with deep reinforcement learning. In Neural Information Processing Systems
(NeurIPS), 2021. 4
[38] Mingzhe Wang and Jia Deng. Learning to prove theorems by learning to generate theorems. In Neural
Information Processing Systems (NeurIPS), 2020. 4, 36
[39] Markus Norman Rabe, Dennis Lee, Kshitij Bansal, and Christian Szegedy. Mathematical reasoning via
self-supervised skip-tree training. In International Conference on Learning Representations (ICLR) ,
2021.
12[40] Yuhuai Wu, Markus N Rabe, Wenda Li, Jimmy Ba, Roger B Grosse, and Christian Szegedy. LIME:
Learning inductive bias for primitives of mathematical reasoning. InInternational Conference on Machine
Learning (ICML), 2021. 4
[41] Sascha Böhme and Tobias Nipkow. Sledgehammer: judgement day. In International Joint Conference on
Automated Reasoning (IJCAR), 2010. 4
[42] Jasmin Christian Blanchette, Cezary Kaliszyk, Lawrence C Paulson, and Josef Urban. Hammering
towards QED. Journal of Formalized Reasoning, 9(1):101–148, 2016.
[43] Łukasz Czajka and Cezary Kaliszyk. Hammer for Coq: Automation for dependent type theory. Journal
of Automated Reasoning, 2018. 4
[44] Linting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts,
and Colin Raffel. ByT5: Towards a token-free future with pre-trained byte-to-byte models. Transactions
of the Association for Computational Linguistics (TACL), 10:291–306, 2022. 4, 8, 24, 32
[45] Christian Szegedy, Markus Rabe, and Henryk Michalewski. Retrieval-augmented proof step synthesis. In
Conference on Artificial Intelligence and Theorem Proving (AITP), 2021. 4
[46] Yuhuai Wu. Formal premise selection with language models. In Conference on Artificial Intelligence and
Theorem Proving (AITP), 2022. 4
[47] Jesse Alama, Tom Heskes, Daniel Kühlwein, Evgeni Tsivtsivadze, and Josef Urban. Premise selection
for mathematics by corpus analysis and kernel methods. Journal of Automated Reasoning, 52:191–213,
2014. 4
[48] Bartosz Piotrowski, Ramon Fernández Mir, and Edward Ayers. Machine-learned premise selection for
Lean. In International Conference on Automated Reasoning with Analytic Tableaux and Related Methods
(TABLEAUX), 2023. 4
[49] Maciej Mikuła, Szymon Antoniak, Szymon Tworkowski, Albert Qiaochu Jiang, Jin Peng Zhou, Christian
Szegedy, Łukasz Kuci´nski, Piotr Miło´s, and Yuhuai Wu. Magnushammer: A transformer-based approach
to premise selection. arXiv preprint arXiv:2303.04488, 2023. 4, 7
[50] Eric Yeh, Briland Hitaj, Sam Owre, Maena Quemener, and Natarajan Shankar. CoProver: A recommender
system for proof construction. arXiv preprint arXiv:2304.10486, 2023. 4
[51] Daniel Huang, Prafulla Dhariwal, Dawn Song, and Ilya Sutskever. GamePad: A learning environment for
theorem proving. In International Conference on Learning Representations (ICLR), 2019. 4
[52] Tom Reichel, R Henderson, Andrew Touchet, Andrew Gardner, and Talia Ringer. Proof repair infras-
tructure for supervised models: Building a large proof repair dataset. In International Conference on
Interactive Theorem Proving (ITP), 2023. 4
[53] Wenda Li, Lei Yu, Yuhuai Wu, and Lawrence C Paulson. IsarStep: a benchmark for high-level mathemat-
ical reasoning. In International Conference on Learning Representations (ICLR), 2021. 4
[54] Kshitij Bansal, Sarah Loos, Markus Rabe, Christian Szegedy, and Stewart Wilcox. HOList: An envi-
ronment for machine learning of higher order logic theorem proving. In International Conference on
Machine Learning (ICML), 2019. 4
[55] Cezary Kaliszyk, François Chollet, and Christian Szegedy. HolStep: A machine learning dataset for
higher-order logic theorem proving. In International Conference on Learning Representations (ICLR),
2017. 4
[56] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,
and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Neural
Information Processing Systems (NeurIPS), Datasets and Benchmarks Track, 2021. 4
[57] Aitor Lewkowycz, Anders Johan Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski,
Vinay Venkatesh Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al.
Solving quantitative reasoning problems with language models. In Neural Information Processing
Systems (NeurIPS), 2022. 8
[58] Deborah Ferreira and André Freitas. Premise selection in natural language mathematical texts. In Annual
Meeting of the Association for Computational Linguistics (ACL), 2020.
13[59] Sean Welleck, Jiacheng Liu, Ronan Le Bras, Hannaneh Hajishirzi, Yejin Choi, and Kyunghyun Cho.
NaturalProofs: Mathematical theorem proving in natural language. In Neural Information Processing
Systems (NeurIPS), Datasets and Benchmarks Track, 2021.
[60] Sean Welleck, Jiacheng Liu, Ximing Lu, Hannaneh Hajishirzi, and Yejin Choi. NaturalProver: Grounded
mathematical proof generation with language models. In Neural Information Processing Systems
(NeurIPS), 2022.
[61] Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay
Rajpurohit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark, et al. Lila: A unified benchmark for
mathematical reasoning. In Conference on Empirical Methods in Natural Language Processing (EMNLP),
2022.
[62] Pan Lu, Liang Qiu, Wenhao Yu, Sean Welleck, and Kai-Wei Chang. A survey of deep learning for
mathematical reasoning. arXiv preprint arXiv:2212.10535, 2022.
[63] Jordan Meadows and Andre Freitas. A survey in mathematical language processing. arXiv preprint
arXiv:2205.15231, 2022. 4
[64] Qingxiang Wang, Cezary Kaliszyk, and Josef Urban. First experiments with neural translation of informal
to formal mathematics. In Conferences on Intelligent Computer Mathematics (CICM), 2018. 4
[65] Matthias Cosler, Christopher Hahn, Daniel Mendoza, Frederik Schmitt, and Caroline Trippel. nl2spec:
Interactively translating unstructured natural language to temporal logics with large language models.
arXiv preprint arXiv:2303.04864, 2023.
[66] Jiayi Pan, Glen Chou, and Dmitry Berenson. Data-efficient learning of natural language to linear temporal
logic translators for robot task specification. In International Conference on Robotics and Automation
(ICRA), 2023.
[67] Christopher Hahn, Frederik Schmitt, Julia J Tillman, Niklas Metzger, Julian Siber, and Bernd Finkbeiner.
Formal specifications from natural language. arXiv preprint arXiv:2206.01962, 2022.
[68] Yuhuai Wu, Albert Qiaochu Jiang, Wenda Li, Markus Rabe, Charles Staats, Mateja Jamnik, and Christian
Szegedy. Autoformalization with large language models. In Neural Information Processing Systems
(NeurIPS), 2022.
[69] Albert Q Jiang, Sean Welleck, Jin Peng Zhou, Wenda Li, Jiacheng Liu, Mateja Jamnik, Timothée Lacroix,
Yuhuai Wu, and Guillaume Lample. Draft, Sketch, and Prove: Guiding formal theorem provers with
informal proofs. In International Conference on Learning Representations (ICLR), 2023. 26
[70] Xueliang Zhao, Wenda Li, and Lingpeng Kong. Decomposing the enigma: Subgoal-based demonstration
learning for formal theorem proving. arXiv preprint arXiv:2305.16366, 2023. 26
[71] Garett Cunningham, Razvan C Bunescu, and David Juedes. Towards autoformalization of mathematics
and code correctness: Experiments with elementary proofs. arXiv preprint arXiv:2301.02195, 2023.
[72] Yongchao Chen, Rujul Gandhi, Yang Zhang, and Chuchu Fan. NL2TL: Transforming natural languages
to temporal logics using large language models. arXiv preprint arXiv:2305.07766, 2023. 4
[73] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization
through memorization: Nearest neighbor language models. In International Conference on Learning
Representations (ICLR), 2020. 4
[74] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented
language model pre-training. In International Conference on Machine Learning (ICML), 2020.
[75] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,
Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for
knowledge-intensive NLP tasks. In Neural Information Processing Systems (NeurIPS), 2020.
[76] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican,
George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving
language models by retrieving from trillions of tokens. In International Conference on Machine Learning
(ICML), 2022.
[77] Zonglin Li, Ruiqi Guo, and Sanjiv Kumar. Decoupled context processing for context augmented language
modeling. In Neural Information Processing Systems (NeurIPS), 2022.
14[78] Zhengbao Jiang, Luyu Gao, Jun Araki, Haibo Ding, Zhiruo Wang, Jamie Callan, and Graham Neubig.
Retrieval as attention: End-to-end learning of retrieval and reading within a single transformer. In
Conference on Empirical Methods in Natural Language Processing (EMNLP), 2022.
[79] Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing transformers.
In International Conference on Learning Representations (ICLR), 2022.
[80] Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-
Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Few-shot learning with retrieval augmented
language models. arXiv preprint arXiv:2208.03299, 2022.
[81] Zexuan Zhong, Tao Lei, and Danqi Chen. Training language models with memory augmentation. In
Conference on Empirical Methods in Natural Language Processing (EMNLP), 2022. 4
[82] Shirley Anugrah Hayati, Raphael Olivier, Pravalika Avvaru, Pengcheng Yin, Anthony Tomasic, and
Graham Neubig. Retrieval-based neural code generation. In Conference on Empirical Methods in Natural
Language Processing (EMNLP), 2018. 4
[83] Md Rizwan Parvez, Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei Chang. Retrieval
augmented code generation and summarization. In Findings of the Association for Computational
Linguistics: EMNLP, 2021.
[84] Shuai Lu, Nan Duan, Hojae Han, Daya Guo, Seung-won Hwang, and Alexey Svyatkovskiy. ReACC: A
retrieval-augmented code completion framework. InAnnual Meeting of the Association for Computational
Linguistics (ACL), 2022.
[85] Shuyan Zhou, Uri Alon, Frank F Xu, Zhengbao Jiang, and Graham Neubig. DocPrompting: Generating
code by retrieving the docs. In International Conference on Learning Representations (ICLR), 2023.
[86] Disha Shrivastava, Hugo Larochelle, and Daniel Tarlow. Repository-level prompt generation for large
language models of code. arXiv preprint arXiv:2206.12839, 2022.
[87] Fengji Zhang, Bei Chen, Yue Zhang, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, and Weizhu Chen.
RepoCoder: Repository-level code completion through iterative retrieval and generation. arXiv preprint
arXiv:2303.12570, 2023.
[88] Yangruibo Ding, Zijian Wang, Wasi Uddin Ahmad, Murali Krishna Ramanathan, Ramesh Nallapati,
Parminder Bhatia, Dan Roth, and Bing Xiang. CoCoMIC: Code completion by jointly modeling in-file
and cross-file context. arXiv preprint arXiv:2212.10007, 2022. 4
[89] David Thrane Christiansen. Functional programming in Lean, 2023. 4
[90] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach,
Hal Daumé Iii, and Kate Crawford. Datasheets for datasets. Communications of the ACM, 64(12):86–92,
2021. 6, 22
[91] Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Jiafeng Guo, Min Zhang, and Shaoping Ma. Optimizing dense
retrieval model training with hard negatives. In International ACM SIGIR Conference on Research and
Development in Information Retrieval (SIGIR), 2021. 8
[92] Jing Lu, Gustavo Hernandez Abrego, Ji Ma, Jianmo Ni, and Yinfei Yang. Multi-stage training with
improved negative contrast for neural passage retrieval. In Conference on Empirical Methods in Natural
Language Processing (EMNLP), 2021. 8
[93] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. LLaMA: Open and efficient foundation
language models. arXiv preprint arXiv:2302.13971, 2023. 8
[94] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou,
Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. StarCoder: may the source be with you!
arXiv preprint arXiv:2305.06161, 2023. 8, 32
[95] Stephen Robertson, Hugo Zaragoza, et al. The probabilistic relevance framework: BM25 and beyond.
Foundations and Trends® in Information Retrieval, 3(4):333–389, 2009. 8
[96] Leonardo de Moura, Jeremy Avigad, Soonho Kong, and Cody Roux. Elaboration in dependent type
theory. arXiv preprint arXiv:1505.04324, 2015. 18
15[97] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.
The Journal of Machine Learning Research, 21(1):5485–5551, 2020. 23
[98] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. DeepSpeed: System optimizations
enable training deep learning models with over 100 billion parameters. In International Conference on
Knowledge Discovery and Data Mining (KDD), 2020. 24
[99] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference
on Learning Representations (ICLR), 2019. 24
[100] Mathlib Community. Mathport: A tool for porting Lean 3 projects to Lean 4. URL https://github.
com/leanprover-community/mathport. 27
[101] OpenAI. ChatGPT plugins. https://openai.com/blog/chatgpt-plugins, 2023. URL https:
//openai.com/blog/chatgpt-plugins. 29
[102] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik
Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint
arXiv:2305.10601, 2023. 31, 33
[103] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and
Caiming Xiong. CodeGen: An open large language model for code with multi-turn program synthesis.
arXiv preprint arXiv:2203.13474, 2022. 32
[104] Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi
Wang, Yang Li, et al. CodeGeeX: A pre-trained model for code generation with multilingual evaluations
on HumanEval-X. arXiv preprint arXiv:2303.17568, 2023. 32
[105] Lili Yu, Dániel Simig, Colin Flaherty, Armen Aghajanyan, Luke Zettlemoyer, and Mike Lewis. MegaByte:
Predicting million-byte sequences with multiscale transformers. arXiv preprint arXiv:2305.07185, 2023.
32
[106] Gautier Izacard and Édouard Grave. Leveraging passage retrieval with generative models for open domain
question answering. In European Chapter of the Association for Computational Linguistics (EACL) ,
2021. 33
[107] Yi Tay, Vinh Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe
Zhao, Jai Gupta, et al. Transformer memory as a differentiable search index. In Neural Information
Processing Systems (NeurIPS), 2022. 34
[108] Michele Bevilacqua, Giuseppe Ottaviano, Patrick Lewis, Scott Yih, Sebastian Riedel, and Fabio Petroni.
Autoregressive search engines: Generating substrings as document identifiers. 2022.
[109] Ronak Pradeep, Kai Hui, Jai Gupta, Adam D Lelkes, Honglei Zhuang, Jimmy Lin, Donald Metzler,
and Vinh Q Tran. How does generative retrieval scale to millions of passages? arXiv preprint
arXiv:2305.11841, 2023. 34
[110] Norman Megill and David A Wheeler. Metamath: a computer language for mathematical proofs. Lulu.
com, 2019. 36
16Appendix
A LeanDojo Technical Details 18
A.1 Extracting Premise Information from Lean’s Elaborator . . . . . . . . . . . . . . . 18
A.2 Reliable Interaction with Lean . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
A.3 Comparison with Existing Tools for Learning-Based Theorem Proving in Lean . . 19
B LeanDojo Benchmark 20
B.1 Dataset Format . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
B.2 Datasheet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
B.3 Data Hosting, Licensing, and Maintenance . . . . . . . . . . . . . . . . . . . . . . 23
C Experiments 23
C.1 Details and Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
C.2 The GPT-4 Baseline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
C.3 Justifications for Not Comparing with Existing LLM-Based Provers . . . . . . . . 26
C.4 Evaluation on MiniF2F and ProofNet . . . . . . . . . . . . . . . . . . . . . . . . 26
D LeanDojo for Lean 4 27
E ChatGPT Plugin for Theorem Proving 29
F Limitations and Future Work 32
17A LeanDojo Technical Details
We provide more information on how LeanDojo extracts data from and interacts with Lean.9 For
further details, please check our open-source implementation.
A.1 Extracting Premise Information from Lean’s Elaborator
“Premises” in this paper belong to a category of Lean expressions called “constants.” In Lean,
definitions of constants are grouped into nested, hierarchical namespaces. Therefore, each premise
has a unique fully-qualified name. For example, mod_selfin Fig. 2 is defined in the namespace nat;
therefore, its fully qualified name is nat.mod_self. However, it would be too verbose if premises
had to be referred to using full names. In practice, tactics often refer to premises using short names
such as mod_self. In case multiple premises share the same short name, Lean automatically infers
the correct one from the context through a process called “name resolution”. LeanDojo is able to
trace the input/output of Lean’s name resolution and thereby extract accurate premise information for
training the retriever.
Name resolution in Lean is implemented in a process called “elaboration,” which happens after
parsing but before the parsed expressions are checked by Lean’s trusted kernel. Elaboration takes
as input user-entered expressions (called “pre-expressions”) that are concise, partially specified,
and potentially ambiguous. It turns them into complete expressions ready to be checked by the
kernel. This is realized by inferring not only full names but also missing types, implicit arguments,
overloading, type coercion, etc. Please refer to de Moura et al. [96] for details on Lean’s elaboration
process. In LeanDojo, we modify Lean’s internal implementation, intercepting the elaborator to
record its input/output:
• Pre-expression: The input to Lean’s elaborator, including where premises are used in proofs.
• Expression: The output of the elaborator, including the premise’s full name and where it is defined.
Locations are spans in the source code, specified by the file name and the row/column numbers of its
start/end. Our modification takes the form of a Git patch that LeanDojo can automatically apply to
any version of Lean 3 after March 24, 2022.
A.2 Reliable Interaction with Lean
Polu et al. [19] introduced lean-gym. To our knowledge, it is the only mature, open-source tool
before LeanDojo for interacting with Lean programmatically. However, we found severe issues with
lean-gym: About 21.1% of the correct, human-written proofs are misjudged as incorrect, leading to
two problems: First, it underestimates the prover’s evaluation performance. Second, the results are
too noisy as feedback signals for reinforcement learning.
After carefully analyzing lean-gym’s implementation, we identified the root cause of the problem.
When proving a theorem, the environment used by lean-gymis subtly different from the original
environment used by humans. Specifically,lean-gymfails to handle namespaces correctly (illustrated
in Fig. A). As a result, name resolution fails unexpectedly when checking correct proofs.
For example, Fig. A compares the correct environment and the environment constructed bylean-
gym. The theorem should be inside the namespace “ buffer”. However, in lean-gym, it merely
opens the namespace. These two scenarios are different when it comes to name resolution. Being
inside a namespace instructs Lean to favor constants defined in that namespace, whereas opening
a namespace does not have such an effect. In this example, the short name “ read” is ambiguous:
We have “monad_reader.read” defined in “init/control/reader.lean” and “buffer.read”
defined in “ data/buffer.lean”. In the correct environment, the “ read” in “ unfold read”
resolves to “ buffer.read”. Whereas in lean-gym’s environment, it incorrectly resolved to
“monad_reader.read”. Lean complains that “ read” is not an equational lemma, because it is
referring to a wrong “read”. LeanDojo does not suffer from this kind of error since it uses a different
mechanism for constructing the environment. Specifically, it wraps the interaction code as a Lean
tactic, which is inserted into the proof. Therefore, the environment is guaranteed to be correct.
9“Lean” in our paper refers to Lean 3 by default. Lean 4 is not backward-compatible but is also supported by
LeanDojo. Our Lean 4 results are in Appendix D.
18We quantitatively compare lean-gym and LeanDojo on the number of proof check-
ing errors. In this study, we use Lean v3.42.1 paired with mathlib version
6e5ca7d0097313e59f7533a42e3ea5197484c775 since they are supported by both tools. We
use LeanDojo to extract all tactic-style proofs and enter them into both tools. These proofs are all
correct, but lean-gymfailed on 21.1% of them. In contrast, LeanDojo only failed on 1.4%, and its
failures are a subset of lean-gym’s. We include this study in our open-source repo and document
example proofs from the remaining 1.4% to provide transparency on LeanDojo’s limitations.10
import data.buffer
universe u
namespace buffer
theorem my_read_eq_read’ { a : Type u } [ inhabited a ]
( b : buffer a ) ( i : nat ) ( h : i < b.size ) :
read b 〈 i , h 〉 = read’ b i := begin
cases b ,
unfold read ,
unfold read’ ,
simp [ array.read_eq_read’ ]
end
end buffer
1
import data.buffer
universe u
open buffer
theorem my_read_eq_read’ { a : Type u } [ inhabited a ]
( b : buffer a ) ( i : nat ) ( h : i < b.size ) :
read b 〈 i , h 〉 = read’ b i := begin
cases b ,
unfold read ,
unfold read’ ,
simp [ array.read_eq_read’ ]
end
1
lean-gym’s environmentCorrectenvironment ERROR: unfold tactic failed, `read` does not have equational lemmas nor is a projection
Figure A: An example of correct proofs misjudged as incorrect by lean-gym, adapted from the
theorem read_eq_read’in “data/buffer.lean” of Lean’s standard library. The error message
is because lean-gym failed to resolve the short name “ read” to the correct fully-qualified name.
The Lean code in this figure is only for illustrative purposes. It does not reflect the implementation
technique used by lean-gymto construct the environment. Instead of generating actual Lean code,
lean-gymuses Lean’s metaprogramming APIs to construct the environment.
A.3 Comparison with Existing Tools for Learning-Based Theorem Proving in Lean
To our knowledge, LeanStep [16]11 and lean-gym[19] are the only published tools for learning-based
theorem proving in Lean. There are a few unpublished prototypes, such as repl, lean-client-python,
and lean-gymfor Lean 4, none of which is mature enough or is under active development. Therefore,
we only compare LeanDojo with LeanStep and lean-gym(summarized in Table A).
Functionality. LeanDojo supports both data extraction and interacting with Lean programmatically.
In contrast, LeanStep is only for data extraction, and lean-gym is only for interacting with Lean.
They are not actively maintained, so they do not support recent versions of mathlib (tested on
August 11, 2023, using mathlib commit 19c869efa56bbb8b500f2724c0b77261edbfa28c). Also,
neither of them support Lean 4 (Appendix D). LeanDojo fully supports recent mathlib and Lean 4.
Furthermore, LeanStep cannot extract premise information and is not applicable to repos other than
mathlib. Last, LeanDojo comes with comprehensive documentation and unit tests, whereas other
tools barely have any.
Implementation details. LeanStep and LeanDojo use different mechanisms to extract ASTs and
proof trees. LeanStep implements an ad-hoc parser in Python for parsing Lean code into ASTs. It
also intercepts Lean’s tactic system to insert logging code. Then the logs are used to reconstruct proof
trees. This implementation is brittle and does not work for the current versions of Lean/mathlib. In
contrast, LeanDojo relies on Lean’s built-in mechanisms for exporting ASTs and proof states (lean
––ast ––tsast ––tspp), which works robustly for recent Lean/mathlib. This mechanism was
developed after LeanStep.
10https://github.com/lean-dojo/LeanDojo/blob/main/tests/interaction/test_
unexpected_errors.py
11LeanStep is technically a dataset. We are referring to the lean_proof_recording tool for extracting it.
19Regarding interaction with Lean, both lean-gymand LeanDojo rely on Lean’s metaprogramming
APIs, and LeanDojo partially builds upon lean-gym’s code. However, lean-gym has a critical
issue in that it misjudges many correct proofs as incorrect (Appendix A.2). The main reason is that
lean-gymfails to distinguish two subtly different cases when constructing the proof environment:
(1) opening a namespace; (2) being inside a namespace. LeanDojo does not suffer from this issue.
Instead of operating as a standalone program in the IO monad, it wraps the interaction code into a
special tactic, which is inserted into the correct location in the proof. Therefore, the interaction code
is guaranteed to run in the same environment as the original human-written proof.
LeanStep [16] lean-gym[19] LeanDojo (ours)
Data extraction
Premise information ✗ N/A ✓
Lean 4 support ✗ N/A ✓
Recent mathlib ✗ N/A ✓
Repos other than mathlib ✗ N/A ✓
Interaction
Estimated errors N/A 21.1% 1.4%
Lean 4 support N/A ✗ ✓
Recent mathlib N/A ✗ ✓
Repos other than mathlib N/A ✓ ✓
Documentation & unit tests ✗ ✗ ✓
Table A: Comparing LeanDojo with existing tools for data extraction and interaction with Lean.
B LeanDojo Benchmark
B.1 Dataset Format
We describe the data format of LeanDojo Benchmark, which has the following directory structure:
/
corpus.jsonl.............All premises defined in mathliband Lean’s standard library
metadata.json.........................................................Metadata
licenses
lean.....................................Attribution to Lean’s Apache 2.0 license
mathlib..............................Attribution to mathlib’s Apache 2.0 license
README.md.........Statement that LeanDojo Benchmark is released under CC BY 2.0
random.........................................Theorems/proofs of the randomsplit
train.json...................................................94,734 theorems
val.json......................................................2,000 theorems
test.json.....................................................2,000 theorems
novel_premises.......................Theorems/proofs of the novel_premisessplit
train.json...................................................94,734 theorems
val.json......................................................2,000 theorems
test.json.....................................................2,000 theorems
Premise Definitions. corpus.jsonlcontains the definition of premises. It has 3,280 lines. Each
line is in JSON format and corresponds to a Lean file. Below is an example for “ init/con-
trol/functor.lean”, which directly imports three other files: “init/core.lean”, “init/func-
tion.lean”, and “init/meta/name.lean”. It defines two constants that can be used as premises:
“functor” and “functor.map_const_rev”. For each premise, we have access to its full name, the
source code, and its start/end location within the file.
"path": "_target/deps/lean/library/init/control/functor.lean",
"imports": [
"_target/deps/lean/library/init/core.lean",
"_target/deps/lean/library/init/function.lean",
"_target/deps/lean/library/init/meta/name.lean"
],
"premises": [
20{
"full_name": "functor",
"code": "class functor (f : Type u → Type v) : Type (max (
u+1) v) :=\n(map : Π {α β : Type u}, ( α → β) → f α →
f β)\n(map_const : Π {α β : Type u}, α → f β → f α :=
λ α β, map ◦ const β)",
"start": [11, 1],
"end": [13, 70],
"kind": "class"
},
{
"full_name": "functor.map_const_rev",
"code": "@[reducible] def functor.map_const_rev {f : Type u
→ Type v} [functor f] { α β : Type u} : f β → α → f α
:=\n λ a b, b <$ a",
"start": [18, 1],
"end": [19, 14],
"kind": "definition"
}
]
Theorems and Tactics. Theorems in LeanDojo Benchmark are split into training/validation/testing
using two different strategies (Sec. 4). They are formatted in JSON, and below is an example
corresponding to the theorem “real.angle.to_real_pi_div_two”. LeanDojo has recorded two
tactics: “split” and “linarith [pi_pos]”. For each tactic, we have the proof states before/after
it. The “ linarith [pi_pos]” tactic illustrates how premises are recorded: They are annotated
using HTML-like strings such as “linarith [<a>pi_pos</a>]”, followed by a “provenance list”.
Each element in the list corresponds to a premise in the tactic.
"url": "https://github.com/leanprover-community/mathlib",
"commit": "19c869efa56bbb8b500f2724c0b77261edbfa28c",
"file_path": "src/analysis/special_functions/trigonometric/
angle.lean",
"full_name": "real.angle.to_real_pi_div_two",
"start": [512, 9],
"end": [513, 56],
"traced_tactics": [
{
"tactic": "split",
"annotated_tactic": ["split", []],
"state_before": " ⊢ -π < π / 2 ∧ π / 2 ≤ π",
"state_after": "2 goals\n ⊢ -π < π / 2\n\n ⊢ π / 2 ≤ π"
},
{
"tactic": "linarith [pi_pos]",
"annotated_tactic": [
"linarith [<a>pi_pos</a>]",
[
{
"full_name": "real.pi_pos",
"def_path": "src/analysis/special_functions/
trigonometric/basic.lean",
"def_pos": [122, 7],
}
]
],
"state_before": " ⊢ -π < π / 2",
"state_after": "no goals"
}
21]
Not all theorems have tactic-style proofs. For those without tactic-style proofs, concatenating the
tactics does not lead to a complete proof of the original theorem. However, this is not an issue when
using the data for theorem proving evaluation or for training tactic generators.
B.2 Datasheet
We present a datasheet [90] for documentation and responsible usage of LeanDojo Benchmark.
Motivation.
• For what purpose was the dataset created? It was created as a benchmark for learning-based
theorem proving in Lean.
• Who created the dataset (e.g., which team, research group) and on behalf of which entity (e.g.,
company, institution, organization)? It was created by the authors of this paper.
• Who funded the creation of the dataset? See the acknowledgments in Sec. 7.
Composition.
• What do the instances that comprise the dataset represent (e.g., documents, photos, people, coun-
tries)? The dataset consists of formal definitions, theorems, and proofs written in Lean [1].
• How many instances are there in total (of each type, if appropriate)? The dataset has 98,734
theorems and their proofs, as well as 130,262 premises defined in 3,384 files.
• Does the dataset contain all possible instances or is it a sample (not necessarily random) of
instances from a larger set? The dataset contains all theorems/proofs that LeanDojo can extract
from the commit 19c869efa56bbb8b500f2724c0b77261edbfa28c of mathlib released on
October 11, 2023.
• What data does each instance consist of? Theorems/proofs in the dataset are Lean code written by
programmers and mathematicians.
• Are relationships between individual instances made explicit? Definitions in the dataset are linked
to proofs using them as premises.
• Are there recommended data splits? Yes, we recommend two data splits: random and
novel_premises. Please see Sec. 4 for details.
• Are there any errors, sources of noise, or redundancies in the dataset?ASTs extracted by LeanDojo
contain a small number of errors due to potential flaws in Lean’s AST exporting mechanism.
However, they do not have a tangible impact on our work.
• Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites,
tweets, other datasets)? The dataset is self-contained.
• Does the dataset contain data that might be considered confidential (e.g., data that is protected by
legal privilege or by doctor-patient confidentiality, data that includes the content of individuals’
non-public communications)? No.
• Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or
might otherwise cause anxiety? No.
Collection Process.
• How was the data associated with each instance acquired? The data is directly observable by
opening mathlibin VS Code with the Lean plugin. However, we had to instrument Lean to export
the data programmatically.
• What mechanisms or procedures were used to collect the data (e.g., hardware apparatuses or
sensors, manual human curation, software programs, software APIs)? The data was generated by
building a Lean repo using our modified Lean and postprocessing the exported data.
22• Who was involved in the data collection process (e.g., students, crowd workers, contractors), and
how were they compensated (e.g., how much were crowd workers paid)? No manual effort was
involved in the data collection process.
• Over what timeframe was the data collected? The final version of the dataset was generated in
October 2023.
Uses.
• Has the dataset been used for any tasks already? We have used the dataset for training and
evaluating machine learning models on the tasks of premise selection and theorem proving.
• Is there a repository that links to any or all papers or systems that use the dataset? Yes, https:
//leandojo.org.
Distribution.
• Will the dataset be distributed to third parties outside of the entity (e.g., company, institution,
organization) on behalf of which the dataset was created? Yes, the dataset is publicly available on
the Internet.
• How will the dataset be distributed (e.g., tarball on website, API, GitHub)? The dataset can be
downloaded as a tarball.
• Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or
under applicable terms of use (ToU)? The dataset is distributed under CC BY 2.0. The data
generation code is distributed under the MIT license. The dataset was extracted from mathlib,
which depends on lean. Both of them are distributed under the Apache 2.0 license. We include
their licenses in the dataset as attribution (Appendix B.1).
• Have any third parties imposed IP-based or other restrictions on the data associated with the
instances? No.
• Do any export controls or other regulatory restrictions apply to the dataset or to individual
instances? No.
Maintenance.
• Who will be supporting/hosting/maintaining the dataset? The authors of this paper.
• How can the owner/curator/manager of the dataset be contacted (e.g., email address)? Please
contact Kaiyu Yang at kaiyuy@caltech.edu.
• Is there an erratum? No.
• Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)?
Please check https://leandojo.orgfor any update.
• If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them
to do so? Yes, they can use our data generation code, which is publicly available.
B.3 Data Hosting, Licensing, and Maintenance
LeanDojo Benchmark is distributed under the CC BY 2.0 license. The data is hosted on zenodo.org (a
long-term data repository operated by CERN). The LeanDojo tool for data extraction and interaction
with Lean is released at https://github.com/lean-dojo/LeanDojo under the MIT license. Our
model checkpoints are hosted on Hugging Face Hub. LeanDojo’s documentation is hosted on Read the
Docs at https://leandojo.readthedocs.io. LeanDojo’s website (https://leandojo.org)
is the entry point for everything related to it, including any future updates or maintenance.
C Experiments
C.1 Details and Hyperparameters
The premise retriever and tactic generator in ReProver are initialized by the google/byt5-small
checkpoint on Hugging Face. It is a T5-like [97] encoder-decoder Transformer that operates directly
23on UTF-8 bytes without tokenization. We choose ByT5 [44] instead of T5 because Lean code makes
extensive use of Unicode math symbols, which may cause problems to T5’s pretrained tokenizer. The
retriever uses the encoder only, whereas the generator uses both the encoder and the decoder.
In training, we use one NVIDIA A100 GPU with 80GB of memory. The code is implemented in
PyTorch and PyTorch Lightning, with bfloat16 mixed precision and DeepSpeed ZeRO Stage 2 [98].
Both the retriever and the generator are optimized using AdamW [99] with a batch size of 8. In the
first 2,000 steps, the learning rate warms up linearly from 0 to the maximum value. Then it decays to
0 following a cosine schedule. The maximum learning rate is 10−4 for the retriever and 5 × 10−4 for
the generator. When training the retriever, we sample 3 negative premises for each example, including
1 in-file negative premise. When training the generator, we apply dropout to retrieved premises with
a dropout rate of 0.5. Then, we truncate the generator’s input to 2,300 tokens.
During evaluation, the tactic generator is combined with best-first search to find proofs. At each
search step, it produces 64 tactic candidates using beam search. Each tactic is associated with a
log-likelihood score. In best-first search, we prioritize the states by the sum of log-likelihoods of
tactics leading to that state.
C.2 The GPT-4 Baseline
Now we describe the GPT-4 [ 27] baseline in Sec. 6. Similar to ReProver, it is a tactic gen-
erator combined with best-first search. However, the tactic generator is based on GPT-4’s
capability to follow instructions in zero-shot. Specifically, given a proof state, we use the
following prompt to instruct GPT-4 to produce a list of tactics, each paired with a confidence score:
Prompt Template:
You are an expert in Lean3 theorem proofs. We are trying to solve the
Lean3 theorem ‘THEOREM_FULL_NAME ’ from the mathlib file ‘FILE_PATH ’.
The current tactic state is: ‘ TACTIC_STATE ’. Suggest exactly 35 unique
tactics to progress in solving ‘THEOREM_FULL_NAME ’, along with their
confidence levels as a float between 0 and 1. Rank them in order of
effectiveness. Present the tactics and their confidence levels as comma-
separated tuples in this format: #(tactic_{1}, confidence_{1})#, #(tac-
tic_{2}, confidence_{2})#, ..., #(tactic_{35 }, confidence_{35 })#.
We adapted the prompt to a particular theorem and state by substituting the variables with the
appropriate values. Given the inherent variability in GPT-4’s outputs, we requested 35 and filtered
out invalid ones. We used a token length limit of 1,024 and kept all other API parameters at their
default values. Below are a few example prompts and GPT-4’s responses:
Example Prompt 1:
You are an expert in Lean3 theorem proofs. We are trying to solve the
Lean3 theorem ‘polynomial.chebyshev.aeval_U’ from the mathlib file
‘mathlib/src/analysis/special_functions/trigonometric/chebyshev.lean’.
The current tactic state is: ‘R : Type u_1, A : Type u_2, _inst_1
: comm_ring R, _inst_2 : comm_ring A, _inst_3 : algebra R A, x
: A, n : N ⊢ ⇑(aeval x) (chebyshev.U R n) = eval x (chebyshev.U A
n)’. Suggest exactly 35 unique tactics to progress in solving ‘polyno-
mial.chebyshev.aeval_U’, along with their confidence levels as a float
between 0 and 1. Rank them in order of effectiveness. Present the
tactics and their confidence levels as comma-separated tuples in this
format: #(tactic_{1}, confidence_{1})#, #(tactic_{2}, confidence_{2})#,
..., #(tactic_{35 }, confidence_{35 })#.
24Example Response 1:
#(rw chebyshev.U, 0.9)#, #(simp only [chebyshev.U], 0.9)#, #(simp_rw
chebyshev.U, 0.8)#, #(apply aeval_add, 0.75)#, #(apply aeval_mul,
0.75)#, #(apply aeval_sub, 0.75)#, #(apply aeval_X, 0.7)#, #(ap-
ply aeval_C, 0.7)#, #(apply aeval_pow, 0.7)#, #(rw \u2190aeval_pow,
0.65)#, #(rw ˘2190aeval_mul, 0.65)#, #(rw \u2190aeval_add, 0.65)#, #(rw
\u2190aeval_sub, 0.65)#, #(apply algebra_map_eq, 0.6)#, #(intro ae-
val_algebra_map_eq, 0.6)#, #(apply eval_map, 0.55)#, #(rw eval_map,
0.55)#, #(apply aeval_eq_eval, 0.55)#, #(rw_for_target aeval_eq_eval,
0.5)#, #(apply aeval_eq_eval’, 0.5)#, #(rw_for_target aeval_eq_eval’,
0.5)#, #(apply aeval_commutes, 0.45)#, #(rw aeval_commutes, 0.45)#, #(ap-
ply aeval_eq_of_is_scalar_tower, 0.4)#, #(rw is_scalar_tower.aeval_apply,
0.4)#, #(try_for 100 exact_mod_cast chebyshev.U_eq_refl, 0.35)#, #(simp
* at *, 0.3)#, #(simp int.coe_nat_ring_hom, 0.25)#, #(apply cheby-
shev.U_is_polynomial, 0.2)#, #(rw_algebra_map_eq_of_ring_hom, 0.15)#,
#(unfold chebyshev.U, 0.1)#, #(congr, 0.05)#, #(change (chebyshev.U R
n).eval x = chebyshev.U A n, 0.05)#, #(change (\u21d1(aeval x) (cheby-
shev.U R n) = chebyshev.U A n), 0.05)#, #(apply_fun aeval x, 0.05)#,
#(apply_fun eval x, 0.05)#, #(simp context_md := reducible [], 0.01)#
Example Prompt 2:
You are an expert in Lean3 theorem proofs. We are trying to solve
the Lean3 theorem ‘euler_sine.integral_cos_pow_pos’ from the math-
lib file ‘mathlib/src/analysis/special_functions/trigonometric
/euler_sine_prod.lean’. The current tactic state is: ‘n : N ⊢ 0 <R
(x : R) in 0..π / 2, cos x ˆ n’. Suggest exactly 35 unique tactics
to progress in solving ‘euler_sine.integral_cos_pow_pos’, along with
their confidence levels as a float between 0 and 1. Rank them in or-
der of effectiveness. Present the tactics and their confidence levels
as comma-separated tuples in this format: #(tactic_1, confidence_1)#,
#(tactic_2, confidence_2)#, ..., #(tactic_35, confidence_35)#.
Example Response 2:
#(exact integral_pos_of_antimono_increasing_on cos.pow_antimono_ in-
creasing_on_cos_even_nnreal, 0.99)#, #(apply integral_pos_of_antimono_
increasing_on, 0.95)#, #(interval_integrable_integral_pos, 0.90)#, #(ex-
act integral_pos_pre, 0.85)#, #(interval_integ_timedout, 0.80)#, #(in-
terval_integrable_pos, 0.75)#, #(exact integral_pos_of_nonneg_dint_val,
0.70)#, #(exact interval_integrable_pos_left, 0.65)#, #(exact inter-
val_ integrable_pos_right, 0.60)#, #(exact integral_pos_of_nonneg_f,
0.55)#, #(rw integral_on_interval, 0.50)#, #(rw interval_integral_pos_
of_supporting, 0.45)#, #(rw integral_pos_of_interval_integrable_pos,
0.40)#, #(apply integral_ pos_of_antimono_increasing_on_cos_even_nnreal,
0.35)#, #(norm_cast apply_pos, 0.30)#, #(apply_pos_of_nonneg_pos,
0.25)#, #(rw interval_integral_pos_of_antimono, 0.20)#, #(use
cos_slow_mono_on_pos, 0.15)#, #(apply cos_slow_mono_on_pos, 0.10)#, #(ex-
act integral_pos_pre_cos_pow_even, 0.09)#, #(norm_cast apply_nonneg_pos,
0.08)#, #(rw integral_pos_of_ monotone_integrable, 0.07)#, #(use
cos_pos_constant_sequence, 0.06)#, #(apply cos_pos_constant_sequence,
0.05)#, #(use integral_pos_of_ascending_sequence, 0.04)#,
#(apply integral_pos_of_ascending_sequence, 0.03)#, #(inter-
val_integral_pos_of_antimono_increasing_on_pos, 0.02)#, #(norm_cast
apply_pos_of_nonzero_pos, 0.01)#, #(apply_pos_of_nonzero_pos, 0.005)#,
#(rw integral_pos_of_nonneg_f_monotone, 0.001)#
25Data contamination is possible. Our GPT-4 experiments were performed in 2023, but many theorems
and proofs in the dataset have been publicly available on GitHub before GPT-4’s data cutoff date
(September 2021).
C.3 Justifications for Not Comparing with Existing LLM-Based Provers
In Table 2, we do not empirically compare ReProver with any existing LLM-based prover. Unfor-
tunately, such a comparison is infeasible. Provers targeting different proof assistants are generally
not comparable, so we focus the discussion on the three existing provers in Lean [16, 17, 19]. Most
importantly, they are impossible to reproduce with reasonable effort, due to private code and pretrain-
ing data. Therefore, the only potential comparison is to evaluate ReProver under their experimental
settings and compare with the numbers reported in their papers. However, that is also impractical for
numerous reasons:
• The data is different. All existing methods used an outdated version of mathlibmore than two
years ago. We cannot use LeanDojo to extract data from this version. As mentioned in Sec. 4,
LeanDojo only supports repos released after March 24, 2022. Also, we cannot use their dataset
directly, since it does not contain premise information required by ReProver.
• Lample et al. [17] trained on a synthetic dataset named Equations, which is not publicly available.
• All existing methods co-train the tactic generator on auxiliary tasks from the PACT dataset [16].
Co-training increases the data/compute requirements by an order of magnitude, which cannot
be afforded by us (or probably most academic labs). All existing methods were developed by
researchers in the industry.
• Polu et al. [ 19] and Lample et al. [ 17] further finetuned their models on new proofs collected
through online interaction with Lean, whereas our method is only trained on human-written proofs.
• The tool for interacting with Lean may impact the performance. Han et al. [16] and Polu et al. [19]
used lean-gym, which has severe limitations (Appendix A.2). Lample et al. [17] developed their
own private tool, which is not publicly available.
Most of these difficulties are due to the private nature of existing methods. By releasing our code and
models, we take a major step in establishing accessible baselines for future work to build upon.
C.4 Evaluation on MiniF2F and ProofNet
We evaluate our ReProver model on MiniF2F [28] and ProofNet [29] (Sec. 6) to test its capability
in proving theorems outside its training data distribution. We use the same hyperparameters and
evaluation setup as the previous experiments (Appendix C.1).
MiniF2F. We use the commit 5271ddec788677c815cf818a06f368ef6498a106 of Meta’s version of
MiniF2F [17]. ReProver achieves a Pass@1 of 26.5% on the test set, which is competitive with state-
of-the-art methods without reinforcement learning (25.9% in Polu et al. [19]). Moreover, ReProver
can prove 33 theorems that currently do not have Lean proofs (examples in Fig. B). For the complete
list of 33 new proofs, please see our pull request to MiniF2F.
There are caveats about quantitatively comparing ReProver with existing methods on MiniF2F. Many
difficulties in Appendix C.3 still apply, e.g., different tools for interacting with Lean may impact the
performance. Also, MiniF2F is a test-only dataset without training theorems, and existing methods
focus on reinforcement learning (RL) to learn from proofs collected via online interaction with the
proof assistant [17, 19]. In contrast, ReProver is trained via supervised learning on a static dataset, so
we only compare with the non-RL baseline in existing methods (Polu et al. [19] achieves a Pass@1 of
25.9% without RL and 29.6% with RL). Furthermore, we do not compare with Lample et al. [17] due
to differences in the evaluation metric. They use Pass@64, which requires running the prover on each
theorem 64 times. We use Pass@1, and it already takes one day for a single evaluation on MiniF2F’s
test set. Therefore, evaluating Pass@64 would be too computationally expensive for the resources we
have access to. Finally, MiniF2F is available in multiple proof assistants [18, 69, 70]. Results across
different proof assistants are not comparable, so we only compare with existing work in Lean.
ProofNet. We use the commit e8645aa830ce17c33a8b8482a8195f0f97d6a74a of ProofNet. Re-
Prover can prove 48 out of 349 theorems, achieving a Pass@1 of 13.8%, which is the first reported
26Figure B: Examples of new proofs discovered by ReProver on MiniF2F [28].
theorem proving result on ProofNet. Moreover, 39 out of the 48 proved theorems do not have existing
Lean proofs (examples in Fig. C), and 3 of them can only be proved with the help of premise retrieval
(Fig. D). We have contributed the 39 new proofs to ProofNet, which helped them reveal and fix
problems in the formalization of 7 theorems (details in our pull request).
D LeanDojo for Lean 4
Lean 3 and Lean 4 are two incompatible major versions of Lean,12 and both are widely used. Lean
3 was the latest stable version until recently (June 2023). Also, Lean 3 and Lean 4 have separate
versions of mathlib. The Lean/mathlib community has recently finished porting theorems and proofs
from mathlib 3 to mathlib 4 [100]. Therefore, Lean 3 will gradually become deprecated, and future
Lean projects will be using Lean 4. Therefore, it is important for LeanDojo to support Lean 4.
Since Lean 4 is relatively new, we are not aware of any existing work on learning-based theorem
proving in Lean 4. Furthermore, no existing tool is available for extracting data from Lean 4.
LeanDojo fills in this gap and fully supports Lean 4. Given any repo in Lean 4, LeanDojo can extract
12https://leanprover.github.io/lean4/doc/lean3changes.html
27Figure C: Examples of new proofs discovered by ReProver on ProofNet [29].
data, including file dependencies, ASTs, proof states, tactics, and premise information. In addition, it
enables the model to interact with Lean 4 through tactics, in the same way as Lean 3 (Sec. 4).
Similar to constructing the Lean 3 version of LeanDojo Benchmark, we extract data from the commit
3ce43c18f614b76e161f911b75a3e1ef641620ff of mathlib4 released on October 21, 2023. The
resulting dataset is named LeanDojo Benchmark 4. It is released under the CC BY 2.0 license and
hosted on zenodo.org with DOI “10.5281/zenodo.8040109”. LeanDojo Benchmark 4 consists
of 102,514 theorems/proofs, 213,067 tactics, and 152,695 premises. We use 2,000 theorems for
28Figure D: Three new proofs discovered by ReProver on ProofNet [ 29] that cannot be
found by a baseline without premise retrieval. All of the three proofs rely on premises:
“finite_field.prod_univ_units_id_eq_neg_one”
, “norm_add_sq_real”, “norm_sub_pow_two_real”, and “exists_countable_basis”.
validation, 2,000 theorems for testing, and the rest for training. LeanDojo Benchmark 4 also has two
different data splits: randomand novel_premises.
We use LeanDojo Benchmark 4 to train and evaluate our method. The model architectures and
experimental details are the same as those in Sec. 6. Results on premise selection are in Table B, and
results on theorem proving are in Table C.
Table B: Premise selection testing performance on LeanDojo Benchmark 4 (Lean 3 results in
Table 1). We train and evaluate two models independently using different data splits (randomand
novel_premises). R@k is the recall for the top k retrieved premises, and MRR is the mean
reciprocal rank metric.
Method random novel_premises
R@1 R@10 MRR R@1 R@10 MRR
Ours 12.8 34.7 0.29 9.8 32.1 0.24
Table C: Theorem proving Pass@1 (%) on the testing data of LeanDojo Benchmark 4 (Lean 3 results
in Table 2).
Method random novel_premises
ReProver 48.6 19.9
W/o retrieval 44.5 16.2
E ChatGPT Plugin for Theorem Proving
LeanDojo provides a general tool for interacting with Lean programmatically. As a demo of how it
might bridge LLMs and theorem proving, we build a ChatGPT plugin [101] enabling ChatGPT to
prove theorems by interacting with Lean through LeanDojo. Plugin developers can wrap any software
29as a web service and describe its APIs to ChatGPT. Then, ChatGPT can automatically call the APIs
and incorporate the results into the response to the user. Below is a summary of our API description
corresponding to the interface in Sec. 4.
Title: Lean
Description: Plugin for proving user-specified theorems
automatically by interacting with Lean. The user enters
information of how to find a theorem (e.g., theorem name
and file path). Based on the user’s input, ChatGPT first
initializes the proof search with the given theorem as the
initial state. Then, ChatGPT will first explain the choice
for the next tactic step using LaTeX and run that tactic
step to the state. If the current state is not promising,
ChatGPT can backtrack to previous states by decrementing
the "state_id" parameter. If applying tactics to the
current state specified by the "state_id" parameter returns
an error message, ChatGPT should explain the error, and if
repetitive errors occur, ChatGPT should decrement the "
state_id" parameter and try a different approach on a
previous state. The theorem is successfully proved if there
are no unsolved goals in the current state.
Endpoints:
initialize_proof_search: Given the theorem name and file
path of a Lean theorem, initialize the proof search.
The response includes the initial state and its state
ID.
Args:
theorem_name (string): The name of the target theorem
to prove.
theorem_file_path (string): The file path of the target
theorem.
run_tactic: Run a tactic on a state (specified by its state
ID), assuming the proof search has been initialized
and some state is available. The response is either the
next state and its state ID or an error message, in
which ChatGPT should explain the error and consider
decrementing the "state_id".
Args:
state_id (string): The ID of the state on which to run
the tactic.
tactic (string): The tactic to run on a state (
specified by its state ID), assuming the proof
search has been initialized.
After exposing the APIs to ChatGPT, we can ask it to prove theorems by specifying the theorem’s
name and path in any public Lean repo on GitHub. Fig. E–L show an example with the GPT-3.5
version of ChatGPT. And Fig. M–O are the same example with the GPT-4 version. The captions
provide detailed step-by-step explanations.
We highlight a few key strengths of ChatGPT observed in multiple examples we evaluated. First,
unlike specialized methods for theorem proving (this paper and its prior works), ChatGPT interleaved
informal mathematics with formal proof steps. This resembles how humans interact with proof
assistants and opens up new avenues for integrating natural language and formal theorem proving.
Second, ChatGPT demonstrated impressive capability in explaining error messages from Lean that
are quite opaque even to humans. It was able to incorporate the error message to refine its proof
strategy. Last, ChatGPT’s behavior is more steerable than specialized provers. In Fig. E, we simply
gave it the theorem to prove, but we could also provide more detailed instructions. For example, we
30could say: “Please describe a high-level proof plan before trying any tactic.” This kind of steerability
enables future research on prompt engineering for theorem proving, and we have already seen initial
benefits in an ongoing work named Sagredo.13
However, these strengths by no means imply ChatGPT can already solve theorem proving. In fact, it
failed to find a proof for most theorems we tried. Hallucination was common. In Fig. L, ChatGPT
falsely asserted the theorem was proved, while we knew it was not, by looking at LeanDojo’s response.
This demonstrates the value of theorem proving as a rigorous benchmark for addressing LLMs’
hallucination problem. Another key limitation of ChatGPT was its inability to search systematically
in a large space. We frequently found it stuck to an unpromising path when the correct solution could
be found by backtracking and exploring alternative paths. This behavior is consistent with the general
observation that LLMs are weak at search and planning. Addressing this weakness is an active area
of research [102].
We emphasize a few caveats about our study of theorem proving with ChatGPT. First, data con-
tamination is likely. Many theorems we evaluated have been publicly available on GitHub before
ChatGPT’s data cutoff date. Therefore, ChatGPT may have seen them in training. Second, our study
is exploratory. A more detailed and quantitative study is needed to characterize ChatGPT’s capability
in theorem proving. Such a study with ChatGPT plugins is challenging, as plugins currently only
support interaction through the browser. Also, OpenAI has taken measures to block automated access
by bots. Using humans may be an option, but that is beyond the scope of this paper.
Figure E: (ChatGPT-3.5, 1/8) After receiving the theorem to prove, ChatGPT first called
“initialize”, which returned the initial state. Then it tried to interpret the theorem in natural
language. Note that it made a mistake here. The theorem was about natural numbers (N), not complex
numbers (C).
13https://www.youtube.com/watch?v=CEwRMT0GpKo
31Figure F: (ChatGPT-3.5, 2/8) ChatGPT tried to rewrite the goal using the lemma “b + c = c + b”.
This was a reasonable but incorrect move. After receiving the error message from Lean, ChatGPT
explained the error in natural language. Here the explanation is quite accurate, which is impressive
given that the error message looks opaque to anyone not familiar with Lean.
F Limitations and Future Work
Our work is one step toward unlocking the potential of LLMs for generating verifiable formal proofs,
and we see abundant space for future exploration. A learning-based prover is a complex system
consisting of multiple components: data extraction, interaction with proof assistants, model training,
and proof search. While navigating the design space spanned by various components, we err on the
side of simplicity and efficiency, instead of pushing performance to the limit. This helps us deliver
a reliable, open, and accessible system, laying the foundation for further research. There are many
directions in which the system can be improved, and we discuss a few of them here.14
Stronger LLMs. Our backbone model, ByT5 [44], was published in 2021 and has 299M parameters,
which is not very large by today’s standard. Recently, there have been a plethora of open-source
LLMs demonstrating strong capabilities in writing code, e.g., CodeGen [103], StarCoder [94], and
CodeGeeX [104]. We are excited to see how they might impact theorem proving and, more generally,
how far we can go by pushing the limit of the model/data scale.
ByT5’s tokenizer-free nature helps us sidestep the difficulty with pretrained tokenizers that may
not work well for Lean’s Unicode-rich code. However, treating texts as raw bytes makes the
sequence length much longer than necessary. Long sequences harm efficiency, as Transformers scale
quadratically w.r.t. the sequence length, which may become a bigger problem when we further scale
up the model. To solve the issue, it might be helpful to pretrain a customized tokenizer or adopt more
advanced tokenizer-free models such as MegaByte [105].
Our ReProver model is based on the pretraining-finetuning paradigm. Recent work on instruction-
following LLMs such as GPT-4 [27] has led to successes in many applications by prompting the
model without any finetuning. Our preliminary results show that GPT-4 and ChatGPT (Appendix C.2
14Additional limitations: https://leandojo.readthedocs.io/en/latest/limitations.html
32Figure G: (ChatGPT-3.5, 3/8) Then it tried to prove the theorem using “ring”. This was another good
move. The ringtactic can prove this theorem, but Lean could not find it since it was not imported
into the current file. Again, ChatGPT was able to interpret the error message correctly and concluded
that ringwas not available. Next, it tried another tactic but failed again.
and E) cannot solve theorem proving out of the box and are currently far behind finetuned models.
However, the way we prompt these models is quite naive, and better strategies, such as Tree of
Thoughts [102], may lead to further improvements. We consider theorem proving as a promising task
for studying LLMs’ capabilities in planning and search.
Improving Premise Retrieval. ReProver uses DPR [26] to retrieve premises and fuses them with
the current proof state by concatenation. This architecture is simple and effective but does not scale
to a large number of retrieved premises. With a length limit of 2,300 tokens, we can fit only 10–15
premises into the input of the tactic generator. To mitigate the problem, we may need an architecture
that fuses the retrieved premises in the hidden space, e.g., Fusion-in-Decoder [106].
33Figure H: (ChatGPT-3.5, 4/8) ChatGPT made another two failed attempts. Here, the second attempt
had the same problem as in Fig. E (“+” is left associative).
In addition, one can also switch from DPR to radically different retrieval architectures. For example,
generative retrieval [107–109] is a recent class of models performing retrieval by directly predicting
the document IDs, which could be the premise names in our task.
Limitations of Imitating Human-Written Proofs. Human-written proofs extracted by LeanDojo
provide valuable data for training the prover. However, we have also observed limitations of using
them as the sole training target:
First, they are relatively scarce for today’s data-hungry LLMs. LeanDojo Benchmark has 98,734
proofs, covering a large portion of available data in Lean (as of October 2023). The number of proofs
34Figure I: (ChatGPT-3.5, 5/8) ChatGPT tried to simplify the goal using the simp tactic but failed.
Then it tried the “rw add_comm a b” tactic, which successfully rewrote the goal from “a + b + c
= a + c + b” to “b + a + c = a + c + b”.
in other proof assistants has the same order of magnitude (tens or hundreds of thousands). Due to
limited data, we cannot constantly improve the performance simply by scaling up the model size.
Second, theorem proving in proof assistants is an interactive process, but the proof only captures the
final successful trajectory. Without the intermediate history of trial and error, it can be quite opaque
how final proofs are derived. Therefore, tactics in human-written proofs can be difficult for the model
to learn from. Third, models trained on proofs in one project often struggle to generalize to theorems
in new domains [9, 20], e.g., from mathlibto MiniF2F and ProofNet (Appendix C.4).
35Figure J: (ChatGPT-3.5, 6/8) Then it successfully performed another rewrite from “b + a + c = a
+ c + b” to “b + a + c = c + a + b”.
To overcome these limitations, existing work has explored learning from auxiliary data or data
collected via online interaction with the proof assistant. For example, Proof Artifact Co-Training
(PACT) co-trains the tactic generator on nine auxiliary tasks, such as predicting types and theorem
names [16]. MetaGen [38] trains a neural network to generate synthetic theorems/proofs as training
data in the Metamath proof assistant [110]. Polu et al. [19] and Lample et al. [17] improve the prover
by training it on successful proofs found by itself. Incorporating these techniques into our system
may lead to substantial improvements.
36Figure K: (ChatGPT-3.5, 7/8) Another rewrite tactic, transforming the goal to “a + b + c = c +
a + b”.
Figure L: (ChatGPT-3.5, 8/8) It performed one last rewrite, which brought the goal back to the
original goal “a + b + c = a + c + b”. At this point, ChatGPT went back to the starting point
without making meaningful progress. However, it asserted that the theorem had been successfully
proved, even though the response from LeanDojo clearly indicated “proof_finished: False ”.
37Figure M: (ChatGPT-4, 1/3) The same theorem using the GPT-4 version of ChatGPT. Despite a minor
error when interpreting the theorem (natural numbers, not integers), it generated a sensible tactic,
rewriting the goal to “a + (b + c) = a + c + b”.
38Figure N: (ChatGPT-4, 2/3) A good move, rewriting the goal to “a + (c + b) = a + c + b”.
39Figure O: (ChatGPT-4, 3/3) It tried therefltactic but failed. reflrequires the goal to be an equation
whose both sides are identical up to trivial transformations. However, “a + (c + b) = a + c +
b” is not trivial since “+” is left associative. ChatGPT was able to interpret this error accurately and
finish the proof using the correct premise “add_assoc”.
40

Retrieval-Augmented Generation for
Knowledge-Intensive NLP Tasks
Patrick Lewis†‡ , Ethan Perez?,
Aleksandra Piktus† , Fabio Petroni† , Vladimir Karpukhin† , Naman Goyal† , Heinrich Küttler† ,
Mike Lewis† , Wen-tau Yih† , Tim Rocktäschel†‡ , Sebastian Riedel†‡ , Douwe Kiela†
† Facebook AI Research;‡ University College London;?New York University;
plewis@fb.com
Abstract
Large pre-trained language models have been shown to store factual knowledge
in their parameters, and achieve state-of-the-art results when ﬁne-tuned on down-
stream NLP tasks. However, their ability to access and precisely manipulate
knowledge is still limited, and hence on knowledge-intensive tasks, their perfor-
mance lags behind task-speciﬁc architectures. Additionally, providing provenance
for their decisions and updating their world knowledge remain open research prob-
lems. Pre-trained models with a differentiable access mechanism to explicit non-
parametric memory can overcome this issue, but have so far been only investigated
for extractive downstream tasks. We explore a general-purpose ﬁne-tuning recipe
for retrieval-augmented generation (RAG) — models which combine pre-trained
parametric and non-parametric memory for language generation. We introduce
RAG models where the parametric memory is a pre-trained seq2seq model and
the non-parametric memory is a dense vector index of Wikipedia, accessed with
a pre-trained neural retriever. We compare two RAG formulations, one which
conditions on the same retrieved passages across the whole generated sequence,
and another which can use different passages per token. We ﬁne-tune and evaluate
our models on a wide range of knowledge-intensive NLP tasks and set the state of
the art on three open domain QA tasks, outperforming parametric seq2seq models
and task-speciﬁc retrieve-and-extract architectures. For language generation tasks,
we ﬁnd that RAG models generate more speciﬁc, diverse and factual language than
a state-of-the-art parametric-only seq2seq baseline.
1 Introduction
Pre-trained neural language models have been shown to learn a substantial amount of in-depth knowl-
edge from data [47]. They can do so without any access to an external memory, as a parameterized
implicit knowledge base [51, 52]. While this development is exciting, such models do have down-
sides: They cannot easily expand or revise their memory, can’t straightforwardly provide insight into
their predictions, and may produce “hallucinations” [38]. Hybrid models that combine parametric
memory with non-parametric (i.e., retrieval-based) memories [20, 26, 48] can address some of these
issues because knowledge can be directly revised and expanded, and accessed knowledge can be
inspected and interpreted. REALM [20] and ORQA [31], two recently introduced models that
combine masked language models [8] with a differentiable retriever, have shown promising results,
34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.TheDiYineComed\(x) T
QXeU\EQcRdeU
T([)
MIPS pθ
GeneUaWoU¬Sѡ(PaUaPeWULc)
MaUgin-ali]eThiV14WhcenWXU\ZoUkiVdiYidedinWo3VecWionV:"InfeUno","PXUgaWoUio"&"PaUadiVo"(y)
End-to-End Backprop through T and¬pθ
BaUackObamaZaVboUninHaZaii.(x)FacW VeUiÀcaWiRQ: FacW QXeU\
VXppoUWV(y)
QXeVWiRQ GeQeUaWiRQ
FacW VeUiÀcaWiRQ:LabeO GeQeUaWiRQ
DRcXmeQWIQde[
Define"middleeaU"(x)
QXeVWiRQ AQVZeUiQg:QXeVWiRQ QXeU\
ThemiddleeaUinclXdeVWheW\mpaniccaYiW\andWheWhUeeoVVicleV.(y)QXeVWiRQ AQVZeUiQg:AQVZeU GeQeUaWiRQReWUieYeU Sη(NRQ-PaUaPeWULc) z4z3z2z1
d(])
JeRSaUd\ QXeVWiRQGeQeUaWiRQ:AQVZeU QXeU\
Figure 1: Overview of our approach. We combine a pre-trained retriever (Query Encoder+ Document
Index) with a pre-trained seq2seq model (Generator) and ﬁne-tune end-to-end. For queryx, we use
Maximum Inner Product Search (MIPS) to ﬁnd the top-K documentszi. For ﬁnal predictiony, we
treat z as a latent variable and marginalize over seq2seq predictions given different documents.
but have only explored open-domain extractive question answering. Here, we bring hybrid parametric
and non-parametric memory to the “workhorse of NLP,” i.e. sequence-to-sequence (seq2seq) models.
We endow pre-trained, parametric-memory generation models with a non-parametric memory through
a general-purpose ﬁne-tuning approach which we refer to as retrieval-augmented generation (RAG).
We build RAG models where the parametric memory is a pre-trained seq2seq transformer, and the
non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural
retriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The
retriever (Dense Passage Retriever [26], henceforth DPR) provides latent documents conditioned on
the input, and the seq2seq model (BART [32]) then conditions on these latent documents together with
the input to generate the output. We marginalize the latent documents with a top-K approximation,
either on a per-output basis (assuming the same document is responsible for all tokens) or a per-token
basis (where different documents are responsible for different tokens). Like T5 [51] or BART, RAG
can be ﬁne-tuned on any seq2seq task, whereby both the generator and retriever are jointly learned.
There has been extensive previous work proposing architectures to enrich systems with non-parametric
memory which are trained from scratch for speciﬁc tasks, e.g. memory networks [64, 55], stack-
augmented networks [25] and memory layers [30]. In contrast, we explore a setting where both
parametric and non-parametric memory components are pre-trained and pre-loaded with extensive
knowledge. Crucially, by using pre-trained access mechanisms, the ability to access knowledge is
present without additional training.
Our results highlight the beneﬁts of combining parametric and non-parametric memory with genera-
tion forknowledge-intensive tasks—tasks that humans could not reasonably be expected to perform
without access to an external knowledge source. Our RAG models achieve state-of-the-art results
on open Natural Questions [29], WebQuestions [3] and CuratedTrec [2] and strongly outperform
recent approaches that use specialised pre-training objectives on TriviaQA [24]. Despite these being
extractive tasks, we ﬁnd that unconstrained generation outperforms previous extractive approaches.
For knowledge-intensive generation, we experiment with MS-MARCO [1] and Jeopardy question
generation, and we ﬁnd that our models generate responses that are more factual, speciﬁc, and
diverse than a BART baseline. For FEVER [56] fact veriﬁcation, we achieve results within 4.3% of
state-of-the-art pipeline models which use strong retrieval supervision. Finally, we demonstrate that
the non-parametric memory can be replaced to update the models’ knowledge as the world changes.1
2 Methods
We explore RAG models, which use the input sequencex to retrieve text documentsz and use them
as additional context when generating the target sequencey. As shown in Figure 1, our models
leverage two components: (i) a retrieverp⌘(z|x) with parameters⌘ that returns (top-K truncated)
distributions over text passages given a queryx and (ii) a generatorp✓(yi|x, z, y1:i 1) parametrized
1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform-
ers Library [66] and can be found athttps://github.com/huggingface/transformers/blob/master/
examples/rag/. An interactive demo of RAG models can be found athttps://huggingface.co/rag/
2by ✓ that generates a current token based on a context of the previousi   1 tokens y1:i 1, the original
input x and a retrieved passagez.
To train the retriever and generator end-to-end, we treat the retrieved document as a latent variable.
We propose two models that marginalize over the latent documents in different ways to produce a
distribution over generated text. In one approach,RAG-Sequence, the model uses the same document
to predict each target token. The second approach,RAG-Token, can predict each target token based
on a different document. In the following, we formally introduce both models and then describe the
p⌘ and p✓ components, as well as the training and decoding procedure.
2.1 Models
RAG-Sequence Model The RAG-Sequence model uses the same retrieved document to generate
the completesequence. Technically, it treats the retrieved document as a single latent variable that
is marginalized to get the seq2seq probabilityp(y|x) via a top-K approximation. Concretely, the
top K documents are retrieved using the retriever, and the generator produces the output sequence
probability for each document, which are then marginalized,
pRAG-Sequence(y|x) ⇡
X
z2top-k(p(·| x))
p⌘(z|x)p✓(y|x, z)=
X
z2top-k(p(·| x))
p⌘(z|x)
NY
i
p✓(yi|x, z, y1:i 1)
RAG-Token Model In the RAG-Token model we can draw a different latent document for each
target token and marginalize accordingly. This allows the generator to choose content from several
documents when producing an answer. Concretely, the top K documents are retrieved using the
retriever, and then the generator produces a distribution for the next output token for each document,
before marginalizing, and repeating the process with the following output token, Formally, we deﬁne:
pRAG-Token(y|x) ⇡
NY
i
X
z2top-k(p(·| x))
p⌘(z|x)p✓(yi|x, zi,y 1:i 1)
Finally, we note that RAG can be used for sequence classiﬁcation tasks by considering the target class
as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent.
2.2 Retriever: DPR
The retrieval componentp⌘(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture:
p⌘(z|x) / exp
 
d(z)>q(x)
 
d(z)= BERTd(z), q(x)= BERTq(x)
where d(z) is a dense representation of a document produced by a BERTBASE document encoder[8],
and q(x) a query representation produced by aquery encoder, also based on BERTBASE. Calculating
top-k(p⌘(·| x)), the list ofk documents z with highest prior probabilityp⌘(z|x), is a Maximum Inner
Product Search (MIPS) problem, which can be approximately solved in sub-linear time [23]. We use
a pre-trained bi-encoder from DPR to initialize our retriever and to build the document index. This
retriever was trained to retrieve documents which contain answers to TriviaQA [24] questions and
Natural Questions [29]. We refer to the document index as thenon-parametric memory.
2.3 Generator: BART
The generator componentp✓(yi|x, z, y1:i 1) could be modelled using any encoder-decoder. We use
BART-large [32], a pre-trained seq2seq transformer [58] with 400M parameters. To combine the input
x with the retrieved contentz when generating from BART, we simply concatenate them. BART was
pre-trained using a denoising objective and a variety of different noising functions. It has obtained
state-of-the-art results on a diverse set of generation tasks and outperforms comparably-sized T5
models [32]. We refer to the BART generator parameters✓ as theparametric memoryhenceforth.
2.4 Training
We jointly train the retriever and generator components without any direct supervision on what
document should be retrieved. Given a ﬁne-tuning training corpus of input/output pairs(xj,y j), we
3minimize the negative marginal log-likelihood of each target,P
j  log p(yj|xj) using stochastic
gradient descent with Adam [28]. Updating the document encoderBERTd during training is costly as
it requires the document index to be periodically updated as REALM does during pre-training [20].
We do not ﬁnd this step necessary for strong performance, and keep the document encoder (and
index) ﬁxed, only ﬁne-tuning the query encoder BERTq and the BART generator.
2.5 Decoding
At test time, RAG-Sequence and RAG-Token require different ways to approximatearg maxy p(y|x).
RAG-Token The RAG-Token model can be seen as a standard, autoregressive seq2seq genera-
tor with transition probability:p0
✓(yi|x, y1:i 1)= P
z2top-k(p(·| x)) p⌘(zi|x)p✓(yi|x, zi,y 1:i 1) To
decode, we can plugp0
✓(yi|x, y1:i 1) into a standard beam decoder.
RAG-Sequence For RAG-Sequence, the likelihoodp(y|x) does not break into a conventional per-
token likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for
each documentz, scoring each hypothesis usingp✓(yi|x, z, y1:i 1). This yields a set of hypotheses
Y , some of which may not have appeared in the beams of all documents. To estimate the probability
of an hypothesisy we run an additional forward pass for each documentz for whichy does not
appear in the beam, multiply generator probability withp⌘(z|x) and then sum the probabilities across
beams for the marginals. We refer to this decoding procedure as “Thorough Decoding.” For longer
output sequences,|Y | can become large, requiring many forward passes. For more efﬁcient decoding,
we can make a further approximation thatp✓(y|x, zi) ⇡ 0 where y was not generated during beam
search fromx, zi. This avoids the need to run additional forward passes once the candidate setY has
been generated. We refer to this decoding procedure as “Fast Decoding.”
3 Experiments
We experiment with RAG in a wide range of knowledge-intensive tasks. For all experiments, we use
a single Wikipedia dump for our non-parametric knowledge source. Following Lee et al.[31] and
Karpukhin et al.[26], we use the December 2018 dump. Each Wikipedia article is split into disjoint
100-word chunks, to make a total of 21M documents. We use the document encoder to compute an
embedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical
Navigable Small World approximation for fast retrieval [37]. During training, we retrieve the top
k documents for each query. We considerk 2{ 5, 10} for training and setk for test time using dev
data. We now discuss experimental details for each task.
3.1 Open-domain Question Answering
Open-domain question answering (QA) is an important real-world application and common testbed
for knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs(x, y)
and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to
the popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved
documents, relying primarily on non-parametric knowledge. We also compare to “Closed-Book
QA” approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead
relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural
Questions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As
CT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG
model. We use the same train/dev/test splits as prior work [31, 26] and report Exact Match (EM)
scores. For TQA, to compare with T5 [52], we also evaluate on the TQA Wiki test set.
3.2 Abstractive Question Answering
RAG models can go beyond simple extractive QA and answer questions with free-form, abstractive
text generation. To test RAG’s natural language generation (NLG) in a knowledge-intensive setting,
we use the MSMARCO NLG task v2.1 [43]. The task consists of questions, ten gold passages
retrieved from a search engine for each question, and a full sentence answer annotated from the
retrieved passages. We do not use the supplied passages, only the questions and answers, to treat
4MSMARCO as an open-domain abstractive QA task. MSMARCO has some questions that cannot be
answered in a way that matches the reference answer without access to the gold passages, such as
“What is the weather in V olcano, CA?” so performance will be lower without using gold passages.
We also note that some MSMARCO questions cannot be answered using Wikipedia alone. Here,
RAG can rely on parametric knowledge to generate reasonable responses.
3.3 Jeopardy Question Generation
To evaluate RAG’s generation abilities in a non-QA setting, we study open-domain question gen-
eration. Rather than use questions from standard open-domain QA tasks, which typically consist
of short, simple questions, we propose the more demanding task of generating Jeopardy questions.
Jeopardy is an unusual format that consists of trying to guess an entity from a fact about that entity.
For example, “The World Cup” is the answer to the question “In 1986 Mexico scored as the ﬁrst
country to host this international sports competition twice.” As Jeopardy questions are precise,
factual statements, generating Jeopardy questions conditioned on their answer entities constitutes a
challenging knowledge-intensive generation task.
We use the splits from SearchQA [10], with 100K train, 14K dev, and 27K test examples. As
this is a new task, we train a BART model for comparison. Following [67], we evaluate using the
SQuAD-tuned Q-BLEU-1 metric [42]. Q-BLEU is a variant of BLEU with a higher weight for
matching entities and has higher correlation with human judgment for question generation than
standard metrics. We also perform two human evaluations, one to assess generation factuality, and
one for speciﬁcity. We deﬁne factuality as whether a statement can be corroborated by trusted external
sources, and speciﬁcity as high mutual dependence between the input and output [33]. We follow
best practice and use pairwise comparative evaluation [34]. Evaluators are shown an answer and two
generated questions, one from BART and one from RAG. They are then asked to pick one of four
options—quuestion A is better, question B is better, both are good, or neither is good.
3.4 Fact Veriﬁcation
FEVER [56] requires classifying whether a natural language claim is supported or refuted by
Wikipedia, or whether there is not enough information to decide. The task requires retrieving
evidence from Wikipedia relating to the claim and then reasoning over this evidence to classify
whether the claim is true, false, or unveriﬁable from Wikipedia alone. FEVER is a retrieval problem
coupled with an challenging entailment reasoning task. It also provides an appropriate testbed for
exploring the RAG models’ ability to handle classiﬁcation rather than generation. We map FEVER
class labels (supports, refutes, or not enough info) to single output tokens and directly train with
claim-class pairs. Crucially, unlike most other approaches to FEVER, we do not use supervision on
retrieved evidence. In many real-world applications, retrieval supervision signals aren’t available, and
models that do not require such supervision will be applicable to a wider range of tasks. We explore
two variants: the standard 3-way classiﬁcation task (supports/refutes/not enough info) and the 2-way
(supports/refutes) task studied in Thorne and Vlachos [57]. In both cases we report label accuracy.
4 Results
4.1 Open-domain Question Answering
Table 1 shows results for RAG along with state-of-the-art models. On all four open-domain QA
tasks, RAG sets a new state of the art (only on the T5-comparable split for TQA). RAG combines
the generation ﬂexibility of the “closed-book” (parametric only) approaches and the performance of
"open-book" retrieval-based approaches. Unlike REALM and T5+SSM, RAG enjoys strong results
without expensive, specialized “salient span masking” pre-training [20]. It is worth noting that RAG’s
retriever is initialized using DPR’s retriever, which uses retrieval supervision on Natural Questions
and TriviaQA. RAG compares favourably to the DPR QA system, which uses a BERT-based “cross-
encoder” to re-rank documents, along with an extractive reader. RAG demonstrates that neither a
re-ranker nor extractive reader is necessary for state-of-the-art performance.
There are several advantages to generating answers even when it is possible to extract them. Docu-
ments with clues about the answer but do not contain the answer verbatim can still contribute towards
a correct answer being generated, which is not possible with standard extractive approaches, leading
5Table 1: Open-Domain QA Test Scores. For TQA,
left column uses the standard test set for Open-
Domain QA, right column uses the TQA-Wiki
test set. See Appendix D for further details.
Model NQ TQA WQ CT
Closed
Book
T5-11B [52] 34.5 - /50.1 37.4 -
T5-11B+SSM[52] 36.6 - /60.5 44.7 -
Open
Book
REALM [20] 40.4 - / - 40.7 46.8
DPR [26] 41.5 57.9/ - 41.1 50.6
RAG-Token 44.1 55.2/66.1 45.5 50.0
RAG-Seq. 44.5 56.8/68.0 45.2 52.2
Table 2: Generation and classiﬁcation Test Scores.
MS-MARCO SotA is [4], FEVER-3 is [68] and
FEVER-2 is [57] *Uses gold context/evidence.
Best model without gold access underlined.
Model Jeopardy MSMARCO FVR3 FVR2
B-1 QB-1 R-L B-1 Label Acc.
SotA - - 49.8* 49.9* 76.8 92.2 *
BART 15.1 19.7 38.2 41.6 64.0 81.1
RAG-Tok. 17.3 22.2 40.1 41.5 72.5 89.5RAG-Seq. 14.7 21.4 40.8 44.2
to more effective marginalization over documents. Furthermore, RAG can generate correct answers
even when the correct answer is not in any retrieved document, achieving 11.8% accuracy in such
cases for NQ, where an extractive model would score 0%.
4.2 Abstractive Question Answering
As shown in Table 2, RAG-Sequence outperforms BART on Open MS-MARCO NLG by 2.6 Bleu
points and 2.6 Rouge-L points. RAG approaches state-of-the-art model performance, which is
impressive given that (i) those models access gold passages with speciﬁc information required to
generate the reference answer, (ii) many questions are unanswerable without the gold passages, and
(iii) not all questions are answerable from Wikipedia alone. Table 3 shows some generated answers
from our models. Qualitatively, we ﬁnd that RAG models hallucinate less and generate factually
correct text more often than BART. Later, we also show that RAG generations are more diverse than
BART generations (see §4.5).
4.3 Jeopardy Question Generation
Table 2 shows that RAG-Token performs better than RAG-Sequence on Jeopardy question generation,
with both models outperforming BART on Q-BLEU-1. Table 4 shows human evaluation results, over
452 pairs of generations from BART and RAG-Token. Evaluators indicated that BART was more
factual than RAG in only 7.1% of cases, while RAG was more factual in 42.7% of cases, and both
RAG and BART were factual in a further 17% of cases, clearly demonstrating the effectiveness of
RAG on the task over a state-of-the-art generation model. Evaluators also ﬁnd RAG generations to
be more speciﬁc by a large margin. Table 3 shows typical generations from each model.
Jeopardy questions often contain two separate pieces of information, and RAG-Token may perform
best because it can generate responses that combine content from several documents. Figure 2 shows
an example. When generating “Sun”, the posterior is high for document 2 which mentions “The
Sun Also Rises”. Similarly, document 1 dominates the posterior when “A Farewell to Arms” is
generated. Intriguingly, after the ﬁrst token of each book is generated, the document posterior ﬂattens.
This observation suggests that the generator can complete the titles without depending on speciﬁc
documents. In other words, the model’s parametric knowledge is sufﬁcient to complete the titles. We
ﬁnd evidence for this hypothesis by feeding the BART-only baseline with the partial decoding"The
Sun. BART completes the generation"The Sun Also Rises" is a novel by this author of "The Sun
Also Rises" indicating the title "The Sun Also Rises" is stored in BART’s parameters. Similarly,
BART will complete the partial decoding"The Sun Also Rises" is a novel by this author of "A
with "The Sun Also Rises" is a novel by this author of "A Farewell to Arms". This example shows
how parametric and non-parametric memorieswork together—the non-parametric component helps
to guide the generation, drawing out speciﬁc knowledge stored in the parametric memory.
4.4 Fact Veriﬁcation
Table 2 shows our results on FEVER. For 3-way classiﬁcation, RAG scores are within 4.3% of
state-of-the-art models, which are complex pipeline systems with domain-speciﬁc architectures and
substantial engineering, trained using intermediate retrieval supervision, which RAG does not require.
6Document 1: his works are considered classics of Americanliterature ... His wartime experiences formed the basis for his novel”A Farewell to Arms”(1929) ...
Document 2: ... artists of the 1920s ”Lost Generation” expatriatecommunity. His debut novel,”The Sun Also Rises”, was publishedin 1926.
BOS”TheSunAlsoRises” is a
novelbythisauthor
of” AFarewelltoArms
”
Doc 1
Doc 2
Doc 3
Doc 4
Doc 5
Figure 2: RAG-Token document posteriorp(zi|x, yi,y  i) for each generated token for input “Hem-
ingway" for Jeopardy generation with 5 retrieved documents. The posterior for document 1 is high
when generating “A Farewell to Arms" and for document 2 when generating “The Sun Also Rises".
Table 3: Examples from generation tasks. RAG models generate more speciﬁc and factually accurate
responses. ‘?’ indicates factually incorrect responses, * indicates partially correct responses.
Task Input Model Generation
MS-
MARCO
deﬁne middle
ear
BART ?The middle ear is the part of the ear between the middle ear and the nose.
RAG-T The middle ear is the portion of the ear internal to the eardrum.
RAG-S The middle ear includes the tympanic cavity and the three ossicles.
what currency
needed in
scotland
BART The currency needed in Scotland is Pound sterling.
RAG-T Pound is the currency needed in Scotland.
RAG-S The currency needed in Scotland is the pound sterling.
Jeopardy
Question
Gener
-ation
Washington
BART ?This state has the largest number of counties in the U.S.
RAG-T It’s the only U.S. state named for a U.S. president
RAG-S It’s the state where you’ll ﬁnd Mount Rainier National Park
The Divine
Comedy
BART *This epic poem by Dante is divided into 3 parts: the Inferno, the Purgatorio & the Purgatorio
RAG-T Dante’s "Inferno" is the ﬁrst part of this epic poem
RAG-S This 14th century work is divided into 3 sections: "Inferno", "Purgatorio" & "Paradiso"
For 2-way classiﬁcation, we compare against Thorne and Vlachos[57], who train RoBERTa [35]
to classify the claim as true or false given the gold evidence sentence. RAG achieves an accuracy
within 2.7% of this model, despite being supplied with only the claim and retrieving its own evidence.
We also analyze whether documents retrieved by RAG correspond to documents annotated as gold
evidence in FEVER. We calculate the overlap in article titles between the topk documents retrieved
by RAG and gold evidence annotations. We ﬁnd that the top retrieved document is from a gold article
in 71% of cases, and a gold article is present in the top 10 retrieved articles in 90% of cases.
4.5 Additional Results
Generation Diversity Section 4.3 shows that RAG models are more factual and speciﬁc than
BART for Jeopardy question generation. Following recent work on diversity-promoting decoding
[33, 59, 39], we also investigate generation diversity by calculating the ratio of distinct ngrams to
total ngrams generated by different models. Table 5 shows that RAG-Sequence’s generations are
more diverse than RAG-Token’s, and both are signiﬁcantly more diverse than BART without needing
any diversity-promoting decoding.
Retrieval Ablations A key feature of RAG is learning to retrieve relevant information for the task.
To assess the effectiveness of the retrieval mechanism, we run ablations where we freeze the retriever
during training. As shown in Table 6, learned retrieval improves results for all tasks. We compare
RAG’s dense retriever to a word overlap-based BM25 retriever [53]. Here, we replace RAG’s retriever
with a ﬁxed BM25 system, and use BM25 retrieval scores as logits when calculatingp(z|x). Table
6 show the results. For FEVER, BM25 performs best, perhaps since FEVER claims are heavily
entity-centric and thus well-suited for word overlap-based retrieval. Differentiable retrieval improves
results on all other tasks, especially for Open-Domain QA, where it is crucial.
Index hot-swapping An advantage of non-parametric memory models like RAG is that knowledge
can be easily updated at test time. Parametric-only models like T5 or BART need further training to
update their behavior as the world changes. To demonstrate, we build an index using the DrQA [5]
Wikipedia dump from December 2016 and compare outputs from RAG using this index to the newer
index from our main results (December 2018). We prepare a list of 82 world leaders who had changed
between these dates and use a template “Who is {position}?” (e.g. “Who is the President of Peru?”)
7Table 4: Human assessments for the Jeopardy
Question Generation Task.
Factuality Speciﬁcity
BART better 7.1% 16.8%
RAG better 42.7% 37.4%
Both good 11.7% 11.8%
Both poor 17.7% 6.9%
No majority 20.8% 20.1%
Table 5: Ratio of distinct to total tri-grams for
generation tasks.
MSMARCO Jeopardy QGen
Gold 89.6% 90.0%
BART 70.7% 32.4%
RAG-Token 77.8% 46.8%
RAG-Seq. 83.5% 53.8%
Table 6: Ablations on the dev set. As FEVER is a classiﬁcation task, both RAG models are equivalent.
Model NQ TQA WQ CT Jeopardy-QGen MSMarco FVR-3 FVR-2
Exact Match B-1 QB-1 R-L B-1 Label Accuracy
RAG-Token-BM25 29.7 41.5 32.1 33.1 17.5 22.3 55.5 48.4 75.1 91.6RAG-Sequence-BM25 31.8 44.1 36.6 33.8 11.1 19.5 56.5 46.9
RAG-Token-Frozen 37.8 50.1 37.1 51.1 16.7 21.7 55.9 49.4 72.9 89.4RAG-Sequence-Frozen 41.2 52.1 41.8 52.6 11.8 19.6 56.7 47.3
RAG-Token 43.5 54.8 46.5 51.9 17.9 22.6 56.2 49.4 74.5 90.6RAG-Sequence 44.0 55.8 44.9 53.4 15.3 21.5 57.2 47.5
to query our NQ RAG model with each index. RAG answers 70% correctly using the 2016 index for
2016 world leaders and 68% using the 2018 index for 2018 world leaders. Accuracy with mismatched
indices is low (12% with the 2018 index and 2016 leaders, 4% with the 2016 index and 2018 leaders).
This shows we can update RAG’s world knowledge by simply replacing its non-parametric memory.
Effect of Retrieving more documentsModels are trained with either 5 or 10 retrieved latent
documents, and we do not observe signiﬁcant differences in performance between them. We have the
ﬂexibility to adjust the number of retrieved documents at test time, which can affect performance and
runtime. Figure 3 (left) shows that retrieving more documents at test time monotonically improves
Open-domain QA results for RAG-Sequence, but performance peaks for RAG-Token at 10 retrieved
documents. Figure 3 (right) shows that retrieving more documents leads to higher Rouge-L for
RAG-Token at the expense of Bleu-1, but the effect is less pronounced for RAG-Sequence.
10 20 30 40 50KR e t r i e v e dD o c s
39
40
41
42
43
44NQ Exact Match RAG-TokRAG-Seq
10 20 30 40 50KR e t r i e v e dD o c s
40
50
60
70
80NQ Answer Recall @ K
RAG-TokRAG-SeqFixed DPRBM25
10 20 30 40 50KR e t r i e v e dD o c s
48
50
52
54
56Bleu-1 / Rouge-L score
RAG-Tok R-LRAG-Tok B-1RAG-Seq R-LRAG-Seq B-1
Figure 3: Left: NQ performance as more documents are retrieved. Center: Retrieval recall perfor-
mance in NQ. Right: MS-MARCO Bleu-1 and Rouge-L as more documents are retrieved.
5 Related Work
Single-Task Retrieval Prior work has shown that retrieval improves performance across a variety of
NLP tasks when considered in isolation. Such tasks include open-domain question answering [5, 29],
fact checking [56], fact completion [48], long-form question answering [12], Wikipedia article
generation [36], dialogue [41, 65, 9, 13], translation [17], and language modeling [19, 27]. Our
work uniﬁes previous successes in incorporating retrieval into individual tasks, showing that a single
retrieval-based architecture is capable of achieving strong performance across several tasks.
8General-Purpose Architectures for NLPPrior work on general-purpose architectures for NLP
tasks has shown great success without the use of retrieval. A single, pre-trained language model
has been shown to achieve strong performance on various classiﬁcation tasks in the GLUE bench-
marks [60, 61] after ﬁne-tuning [49, 8]. GPT-2 [50] later showed that a single, left-to-right, pre-trained
language model could achieve strong performance across both discriminative and generative tasks.
For further improvement, BART [32] and T5 [51, 52] propose a single, pre-trained encoder-decoder
model that leverages bi-directional attention to achieve stronger performance on discriminative
and generative tasks. Our work aims to expand the space of possible tasks with a single, uniﬁed
architecture, by learning a retrieval module to augment pre-trained, generative language models.
Learned Retrieval There is signiﬁcant work on learning to retrieve documents in information
retrieval, more recently with pre-trained, neural language models [44, 26] similar to ours. Some
work optimizes the retrieval module to aid in a speciﬁc, downstream task such as question answering,
using search [46], reinforcement learning [6, 63, 62], or a latent variable approach [31, 20] as in our
work. These successes leverage different retrieval-based architectures and optimization techniques to
achieve strong performance on a single task, while we show that a single retrieval-based architecture
can be ﬁne-tuned for strong performance on a variety of tasks.
Memory-based Architectures Our document index can be seen as a large external memory for
neural networks to attend to, analogous to memory networks [64, 55]. Concurrent work [14] learns
to retrieve a trained embedding for each entity in the input, rather than to retrieve raw text as in our
work. Other work improves the ability of dialog models to generate factual text by attending over
fact embeddings [9, 13] or, closer to our work, over retrieved text directly [15]. A key feature of our
memory is that it is comprised of raw text rather distributed representations, which makes the memory
both (i) human-readable, lending a form of interpretability to our model, and (ii) human-writable,
enabling us to dynamically update the model’s memory by editing the document index.
Retrieve-and-Edit approaches Our method shares some similarities with retrieve-and-edit style
approaches, where a similar training input-output pair is retrieved for a given input, and then edited
to provide a ﬁnal output. These approaches have proved successful in a number of domains including
Machine Translation [18, 22] and Semantic Parsing [21]. Our approach does have several differences,
including less of emphasis on lightly editing a retrieved item, but on aggregating content from several
pieces of retrieved content, as well as learning latent retrieval, and retrieving evidence documents
rather than related training pairs. This said, RAG techniques may work well in these settings, and
could represent promising future work.
6 Discussion
In this work, we presented hybrid generation models with access to parametric and non-parametric
memory. We showed that our RAG models obtain state of the art results on open-domain QA. We
found that people prefer RAG’s generation over purely parametric BART, ﬁnding RAG more factual
and speciﬁc. We conducted an thorough investigation of the learned retrieval component, validating
its effectiveness, and we illustrated how the retrieval index can be hot-swapped to update the model
without requiring any retraining. In future work, it may be fruitful to investigate if the two components
can be jointly pre-trained from scratch, either with a denoising objective similar to BART or some
another objective. Our work opens up new research directions on how parametric and non-parametric
memories interact and how to most effectively combine them, showing promise in being applied to a
wide variety of NLP tasks.
9Broader Impact
This work offers several positive societal beneﬁts over previous work: the fact that it is more
strongly grounded in real factual knowledge (in this case Wikipedia) makes it “hallucinate” less
with generations that are more factual, and offers more control and interpretability. RAG could be
employed in a wide variety of scenarios with direct beneﬁt to society, for example by endowing it
with a medical index and asking it open-domain questions on that topic, or by helping people be more
effective at their jobs.
With these advantages also come potential downsides: Wikipedia, or any potential external knowledge
source, will probably never be entirely factual and completely devoid of bias. Since RAG can be
employed as a language model, similar concerns as for GPT-2 [50] are valid here, although arguably
to a lesser extent, including that it might be used to generate abuse, faked or misleading content in
the news or on social media; to impersonate others; or to automate the production of spam/phishing
content [54]. Advanced language models may also lead to the automation of various jobs in the
coming decades [16]. In order to mitigate these risks, AI systems could be employed to ﬁght against
misleading content and automated spam/phishing.
Acknowledgments
The authors would like to thank the reviewers for their thoughtful and constructive feedback on this
paper, as well as HuggingFace for their help in open-sourcing code to run RAG models. The authors
would also like to thank Kyunghyun Cho and Sewon Min for productive discussions and advice.
Funding Disclosure
EP thanks supports from the NSF Graduate Research Fellowship. PL is supported by the FAIR PhD
program. This work was funded by Facebook.
References
[1] Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan
Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina
Stoica, Saurabh Tiwary, and Tong Wang. MS MARCO: A Human Generated MAchine
Reading COmprehension Dataset. arXiv:1611.09268 [cs], November 2016. URL http:
//arxiv.org/abs/1611.09268. arXiv: 1611.09268.
[2] Petr Baudiš and Jan Šediv`y. Modeling of the question answering task in the yodaqa system. In
International Conference of the Cross-Language Evaluation Forum for European Languages,
pages 222–228. Springer, 2015. URLhttps://link.springer.com/chapter/10.1007%
2F978-3-319-24027-5_20 .
[3] Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic Parsing on Freebase
from Question-Answer Pairs. InProceedings of the 2013 Conference on Empirical Methods
in Natural Language Processing, pages 1533–1544, Seattle, Washington, USA, October 2013.
Association for Computational Linguistics. URLhttp://www.aclweb.org/anthology/
D13-1160.
[4] Bin Bi, Chenliang Li, Chen Wu, Ming Yan, and Wei Wang. Palm: Pre-training an autoencod-
ing&autoregressive language model for context-conditioned generation.ArXiv, abs/2004.07159,
2020. URLhttps://arxiv.org/abs/2004.07159.
[5] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading Wikipedia to Answer
Open-Domain Questions. InProceedings of the 55th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), pages 1870–1879, Vancouver, Canada,
July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1171. URL
https://www.aclweb.org/anthology/P17-1171.
[6] Eunsol Choi, Daniel Hewlett, Jakob Uszkoreit, Illia Polosukhin, Alexandre Lacoste, and
Jonathan Berant. Coarse-to-ﬁne question answering for long documents. InProceedings of the
1055th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),
pages 209–220, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi:
10.18653/v1/P17-1020. URL https://www.aclweb.org/anthology/P17-1020.
[7] Christopher Clark and Matt Gardner. Simple and Effective Multi-Paragraph Reading Compre-
hension. arXiv:1710.10723 [cs], October 2017. URLhttp://arxiv.org/abs/1710.10723.
arXiv: 1710.10723.
[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of
Deep Bidirectional Transformers for Language Understanding. InProceedings of the 2019 Con-
ference of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis,
Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423.
URL https://www.aclweb.org/anthology/N19-1423.
[9] Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wiz-
ard of wikipedia: Knowledge-powered conversational agents. InInternational Conference on
Learning Representations, 2019. URLhttps://openreview.net/forum?id=r1l73iRqKm.
[10] Matthew Dunn, Levent Sagun, Mike Higgins, V . Ugur Guney, V olkan Cirik, and Kyunghyun
Cho. SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine.
arXiv:1704.05179 [cs], April 2017. URL http://arxiv.org/abs/1704.05179. arXiv:
1704.05179.
[11] Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. InProceed-
ings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1:
Long Papers), pages 889–898, Melbourne, Australia, July 2018. Association for Computational
Linguistics. doi: 10.18653/v1/P18-1082. URL https://www.aclweb.org/anthology/
P18-1082.
[12] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5:
Long form question answering. InProceedings of the 57th Annual Meeting of the Association
for Computational Linguistics, pages 3558–3567, Florence, Italy, July 2019. Association for
Computational Linguistics. doi: 10.18653/v1/P19-1346. URLhttps://www.aclweb.org/
anthology/P19-1346.
[13] Angela Fan, Claire Gardent, Chloe Braud, and Antoine Bordes. Augmenting transformers
with KNN-based composite memory, 2020. URLhttps://openreview.net/forum?id=
H1gx1CNKPH.
[14] Thibault Févry, Livio Baldini Soares, Nicholas FitzGerald, Eunsol Choi, and Tom Kwiatkowski.
Entities as experts: Sparse memory access with entity supervision.ArXiv, abs/2004.07202,
2020. URLhttps://arxiv.org/abs/2004.07202.
[15] Marjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan, Jianfeng Gao, Wen
tau Yih, and Michel Galley. A knowledge-grounded neural conversation model. InAAAI
Conference on Artiﬁcial Intelligence, 2018. URLhttps://www.aaai.org/ocs/index.php/
AAAI/AAAI18/paper/view/16710.
[16] Katja Grace, John Salvatier, Allan Dafoe, Baobao Zhang, and Owain Evans. When will AI
exceed human performance? evidence from AI experts.CoRR, abs/1705.08807, 2017. URL
http://arxiv.org/abs/1705.08807.
[17] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural
machine translation. In AAAI Conference on Artiﬁcial Intelligence, 2018. URL https:
//www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17282.
[18] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Search engine guided neural
machine translation. In32nd AAAI Conference on Artiﬁcial Intelligence, AAAI 2018, 32nd
AAAI Conference on Artiﬁcial Intelligence, AAAI 2018, pages 5133–5140. AAAI press, 2018.
32nd AAAI Conference on Artiﬁcial Intelligence, AAAI 2018 ; Conference date: 02-02-2018
Through 07-02-2018.
11[19] Kelvin Guu, Tatsunori B. Hashimoto, Yonatan Oren, and Percy Liang. Generating sentences by
editing prototypes.Transactions of the Association for Computational Linguistics, 6:437–450,
2018. doi: 10.1162/tacl_a_00030. URLhttps://www.aclweb.org/anthology/Q18-1031.
[20] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. REALM:
Retrieval-augmented language model pre-training.ArXiv, abs/2002.08909, 2020. URLhttps:
//arxiv.org/abs/2002.08909.
[21] Tatsunori B Hashimoto, Kelvin Guu, Yonatan Oren, and Percy S Liang. A
retrieve-and-edit framework for predicting structured outputs. In S. Bengio,
H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, ed-
itors, Advances in Neural Information Processing Systems 31 , pages 10052–
10062. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/
8209-a-retrieve-and-edit-framework-for-predicting-structured-outputs.
pdf.
[22] Nabil Hossain, Marjan Ghazvininejad, and Luke Zettlemoyer. Simple and effective retrieve-
edit-rerank text generation. InProceedings of the 58th Annual Meeting of the Association for
Computational Linguistics, pages 2532–2538, Online, July 2020. Association for Computa-
tional Linguistics. doi: 10.18653/v1/2020.acl-main.228. URLhttps://www.aclweb.org/
anthology/2020.acl-main.228.
[23] Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with gpus.arXiv
preprint arXiv:1702.08734, 2017. URLhttps://arxiv.org/abs/1702.08734.
[24] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A Large Scale
Distantly Supervised Challenge Dataset for Reading Comprehension. InProceedings of the
55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),
pages 1601–1611, Vancouver, Canada, July 2017. Association for Computational Linguistics.
doi: 10.18653/v1/P17-1147. URLhttps://www.aclweb.org/anthology/P17-1147.
[25] Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stack-
augmented recurrent nets. In Proceedings of the 28th International Conference on
Neural Information Processing Systems - Volume 1 , NIPS’15, page 190–198, Cam-
bridge, MA, USA, 2015. MIT Press. URL https://papers.nips.cc/paper/
5857-inferring-algorithmic-patterns-with-stack-augmented-recurrent-nets .
[26] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Ledell Wu, Sergey Edunov, Danqi Chen, and
Wen-tau Yih. Dense passage retrieval for open-domain question answering.arXiv preprint
arXiv:2004.04906, 2020. URLhttps://arxiv.org/abs/2004.04906.
[27] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generaliza-
tion through memorization: Nearest neighbor language models. InInternational Conference on
Learning Representations, 2020. URLhttps://openreview.net/forum?id=HklBjCEKvH.
[28] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua
Bengio and Yann LeCun, editors,3rd International Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL
http://arxiv.org/abs/1412.6980.
[29] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins, Ankur Parikh,
Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey, Jacob Devlin, Ken-
ton Lee, Kristina N. Toutanova, Llion Jones, Ming-Wei Chang, Andrew Dai, Jakob
Uszkoreit, Quoc Le, and Slav Petrov. Natural Questions: a Benchmark for Ques-
tion Answering Research. Transactions of the Association of Computational Lin-
guistics, 2019. URL https://tomkwiat.users.x20web.corp.google.com/papers/
natural-questions/main-1455-kwiatkowski.pdf .
[30] Guillaume Lample, Alexandre Sablayrolles, Marc’ Aurelio Ranzato, Ludovic Denoyer, and
Herve Jegou. Large memory layers with product keys. In H. Wallach, H. Larochelle,
A. Beygelzimer, F. d’ Alché-Buc, E. Fox, and R. Garnett, editors,Advances in Neural In-
formation Processing Systems 32, pages 8548–8559. Curran Associates, Inc., 2019. URLhttp:
//papers.nips.cc/paper/9061-large-memory-layers-with-product-keys.pdf .
12[31] Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised
open domain question answering. InProceedings of the 57th Annual Meeting of the Association
for Computational Linguistics, pages 6086–6096, Florence, Italy, July 2019. Association for
Computational Linguistics. doi: 10.18653/v1/P19-1612. URLhttps://www.aclweb.org/
anthology/P19-1612.
[32] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,
Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence
pre-training for natural language generation, translation, and comprehension.arXiv preprint
arXiv:1910.13461, 2019. URLhttps://arxiv.org/abs/1910.13461.
[33] Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting
objective function for neural conversation models. InProceedings of the 2016 Conference of the
North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, pages 110–119, San Diego, California, June 2016. Association for Computational
Linguistics. doi: 10.18653/v1/N16-1014. URL https://www.aclweb.org/anthology/
N16-1014.
[34] Margaret Li, Jason Weston, and Stephen Roller. Acute-eval: Improved dialogue evaluation
with optimized questions and multi-turn comparisons.ArXiv, abs/1909.03087, 2019. URL
https://arxiv.org/abs/1909.03087.
[35] Hairong Liu, Mingbo Ma, Liang Huang, Hao Xiong, and Zhongjun He. Robust neural machine
translation with joint textual and phonetic embedding. InProceedings of the 57th Annual
Meeting of the Association for Computational Linguistics, pages 3044–3049, Florence, Italy,
July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1291. URL
https://www.aclweb.org/anthology/P19-1291.
[36] Peter J. Liu*, Mohammad Saleh*, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser,
and Noam Shazeer. Generating wikipedia by summarizing long sequences. InInternational
Conference on Learning Representations, 2018. URLhttps://openreview.net/forum?
id=Hyg0vbWC-.
[37] Yury A. Malkov and D. A. Yashunin. Efﬁcient and robust approximate nearest neighbor search
using hierarchical navigable small world graphs.IEEE Transactions on Pattern Analysis and
Machine Intelligence, 42:824–836, 2016. URLhttps://arxiv.org/abs/1603.09320.
[38] Gary Marcus. The next decade in ai: four steps towards robust artiﬁcial intelligence.arXiv
preprint arXiv:2002.06177, 2020. URLhttps://arxiv.org/abs/2002.06177.
[39] Luca Massarelli, Fabio Petroni, Aleksandra Piktus, Myle Ott, Tim Rocktäschel, Vassilis
Plachouras, Fabrizio Silvestri, and Sebastian Riedel. How decoding strategies affect the
veriﬁability of generated text. arXiv preprint arXiv:1911.03587, 2019. URL https:
//arxiv.org/abs/1911.03587.
[40] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia,
Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed
precision training. InICLR, 2018. URLhttps://openreview.net/forum?id=r1gs9JgRZ.
[41] Nikita Moghe, Siddhartha Arora, Suman Banerjee, and Mitesh M. Khapra. Towards exploit-
ing background knowledge for building conversation systems. InProceedings of the 2018
Conference on Empirical Methods in Natural Language Processing, pages 2322–2332, Brus-
sels, Belgium, October-November 2018. Association for Computational Linguistics. doi:
10.18653/v1/D18-1255. URL https://www.aclweb.org/anthology/D18-1255.
[42] Preksha Nema and Mitesh M. Khapra. Towards a better metric for evaluating question generation
systems. InProceedings of the 2018 Conference on Empirical Methods in Natural Language
Processing, pages 3950–3959, Brussels, Belgium, October-November 2018. Association for
Computational Linguistics. doi: 10.18653/v1/D18-1429. URLhttps://www.aclweb.org/
anthology/D18-1429.
13[43] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder,
and Li Deng. MS MARCO: A human generated machine reading comprehension dataset. In
Tarek Richard Besold, Antoine Bordes, Artur S. d’Avila Garcez, and Greg Wayne, editors,
Proceedings of the Workshop on Cognitive Computation: Integrating neural and symbolic
approaches 2016 co-located with the 30th Annual Conference on Neural Information Processing
Systems (NIPS 2016), Barcelona, Spain, December 9, 2016, volume 1773 ofCEUR Workshop
Proceedings. CEUR-WS.org, 2016. URL http://ceur-ws.org/Vol-1773/CoCoNIPS_
2016_paper9.pdf.
[44] Rodrigo Nogueira and Kyunghyun Cho. Passage re-ranking with BERT.arXiv preprint
arXiv:1901.04085, 2019. URLhttps://arxiv.org/abs/1901.04085.
[45] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier,
and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. InProceedings
of the 2019 Conference of the North American Chapter of the Association for Computational
Linguistics (Demonstrations), pages 48–53, Minneapolis, Minnesota, June 2019. Association
for Computational Linguistics. doi: 10.18653/v1/N19-4009. URLhttps://www.aclweb.
org/anthology/N19-4009.
[46] Ethan Perez, Siddharth Karamcheti, Rob Fergus, Jason Weston, Douwe Kiela, and Kyunghyun
Cho. Finding generalizable evidence by learning to convince q&a models. InProceedings
of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th
International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages
2402–2411, Hong Kong, China, November 2019. Association for Computational Linguistics.
doi: 10.18653/v1/D19-1244. URLhttps://www.aclweb.org/anthology/D19-1244.
[47] Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu,
and Alexander Miller. Language models as knowledge bases? InProceedings of the 2019
Conference on Empirical Methods in Natural Language Processing and the 9th International
Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2463–2473, Hong
Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/
D19-1250. URL https://www.aclweb.org/anthology/D19-1250.
[48] Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rocktäschel, Yuxiang Wu, Alexander H.
Miller, and Sebastian Riedel. How context affects language models’ factual predictions. In
Automated Knowledge Base Construction, 2020. URLhttps://openreview.net/forum?
id=025X0zPfn.
[49] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Im-
proving Language Understanding by Generative Pre-Training, 2018. URL
https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/
language-unsupervised/language_understanding_paper.pdf.
[50] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
Sutskever. Language models are unsupervised multitask learners, 2019. URL
https://d4mucfpksywv.cloudfront.net/better-language-models/language_
models_are_unsupervised_multitask_learners.pdf.
[51] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,
Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed
text-to-text transformer.arXiv e-prints, 2019. URLhttps://arxiv.org/abs/1910.10683.
[52] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into
the parameters of a language model?arXiv e-prints, 2020. URLhttps://arxiv.org/abs/
2002.08910.
[53] Stephen Robertson and Hugo Zaragoza. The probabilistic relevance framework: Bm25 and
beyond. Found. Trends Inf. Retr., 3(4):333–389, April 2009. ISSN 1554-0669. doi: 10.1561/
1500000019. URLhttps://doi.org/10.1561/1500000019.
[54] Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-V oss, Jeff Wu, Alec
Radford, and Jian-Bing Wang. Release strategies and the social impacts of language models.
ArXiv, abs/1908.09203, 2019.
14[55] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory net-
works. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,Advances
in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates, Inc., 2015.
URL http://papers.nips.cc/paper/5846-end-to-end-memory-networks.pdf .
[56] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. FEVER: a
large-scale dataset for fact extraction and VERiﬁcation. InProceedings of the 2018 Conference
of the North American Chapter of the Association for Computational Linguistics: Human
Language Technologies, Volume 1 (Long Papers), pages 809–819, New Orleans, Louisiana,
June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1074. URL
https://www.aclweb.org/anthology/N18-1074.
[57] James H. Thorne and Andreas Vlachos. Avoiding catastrophic forgetting in mitigating model
biases in sentence-pair classiﬁcation with elastic weight consolidation.ArXiv, abs/2004.14366,
2020. URLhttps://arxiv.org/abs/2004.14366.
[58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V . Luxburg,
S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors,Advances in Neural
Information Processing Systems 30, pages 5998–6008. Curran Associates, Inc., 2017. URL
http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf .
[59] Ashwin Vijayakumar, Michael Cogswell, Ramprasaath Selvaraju, Qing Sun, Stefan Lee, David
Crandall, and Dhruv Batra. Diverse beam search for improved description of complex scenes.
AAAI Conference on Artiﬁcial Intelligence, 2018. URLhttps://www.aaai.org/ocs/index.
php/AAAI/AAAI18/paper/view/17329.
[60] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman.
GLUE: A multi-task benchmark and analysis platform for natural language understanding.
In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting
Neural Networks for NLP, pages 353–355, Brussels, Belgium, November 2018. Association for
Computational Linguistics. doi: 10.18653/v1/W18-5446. URLhttps://www.aclweb.org/
anthology/W18-5446.
[61] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel Bowman. SuperGLUE: A Stickier Benchmark for General-
Purpose Language Understanding Systems. In H. Wallach, H. Larochelle, A. Beygelzimer,
F. d\textquotesingle Alché-Buc, E. Fox, and R. Garnett, editors,Advances in Neural Information
Processing Systems 32, pages 3261–3275. Curran Associates, Inc., 2019. URLhttps://
arxiv.org/abs/1905.00537.
[62] Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang,
Gerry Tesauro, Bowen Zhou, and Jing Jiang. R3: Reinforced ranker-reader for open-domain
question answering. In Sheila A. McIlraith and Kilian Q. Weinberger, editors,Proceedings of
the Thirty-Second AAAI Conference on Artiﬁcial Intelligence, (AAAI-18), the 30th innovative
Applications of Artiﬁcial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational
Advances in Artiﬁcial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7,
2018, pages 5981–5988. AAAI Press, 2018. URLhttps://www.aaai.org/ocs/index.
php/AAAI/AAAI18/paper/view/16712.
[63] Shuohang Wang, Mo Yu, Jing Jiang, Wei Zhang, Xiaoxiao Guo, Shiyu Chang, Zhiguo Wang,
Tim Klinger, Gerald Tesauro, and Murray Campbell. Evidence aggregation for answer re-
ranking in open-domain question answering. InICLR, 2018. URLhttps://openreview.
net/forum?id=rJl3yM-Ab.
[64] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. In Yoshua Bengio
and Yann LeCun, editors,3rd International Conference on Learning Representations, ICLR
2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL
http://arxiv.org/abs/1410.3916.
[65] Jason Weston, Emily Dinan, and Alexander Miller. Retrieve and reﬁne: Improved sequence
generation models for dialogue. InProceedings of the 2018 EMNLP Workshop SCAI: The 2nd
15International Workshop on Search-Oriented Conversational AI, pages 87–92, Brussels, Belgium,
October 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5713. URL
https://www.aclweb.org/anthology/W18-5713.
[66] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony
Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer,
Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain
Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Huggingface’s transformers:
State-of-the-art natural language processing.ArXiv, abs/1910.03771, 2019.
[67] Shiyue Zhang and Mohit Bansal. Addressing semantic drift in question generation for semi-
supervised question answering. InProceedings of the 2019 Conference on Empirical Meth-
ods in Natural Language Processing and the 9th International Joint Conference on Natural
Language Processing (EMNLP-IJCNLP), pages 2495–2509, Hong Kong, China, Novem-
ber 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1253. URL
https://www.aclweb.org/anthology/D19-1253.
[68] Wanjun Zhong, Jingjing Xu, Duyu Tang, Zenan Xu, Nan Duan, Ming Zhou, Jiahai Wang, and
Jian Yin. Reasoning over semantic-level graph for fact checking.ArXiv, abs/1909.03745, 2019.
URL https://arxiv.org/abs/1909.03745.
16

Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics
System Demonstrations, pages 150–158
March 17-22, 2024c⃝2024 Association for Computational Linguistics
RAGA S: Automated Evaluation of Retrieval Augmented Generation
Shahul Es†, Jithin James†, Luis Espinosa-Anke∗♢, Steven Schockaert∗
†Exploding Gradients
∗CardiffNLP, Cardiff University, United Kingdom
♢AMPLYFI, United Kingdom
{shahules786,jamesjithin97}@gmail.com
{espinosa-ankel,schockaerts1}@cardiff.ac.uk
Abstract
We introduce RAGA S1 (Retrieval Augmented
Generation Assessment), a framework for
reference-free evaluation of Retrieval Aug-
mented Generation (RAG) pipelines. RAG sys-
tems are composed of a retrieval module and an
LLM based generation module. They provide
LLMs with knowledge from a reference corpus,
which can help to keep LLM based systems
up-to-date and can reduce the risk of halluci-
nations, among others. However, evaluating
RAG architectures is challenging because there
are several dimensions to consider: the abil-
ity of the retrieval system to identify relevant
and focused context passages, the ability of the
LLM to exploit such passages in a faithful way,
and the quality of the generation itself. With
RAGA S, we put forward a suite of metrics
which can be used to evaluate these different
dimensions without having to rely on ground
truth human annotations. We posit that such
a framework can crucially contribute to faster
evaluation cycles of RAG architectures, which
is especially important given the fast adoption
of LLMs.
1 Introduction
Language Models (LMs) capture a vast amount
of knowledge about the world, which allows them
to answer questions without accessing any exter-
nal sources. This idea of LMs as repositories of
knowledge emerged shortly after the introduction
of BERT (Devlin et al., 2019) and became more
firmly established with the introduction of ever
larger LMs (Roberts et al., 2020). While the most
recent Large Language Models (LLMs) capture
enough knowledge to rival human performance
across a wide variety of question answering bench-
marks (Bubeck et al., 2023), the idea of using
LLMs as knowledge bases still has two fundamen-
tal limitations. First, LLMs are not able to answer
1RAGA S is available at https://github.com/
explodinggradients/ragas.
questions about events that have happened after
they were trained. Second, even the largest models
struggle to memorise knowledge that is only rarely
mentioned in the training corpus (Kandpal et al.,
2022; Mallen et al., 2023). The standard solution
to these issues is to rely on Retrieval Augmented
Generation (RAG) (Lee et al., 2019; Lewis et al.,
2020; Guu et al., 2020). Answering a question
then essentially involves retrieving relevant pas-
sages from a corpus and feeding these passages,
along with the original question, to the LM. While
initial approaches relied on specialised LMs for
retrieval-augmented language modelling (Khandel-
wal et al., 2020; Borgeaud et al., 2022), recent work
has suggested that simply adding retrieved docu-
ments to the input of a standard LM can also work
well (Khattab et al., 2022; Ram et al., 2023; Shi
et al., 2023), thus making it possible to use retrieval-
augmented strategies in combination with LLMs
that are only available through APIs.
While the usefulness of retrieval-augmented
strategies is clear, their implementation requires
a significant amount of tuning, as the overall per-
formance will be affected by the retrieval model,
the considered corpus, the LM and the prompt for-
mulation, among others. Automated evaluation of
retrieval-augmented systems is thus paramount. In
practice, RAG systems are often evaluated in terms
of the language modelling task itself, i.e. by mea-
suring perplexity on some reference corpus. How-
ever, such evaluations are not always predictive
of downstream performance (Wang et al., 2023c).
Moreover, this evaluation strategy relies on the LM
probabilities, which are not accessible for some
closed models (e.g. ChatGPT and GPT-4). Ques-
tion answering is another common evaluation task,
but usually only datasets with short extractive an-
swers are considered, which may not be represen-
tative of how the system will be used.
To address these issues, in this paper we present
RAGA S, a framework for the automated assess-
150ment of retrieval augmented generation systems.
We focus on settings where reference answers may
not be available, and where we want to estimate
different proxies for correctness, in addition to the
usefulness of the retrieved passages. The RAGA S
framework provides an integration with both llama-
index and Langchain, the most widely used frame-
works for building RAG solutions, thus enabling
developers to easily integrate RAGA S into their
standard workflow.
2 Related Work
Estimating faithfulness using LLMsThe prob-
lem of detecting hallucinations in LLM-generated
responses has been extensively studied (Ji et al.,
2023). Some authors have suggested the idea of
predicting factuality using a few-shot prompting
strategy (Zhang et al., 2023). Recent analyses, how-
ever, suggest that existing models struggle with de-
tecting hallucination when using standard prompt-
ing strategies (Li et al., 2023; Azaria and Mitchell,
2023). Other approaches rely on linking the gener-
ated responses to facts from an external knowledge
base (Min et al., 2023), but this is not always possi-
ble.
Yet another strategy is to inspect the probabili-
ties assigned to individual tokens, where we would
expect the model to be less confident in halluci-
nated answers than in factual ones. For instance,
BARTScore (Yuan et al., 2021) estimates factuality
by looking at the conditional probability of the gen-
erated text given the input. Kadavath et al. (2022)
use a variation of this idea. Starting from the ob-
servation that LLMs provide well-calibrated proba-
bilities when answering multiple-choice questions,
they essentially convert the problem of validating
model generated answers into a multiple-choice
question which asks whether the answer is true or
false. Rather than looking at the output probabil-
ities, Azaria and Mitchell (2023) propose to train
a supervised classifier on the weights from one of
the hidden layers of the LLM, to predict whether a
given statement is true or not. While the approach
performs well, the need to access the hidden states
of the model makes it unsuitable for systems that
access LLMs through an API.
For models that do not provide access to token
probabilities, such as ChatGPT and GPT-4, differ-
ent methods are needed. SelfCheckGPT (Manakul
et al., 2023) addresses this problem by instead sam-
pling multiple answers. Their core idea is that
factual answers are more stable: when an answer is
factual, we can expect that different samples will
tend to be semantically similar, whereas this is less
likely to be the case for hallucinated answers.
Automated evaluation of text generation systems
LLMs have also been leveraged to automatically
evaluate other aspects of generated text fragments,
beyond factuality. For instance, GPTScore (Fu
et al., 2023) uses a prompt that specifies the consid-
ered aspect (e.g. fluency) and then scores passages
based on the average probability of the generated
tokens, according to a given autoregressive LM.
This idea of using prompts was previously also
considered by Yuan et al. (2021), although they
used a smaller fine-tuned LM (i.e. BART) and did
not observe a clear benefit from using prompts. An-
other approach directly asks ChatGPT to evaluate
a particular aspect of the given answer by provid-
ing a score between 0 and 100, or by providing a
rating on a 5-star scale (Wang et al., 2023a). Re-
markably, strong results can be obtained in this
way, although it comes with the limitation of being
sensitive to the design of the prompt. Rather than
scoring individual answers, some authors have also
focused on using an LLM to select the best answer
among a number of candidates (Wang et al., 2023b),
typically to compare the performance of different
LLMs. However, care is needed with this approach,
as the order in which the answers are presented can
influence the result (Wang et al., 2023b).
More generally, however, most approaches have
relied on the availability of one or more refer-
ence answers for evaluating text generation sys-
tems. For instance, BERTScore (Zhang et al., 2020)
and MoverScore (Zhao et al., 2019) use contex-
tualised embeddings, produced by a pre-trained
BERT model, to compare the similarity between
the generated answer and the reference answers.
BARTScore (Yuan et al., 2021) similarly uses refer-
ence answers to compute aspects such as precision
(estimated as the probability of generating the gen-
erated answer given the reference) and recall (esti-
mated as the probability of generating the reference
given the generated answer).
3 Evaluation Strategies
We consider a standard RAG setting, where given a
question q, the system first retrieves some context
c(q) and then uses the retrieved context to generate
an answer as(q). When building a RAG system,
we usually do not have access to human-annotated
151datasets or reference answers. We therefore fo-
cus on metrics that are fully self-contained and
reference-free. We focus in particular three quality
aspects, which we argue are of central importance.
First, Faithfulness refers to the idea that the
answer should be grounded in the given context.
This is important to avoid hallucinations, and to
ensure that the retrieved context can act as a jus-
tification for the generated answer. Indeed, RAG
systems are often used in applications where the
factual consistency of the generated text w.r.t. the
grounded sources is highly important, e.g. in do-
mains such as law, where information is constantly
evolving. Second, Answer Relevancerefers to the
idea that the generated answer should address the
actual question that was provided. Finally, Con-
text Relevancerefers to the idea that the retrieved
context should be focused, containing as little ir-
relevant information as possible. This is important
given the cost associated with feeding long context
passages to LLMs. Moreover, when context pas-
sages are too long, LLMs are often less effective
in exploiting that context, especially for informa-
tion that is provided in the middle of the context
passage (Liu et al., 2023).
We now explain how these three quality aspects
can be measured in a fully automated way, by
prompting an LLM. In our implementation and
experiments, all prompts are evaluated using the
gpt-3.5-turbo-16k model, which is available
through the OpenAI API2.
Faithfulness We say that the answer as(q) is
faithful to the context c(q) if the claims that are
made in the answer can be inferred from the con-
text. To estimate faithfulness, we first use an LLM
to extract a set of statements, S(as(q)). The aim
of this step is to decompose longer sentences into
shorter and more focused assertions. We use the
following prompt for this step3:
Given a question and answer, create one
or more statements from each sentence
in the given answer.
question: [question]
answer: [answer]
where [question] and [answer] refer to the
given question and answer. For each statementsi in
2https://platform.openai.com
3To help clarify the task, we include a demonstration as
part of the prompt. This demonstration is not explicitly shown
in the listing of the prompts throughout this paper.
S(as(q)), the LLM determines if si can be inferred
from c(q) using a verification function v(si, c(q)).
This verification step is carried out using the fol-
lowing prompt:
Consider the given context and following
statements, then determine whether they
are supported by the information present
in the context. Provide a brief explana-
tion for each statement before arriving
at the verdict (Yes/No). Provide a final
verdict for each statement in order at the
end in the given format. Do not deviate
from the specified format.
statement: [statement 1]
...
statement: [statement n]
The final faithfulness score, F, is then computed
as F = |V |
|S|, where |V | is the number of statements
that were supported according to the LLM and |S|
is the total number of statements in S(as(q)).
Answer relevance We say that the answer as(q)
is relevant if it directly addresses the question in
an appropriate way. In particular, our assessment
of answer relevance does not take into account fac-
tuality, but penalises cases where the answer is
incomplete or where it contains redundant informa-
tion. To estimate answer relevance, for the given
answer as(q), we prompt the LLM to generate n
potential questions qi based on as(q), as follows:
Generate a question for the given an-
swer.
answer: [answer]
We then obtain embeddings for all questions us-
ing the text-embedding-ada-002 model, avail-
able from the OpenAI API. For each qi, we cal-
culate the similarity sim(q, qi) with the original
question q, as the cosine between the correspond-
ing embeddings. The answer relevance score, AR,
for question q is then computed as:
AR = 1
n
n∑
i=1
sim(q, qi) (1)
This metric evaluates how closely the generated
answer aligns with the initial question or instruc-
tion.
152Context relevance The context c(q) is consid-
ered relevant to the extent that it exclusively con-
tains information that is needed to answer the ques-
tion. In particular, this metric aims to penalise the
inclusion of redundant information. To estimate
context relevance, given a question q and its con-
text c(q), the LLM extracts a subset of sentences,
Sext, from c(q) that are crucial to answer q, using
the following prompt:
Please extract relevant sentences from
the provided context that can potentially
help answer the following question. If no
relevant sentences are found, or if you
believe the question cannot be answered
from the given context, return the phrase
"Insufficient Information". While extract-
ing candidate sentences you’re not al-
lowed to make any changes to sentences
from given context.
The context relevance score is then computed as:
CR = number of extracted sentences
total number of sentences in c(q) (2)
4 The WikiEval Dataset
To evaluate the proposed framework, we ideally
need examples of question-context-answer triples
which are annotated with human judgments. We
can then verify to what extent our metrics agree
with human assessments of faithfulness, answer
relevance and context relevance. Since we are not
aware of any publicly available datasets that could
be used for this purpose, we created a new dataset,
which we refer to as WikiEval4. To construct the
dataset, we first selected 50 Wikipedia pages cov-
ering events that have happened since the start of
20225. In selecting these pages, we prioritised
those with recent edits. For each of the 50 pages,
we then asked ChatGPT to suggest a question that
can be answered based on the introductory section
of the page, using the following prompt:
Your task is to formulate a question from
given context satisfying the rules given
below:
1. The question should be fully answered
from the given context.
4https://huggingface.co/datasets/
explodinggradients/WikiEval.
5That is, beyond the reported training cutoff of the model
we used in our experiments.
2. The question should be framed from
a part that contains non-trivial informa-
tion.
3. The answer should not contain any
links.
4. The question should be of moderate
difficulty.
5. The question must be reasonable and
must be understood and responded to by
humans.
6. Do not use phrases that ‘provided con-
text’, etc in the question
context:
We also used ChatGPT to answer the generated
question, when given the corresponding introduc-
tory section as context, using the following prompt:
Answer the question using the informa-
tion from the given context.
question: [question]
context: [context]
All questions were annotated along the three con-
sidered quality dimensions by two annotators. Both
annotators were fluent in English and were given
clear instructions about the meaning of the three
considered quality dimensions. For faithfulness
and context relevance, the two annotators agreed in
around 95% of cases. For answer relevance, they
agreed in around 90% of the cases. Disagreements
were resolved after a discussion between the anno-
tators. Tables 1, 2 and 3 show examples from the
WikiEval dataset, focusing in particular on answers
with high and low faithfulness (Table 1), high and
low answer relevance (Table 2), and high and low
context relevance (Table 3).
Faithfulness To obtain human judgements about
faithfulness, we first used ChatGPT to answer the
question without access to any additional context.
We then asked the annotators to judge which of the
two answers was the most faithful (i.e. the standard
one or the one generated without context), given
the question and corresponding Wikipedia page.
Answer relevance We first used ChatGPT to
obtain candidate answers with lower answer rel-
evance, using the following prompt:
Answer the given question in an incom-
plete manner.
question: [question]
153Question Context Answer
Who directed the film Op-
penheimer and who stars
as J. Robert Oppenheimer
in the film?
Oppenheimer is a 2023 biographical thriller film written
and directed by Christopher Nolan. Based on the 2005
biography American Prometheus by Kai Bird and Mar-
tin J. Sherwin, the film chronicles the life of J. Robert
Oppenheimer, a theoretical physicist who was pivotal in
developing the first nuclear weapons as part of the Man-
hattan Project, and thereby ushering in the Atomic Age.
Cillian Murphy stars as Oppenheimer, with Emily Blunt
as Oppenheimer’s wife Katherine "Kitty" Oppenheimer.
High Faithfulness : Christopher
Nolan directed the film Oppen-
heimer. Cillian Murphy stars as J.
Robert Oppenheimer in the film.
Low Faithfulness : James
Cameron directed the film Op-
penheimer. Tom Cruise stars as J.
Robert Oppenheimer in the film.
Table 1: Example from WikiEval, showing answers with high and low faithfulness.
Question Answer
When is the scheduled
launch date and time for
the PSLV-C56 mission,
and where will it be
launched from?
High answer relevance: The PSLV-C56 mission is scheduled to be launched on Sunday, 30
July 2023 at 06:30 IST / 01:00 UTC. It will be launched from the Satish Dhawan Space
Centre, Sriharikota, Andhra Pradesh, India.
Low answer relevance: The scheduled launch date and time for the PSLV-C56 mission have
not been provided.The PSLV-C56 mission is an important space mission for India. It aims to
launch a satellite into orbit to study weather patterns.
Table 2: Example from WikiEval, showing answers with high and low answer relevance.
We then asked human annotators to compare this
answer, and indicate which of the two answers had
the highest answer relevance.
Context relevance To measure this aspect, we
first added additional sentences to the context by
scraping back-links to the corresponding Wikipedia
page. In this way, we were able to add information
to the context that was related but less relevant for
answering the question. For the few pages with-
out any back-links, we instead used ChatGPT to
complete the given context.
5 Experiments
Table 4 analyses the agreement between the met-
rics proposed in Section 3 and the human assess-
ments from the proposed WikiEval dataset. Each
WikiEval instance requires the model to compare
two answers or two context fragments. We count
how often the answer/context preferred by the
model (i.e. with highest estimated faithfulness, an-
swer relevance, or context relevance) coincides
with the answer/context preferred by the human
annotators. We report the results in terms of ac-
curacy (i.e. the fraction of instances on which the
model agrees with the annotators).
To put the results in context, we compare our
proposed metrics (shown as RAGA S in Table 4)
with two baseline methods. For the first method,
shown as GPT Score, we ask ChatGPT to assign a
score between 0 and 10 for the three quality dimen-
sions. To this end, we use a prompt that describes
the meaning of the quality metric and then asks
to score the given answer/context in line with that
definition. For instance, for evaluating faithfulness,
we used the following prompt:
Faithfulness measures the information
consistency of the answer against the
given context. Any claims that are made
in the answer that cannot be deduced
from context should be penalized.
Given an answer and context, assign a
score for faithfulness in the range 0-10.
context: [context]
answer: [answer]
Ties, where the same score is assigned by the LLM
to both answer candidates, were broken randomly.
The second baseline, shown as GPT Ranking, in-
stead asks ChatGPT to select the preferred answer/-
context. In this case, the prompt again includes
a definition of the considered quality metric. For
instance, to evaluate answer relevance, we used the
following prompt:
Answer Relevancy measures the degree
to which a response directly addresses
and is appropriate for a given question.
It penalizes the present of redundant in-
formation or incomplete answers given a
question. Given an question and answer,
rank each answer based on Answer Rele-
vancy.
question: [question]
154Question Context
When was the Chimnabai
Clock Tower completed,
and who was it named af-
ter?
High context relevance: The Chimnabai Clock Tower, also known as the Raopura Tower, is
a clock tower situated in the Raopura area of Vadodara, Gujarat, India. It was completed
in 1896 and named in memory of Chimnabai I (1864–1885), a queen and the first wife of
Sayajirao Gaekwad III of Baroda State.
Low context relevance: The Chimnabai Clock Tower, also known as the Raopura Tower, is
a clock tower situated in the Raopura area of Vadodara, Gujarat, India. It was completed
in 1896 and named in memory of Chimnabai I (1864–1885), a queen and the first wife of
Sayajirao Gaekwad III of Baroda State. It was built in Indo-Saracenic architecture style.
History. Chimnabai Clock Tower was built in 1896. The tower was named after Chimnabai
I (1864–1885), a queen and the first wife of Sayajirao Gaekwad III of Baroda State. It was
inaugurated by Mir Kamaluddin Hussainkhan, the last Nawab of Baroda. During the rule of
Gaekwad, it was a stoppage for horse drawn trams. The clock tower was erected at the cost
of 25,000 (equivalent to 9.2 million or USD 120,000 in 2023).
Table 3: Example from WikiEval, showing answers with high and low context relevance.
Faith. Ans. Rel. Cont. Rel.
RAGA S 0.95 0.78 0.70
GPT Score 0.72 0.52 0.63
GPT Ranking 0.54 0.40 0.52
Table 4: Agreement with human annotators in pairwise
comparisons of faithfulness, answer relevance and con-
text relevance, using the WikEval dataset (accuracy).
answer 1: [answer 1]
answer 2: [answer 2]
The results in Table 4 show that our proposed
metrics are much closer aligned with the human
judgements than the predictions from the two base-
lines. For faithfulness, the RAGA S prediction are
in general highly accurate. For answer relevance,
the agreement is lower, but this is largely due to the
fact that the differences between the two candidate
answers are often very subtle. We found context
relevance to be the hardest quality dimension to
evaluate. In particular, we observed that ChatGPT
often struggles with the task of selecting the sen-
tences from the context that are crucial, especially
for longer contexts.
5.1 Reproducibility
Obtaining reproducible results with (large) lan-
guage models is challenging. For this reason, reli-
able software that uses prompts should account not
only for hallucinations, but for the fact that several
runs of the same experiment under the same config-
uration might yield different results, e.g., because
of an undocumented change in the underlying API,
or because of the inherent randomness in neural
networks. Furthermore, we require the LLM to gen-
erate outputs in structured JSON format. We found
that this largely makes RAGA S compatible with
different LLMs, and ultimately lowers the error rate
when consuming LLM generated text. To measure
the effectiveness of the JSON-formatting, we mea-
sured the correlation between RAGA S scores in
successive runs with and without JSON formatting.
As shown in Figure 1, the scores are clearly more
consistent when JSON formatted outputs are used.
Figure 1: We compare the consistency of RAGA S
scores across two different runs of the model, with
JSON formatting (left) and without (right). The use
of JSON formatting leads to more consistent scores.
6 Python API
RAGA S provides access to metrics and datasets via
an easy-to-use Python API. Its syntax is similar to
other well-known libraries such as transformers
or datasets. As an example, once installed, load-
ing a dataset, evaluating a pipeline with the de-
sired metrics, and exporting the results to a pandas
dataframe can be accomplished with the snippet be-
low. The metrics available at ragas.metrics use
OpenAI’s API by default, which requires having
the appropriate environment variables set up. It is
however possible to experiment with other LLMs
155for evaluation6.
# import required modules
from ragas . metrics import (
answer_relevancy ,
faithfulness ,
context_relevancy ,
)
from ragas import evaluate
from datasets import load_dataset
# loading the eval dataset
amnesty_qa = load_dataset (
' explodinggradients / amnesty_qa ',
'english_v2 '
)
# evaluate
from ragas import evaluate
result = evaluate (
amnesty_qa [" eval "],
metrics =[
faithfulness ,
answer_relevancy ,
context_relevancy ,
],
)
# export results to pandas dataframe
df = result . to_pandas ()
7 Conclusions
We have highlighted the need for automated
reference-free evaluation of RAG systems. In par-
ticular, we have argued the need for an evaluation
framework that can assess faithfulness (i.e. is the
answer grounded in the retrieved context), answer
relevance (i.e. does the answer address the ques-
tion) and context relevance (i.e. is the retrieved
context sufficiently focused). To support the devel-
opment of such a framework, we have introduced
WikiEval, a dataset which human judgements of
these three different aspects. Finally, we have also
described RAGA S, our implementation of the three
considered quality aspects. This framework is easy
to use and can provide developers of RAG sys-
tems with valuable insights, even in the absence
of any ground truth. Our evaluation on WikiEval
has shown that the predictions from RAGA S are
closely aligned with human judgments, especially
for faithfulness and answer relevance.
8 Limitations
This paper introduces a toolkit aimed at provid-
ing an end-to-end evaluation framework for RAG
6See https://github.com/explodinggradients/
ragas/blob/main/docs/howtos/customisations/llms.
ipynb.
systems. It relies heavily on the performance of
the LLMs used for evaluating the different compo-
nents. While the current set of experiments demon-
strate high correlation of these metrics with human
judgements, we acknowledge that relying on LLMs
comes with known limitations. Therefore, careful
reviewing of the suitability of LLMs (ideally pri-
oritizing open models over full-fledged products
behind paid APIs) is critical for RAGA S, and other
contributions in this space, to nurture a healthy
environment. We are also aware of the potential
implications of enabling large user bases into using
more accurate RAG systems, and therefore we will
continue to encourage applications of fair systems
on top of RAGA S.
References
Amos Azaria and Tom M. Mitchell. 2023. The inter-
nal state of an LLM knows when its lying. CoRR,
abs/2304.13734.
Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,
Trevor Cai, Eliza Rutherford, Katie Millican, George
van den Driessche, Jean-Baptiste Lespiau, Bogdan
Damoc, Aidan Clark, Diego de Las Casas, Aurelia
Guy, Jacob Menick, Roman Ring, Tom Hennigan,
Saffron Huang, Loren Maggiore, Chris Jones, Albin
Cassirer, Andy Brock, Michela Paganini, Geoffrey
Irving, Oriol Vinyals, Simon Osindero, Karen Si-
monyan, Jack W. Rae, Erich Elsen, and Laurent Sifre.
2022. Improving language models by retrieving from
trillions of tokens. In International Conference on
Machine Learning, ICML 2022, 17-23 July 2022, Bal-
timore, Maryland, USA, volume 162 of Proceedings
of Machine Learning Research , pages 2206–2240.
PMLR.
Sébastien Bubeck, Varun Chandrasekaran, Ronen El-
dan, Johannes Gehrke, Eric Horvitz, Ece Kamar,
Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-
berg, et al. 2023. Sparks of artificial general intelli-
gence: Early experiments with gpt-4. arXiv preprint
arXiv:2303.12712.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of
deep bidirectional transformers for language under-
standing. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers), pages
4171–4186, Minneapolis, Minnesota. Association for
Computational Linguistics.
Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei
Liu. 2023. Gptscore: Evaluate as you desire. CoRR,
abs/2302.04166.
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-
pat, and Mingwei Chang. 2020. Retrieval augmented
156language model pre-training. In International confer-
ence on machine learning, pages 3929–3938. PMLR.
Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan
Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea
Madotto, and Pascale Fung. 2023. Survey of halluci-
nation in natural language generation. ACM Comput-
ing Surveys, 55(12):1–38.
Saurav Kadavath, Tom Conerly, Amanda Askell, Tom
Henighan, Dawn Drain, Ethan Perez, Nicholas
Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli
Tran-Johnson, Scott Johnston, Sheer El Showk, Andy
Jones, Nelson Elhage, Tristan Hume, Anna Chen,
Yuntao Bai, Sam Bowman, Stanislav Fort, Deep
Ganguli, Danny Hernandez, Josh Jacobson, Jack-
son Kernion, Shauna Kravec, Liane Lovitt, Ka-
mal Ndousse, Catherine Olsson, Sam Ringer, Dario
Amodei, Tom Brown, Jack Clark, Nicholas Joseph,
Ben Mann, Sam McCandlish, Chris Olah, and Jared
Kaplan. 2022. Language models (mostly) know what
they know. CoRR, abs/2207.05221.
Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric
Wallace, and Colin Raffel. 2022. Large language
models struggle to learn long-tail knowledge. CoRR,
abs/2211.08411.
Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke
Zettlemoyer, and Mike Lewis. 2020. Generalization
through memorization: Nearest neighbor language
models. In 8th International Conference on Learning
Representations, ICLR 2020, Addis Ababa, Ethiopia,
April 26-30, 2020. OpenReview.net.
Omar Khattab, Keshav Santhanam, Xiang Lisa Li,
David Hall, Percy Liang, Christopher Potts, and
Matei Zaharia. 2022. Demonstrate-search-predict:
Composing retrieval and language models for
knowledge-intensive NLP. CoRR, abs/2212.14024.
Kenton Lee, Ming-Wei Chang, and Kristina Toutanova.
2019. Latent retrieval for weakly supervised open do-
main question answering. In Proceedings of the 57th
Annual Meeting of the Association for Computational
Linguistics, pages 6086–6096.
Patrick S. H. Lewis, Ethan Perez, Aleksandra Pik-
tus, Fabio Petroni, Vladimir Karpukhin, Naman
Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih,
Tim Rocktäschel, Sebastian Riedel, and Douwe
Kiela. 2020. Retrieval-augmented generation for
knowledge-intensive NLP tasks. In Advances in Neu-
ral Information Processing Systems 33: Annual Con-
ference on Neural Information Processing Systems
2020, NeurIPS 2020, December 6-12, 2020, virtual.
Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun
Nie, and Ji-Rong Wen. 2023. Halueval: A large-
scale hallucination evaluation benchmark for large
language models. CoRR, abs/2305.11747.
Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paran-
jape, Michele Bevilacqua, Fabio Petroni, and Percy
Liang. 2023. Lost in the middle: How language
models use long contexts.
Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,
Daniel Khashabi, and Hannaneh Hajishirzi. 2023.
When not to trust language models: Investigating
effectiveness of parametric and non-parametric mem-
ories. In Proceedings of the 61st Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers) , pages 9802–9822, Toronto,
Canada. Association for Computational Linguistics.
Potsawee Manakul, Adian Liusie, and Mark J. F. Gales.
2023. Selfcheckgpt: Zero-resource black-box hal-
lucination detection for generative large language
models. CoRR, abs/2303.08896.
Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike
Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer,
Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023.
Factscore: Fine-grained atomic evaluation of fac-
tual precision in long form text generation. CoRR,
abs/2305.14251.
Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,
Amnon Shashua, Kevin Leyton-Brown, and Yoav
Shoham. 2023. In-context retrieval-augmented lan-
guage models. CoRR, abs/2302.00083.
Adam Roberts, Colin Raffel, and Noam Shazeer. 2020.
How much knowledge can you pack into the param-
eters of a language model? In Proceedings of the
2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 5418–5426,
Online. Association for Computational Linguistics.
Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon
Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and
Wen-tau Yih. 2023. REPLUG: retrieval-augmented
black-box language models. CoRR, abs/2301.12652.
Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxi-
ang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie
Zhou. 2023a. Is chatgpt a good NLG evaluator? A
preliminary study. CoRR, abs/2303.04048.
Peiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai
Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui.
2023b. Large language models are not fair evaluators.
CoRR, abs/2305.17926.
Shufan Wang, Yixiao Song, Andrew Drozdov, Aparna
Garimella, Varun Manjunatha, and Mohit Iyyer.
2023c. KNN-LM does not improve open-ended text
generation. CoRR, abs/2305.14625.
Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.
Bartscore: Evaluating generated text as text genera-
tion. In Advances in Neural Information Processing
Systems 34: Annual Conference on Neural Informa-
tion Processing Systems 2021, NeurIPS 2021, De-
cember 6-14, 2021, virtual, pages 27263–27277.
Tianhua Zhang, Hongyin Luo, Yung-Sung Chuang, Wei
Fang, Luc Gaitskell, Thomas Hartvigsen, Xixin Wu,
Danny Fox, Helen Meng, and James R. Glass. 2023.
Interpretable unified language checking. CoRR,
abs/2304.03728.
157Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.
Weinberger, and Yoav Artzi. 2020. Bertscore: Evalu-
ating text generation with BERT. In8th International
Conference on Learning Representations, ICLR 2020,
Addis Ababa, Ethiopia, April 26-30, 2020. OpenRe-
view.net.
Wei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Chris-
tian M. Meyer, and Steffen Eger. 2019. MoverScore:
Text generation evaluating with contextualized em-
beddings and earth mover distance. In Proceedings
of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th Interna-
tional Joint Conference on Natural Language Pro-
cessing (EMNLP-IJCNLP), pages 563–578, Hong
Kong, China. Association for Computational Lin-
guistics.
158

Evaluating Retrieval Quality in Retrieval-Augmented Generation
Alireza Salemi
University of Massachusetts Amherst
Amherst, MA, United States
asalemi@cs.umass.edu
Hamed Zamani
University of Massachusetts Amherst
Amherst, MA, United States
zamani@cs.umass.edu
ABSTRACT
Evaluating retrieval-augmented generation (RAG) presents chal-
lenges, particularly for retrieval models within these systems. Tra-
ditional end-to-end evaluation methods are computationally expen-
sive. Furthermore, evaluation of the retrieval model’s performance
based on query-document relevance labels shows a small correla-
tion with the RAG system’s downstream performance. We propose
a novel evaluation approach, eRAG, where each document in the
retrieval list is individually utilized by the large language model
within the RAG system. The output generated for each document is
then evaluated based on the downstream task ground truth labels.
In this manner, the downstream performance for each document
serves as its relevance label. We employ various downstream task
metrics to obtain document-level annotations and aggregate them
using set-based or ranking metrics. Extensive experiments on a
wide range of datasets demonstrate that eRAG achieves a higher
correlation with downstream RAG performance compared to base-
line methods, with improvements in Kendall’s𝜏 correlation ranging
from 0.168 to 0.494. Additionally, eRAG offers significant compu-
tational advantages, improving runtime and consuming up to 50
times less GPU memory than end-to-end evaluation.
CCS CONCEPTS
• Computing methodologies →Natural language generation;
• Information systems →Evaluation of retrieval results.
KEYWORDS
Evaluation; retrieval quality; retrieval-augmented generation
ACM Reference Format:
Alireza Salemi and Hamed Zamani. 2024. Evaluating Retrieval Quality in
Retrieval-Augmented Generation. In Proceedings of the 47th International
ACM SIGIR Conference on Research and Development in Information Retrieval
(SIGIR ’24), July 14–18, 2024, Washington, DC, USA. ACM, New York, NY,
USA, 6 pages. https://doi.org/10.1145/3626772.3657957
1 INTRODUCTION
Retrieval-augmented generation (RAG) has emerged as a prominent
approach in natural language processing, combining the strengths
of retrieval and generation models [35], with use cases in decreas-
ing hallucination [ 1, 29], knowledge-grounding [ 9, 16, 34], and
personalization [25, 26]. Evaluating RAG systems is important as
it ensures the effectiveness of integrating retrieval-based methods
This work is licensed under a Creative Commons Attribution
International 4.0 License.
SIGIR ’24, July 14–18, 2024, Washington, DC, USA
© 2024 Copyright held by the owner/author(s).
ACM ISBN 979-8-4007-0431-4/24/07.
https://doi.org/10.1145/3626772.3657957
with generative models [10, 23]. Traditionally, RAG evaluation has
primarily relied on end-to-end assessment, which entails compar-
ing the generated output with one or more ground truth references
[20]. While this is crucial, it presents several limitations, especially,
for evaluating retrieval models in RAG systems.
First, end-to-end evaluation lacks transparency regarding which
retrieved document contributed to the generated output, hindering
interpretability of the system’s behavior. Secondly, it is resource-
intensive, consuming significant time and computational power,
particularly when dealing with a large set of retrieval results con-
sumed by the LLM. To process long input sequences resulting from
the utilization of all retrieved documents by the LLM, GPUs with
substantial memory capacities are essential for end-to-end evalu-
ation. Moreover, many ranking systems rely on interleaving (i.e.,
replacing one or more documents in the result list) for evaluation
and optimization, which further complicates the evaluation, as
slight variations in retrieval results necessitate re-computation of
the RAG pipeline. Finally, optimizing ranking models often requires
document-level feedback, such as user clicks [3, 6]. However, end-
to-end evaluation only provides list-level feedback for the retrieval
results. That said, this paper studies retrieval evaluation in RAG.
Human annotations can be a potential solution for evaluating
retrieval models in RAG, however, accurate annotations are often
challenging and costly to obtain. More recently, with the emergence
of large language models (LLMs) and their advanced capabilities
in reasoning and text comprehension, they have been utilized to
annotate documents for retrieval evaluation [10, 23]. Nevertheless,
these approaches predominantly evaluate the retriever in RAG sys-
tems based on human preferences, whereas the primary objective
of the retrieval model in RAG is to serve the LLM that leverages
the retrieved results [35]. That said, our extensive investigation on
a diverse set of RAG systems for open-domain question answer-
ing, fact verification, and dialogue systems reveals that employing
human annotations, such as theprovenance labels in the KILT bench-
mark [20], for evaluating the retrieval models within a RAG system
exhibits only a minor correlation with the downstream RAG per-
formance. This indicates a lack of meaningful relationship between
the evaluated metrics and the downstream performance of RAG.
In this paper, we propose eRAG, a new approach for evaluating
retrievers in RAG systems, where we apply the LLM in RAG system
on each document in the retrieval result list individually and use
the LLM’s output to provide document-level annotations. These
annotations can be obtained using any arbitrary downstream task
metric, such as accuracy, exact match, or ROUGE [17]. We can then
apply a set-based or ranking metric as an aggregation function to
obtain a single evaluation score for each retrieval result list.
We evaluate our proposed approach on question answering, fact-
checking, and dialogue generation from the knowledge-intensive
language tasks (KILT) benchmark [20]. Our results demonstrate that
2395
SIGIR ’24, July 14–18, 2024, Washington, DC, USA Alireza Salemi and Hamed Zamani
our proposed approach achieves the highest correlation with the
downstream performance of the RAG system in comparison with
the baselines. Specifically, we observe an absolute improvement in
Kendall’s tau correlation ranging between 0.168 and 0.494 across the
evaluated datasets. Furthermore, we investigate the impact of differ-
ent retrieval augmentation methods, the quantity of retrieved docu-
ments, and the LLM size on correlation. Finally, we demonstrate that
our approach offers significant computational advantages, consum-
ing up to 50 times less memory compared to end-to-end evaluation.
To facilitate research in this domain, we make eRAG’s implementa-
tion publicly available at: https://github.com/alirezasalemi7/eRAG.
2 EVALUATING RETRIEVERS IN RAG
Generally, two predominant methods are used for obtaining rele-
vance labels for retrieval evaluation. The first approach involves
human judgment to assess the relevance of a query to documents
within a corpus. The main issue with this approach is that human
annotation can be costly and is often impractical for evaluating all
documents in a corpus [ 28]. Moreover, human annotation relies
on human preferences to judge the relevance of documents to a
query. However, a document deemed relevant based on human
preferences may not be useful for an LLM in fulfilling its task.
The second approach utilizes the downstream ground truth out-
put associated with the query to provide weak relevance labels.
In this method, a retrieved document containing the downstream
ground truth is considered relevant [8, 14, 24, 27]. This method also
presents its own challenges. This approach is impractical, partic-
ularly in scenarios where the task involves long-text generation
or text classification, as downstream task labels might not exist
within documents. Also, one document can be useful for an LLM
in fulfilling its task without containing the ground truth labels.
Even though we are not aware any work that use LLMs for
evaluating retrieval models in RAG, LLMs can be leveraged to la-
bel documents based on their relevance to a query. Inspired by
Thomas et al. [30], the LLM functions as a binary classifier, indi-
cating whether a document is relevant to the query or not. The
mentioned challenges persist even with the judgment of LLMs, espe-
cially if the LLM responsible for labeling differs from the LLM in the
RAG pipeline. Besides, employing LLMs as judges in this scenario
can pose challenges due to the computational cost of running them
on a large set of retrieved documents and memory constraints.
To mitigate these problems, we propose eRAG, a novel approach
that involves utilizing the LLM in RAG system itself as the arbiter
for generating labels to evaluate the retrieval model.
Using Downstream Large Language Model in RAG as Doc-
ument Annotator. Consider a retrieval model Rthat produces
a ranked list R𝑘 with 𝑘 documents for the LLM Mtasked with
performing a specific task, utilizing a downstream evaluation func-
tion EM. The LLM Mtakes a ranked list of documents as its input
along with the query 𝑞, and generates an output represented as
¯𝑦 = M(𝑞, R𝑘 ). For the documents in R𝑘 , we feed each document in-
dividually to the LLMMwith the query and evaluate the generated
answer to create the label for each document, expressed as:
G𝑞 [𝑑]= EM(M(𝑞, {𝑑}), 𝑦) : ∀𝑑 ∈R𝑘 (1)
where 𝑦 is the expected downstream output for the query. We can
employ the created G𝑞 to utilize any ranking metric to evaluate R.
Note that the runtime cost of a vanilla transformer [32] scales
quadratically with its input length. Consequently, for end-to-end
evaluation, the cost of running a transformer on a ranked list with
𝑘 documents, with an average length of 𝑑, to generate an output
with length 𝑙 is 𝑂 (𝑙𝑘 2𝑑2). Conversely, in our approach, as each
document is individually fed to the LLM for k times, the cost is
𝑂 (𝑙𝑘𝑑 2), proving to be more efficient than end-to-end evaluation.
Retrieval Evaluation Metrics. For a ranked list R𝑘 , comprising
𝑘 retrieved documents generated by a retrieval model R, an evalua-
tion metric ERassigns a score ER(R𝑘, G𝑞)∈[ 0, 1], by comparing
the ranked list with the relevance scoresG𝑞, which is a function that
maps each document to a scalar relevance score for the document
with respect to the query𝑞 (i.e., G𝑞 (𝑑)= 𝑠𝑑 ). Various definitions ex-
ist for the evaluation metricER; in this paper, we examine Precision
(P), Recall (R), Mean Average Precision (MAP), Mean Reciprocal
Rank (MRR) [2], Normalized Discounted Cumulative Gain (NDCG)
[11], and Hit Rate. Note that when dealing with non-binary rele-
vance labels, precision considers the average value of relevance
labels, while Hit Ratio considers the maximum value among them.
3 EXPERIMENTS
3.1 Setup
Datasets and Evaluation. We use Natural Questions (NQ) [15],
TriviaQA [13], HotpotQA [33], FEVER [31], and Wizard of Wikipedia
(WoW) [4] datasets from the KILT [20] benchmark. Due to the un-
availability of ground truth labels for the test set, we utilize the
publicly accessible validation set. As the retrieval corpus, we employ
the Wikipedia dump of the KILT benchmark and adhere to the pre-
processing outlined by Karpukhin et al. [14], where each document
is segmented into passages, each constrained to a maximum length
of 100 words. The concatenation of the article title and passage
is used as a document. The KILT benchmark furnishes document-
level relevance labels (called Provenance) for its datasets, and these
are employed for evaluating retrieval performance. In line with
our preprocessing method, we define all passages within a positive
document as positive passages for our evaluation. For relevance
evaluation using an LLM, we employ Mistral1 [12] to annotate each
document within the retrieved list, determining whether it is rele-
vant to the query or not. We adopt the metrics recommended by
the KILT benchmark, namely Exact Match (EM) for NQ, TriviaQA,
and HotpotQA, Accuracy for FEVER, and F1 for the WoW dataset.
Experiments Configuration. In all experiments, unless explicitly
stated otherwise, we employ T5-small [21] with Fusion-in-Decoder
(FiD) [9] as the LLM. We employ AdamW [19] with a weight decay
of 10−2 and a learning rate of 5 ×10−5 for 10 epochs, incorporating
linear warmup for the initial 5% of training steps. The effective
batch size is set to 64. Each model is trained using an A100 Nvidia
GPU. For document retrieval during training, we utilize BM25 [22]
implemented in Pyserini [18] to retrieve 50 documents to augment
the input with them. For fast vector search in dense retrieval with
Contriever2 [7], we use Faiss [5] flat index.
1https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2
2https://huggingface.co/facebook/contriever
2396Evaluating Retrieval Quality in Retrieval-Augmented Generation SIGIR ’24, July 14–18, 2024, Washington, DC, USA
Table 1: The correlation between each evaluation approach and the downstream performance of the LLM. T5-small with FiD
with 50 retrieved documents is used. We do not report correlation for the Answers method for FEVER and WOW datasets
because the answers to queries do not exist in the document since FEVER is a classification dataset and WoW is long-text
generation. For the WoW dataset, we only report correlation on Precision and Hit Ratio because other metrics do not support
non-integer relevance labels. Tau is Kendall’s tau and rho is Spearman’s rho.
Rele
vance
Annotation Metric
BM25 Contrie
ver
NQ T
riviaQA Hotp
otQA FEVER WoW NQ T
riviaQA HotpotQA FEVER WoW
tau rho tau rho tau rho tau rho tau rho tau rho tau rho tau rho tau rho tau rho
Containing
the
Answ
er
MAP 0.349 0.417 0.298 0.364 0.359 0.423 - - - - 0.303 0.366 0.265 0.325 0.379 0.429 - - - -
MRR 0.361 0.417 0.313 0.340 0.398 0.449 - - - - 0.301 0.353 0.257 0.292 0.384 0.430 - - - -
NDCG 0.357 0.427 0.298 0.365 0.370 0.435 - - - - 0.313 0.378 0.270 0.331 0.385 0.437 - - - -
P 0.353 0.411 0.276 0.333 0.396 0.454 - - - - 0.346 0.403 0.283 0.340 0.406 0.449 - - - -
R 0.325 0.325 0.232 0.232 0.375 0.375 - - - - 0.319 0.319 0.215 0.215 0.401 0.401 - - - -
Hit
Ratio 0.325 0.325 0.232 0.232 0.375 0.375 - - - - 0.319 0.319 0.215 0.215 0.401 0.401 - - - -
KILT
Pr
ovenance
MAP 0.181 0.218 0.142 0.172 0.007 0.009 0.026 0.032 0.015 0.021 0.161 0.196 0.113 0.137 0.128 0.155 0.045 0.056 0.055 0.080
MRR 0.177 0.205 0.151 0.175 0.074 0.080 0.036 0.040 0.013 0.017 0.152 0.173 0.120 0.136 0.151 0.169 0.045 0.049 0.059 0.081
NDCG 0.179 0.216 0.142 0.172 0.021 0.026 0.029 0.036 0.013 0.019 0.159 0.193 0.115 0.140 0.134 0.162 0.045 0.056 0.056 0.081
P 0.163 0.192 0.140 0.165 0.139 0.164 0.043 0.051 0.011 0.015 0.131 0.157 0.108 0.130 0.181 0.215 0.033 0.040 0.045 0.064
R 0.216 0.216 0.187 0.187 0.113 0.113 0.050 0.050 0.019 0.023 0.157 0.157 0.135 0.135 0.163 0.163 0.038 0.038 0.056 0.068
Hit
Ratio 0.216 0.216 0.187 0.187 0.113 0.113 0.050 0.050 0.019 0.023 0.157 0.157 0.135 0.135 0.163 0.163 0.038 0.038 0.056 0.068
Rele
vance
Annotation
with LLM
(Mistral 7B)
MAP 0.045 0.055 0.176 0.216 0.034 0.042 0.018 0.022 -0.005 -0.008 0.032 0.039 0.174 0.213 0.051 0.063 0.021 0.026 -0.002 -0.003
MRR 0.060 0.062 0.189 0.196 0.001 0.001 -0.021 -0.022 -0.008 -0.011 0.048 0.050 0.143 0.151 0.034 0.038 -0.007 -0.007 0.004 0.005
NDCG 0.049 0.060 0.178 0.218 0.032 0.039 0.018 0.022 -0.006 -0.009 0.036 0.044 0.175 0.214 0.049 0.060 0.022 0.028 0.000 0.000
P 0.028 0.034 0.137 0.166 -0.004 -0.006 0.021 0.025 -0.005 -0.008 0.002 0.003 0.138 0.167 0.010 0.013 0.014 0.017 -0.006 -0.010
R 0.014 0.014 0.032 0.032 -0.016 -0.016 0.019 0.019 0.003 0.003 0.000 0.000 0.039 0.039 -0.042 -0.042 -0.017 -0.017 0.017 0.021
Hit
Ratio 0.014 0.014 0.032 0.032 -0.016 -0.016 0.019 0.019 0.003 0.003 0.000 0.000 0.039 0.039 -0.042 -0.042 -0.017 -0.017 0.017 0.021
eRA
G
Annotations
MAP 0.492 0.575 0.474 0.578 0.610 0.694 0.386 0.463 - - 0.467 0.544 0.427 0.519 0.634 0.705 0.399 0.479 - -
MRR 0.503 0.577 0.486 0.553 0.629 0.695 0.592 0.611 - - 0.466 0.537 0.424 0.495 0.639 0.698 0.481 0.504 - -
NDCG 0.505 0.590 0.486 0.592 0.612 0.697 0.404 0.484 - - 0.481 0.560 0.440 0.536 0.635 0.705 0.403 0.484 - -
Pa 0.529 0.598 0.484 0.577 0.594 0.663 0.329 0.391 0.504 0.669 0.522 0.586 0.482 0.571 0.633 0.695 0.378 0.449 0.540 0.712
R 0.519 0.519 0.426 0.426 0.619 0.619 0.301 0.301 - - 0.488 0.488 0.408 0.408 0.631 0.631 0.299 0.299 - -
Hit
Ratiob 0.519 0.519 0.426 0.426 0.619 0.619 0.301 0.301 0.390 0.532 0.488 0.488 0.408 0.408 0.631 0.631 0.299 0.299 0.414 0.561
a For
non-integer relevance labels, precision is equal to average of the relevance labels.
b For non-integer relevance labels, hit ratio is equal to maximum of the relevance labels.
3.2
Main Findings
How do different retrieval evaluation methods correlate with
the end-to-end downstream performance in RAG?. To com-
pare the different evaluation strategies for evaluating retriever in
RAG, we report the correlation between the scores generated for
each method and the downstream performance of the LLM (i.e.,
T5-small with FiD and 50 retrieved documents) in Table 1. The
results indicate that eRAG attains the highest correlation compared
to other evaluation approaches. Furthermore, the results show that
regardless of the retrieval model employed, eRAG consistently out-
performs others in terms of correlation with the LLM’s downstream
performance. Interestingly, the most common approaches, KILT
Provenance and Annotation with LLMs, that are, document-level
relevance labels and using LLMs to assign a relevance label to each
retrieved document, have the lowest correlation with the down-
stream performance of the LLM. This finding confirms that the
LLM as the consumer of the retrieved results in RAG is the best
judge for the performance of the retrieval model.
How do different retrieval evaluation methods in RAG per-
form as the size of retrieval results increases? To address this,
we varied the number of retrieved documents and computed the
correlation between the metric with highest correlation for each
method in Table 1 at each specified number of retrieved documents
and the downstream performance of the LLM given that number
of retrieved documents. For the sake of space, we limit our exper-
iments to three datasets: NQ for question answering, FEVER for
fact-checking, and WoW for long-text generation. The results of
this experiment are shown in Figure 1. The outcomes of this experi-
ment reveal that irrespective of the quantity of retrieved documents,
Figure 1: The correlation between evaluation approaches
and the LLM’s downstream performance varying number
of retrieved documents by BM25. T5-small with FiD is used.
The metric with the highest correlation in Table 1 is used.
our suggested evaluation strategy consistently exhibits a higher
correlation with the downstream performance of the LLM. Further-
more, the results illustrate that augmenting the number of retrieved
documents leads to a decline in correlation—a intuitive observation,
as all metrics assess each document-relevance label independently
for scoring a ranked list, while the LLM uses information from the
entirety of these documents to accomplish its task.
How does our method correlate with the downstream RAG
performance as the size of large language models increases?
In addressing this question, we computed the correlation between
our retrieval evaluation strategy and the downstream performance
of the LMs with two distinct sizes (i.e., T5-small with FiD consisting
of 60M and T5-base with FiD consisting of 220M parameters). For
the sake of space, we limit our experiments to three datasets: NQ
for question answering, FEVER for fact-checking, and WoW for
long-text generation. The results illustrated in Figure 2 indicate that,
for certain datasets, there is a higher correlation with the smaller
LLM, while for others, a higher correlation is observed with the
2397SIGIR ’24, July 14–18, 2024, Washington, DC, USA Alireza Salemi and Hamed Zamani
Figure 2: The correlation between eRAG and the downstream
performance of different LLM sizes. In this experiment, T5-
small (60M parameters) and T5-base (220M parameters) with
FiD are used. The documents are retrieved using BM25.
Figure 3: The correlation between eRAG and the downstream
performance of FiD and IPA LLMs. T5-small with 10 docu-
ments retrieved by BM25 is used. The number of documents
is chosen based on the limitations of the input size in IPA.
larger model. Nonetheless, in none of the cases is there a significant
difference between the correlations, suggesting that the proposed
approach is effective regardless of the LLM size.
How does different retrieval-augmentation approaches af-
fect the correlation between eRAG and the downstream RAG
performance? We applied eRAG to two LLMs. One LLM utilizes
In-Prompt Augmentation (IPA), where the retrieved results are ap-
pended to the input of the LLM. The other LLM employs Fusion-in-
Decoder (FiD) [9], wherein each retrieved document is individually
processed by the encoder, and subsequently, the representations
for all documents are concatenated together and fed to the decoder.
For the sake of space, we limit our experiments to NQ for question
answering, FEVER for fact-checking, and WoW for long-text gen-
eration. The correlation between eRAG and the outputs of each
LLM is illustrated in Figure 3. Interestingly, the results suggest that
although there is no significant difference between the correlation
of eRAG with IPA and FiD LLMs, eRAG consistently exhibits a
higher correlation with the FiD. This observation can be elucidated
by considering the distinction between IPA and FiD methodologies.
In IPA, all documents are concatenated together and then presented
as a single input to the LLM. In contrast, FiD processes each docu-
ment individually by feeding them separately to the LLM’s encoder.
Given that our approach aligns more closely with FiD, we believe
this alignment is a contributing factor to the higher correlation
between eRAG and the downstream performance of FiD.
How much more efficient is eRAG compared to the end-to-
end evaluation? Here, we consider two factors: inference time
and memory consumption. For inference time, we compare the total
time required for end-to-end evaluation to generate scores with the
total time used by eRAG. In this experiment, we opt for the batch
size of each approach to be as large as possible, maximizing the
Table 2: The runtime and memory consumption of eRAG in
comparison with end-to-end evaluation. T5-small with FiD,
consuming 50 documents is used.
Dataset Runtime
(GPU) Memor
y Consumption (GPU)
E2E eRA
G E2E eRA
G-Query eRA
G-Document
NQ 918
sec 351
sec 75.0
GB 4.9
GB 1.5
GB
TriviaQA 1819
sec 686
sec 46.2
GB 5.4
GB 1.5
GB
HotpotQA 1844
sec 712
sec 52.4
GB 5.5
GB 1.5
GB
FEVER 3395
sec 1044
sec 66.5
GB 4.1
GB 1.5
GB
WoW 912
sec 740
sec 47.9
GB 6.5
GB 1.5
GB
utilization of the entire GPU memory. The results of this experiment
are reported in Table 2. The findings indicate that, on average, eRAG
is 2.468 times faster than end-to-end evaluation. Further elaborating,
the speedup for eRAG ranges from 1.232 to 3.252 times compared to
end-to-end evaluation across the datasets, where the least speedup
is for the long-text generation task (i.e., WoW).
To compare memory consumption between eRAG and end-to-
end evaluation, we conducted two experiments. First, we compared
the maximum memory required by end-to-end evaluation to assess
a query with the maximum memory demanded by eRAG for the
same evaluation. To carry out this comparison, we configured the
batch size for end-to-end evaluation to 1, while for eRAG, we set
it to the same number of documents used for one query by end-
to-end evaluation (we call this query-level configuration). In the
subsequent experiments, we set both batch sizes to 1 to assess the
extent to which eRAG demonstrates superior memory efficiency
compared to end-to-end evaluation under the most efficient con-
figuration (we call this document-level configuration). The results
of these experiments are reported in Table 2. The findings indi-
cate that in the query-level configuration, eRAG exhibits between
7 to 15 times greater memory efficiency compared to end-to-end
evaluation. Furthermore, in the document-level configuration, this
efficiency gap widens, with eRAG demonstrating 30 to 48 times
more memory efficiency than end-to-end evaluation across differ-
ent dataset. In summary, these experiments suggest that eRAG is
more efficient than end-to-end evaluation of a vanilla transformer,
excelling in both inference time and memory utilization.
4 CONCLUSION
This paper explores various approaches for evaluating retrieval
models within a RAG pipeline. Additionally, it introduces eRAG, a
novel approach for evaluating retrieval models in the RAG pipeline.
eRAG leverages the per-document performance of the LLM on the
downstream task to generate relevance labels. The findings suggest
that the proposed approach exhibits significantly higher correlation
with the downstream performance of the LLM. Furthermore, eRAG
demonstrates greater efficiency than end-to-end evaluation in terms
of both memory consumption and inference time.
ACKNOWLEDGMENT
This work was supported in part by the Center for Intelligent In-
formation Retrieval, in part by Lowe’s, and in part by an Amazon
Research Award, Fall 2022 CFP. Any opinions, findings and conclu-
sions or recommendations expressed in this material are those of
the authors and do not necessarily reflect those of the sponsor.
2398Evaluating Retrieval Quality in Retrieval-Augmented Generation SIGIR ’24, July 14–18, 2024, Washington, DC, USA
REFERENCES
[1] Garima Agrawal, Tharindu Kumarage, Zeyad Alghami, and Huan Liu.
2023. Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey.
arXiv:2311.07914 [cs.CL]
[2] Nick Craswell. 2009. Mean Reciprocal Rank. Springer US, Boston, MA, 1703–1703.
https://doi.org/10.1007/978-0-387-39940-9_488
[3] Romain Deffayet, Philipp Hager, Jean-Michel Renders, and Maarten de Rijke. 2023.
An Offline Metric for the Debiasedness of Click Models. InProceedings of the 46th
International ACM SIGIR Conference on Research and Development in Information
Retrieval (, Taipei, Taiwan,) (SIGIR ’23). Association for Computing Machinery,
New York, NY, USA, 558–568. https://doi.org/10.1145/3539618.3591639
[4] Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason
Weston. 2019. Wizard of Wikipedia: Knowledge-Powered Conversational Agents.
In International Conference on Learning Representations. https://openreview.net/
forum?id=r1l73iRqKm
[5] Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy,
Pierre-Emmanuel Mazaré, Maria Lomeli, Lucas Hosseini, and Hervé Jégou. 2024.
The Faiss library. (2024). arXiv:2401.08281 [cs.LG]
[6] Fan Guo, Chao Liu, and Yi Min Wang. 2009. Efficient multiple-click models
in web search. InProceedings of the Second ACM International Conference on
Web Search and Data Mining (Barcelona, Spain) (WSDM ’09). Association for
Computing Machinery, New York, NY, USA, 124–131. https://doi.org/10.1145/
1498759.1498818
[7] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bo-
janowski, Armand Joulin, and Edouard Grave. 2022. Unsupervised Dense Infor-
mation Retrieval with Contrastive Learning. Transactions on Machine Learning
Research (2022). https://openreview.net/forum?id=jKN1pXi7b0
[8] Gautier Izacard and Edouard Grave. 2021. Distilling Knowledge from Reader
to Retriever for Question Answering. InInternational Conference on Learning
Representations. https://openreview.net/forum?id=NTEz-6wysdb
[9] Gautier Izacard and Edouard Grave. 2021. Leveraging Passage Retrieval with
Generative Models for Open Domain Question Answering. In Proceedings of the
16th Conference of the European Chapter of the Association for Computational
Linguistics: Main Volume, Paola Merlo, Jorg Tiedemann, and Reut Tsarfaty (Eds.).
Association for Computational Linguistics, Online, 874–880. https://doi.org/10.
18653/v1/2021.eacl-main.74
[10] Jithin James and Shahul Es. 2023. Ragas: Evaluation framework for your retrieval
augmented generation (rag) pipelines. https://github.com/explodinggradients/
ragas.
[11] Kalervo Järvelin and Jaana Kekäläinen. 2000. IR evaluation methods for retrieving
highly relevant documents. In Proceedings of the 23rd Annual International ACM
SIGIR Conference on Research and Development in Information Retrieval (Athens,
Greece) (SIGIR ’00). Association for Computing Machinery, New York, NY, USA,
41–48. https://doi.org/10.1145/345508.345545
[12] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, De-
vendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,
Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,
Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,
and William El Sayed. 2023. Mistral 7B. arXiv:2310.06825 [cs.CL]
[13] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. TriviaQA: A
Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension.
In Proceedings of the 55th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), Regina Barzilay and Min-Yen Kan (Eds.).
Association for Computational Linguistics, Vancouver, Canada, 1601–1611. https:
//doi.org/10.18653/v1/P17-1147
[14] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey
Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for Open-
Domain Question Answering. In Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP), Bonnie Webber, Trevor Cohn,
Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics, Online,
6769–6781. https://doi.org/10.18653/v1/2020.emnlp-main.550
[15] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur
Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee,
Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M.
Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019. Natural Questions: A
Benchmark for Question Answering Research. Transactions of the Association for
Computational Linguistics 7 (2019), 452–466. https://doi.org/10.1162/tacl_a_00276
[16] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,
Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel,
Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-augmented generation for
knowledge-intensive NLP tasks. InProceedings of the 34th International Conference
on Neural Information Processing Systems (Vancouver, BC, Canada) (NIPS’20).
Curran Associates Inc., Red Hook, NY, USA, Article 793, 16 pages.
[17] Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries.
In Text Summarization Branches Out. Association for Computational Linguistics,
Barcelona, Spain, 74–81. https://aclanthology.org/W04-1013
[18] Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-Hong Yang, Ronak Pradeep,
and Rodrigo Nogueira. 2021. Pyserini: A Python Toolkit for Reproducible In-
formation Retrieval Research with Sparse and Dense Representations. InPro-
ceedings of the 44th International ACM SIGIR Conference on Research and De-
velopment in Information Retrieval(Virtual Event, Canada) (SIGIR ’21). Asso-
ciation for Computing Machinery, New York, NY, USA, 2356–2362. https:
//doi.org/10.1145/3404835.3463238
[19] Ilya Loshchilov and Frank Hutter. 2019. Decoupled Weight Decay Regularization.
In International Conference on Learning Representations. https://openreview.net/
forum?id=Bkg6RiCqY7
[20] Fabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani,
Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Mail-
lard, Vassilis Plachouras, Tim Rocktäschel, and Sebastian Riedel. 2021. KILT: a
Benchmark for Knowledge Intensive Language Tasks. In Proceedings of the 2021
Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, Kristina Toutanova, Anna Rumshisky,
Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell,
Tanmoy Chakraborty, and Yichao Zhou (Eds.). Association for Computational
Linguistics, Online, 2523–2544. https://doi.org/10.18653/v1/2021.naacl-main.200
[21] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the
limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn.
Res. 21, 1, Article 140 (jan 2020), 67 pages.
[22] Stephen E. Robertson, Steve Walker, Susan Jones, Micheline Hancock-Beaulieu,
and Mike Gatford. 1994. Okapi at TREC-3. In Text Retrieval Conference. https:
//api.semanticscholar.org/CorpusID:3946054
[23] Jon Saad-Falcon, Omar Khattab, Christopher Potts, and Matei Zaharia. 2023.
ARES: An Automated Evaluation Framework for Retrieval-Augmented Genera-
tion Systems. arXiv:2311.09476 [cs.CL]
[24] Alireza Salemi, Juan Altmayer Pizzorno, and Hamed Zamani. 2023. A Symmet-
ric Dual Encoding Dense Retrieval Framework for Knowledge-Intensive Visual
Question Answering. In Proceedings of the 46th International ACM SIGIR Con-
ference on Research and Development in Information Retrieval (Taipei, Taiwan)
(SIGIR ’23). Association for Computing Machinery, New York, NY, USA, 110–120.
https://doi.org/10.1145/3539618.3591629
[25] Alireza Salemi, Surya Kallumadi, and Hamed Zamani. 2024. Optimization Meth-
ods for Personalizing Large Language Models through Retrieval Augmentation.
In Proceedings of the 47th Annual International ACM SIGIR Conference on Research
and Development in Information Retrieval (Washington, DC, USA) (SIGIR ’24). (to
appear).
[26] Alireza Salemi, Sheshera Mysore, Michael Bendersky, and Hamed Za-
mani. 2023. LaMP: When Large Language Models Meet Personalization.
arXiv:2304.11406 [cs.CL]
[27] Alireza Salemi, Mahta Rafiee, and Hamed Zamani. 2023. Pre-Training Multi-
Modal Dense Retrievers for Outside-Knowledge Visual Question Answering. In
Proceedings of the 2023 ACM SIGIR International Conference on Theory of Informa-
tion Retrieval(Taipei, Taiwan) (ICTIR ’23). Association for Computing Machinery,
New York, NY, USA, 169–176. https://doi.org/10.1145/3578337.3605137
[28] Donia Scott, Rossano Barone, and Rob Koeling. 2012. Corpus Annotation as a
Scientific Task. In Proceedings of the Eighth International Conference on Language
Resources and Evaluation (LREC’12), Nicoletta Calzolari, Khalid Choukri, Thierry
Declerck, Mehmet Uğur Doğan, Bente Maegaard, Joseph Mariani, Asuncion
Moreno, Jan Odijk, and Stelios Piperidis (Eds.). European Language Resources
Association (ELRA), Istanbul, Turkey, 1481–1485. http://www.lrec-conf.org/
proceedings/lrec2012/pdf/569_Paper.pdf
[29] Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. 2021.
Retrieval Augmentation Reduces Hallucination in Conversation. In Findings
of the Association for Computational Linguistics: EMNLP 2021. Association for
Computational Linguistics, Punta Cana, Dominican Republic, 3784–3803. https:
//doi.org/10.18653/v1/2021.findings-emnlp.320
[30] Paul Thomas, Seth Spielman, Nick Craswell, and Bhaskar Mitra. 2023.
Large language models can accurately predict searcher preferences.
arXiv:2309.10621 [cs.IR]
[31] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal.
2018. FEVER: a Large-scale Dataset for Fact Extraction and VERification. In
Proceedings of the 2018 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, Volume 1 (Long
Papers), Marilyn Walker, Heng Ji, and Amanda Stent (Eds.). Association for
Computational Linguistics, New Orleans, Louisiana, 809–819. https://doi.org/10.
18653/v1/N18-1074
[32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All
you Need. In Advances in Neural Information Processing Systems, I. Guyon, U. Von
Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.),
Vol. 30. Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/
2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf
[33] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan
Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A Dataset for
Diverse, Explainable Multi-hop Question Answering. In Proceedings of the 2018
Conference on Empirical Methods in Natural Language Processing, Ellen Riloff,
2399SIGIR ’24, July 14–18, 2024, Washington, DC, USA Alireza Salemi and Hamed Zamani
David Chiang, Julia Hockenmaier, and Jun’ichi Tsujii (Eds.). Association for
Computational Linguistics, Brussels, Belgium, 2369–2380. https://doi.org/10.
18653/v1/D18-1259
[34] Hamed Zamani and Michael Bendersky. 2024. Stochastic RAG: End-to-End
Retrieval-Augmented Generation through Expected Utility Maximization. In
Proceedings of the 47th Annual International ACM SIGIR Conference on Research
and Development in Information Retrieval (SIGIR ’24). (to appear).
[35] Hamed Zamani, Fernando Diaz, Mostafa Dehghani, Donald Metzler, and Michael
Bendersky. 2022. Retrieval-Enhanced Machine Learning. InProceedings of the 45th
International ACM SIGIR Conference on Research and Development in Information
Retrieval (Madrid, Spain) (SIGIR ’22). Association for Computing Machinery, New
York, NY, USA, 2875–2886. https://doi.org/10.1145/3477495.3531722
2400

Published in Transactions on Machine Learning Research (06/2023)
Augmented Language Models: a Survey
Grégoire Mialon∗ gmialon@meta.com
Roberto Dessì∗† rdessi@meta.com
Maria Lomeli∗ marialomeli@meta.com
Christoforos Nalmpantis∗ christoforos@meta.com
Ram Pasunuru∗ rpasunuru@meta.com
Roberta Raileanu∗ raileanu@meta.com
Baptiste Rozière∗ broz@meta.com
Timo Schick∗ schick@meta.com
Jane Dwivedi-Yu∗ janeyu@meta.com
Asli Celikyilmaz∗ aslic@meta.com
Edouard Grave∗ egrave@meta.com
Yann LeCun∗ yann@meta.com
Thomas Scialom∗ tscialom@meta.com
∗Meta AI †Universitat Pompeu Fabra
Reviewed on OpenReview:https://openreview.net/forum?id=jh7wH2AzKK
Abstract
This survey reviews works in which language models (LMs) are augmented with reasoning
skills and the ability to use tools. The former is defined as decomposing a potentially complex
task into simpler subtasks while the latter consists in calling external modules such as a
code interpreter. LMs can leverage these augmentations separately or in combination via
heuristics, or learn to do so from demonstrations. While adhering to a standard missing
tokens prediction objective, such augmented LMs can use various, possibly non-parametric
external modules to expand their context processing ability, thus departing from the pure
language modeling paradigm. We therefore refer to them as Augmented Language Models
(ALMs). The missing token objective allows ALMs to learn to reason, use tools, and even
act, while still performing standard natural language tasks and even outperforming most
regular LMs on several benchmarks. In this work, after reviewing current advance in ALMs,
we conclude that this new research direction has the potential to address common limitations
of traditional LMs such as interpretability, consistency, and scalability issues.
1 Introduction: motivation for the survey and definitions
1.1 Motivation and Definitions
Large Language Models (LLMs) (Devlin et al., 2019; Brown et al., 2020; Chowdhery et al., 2022) have fueled
dramatic progress in Natural Language Processing (NLP) and are already core in several products with
millions of users, such as the coding assistant Copilot (Chen et al., 2021), Google search engine1 or more
recently ChatGPT2. Memorization (Tirumala et al., 2022) combined with compositionality (Zhou et al., 2022)
capabilities made LLMs able to execute various tasks such as language understanding or conditional and
1See e.g. https://blog.google/products/search/search-language-understanding-bert/
2https://openai.com/blog/chatgpt/
1Published in Transactions on Machine Learning Research (06/2023)
unconditional text generation at an unprecedented level of performance, thus opening a realistic path towards
higher-bandwidth human-computer interactions.
However, LLMs suffer from important limitations hindering a broader deployment. LLMs often provide
non-factual but seemingly plausible predictions, often referred to as hallucinations (Welleck et al., 2020).
This leads to many avoidable mistakes, for example in the context of arithmetics (Qian et al., 2022) or within
a reasoning chain (Wei et al., 2022c). Moreover, many LLMs groundbreaking capabilities seem to emerge
with size, measured by the number of trainable parameters: for example, Wei et al. (2022b) demonstrate
that LLMs become able to perform some BIG-bench tasks3 via few-shot prompting once a certain scale is
attained. Although a recent line of work yielded smaller LMs that retain some capabilities from their largest
counterpart (Hoffmann et al., 2022), the size and need for data of LLMs can be impractical for training but
also maintenance: continual learning for large models remains an open research question (Scialom et al.,
2022). Other limitations of LLMs are discussed by Goldberg (2023) in the context ofChatGPT, a chatbot
built uponGPT3.
We argue these issues stem from a fundamental defect of LLMs: they are generally trained to perform
statistical language modeling given (i) a single parametric model and (ii) a limited context, typically then
previous or surrounding tokens. Whilen has been growing in recent years thanks to software and hardware
innovations, most models still use a relatively small context size compared to the potentially large context
needed to always correctly perform language modeling. Hence, massive scale is required to store knowledge
that is not present in the context but necessary to perform the task at hand.
As a consequence, a growing research trend emerged with the goal to solve these issues, slightly moving
away from the pure statistical language modeling paradigm described above. For example, a line of work
circumvents the limited context size of LLMs by increasing its relevance: this is done by adding information
extracted from relevant external documents. Through equipping LMs with a module that retrieves such
documents from a database given a context, it is possible to match certain capabilities of some of the largest
LMs while having less parameters (Borgeaud et al., 2022; Izacard et al., 2022). Note that the resulting model
is now non-parametric, since it can query external data sources. More generally, LMs can also improve their
context via reasoning strategies (Wei et al. (2022c); Taylor et al. (2022); Yang et al. (2022c)inter alia) so that
a more relevant context is produced in exchange for more computation before generating an answer. Another
strategy is to allow LMs to leverage external tools (Press et al. (2022); Gao et al. (2022); Liu et al. (2022b)
inter alia) to augment the current context with important missing information that was not contained in
the LM’s weights. Although most of these works aim to alleviate the downfalls of LMs mentioned above
separately, it is straightforward to think that more systematically augmenting LMs with both reasoning
and tools may lead to significantly more powerful agents. We will refer to these models asAugmented
Language Models (ALMs). As this trend is accelerating, keeping track and understanding the scope of
the numerous results becomes arduous. This calls for a taxonomy of ALMs works and definitions of technical
terms that are used with sometimes different intents.
Definitions. We now provide definitions for terms that will be used throughout the survey.
• Reasoning. In the context of ALMs, reasoning is decomposing a potentially complex task into
simpler subtasks the LM can solve more easily by itself or using tools. There exist various ways to
decompose into subtasks, such as recursion or iteration. In that sense, reasoning is akin to planning
as defined for example in LeCun (2022). In this survey, reasoning will very often refer to the various
strategies to improve reasoning skills in LMs, such as step-by-step reasoning using few-shot examples.
It is not yet fully understood whether the LM is really reasoning, or simply producing a larger context
that increases the likelihood of correctly predicting the missing tokens. We refer to Huang and Chang
(2022) for a discussion on this topic: although reasoning may currently be an abuse of language given
the current state of the art, the term is already in use within the community. A more pragmatic
definition of reasoning in the context of ALMs is giving more computation steps to the model before
yielding the answer to a prompt.
3https://github.com/google/BIG-bench
2Published in Transactions on Machine Learning Research (06/2023)
• Tool. For ALMs, a tool is an external module that is typically called using a rule or a special token
and whose output is included in the ALM’s context. The tool can gather external information, or
have an effect on the virtual or physical world (generally perceived by the ALM). An example of
a tool fetching external information is a document retriever, while a tool having an external effect
could be a robotic arm. A tool can be called at training or at inference time. More generally, learning
to interact with a tool may consist in learning to call its API.
• Act. For ALMs, calling a tool that modifies a state in a virtual or physical object, and observing
the result, typically by including it in the ALM’s current context. For example, some works from
the survey discuss searching the web, or robotic arm manipulation via LMs. With a slight abuse of
term, we will sometimes denote the call of a tool by an ALM as an action, even if it does not have
an external effect.
Why jointly discussing reasoning and tools?The combination of reasoning and tools within LMs
should allow solving a broad range of complex tasks without heuristics, hence with better generalization
capabilities. Typically, reasoning would foster the LM to decompose a given problem into potentially simpler
subtasks while tools would help getting each step right, for example obtaining the result from a mathematical
operation. Put it differently, reasoning is a way for LMs to combine different tools in order to solve complex
tasks, and tools are a way to not fail a reasoning with valid decomposition. Both should benefit from the
other. Moreover, reasoning and tools can be put under the same hood, as both augment the context of the
LM so that it better predicts the missing tokens, albeit in a different way.
Why jointly discussing tools and actions?Tools that gather additional information and tools that
have an effect on the virtual or physical world can be called in the same fashion by the LM. For example,
there is seemingly no difference between a LM outputting python code for solving a mathematical operation,
and a LM outputting python code to manipulate a robotic arm. A few works discussed in the survey are
already using LMs that have effects on the virtual or physical world: under this view, we can say that the
LM have the potential to act, and expect important advances in the direction of LMs as autonomous agents.
1.2 Our classification
We decompose the works included in the survey under three axes. Section 2 studies works which augment
LM’s reasoning capabilities as defined above. Section 3 focuses on works allowing LMs to interact with
external tools and act. Finally, Section 4 explores whether reasoning and tools usage are implemented via
heuristics or learned,e.g. via supervision or reinforcement. Other axes could naturally have been chosen
for this survey and are discussed in Section 5. For conciseness, the survey focuses on works that combine
reasoning or tools with LMs. However, the reader should keep in mind that many of these techniques were
originally introduced in another context than LMs, and consult the introduction and related work section of
the papers we mention if needed. Finally, although we focus on LLMs, not all works we consider employ
large models, hence we stick to LMs for correctness in the remainder of the survey.
2 Reasoning
In general, reasoning is the ability to make inferences using evidence and logic. Reasoning can be divided
into multiple types of skills such as commonsense reasoning (McCarthy et al., 1960; Levesque et al., 2012),
mathematical reasoning (Cobbe et al., 2021), symbolic reasoning (Wei et al., 2022c), etc. Often, reasoning
involves deductions from inference chains, called as multi-step reasoning. In the context of LMs, we will
use the definition of reasoning provided in Section 1. Previous work has shown that LLMs can solve simple
reasoning problems but fail at complex reasoning (Creswell et al., 2022): hence, this section focuses on various
strategies to augment LM’s reasoning skills. One of the challenges with complex reasoning problems for LMs
is to correctly obtain the solution by composing the correct answers predicted by it to the sub-problems.
For example, a LM may correctly predict the dates of birth and death of a celebrity, but may not correctly
predict the age. Press et al. (2022) call this discrepancy the compositionality gap for LMs. For the rest of
this section, we discuss the works related to three popular paradigms for eliciting reasoning in LMs. Note
3Published in Transactions on Machine Learning Research (06/2023)
that Huang and Chang (2022) propose a survey on reasoning in language models. Qiao et al. (2022) also
propose a survey on reasoning albeit with a focus on prompting. Since our present work focuses on reasoning
combined with tools, we refer the reader to Huang and Chang (2022); Qiao et al. (2022) for a more in-depth
review of works on reasoning for LLMs.
2.1 Eliciting reasoning with prompting
In recent years, prompting LMs to solve various downstream tasks has become a dominant paradigm (Brown
et al., 2020). In prompting, examples from a downstream task are transformed such that they are formulated
as a language modeling problem. Prompting typically takes one of the two forms: zero-shot, where the
model is directly prompted with a test example’s input; and few-shot, where few examples of a task are
prepended along with a test example’s input. This few-shot prompting is also known as in-context learning
or few-shot learning. As opposed to “naive” prompting that requires an input to be directly followed by
the output/answer, elicitive prompts encourage LMs to solve tasks by following intermediate steps before
predicting the output/answer. While Nye et al. (2021) provides the first example of few-shot prompting LLMs
with reasoning examples and Cobbe et al. (2021) generalizes the use of reasoning examples to non-algorithmic
tasks, Wei et al. (2022c) extensively studies how elicitive prompting enables LMs to be better reasoners in a
few-shot setting. Later, Kojima et al. (2022) showed similar ability in a zero-shot setting. We discuss them in
detail in the following paragraphs.
Few-shot setting. Wei et al. (2022c) popularized chain-of-thought (CoT), a few-shot prompting technique
for LMs. The prompt consists of examples of a task, with inputs followed by intermediate reasoning steps
leading to the final output, as depicted in Figure 1. Table 1 shows that CoT outperforms standard prompting
methods. Wei et al. (2022b) observe that the success of the few-shot strategy emerges with scale, while Tay
et al. (2022) add that without fine-tuning, successful use of CoT generally requires 100B+ parameters LMs
such asLaMDA (Thoppilan et al., 2022),PaLM (Chowdhery et al., 2022) orGPT3 (Brown et al., 2020;
Ouyang et al., 2022), before proposingUL2, a 20B open source model that can perform CoT. Using few-shot
CoT prompting,Minerva (Lewkowycz et al., 2022) achieves excellent performance on math benchmarks such
as GSM8K (Cobbe et al., 2021). Wang et al. (2022c) further improve CoT withSelf-consistency: diverse
reasoning paths are sampled from a given language model using CoT, and the most consistent answer is
selected as the final answer. Press et al. (2022) introduceSelf-ask, a prompt in the spirit of CoT. Instead of
providing the model with a continuous chain of thought as in Figure 1,Self-ask explicitly states the follow-up
question before answering it and relies on a scaffold (e.g,“Follow-up question:” or “So the final answer is:”),
so that the answers are more easily parseable. The authors demonstrate an improvement over CoT on their
introduced datasets aiming at measuring the compositionality gap. They observe that this gap does not
narrow when increasing the size of the model. Note that Press et al. (2022) focus on 2-hop questions,i.e.,
questions for which the model only needs to compose two facts to obtain the answer. Interestingly,Self-ask
can easily be augmented with a search engine (see Section 3).ReAct (Yao et al., 2022b) is another few-shot
prompting approach eliciting reasoning that can query three tools throughout the reasoning steps:search
and lookup in Wikipedia, andfinish to return the answer.ReAct will be discussed in more detail in the
next sections.
Zero-shot setting. Kojima et al. (2022) extend the idea of eliciting reasoning in LMs to zero-shot prompting.
Whereas few-shot provides examples of the task at hand, zero-shot conditions the LM on a single prompt that
is not an example. Here, Kojima et al. (2022) simply appendLet’s think step by stepto the input question
before querying the model (see Figure 2), and demonstrate that zero-shot-CoT for large LMs does well on
reasoning tasks such as GSM8K although not as much as few-shot-CoT.
2.2 Recursive prompting
Several works attempt to improve LM’s reasoning by explicitly decomposing problems into sub-problems in
order to solve the problem in a divide and conquer manner. This recursive approach slightly differs from CoT
since the latter does not explicitly formulate sub-problems, and can be especially useful for complex tasks,
given that compositional generalization can be challenging for LMs (Lake and Baroni, 2018; Keysers et al.,
4Published in Transactions on Machine Learning Research (06/2023)
Question: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls.
How many tennis balls does he have now?
Answer: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The
answer is 11.
Question: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many
apples do they have?
Answer:
<LM>
Figure 1: An example of few-shot Chain-of-Thought prompt.<LM> denotes call to the LM with the above
prompt.
Question: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many
apples do they have?
Answer: Let’s think step by step
<LM>
Figure 2: An example of zero-shot Chain-of-Thought prompt.<LM> denotes call to the LM with the above
prompt.
2019; Li et al., 2022a). Methods that employ problem decomposition can either then solve the sub-problems
independently, where these answers are aggregated to generate the final answer (Perez et al., 2020; Min et al.,
2019), or solve the sub-problems sequentially, where the solution to the next sub-problem depends on the
answer to the previous ones (Yang et al., 2022a; Zhou et al., 2022; Drozdov et al., 2022; Dua et al., 2022;
Khot et al., 2022; Wang et al., 2022a; Wu et al., 2022b; Mishra and Nouri, 2022). For instance, in the context
of math problems,Least-to-most prompting (Zhou et al., 2022) allows a language model to solve harder
problems than the demonstration examples by decomposing a complex problem into a list of sub-problems. It
first employs few-shot prompting to decompose the complex problem into sub-problems, before sequentially
solving the extracted sub-problems, using the solution to the previous sub-problems to answer the next one.
Patel et al. (2022) show that modifying existing benchmarks by letting human decompose questions into
subquestions that are relatively easier for models to solve leads to significant improvement forGPT3 and
RoBERTa-SQuADequipped with a symbolic calculator.
While many earlier works include learning to decompose through distant supervision (Perez et al., 2020;
Talmor and Berant, 2018; Min et al., 2019), like Zhou et al. (2022), many recent works employ in-context
learning to do so (Yang et al., 2022a; Khot et al., 2022; Dua et al., 2022; Kazemi et al., 2022). Among these,
there are further differences. For instance, Drozdov et al. (2022) is a follow-up work to Zhou et al. (2022),
but differs by using a series of prompts to perform recursive syntactic parses of the input rather than a linear
decomposition, and also differs by choosing the exemplars automatically through various heuristics. Dua et al.
(2022) is concurrent work with Zhou et al. (2022) but differs by interweaving the question decomposition
and answering stages, i.e., the next sub-question prediction has access to the previous questions and answers
as opposed to generating all sub-questions independently of any previous answers. Yang et al. (2022a), on
the other hand, decomposes using rule-based principles and slot-filling prompting to translate questions into
a series of SQL operations. Khot et al. (2022) also employs prompts to decompose into specific operations,
but then allows each sub-problem to be solved using a library of specialized handlers, where each is devoted
to a particular sub-task (e.g., retrieval). Finally, Kazemi et al. (2022) decompose a given problem in a
backward fashion: starting from the goal and a set of rules, the system decompose the goal into sub-goals
and recursively check whether the sub-goals can be proved. Here again, the modules are implemented by
few-shot prompting a pre-trained LM.
5Published in Transactions on Machine Learning Research (06/2023)
Model Accuracy (%)
OpenAI (text-davinci-002)[1] 15.6
OpenAI (text-davinci-002) + CoT[1] 46.9
OpenAI (text-davinci-002) + CoT + Calculator[1] 46.9
OpenAI (code-davinci-002)[1] 19.7
OpenAI (code-davinci-002) + CoT[1] 63.1
OpenAI (code-davinci-002) + CoT + Calculator[1] 65.4
GPT-3 175B + FT + CoT + Calculator[2] 34.0
GPT-3 175B + FT + CoT + Calculator + Verifier[2] 55.0
PaLM 540B[3] 17.0
PaLM 540B+CoT[3] 54.0
PaLM 540B+CoT+Calculator[3] 58.0
PAL[4] 72.0
Table 1: Evaluation of different reasoning methods on GSM8K, a popular reasoning benchmark. FT denotes
fine-tuning and CoT denotes chain-of-thought. The reported accuracies are based on [1]: (Wei et al., 2022c);
[2]: (Cobbe et al., 2021); [3]: (Chowdhery et al., 2022); and [4]: (Gao et al., 2022).
2.3 Explicitly teaching language models to reason
Despite their spectacular results, prompting approaches have some drawbacks in addition to requiring model
scale. Namely, they require to discover prompts that elicit e.g. step-by-step reasoning, manually providing
examples when it comes to few-shot for a new task. Moreover, prompting is computationally expensive in the
case of long prompts, and it is harder to benefit from a relatively large number of examples due to limited
context size of the model. Recent works suggest to circumvent these issues by training LMs to use, as humans,
a working memory when more than one step are required to solve a task correctly. Nye et al. (2021) introduce
the notion of scratchpad, allowing a LM to better perform on multi-step computation tasks such as addition
or code execution. More precisely, at training time, the LM sees input tasks such as addition along with
associated intermediate steps: the ensemble is called a scratchpad. At test time, the model is required to
predict the steps and the answer from the input task. Scratchpads differ from the above prompting strategies
in that they are fine-tuned on example tasks with associated computation steps. Note however that Nye
et al. (2021) also perform experiments in the few-shot regime. Taylor et al. (2022) use a similar approach in
the context of large LM pre-training:Galactica was trained on a corpus of scientific data including some
documents where step-by-step reasoning is wrapped with a special token<work> and </work> to mimic an
internal working memory. At inference time, the model can be asked explicitly to activate this reasoning mode
via the<work> token. Taylor et al. (2022) argue that one more problem arise when training on reasoning
examples: many intermediate reasoning steps may be missing in the training data curated from the internet,
as humans do not explicitly write all their reasoning steps. To circumvent the issue of missing steps, the
authors created datasets with detailed reasoning process. An example of prompt seen duringGalactica’s
pre-training is presented in Figure 4.
Other recent works improve the reasoning abilities of pre-trained LMs via fine-tuning. Zelikman et al. (2022)
propose a bootstrap approach to generate reasoning steps (also called rationales) for a large set of unlabeled
data and use that data to fine-tune the model. Lengerich et al. (2022) propose a self-supervised method which
extracts reasoning capabilities of a teacher model by asking the question “why” given the student’s initial
responses to various NLP problems. Next, the student model is fine-tuned by using contrastive distillation
via sampling evidence from memory. Yu et al. (2022) show that standard LM fine-tuning on reasoning tasks
lead to better reasoning skills such as textual entailment, abductive reasoning, and analogical reasoning,
compared to pre-trained models. Further, several instruction fine-tuning approaches (Ouyang et al., 2022;
Chung et al., 2022; Iyer et al., 2022; Ho et al., 2022) use chain-of-thought style prompts to achieve remarkable
improvements on popular benchmarks such as BBH (Srivastava et al., 2022) and MMLU (Hendrycks et al.,
2021). Interestingly, all these works also show that small scale instruction-finetuned models can perform
better than un-finetuned large scale models, especially in the tasks where instruction following is important.
6Published in Transactions on Machine Learning Research (06/2023)
Prompt 0
Question: It takes Amy 4 minutes to climb to the top of a slide. It takes her 1 minute to
slide down. The water slide closes in 15 minutes. How many times can she slide before it closes?
<LM>
Answer: To solve “ How many times can she slide before it closes?”, we need to first solve:
“ How long does each trip take? ”
</LM>
Prompt 1
It takes Amy 4 minutes to climb to the top of a slide. It takes her 1 minute to slide down.
The water slide closes in 15 minutes.
Subquestion 1: How long does each trip take?
<LM>
Answer 1: It takes Amy 4 minutes to climb and 1 minute to slide down. 4 + 1 = 5. So each trip
takes 5 minutes.
</LM>
Prompt 2
It takes Amy 4 minutes to climb to the top of a slide. It takes her 1 minute to slide down.
The slide closes in 15 minutes.
Subquestion 1: How long does each trip take?
Answer 1: It takes Amy 4 minutes to climb and 1 minute to slide down. 4 + 1 = 5. So each trip
takes 5 minutes.
Subquestion 2: How many times can she slide before it closes?
<LM>
Answer 2: The water slide closes in 15 minutes. Each trip takes 5 minutes. So Amy can slide 15÷ 5
= 3 times before it closes.
</LM>
Figure 3: Recursive prompting example.<LM> denotes the start of the LM’s output to the prompt, while
</LM> denotes the end. The problem is first decomposed into subproblems inPrompt 0. Then, Answer
2 to Subquestion 2and Answer 1to Subquestion 1are sequentially fed toPrompt 2and Prompt 1.
The few-shot examples for each stage’s prompt are omitted. Inspired from Figure 1 in Zhou et al. (2022).
2.4 Comparison and limitations of abstract reasoning
Overall, reasoning can be seen as decomposing a problem into a sequence of sub-problems either iteratively
or recursively.4 Exploring as many reasoning paths as possible is hard and there is no guarantee that the
intermediate steps are valid. A way to produce faithful reasoning traces is to generate pairs of questions
and their corresponding answers for each reasoning step (Creswell and Shanahan, 2022), but there is still
no guarantee of the correctness of these intermediate steps. Overall, a reasoning LM seeks to improve its
context by itself so that it has more chance to output the correct answer. To what extent LMs actually use
the stated reasoning steps to support the final prediction remains poorly understood (Yu et al., 2022).
4Here, reasoning is described as a sequential operation. However, other reasoning structures such as trees could be considered.
For example, Lample et al. (2022) leverage trees to model the different strategies leading to a proof for a given theorem. A
strategy is a set of intermediate results that must be either true or themselves proved, hence decomposed into another new
subset of intermediate results.
7Published in Transactions on Machine Learning Research (06/2023)
Question: A needle 35 mm long rests on a water surface at 20◦C. What force over and above the
needle’s weight is required to lift the needle from contact with the water surface?σ = 0.0728m.
<work>
σ= 0.0728N/m
σ= F/L
0.0728 =F/(2 ×0.035)
F = 0.0728(2 ×0.035)
calculate.py
“‘
f = 0.0728*(2*0.035)
with open("output.txt", "w") as file:
file.write(str(round(f, 5)))
”’
«run: calculate.py»
«read: output.txt»
0.0051
</work>
Answer: F = 0.0051N
Figure 4: Working memory example from Taylor et al. (2022). This prompt and its output are seen during
LM pre-training.
In many cases, some reasoning steps may suffer from avoidable mistakes that compromise the correctness of
the output. For example, mistakes on nontrivial mathematical operations in a reasoning step may lead to
the wrong final output. The same goes with known facts such as the identity of a president at a given year.
Some of the works studied above (Yao et al., 2022b; Press et al., 2022) already leverage simple external tools
such as asearch engine or acalculator to validate intermediate steps. More generally, the next section
of the survey focuses on the various tools that can be queried by LMs to increase the chance of outputting a
correct answer.
3 Using Tools and Act
A recent line of LM research allows the model to access knowledge that is not necessarily stored in its
weights, such as a given piece of factual knowledge. More precisely, tasks such as exact computation or
information retrieval for example can be offloaded to external modules such as apython interpreter or a
search engine that are queried by the model which, in that respect, use tools. Additionally, we can say the
LM performs an action when the tool has an effect on the external world. The possibility to easily include
tools and actions in the form of special tokens is a convenient feature of language modeling coupled with
transformers.
3.1 Calling another model
In many cases, the tool can simply be another neural network or the LM itself.
8Published in Transactions on Machine Learning Research (06/2023)
Iteration 0
Text: Brittney Reese (born September 9, 1986 in Gulfport, Mississippi) is an American
long jumper.
<LM>
Plan: Remove incorrect information
Edit: Brittney Reese (born September 9, 1986in Gulfport, Mississippi) is an American long jumper.
</LM>
Iteration 1
Text: Brittney Reese (born September 9, 1986) is an American long jumper.
<LM>
Plan: Add information about her career
Edit: Brittney Reese (born September 9, 1986) is an American long
jumper , who competed at the 2008 Summer Olympics, and is a 4-time World Champion .
</LM>
Iteration 2
Text: Brittney Reese (born September 9, 1986) is an American long jumper, who competed at the
2008 Summer Olympics, and is a 4-time World Champion.
<LM>
Plan: Add her birthplace
Edit: Brittney Reese (born September 9, 1986 in Inglewood, California) is an American long
jumper, who competed at the 2008 Summer Olympics, and is a 4-time World Champion.
</LM>
Figure 5: Iterative prompting example using PEER (Schick et al., 2022), a LM trained to produce a plan of
action and edit to the input text at each step. This process can be repeated until the generated text requires
no further updates.<LM> denotes the start of the LM’s output to the prompt, while</LM> denotes the
end.
Iterative LM calling. As an alternative to improving the LM’s context for better outputs after a single
inference pass, an alternative and intuitive way to get better results from LMs consists of repeatedly calling
the model to iteratively refine its output.Re3 (Yang et al., 2022c) exploits this idea to automatically generate
stories of over two thousand words. More precisely,Re3 first generates a plan, setting, and characters by
prompting GPT3 (Brown et al., 2020) with a premise. Then,Re3 iteratively injects information from both
the plan and current story state into a newGPT3 prompt to generate new story passages. This work is
improved upon in Yang et al. (2022b) with the use of a learned detailed outliner that iteratively expands the
brief initial outline to any desired level of granularity. Other approaches that teach models to iteratively
improve texts in an unsupervised fashion range from applications such as blank filling (Shen et al., 2020;
Donahue et al., 2020) to denoising a sequence of Gaussian vectors into word vectors (Li et al., 2022c).
PEER (Schick et al., 2022), for example, is a model initialized fromLM-Adapted T5(Raffel et al., 2020)
and trained on Wikipedia edits, learning both how to carry out edits and how to plan for the next steps.
Consequently, PEER is able to develop articles by repeatedly planning and editing as in Figure 5. The
iterative approach has the additional benefit of allowing a complex task like story and article generation to be
decomposed into smaller subtasks. Importantly and apart fromPEER, the works mentioned above employ
heuristics to call the LM. A future research direction may consist in allowing the LM to call itself repeatedly
until the output satisfies a certain criterion. Rather than just calling a single model repeatedly, Wu et al.
9Published in Transactions on Machine Learning Research (06/2023)
(2022a) propose an interactive interface for a pipeline allowing chaining of multiple LMs together, where the
output of one step is passed as input to the next. Such contributions allow non-AI-experts to refine solutions
to complex tasks that cannot be appropriately handled by a single LM.
Leveraging other modalities. Prompts under the form of text may not contain enough context to
correctly perform a given task. For example, a question does not call for the same answer if it is asked with a
serious or ironic tone. Including various modalities into the context would probably be useful for LMs such
as chatbots. As recently demonstrated by Hao et al. (2022) and Alayrac et al. (2022), LMs can also be used
as a general-purpose interface with models pre-trained on different modalities. For example, Hao et al. (2022)
take a number of pre-trained encoders that can process diverse modalities such as vision and language, and
connect them to a LM that serves as a universal task layer. The interface and modular encoders are jointly
pre-trained via a semi-causal language modeling objective. This approach combines the benefits of causal
and non-causal language modeling, enabling both in-context learning and open-ended generation, as well
as easy fine-tuning of the encoders. Similarly, Alayrac et al. (2022) introduceFlamingo, a family of Visual
Language Models (VLMs) that can handle any interleaved sequences of visual and textual data.Flamingo
models are trained on large-scale multimodal web corpora containing interleaved text and images, which
enables them to display in-context few-shot learning capabilities of multimodal tasks. With only a handful of
annotated examples,Flamingo can easily adapt to both generation tasks such as visual question-answering
and captioning, as well as classification tasks such as multiple-choice visual question-answering. Zeng et al.
(2022) introduce Socratic Models, a modular framework in which various models pre-trained on different
modalities can be composed zero-shot. This allows models to exchange information with each other and
acquire new multimodal capabilities without additional finetuning. Socratic Models enable new applications
such as robot perception and planning, free-form question-answering about egocentric videos, or multimodal
assistive dialogue by interfacing with external APIs and databases such as search engines. Interestingly,
other modalities such as images can be incorporated to improve reasoning capabilities of moderate size LMs
(1B) (Zhang et al., 2023), and enabling multimodal chain of thought reasoning (Lu et al., 2022a).
3.2 Information retrieval
LMs can be augmented with memory units, for example via a neural cache of recent inputs (Grave et al.,
2017; Merity et al., 2017), to improve their reasoning abilities. Alternatively, knowledge in the form of natural
language can be offloaded completely from the LM by retrieving from an external knowledge source. Memory
augmentation strategies help the language model to avoid producing non-factual and out-of-date information
as well as reducing the number of parameters required to achieve comparable performance to large LMs.
3.2.1 Retrieval-augmented language models
Dense and sparse retrievers. There exist two types of retrievers that can be used to augment a LM:
dense and sparse. Sparse retrievers work with sparse bag-of-words representations of the documents and the
queries (Robertson and Zaragoza, 2009). In contrast, dense neural retrievers use a dense query and dense
document vectors obtained from a neural network (Asai et al., 2021). Both types of retrievers assess the
relevance of a document to an information-seeking query. This can be done by (i) checking for precise term
overlap or (ii) computing the semantic similarity across related concepts. Sparse retrievers excel at the first
sub-problem, while dense retrievers can be better at the second (Luan et al., 2021).
Conditioning LMs on retrieved documents.Various works augment LMs with adense retriever
by adding the retrieved documents to the current context (Chen et al., 2017; Clark and Gardner, 2017; Lee
et al., 2019; Guu et al., 2020; Khandelwal et al., 2020; Lewis et al., 2020; Izacard and Grave, 2020; Zhong
et al., 2022; Borgeaud et al., 2022; Izacard et al., 2022; Shi et al., 2023). Even though the idea of retrieving
documents to perform question answering is not new, retrieval-augmented LMs have recently demonstrated
strong performance in other knowledge intensive tasks besides Q&A. These proposals close the performance
gap compared to larger LMs that use significantly more parameters.REALM (Guu et al., 2020) was the
first method to jointly train end-to-end a retrieval system with an encoder LM.RAG (Lewis et al., 2020)
jointly fine-tunes the retriever with a sequence-to-sequence model. Izacard and Grave (2020) introduced a
10Published in Transactions on Machine Learning Research (06/2023)
modification of the seq2seq architecture to efficiently process many retrieved documents. Borgeaud et al.
(2022) focuses on an auto-regressive LM, calledRETRO, and use a large-scale corpus indexed by frozen
BERT embeddings for the retriever module. Crucially, the authors show that at scale (retrieval corpus and
language model), this approach does not require to train and update the retriever. In particular,RETRO
obtains comparable performance toGPT3 on different downstream tasks. AlthoughRETRO was trained
with retrieval from scratch, their approach allows the integration of retrieval into existing pre-trained LMs.
Atlas (Izacard et al., 2022) jointly trains a retriever with a sequence-to-sequence model to obtain a LM with
strong few-shot learning capabilities in spite of being orders of magnitude smaller than many other large
LMs. Table 2 compares the main characteristics of the models discussed, notably how the retrieval results
are integrated into the LM’s context. In all the aforementioned cases, the query corresponds to the prompt
but this has been relaxed, see the chain-of-thought subsection below.
Model # Retrieval tokens Granularity Retriever training Retrieval integration
REALM (Guu et al., 2020) O(109) Prompt End-to-End Append to prompt
RAG (Lewis et al., 2020) O(109) Prompt Fine-tuning Cross-attention
RETRO (Borgeaud et al., 2022) O(1012) Chunk Frozen Chunked cross-attn.
Atlas (Izacard et al., 2022) O(109) Prompt Fine-tuning Cross-attention
Table 2: Comparison between database retrieval augmented languages models. Inspired by Table 3
from Borgeaud et al. (2022).
Efficient large scale retrieval.Since retrieval-augmented language models store knowledge in an external
data store, it is crucial that the information retrieval step is efficient, especially when dealing with a large
number of document/passage/sentence embeddings and/or a large number (billions) of query vectors. The
literature offers a variety of optimisations to boost performance of retrievers as well as reducing the memory
footprint of the data store.
First, an index structure can be built for the document/passage/sentence embeddings to perform efficient
similarity search, for instance, using the Facebook AI Similarity Search library (faiss) (Johnson et al., 2021).
Leveraging indexing structures enables efficient search operations by partitioning the vector space and enabling
fast pruning of irrelevant vectors.
Second, if there exist memory constraints due to a large number of document/passage/sentence embeddings, a
multi-node, multi-gpu distributed framework can be used to store the index corresponding to only part of the
document/passage/sentence embeddings per process. The query embeddings can also be split in batches and
obtain the top-k results in parallel using gpus for further speed ups for approximate or exact search. When
exact search becomes intractable due to scale, approximate nearest neighbor algorithms can be used. These
algorithms trade off accuracy for speed, allowing significantly faster retrieval while maintaining reasonably
accurate results, see Johnson et al. (2021) for further details.
Finally, in order to further reduce the memory footprint of each of the index shards corresponding to a subset
of the document/passage/sentence embeddings, different compression techniques can be used. For instance,
the Atlas retrieval-augmented language model (Izacard et al., 2022) uses product quantisation (Jégou et al.,
2011) to reduce the memory footprint of the retriever without sacrificing accuracy in downstream tasks in
terms of exact match and recall@50 metrics of Q&A.
Currently, there also exist a variety of vector database frameworks that have some of these optimisations
implemented out of the box and can be leveraged to use retrieval as a service without requiring the user to
implement all optimisations from scratch.
Chain-of-thought prompting and retrievers. Recent works (He et al., 2022; Trivedi et al., 2022)
propose to combine aretriever with reasoning via chain-of-thoughts (CoT) prompting to augment a LM.
He et al. (2022) use the CoT prompt to generate reasoning paths consisting of an explanation and prediction
pair. Then, knowledge is retrieved to support the explanations and the prediction that is mostly supported by
the evidence is selected. This approach does not require any additional training or fine-tuning. Trivedi et al.
(2022) propose an information retrieval chain-of-thought approach (IRCoT) which consists of interleaving
11Published in Transactions on Machine Learning Research (06/2023)
retrieval with CoT for multi-step QA. The idea is to use retrieval to guide the CoT reasoning steps and
conversely, using CoT reasoning to guide the retrieval step.
In all these works, aretriever is systematically called for every query in order to get the corresponding
documents to augment the LM. These approaches also assume that the intent is contained in the query. The
query could be augmented with the user’s intent by providing a natural language description of the search
task (instruction) in order to disambiguate the intent, as proposed by Asai et al. (2022). Also, the LM could
query the retriever only occasionally—when a prompt suggests it to do so—which is discussed in the next
subsection.
3.2.2 Querying search engines
In the previous paragraph, the information retrieval query corresponds to the LM’s context. However, the
LM can also have the ability to generate a query based on the prompt, thus enlarging its action space and
becoming more active.
LaMDAis one example of an agent-like LM designed for dialogue applications. The authors pre-train the
model on dialog data as well as other public web documents. In addition to this, to ensure that the model
is factually grounded as well as enhancing its conversational abilities, it is augmented withretrieval, a
calculator, and atranslator (Thoppilan et al., 2022). Furthermore, to improve the model’s safety,LaMDA
is fine-tuned with annotated data. Another example isBlenderBot (Shuster et al., 2022b), where the LM
decides to generate a query based on a prompt. In this case, the prompt corresponds to the instruction of
calling the search engine tool.BlenderBot is capable of open-domain conversation, it has been deployed on a
public website to further improve the model via continual learning with humans in the loop. Similarly,ReAct
uses few-shot prompting to teach a LM how to use different tools such assearch and lookup in Wikipedia,
and finish to return the answer (Yao et al., 2022b). Similarly, Komeili et al. (2021); Shuster et al. (2022a)
propose a model that learns to generate an internet search query based on the context, and then conditions on
the search results to generate a response.ReAct interleaves reasoning and acting, allowing for greater synergy
between the two and improved performance on both language and decision making tasks.ReAct performs
well on a diverse set of language and decision making tasks such as question answering, fact verification, or
web and home navigation.
In general, reasoning can improve decision making by making better inferences and predictions, while the
ability to use external tools can improve reasoning by gathering additional information from knowledge bases
or environments.
3.2.3 Searching and navigating the web
It is also possible to train agents that can navigate the open-ended internet in pursuit of specified goals
such as searching information or buying items. For example,WebGPT (Nakano et al., 2021) is a LM-based
agent which can interact with a custom text-based web-browsing environment in order to answer long-form
questions. In contrast with other models that only learn how to query retrievers or search engines like
LaMDA(Thoppilan et al., 2022) orBlenderBot (Shuster et al., 2022b),WebGPT learns to interact with a
web-browser, which allows it to further refine the initial query or perform additional actions based on its
interactions with the tool. More specifically,WebGPT can search the internet,navigate webpages, follow
links, and cite sources (see Table 3 for the full list of available actions). By accessing the internet, the
agent is able to enhance its question-answering abilities, even surpassing those of humans as determined
by human evaluators. The best model is obtained by fine-tuningGPT3 on human demonstrations, and
then performing rejection sampling against a reward model trained to predict human preferences. Similarly,
WebShop (Yao et al., 2022a) is a simulated e-commerce website where an agent has to find, customize, and
purchase a product according to a given instruction. To accomplish this, the agent must understand and
reason about noisy text, follow complex instructions, reformulate queries, navigate different types of webpages,
take actions to collect additional information when needed, and make strategic decisions to achieve its goals.
Both the observations and the actions are expressed in natural language, making the environment well-suited
for LM-based agents. The agent consists of a LM fine-tuned with behavior cloning of human demonstrations
(i.e., question-human demonstration pairs) and reinforcement learning using a hard-coded reward function
12Published in Transactions on Machine Learning Research (06/2023)
that verifies whether the purchased item matches the given description. While there are other works on
web navigation and computer-control, most of them assume the typical human interface, that takes as input
images of a computer screen and output keyboard commands in order to solve digital tasks (Shi et al., 2017;
Gur et al., 2019; 2021; Toyama et al., 2021; Humphreys et al., 2022; Gur et al., 2022). Since our survey
focuses on LM-based agents, we will not discuss these works in detail.
3.3 Computing via Symbolic Modules and Code Interpreters
Although recent LMs are able to correctly decompose many problems, they are still prone to errors when
dealing with large numbers or performing complex arithmetics (Mishra et al., 2022b). For example, vanilla
GPT3 cannot perform out-of-distribution addition,i.e. addition on larger numbers than those seen during
the training even when provided with examples with annotated steps (Qian et al., 2022). In the context of
reinforcement learning, the action space of a transformer agent is equipped with symbolic modules to perform
e.g. arithmetic or navigation in Wang et al. (2022b).Mind’s Eye (Liu et al., 2022b) invokes aphysics
engine to ground LMs physical reasoning. More precisely, a text-to-code LM is used to produce rendering
code for the physics engine. The outcome of the simulation that is relevant to answer the question is then
appended in natural language form to the LM prompt. As a result,Mind’s Eyeis able to outperform the
largest LMs on some specific physical reasoning tasks while having two order of magnitude less parameters.
For reasoning graph generation,CoCoGen (Madaan et al., 2022) propose to generate python code generating
a graph instead of a serialized version of the graph.PAL (Gao et al., 2022) relies on CoT prompting of
large LMs to decompose symbolic reasoning, mathematical reasoning, or algorithmic tasks into intermediate
steps along with python code for each step (see Figure 6). The python steps are then offloaded to apython
interpreter outputting the final result. They outperform CoT prompting on several benchmarks, especially
on GSM-HARD, a version of GSM8K with larger numbers. See Table 1 for a comparison betweenPAL and
other models on GSM8K. Similarly, Drori et al. (2022); Chen et al. (2022b) promptsCodex (Chen et al.,
2021) to generate executable code-based solutions to university-level problems, math word problems, or
financial QA. For code generation, Shi et al. (2022) execute several sampled generation on a small number of
test inputs, and use the output to select a solution. Instead, Mishra et al. (2022a) extends existing datasets
with solutions written as python programs. Then, they finetune a model to generate python solutions to
mathematical reasoning problems. In the context of theorem proving, Wu et al. (2022c) uses large LMs to
automatically formalize informal mathematical competition problem statements in Isabelle or HOL. Jiang
et al. (2022) generate formal proof sketches, which are then fed to a prover.
3.4 Acting on the virtual and physical world
While the previous tools gather external information in order to improve the LM’s predictions or performance
on a given task, other tools allow the LM to act on the virtual or physical world. In order to do this, the LM
needs to ground itself in the real-world by learning about affordances i.e. what actions are possible in a given
state, and their effect on the world.
Controlling Virtual Agents. Recent works demonstrated the ability of LMs to control virtual agents in
simulated 2D and 3D environments by outputting functions which can then be executed by computers in the
corresponding environment, be it a simulation or the real-world. For example, Li et al. (2022b) fine-tune a
pre-trained GPT2 (Radford et al., 2019) on sequential decision-making problems by representing the goals
and observations as a sequence of embeddings and predicting the next action. This framework enables strong
combinatorial generalization across different domains including a simulated household environment. This
suggests that LMs can produce representations that are useful for modeling not only language but also
sequential goals and plans, so that they can improve learning and generalization on tasks that go beyond
language processing. Similarly, Huang et al. (2022a) investigate whether it is possible to use the world
knowledge captured by LMs to take specific actions in response to high-level tasks written in natural language
such as “make breakfast”. This work was the first to demonstrate that if the LM is large enough and correctly
prompted, it can break down high-level tasks into a series of simple commands without additional training.
However, the agent has access to a predetermined set of actions, so not all natural language commands can be
executed in the environment. To address this issue, the authors propose to map the commands suggested by
13Published in Transactions on Machine Learning Research (06/2023)
Question: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls.
How many tennis balls does he have now?
Answer: Roger started with 5 balls.
tennis_balls = 5
2 cans of 3 tennis balls each is
bought_balls = 2 * 3
tennis balls. The answer is
answer = tennis_balls + bought_balls
Question: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many
apples do they have?
Answer:
<LM>
Figure 6: An example of few-shot PAL (Gao et al., 2022) prompt.<LM> denotes call to the LM with
the above prompt. The prompts are based on the chain-of-thoughts prompting shown on Figure 1, and the
parts taken from it arehighlighted in green. In PAL, the prompts also containexecutable python code ,
which performs operations and stores the results in theanswer variable. When prompted with a new
question, PAL generates a mix of executable code and explanation. The answer is obtained by executing the
code and print(answer) .
the LM into feasible actions for the agent using the cosine similarity function. The approach is evaluated in a
virtual household environment and displays an improvement in the ability to execute tasks compared to using
the plans generated by the LM without the additional mapping. While these works have demonstrated the
usefulness of LMs for controlling virtual robots, the following paragraph cover works on physical robots. Zeng
et al. (2022) combine a LM with a visual-language model (VLM) and a pre-trained language-conditioned
policy for controlling a simulated robotic arm. The LM is used as a multi-step planner to break down a
high-level task into subgoals, while the VLM is used to describe the objects in the scene. Both are passed
to the policy which then executes actions according to the specified goal and observed state of the world.
Dasgupta et al. (2023) use 7B and 70BChinchilla as planners for an agent that acts and observes the result
in a PycoLab environment. Additionally, a reporter module converts actions and observations from pixel to
text space. Finally, the agent in Carta et al. (2023) uses a LM to generate action policies for text-based tasks.
Interactively learning via online RL allows to ground the LM internal representations to the environment,
thus partly departing from the knowledge about statistical surface structure of text that was acquired during
pre-training.
Command Effect
search <query> Send <query> to the Bing API and display a search results page
clicked on link <link ID> Follow the link with the given ID to a new page
find in page: <text> Find the next occurrence of <text> and scroll to it
quote: <text> If <text> is found in the current page, add it as a reference
scrolled down <1, 2, 3> Scroll down a number of times
scrolled up <1, 2, 3> Scroll up a number of times
Top Scroll to the top of the page
back Go to the previous page
end: answer End browsing and move to answering phase
end: <nonsense, controversial> End browsing and skip answering phase
Table 3: The actionsWebGPT can perform, taken from Nakano et al. (2021).
14Published in Transactions on Machine Learning Research (06/2023)
Controlling Physical Robots. Liang et al. (2022) use a LM to write robot policy code given natural
language commands by prompting the model with a few demonstrations. By combining classic logic structures
and referencing external libraries, e.g., for arithmetic operations, LMs can create policies that exhibit spatial-
geometric reasoning, generalize to new instructions, and provide precise values for ambiguous descriptions.
The effectiveness of the approach is demonstrated on multiple real robot platforms. LMs encode common sense
knowledge about the world which can be useful in getting robots to follow complex high-level instructions
expressed in natural language. However, they lack contextual grounding which makes it difficult to use
them for decision making in the real-world since they do not know what actions are feasible in a particular
situation. To mitigate this problem, Ahn et al. (2022) propose to teach the robot a number of low-level
skills (such as “find a sponge”, “pick up the apple”, “go to the kitchen”) and learn to predict how feasible
they are at any given state. Then, the LM can be used to split complex high-level instructions into simpler
subgoals from the robot’s repertoire. The LM can then select the most valuable yet feasible skills for the
robot to perform. This way, the robot can use its physical abilities to carry out the LM’s instructions, while
the LM provides semantic knowledge about the task. The authors test their approach, calledSayCan, on
various real-world tasks and find that it can successfully complete long, abstract instructions in a variety of
environments. To address the grounding problem, Chen et al. (2022a) proposeNLMap-SayCan, a framework
to gather and integrate contextual information into LM planners.NLMap uses a Visual Language Model
(VLM) to create an open-vocabulary queryable scene representation before generating a context-conditioned
plan. An alternative way of incorporating contextual information into the agent’s decisions is to utilize
linguistic feedback from the environment such as success detection, object recognition, scene description, or
human interaction (Huang et al., 2022b). This results in improved performance on robotic control tasks such
as table top rearrangement and mobile manipulation in a real kitchen. Finally,RT-1 (Brohan et al., 2022)
leverages large-scale, diverse, task-agnostic robotic datasets to learn a model that can follow over 700 natural
language instructions, as well as generalize to new tasks, environments, and objects.RT-1 makes use of
DIAL (Xiao et al., 2022), an approach for automatically labeling robot demonstrations with linguistic labels
via the vision-language alignment modelCLIP (Radford et al., 2019).
4 Learning to reason, use tools, and act
The previous sections reviewedwhat LMs can be augmented with in order to endow them with reasoning and
tools. We will now present approaches onhow to teach them such abilities.
4.1 Supervision
A straightforward way of teaching LMs both to reason and to act consists in providing them with human-
written demonstrations of the desired behaviours. Common ways of doing so are (i) via few-shot prompting as
first suggested by Brown et al. (2020), where the LM is provided a few examples as additional context during
inference, but no parameter updates are performed, or (ii) via regular gradient-based learning. Typically,
supervised learning is doneafter an initial pre-training with a language modeling objective (Ouyang et al.,
2022; Chung et al., 2022); an exception to this is recent work by Taylor et al. (2022), who propose to mix
pre-training texts with human-annotated examples containing some form of explicit reasoning, marked with a
special token. Some authors use supervised fine-tuning as an intermediate step, followed by reinforcement
learning from human feedback (Nakano et al., 2021; Ouyang et al., 2022); see Section 4.2 for an in-depth
discussion of such methods.
Few-shot prompting. Providing LMs with a few human-writtenin-context demonstrations of a desired
behaviour is a common approach both for teaching them to reason (Wei et al., 2022c;b; Suzgun et al., 2022;
Press et al., 2022) and for teaching them to use tools and act (Gao et al., 2022; Lazaridou et al., 2022; Yao
et al., 2022b). This is mainly due to its ease of use: few-shot prompting only requires a handful of manually
labeled examples and enables very fast experimentation as no model fine-tuning is required; moreover, it
enables reusing the very same model for different reasoning tasks and tools, just by changing the provided
prompt (Brown et al., 2020; Wei et al., 2022c). On the other hand, the ability to perform reasoning with
chain-of-thoughts from a few in-context examples only emerges as models reach a certain size (Wei et al.,
2022b; Chung et al., 2022), and performance depends heavily on the format in which examples are presented
15Published in Transactions on Machine Learning Research (06/2023)
(Jiang et al., 2020; Min et al., 2022), the choice of few-shot examples, and the order in which they are
presented (Kumar and Talukdar, 2021; Lu et al., 2022b; Zhou et al., 2022). Another issue is that the amount
of supervision that can be provided is limited by the number of examples that fit into the LM’s context
window; this is especially relevant if (i) a new behaviour is so difficult to learn that it requires more than a
handful of examples, or (ii) we have a large space of possible actions that we want a model to learn. Beyond
that, as no weight updates are performed, the LM’s reasoning and acting abilities are tied entirely to the
provided prompt; removing it also removes these abilities.
Fine-tuning. As an alternative to few-shot prompting, the reasoning and acting abilities of a pre-trained
LM can also be elicited by updating its parameters with standard supervised learning. This approach has
been used both for teaching models to use tools, including search engines (Komeili et al., 2021; Shuster et al.,
2022b), web browsers (Nakano et al., 2021), calculators and translation systems (Thoppilan et al., 2022), and
for improving reasoning abilities (Chung et al., 2022). For the latter, examples of reasoning are typically used
in the larger context ofinstruction tuning(Mishra et al., 2021; Sanh et al., 2022; Wang et al., 2022d; Ouyang
et al., 2022), where, more generally, an LM’s ability to follow instructions is improved based on human-labeled
examples. Examples are typically collected from crowd workers. In some cases, they can instead be obtained
automatically: Nye et al. (2021) use execution traces as a form of supervision for reasoning, while Andor
et al. (2019) use heuristics to collect supervised data for teaching a language model to use a calculator.
Prompt pre-training. A potential risk of finetuningafter the pre-training phase is that the LM might
deviate far from the original distribution and overfit the distribution of the examples provided during
fine-tuning. To alleviate this issue, Taylor et al. (2022) propose to mix pre-training data with labeled
demonstrations of reasoning, similar to how earlier work mixes pre-training data with examples from various
downstream tasks (Raffel et al., 2020); however, the exact gains from this mixing, compared to having a
separate fine-tuning stage, have not yet been empirically studied. With a similar goal in mind, Ouyang et al.
(2022) and Iyer et al. (2022) include examples from pre-training during the fine-tuning stage.
Bootstrapping. As an alternative to standard fine-tuning, several authors propose to usebootstrapping
techniques (e.g. Yarowsky, 1995; Brin, 1999) to leverage some form of indirect supervision. This typically
works by prompting a LM to reason or act in a few-shot setup followed by a final prediction; examples for
which the actions or reasoning steps performed didnot lead to a correct final prediction are then discarded.
For example, STaR (Zelikman et al., 2022) prompts a model to generate chain-of-thought reasoning sequences
in a common sense question answering setup, but only keeps those chains that lead to the correct final answer
for a given question. Finally, either the original LM or another (typically smaller) model is fine-tuned on all
correct examples. As such, bootstrapping combines the data efficiency of few-shot prompting with some of
the advantages of fine-tuning and can be successfully applied both to teach models to reason (Shridhar et al.,
2022) and to use tools (Parisi et al., 2022).
4.2 Reinforcement learning
Supervised learning from human-created prompts is effective to teach models to reason and act. However,
such data is difficult and costly to obtain. Human preference data — such as rankings or likes/dislikes — is
much easier, faster, and cheaper to obtain than full demonstrations. For instance, it might be easier for a
human to evaluate the quality of a summary than write one from scratch. Going further, it might be even
easier to rank different summaries than scoring each separately. Such data cannot be used in a supervised
setting, but can provide rewards in the context of Reinforcement Learning (RL) (Sutton and Barto, 2018).
RL has proven successful for learning complex behaviors through feedback-based interaction with an en-
vironment, and it has been used for applications such as playing games (Mnih et al., 2015; Silver et al.,
2016; Vinyals et al., 2019; Team et al., 2021; Bakhtin et al., 2022) or controlling robots (Gu et al., 2017;
Kalashnikov et al., 2018; Akkaya et al., 2019; Lee et al., 2020). When training a LM with RL, the LM can be
considered an agent that learns a policy (i.e. a distribution over the model’s vocabulary from which the next
token is sampled) in order to optimize some reward function. Most of the existing work on RL and ALMs
has focused on teaching LMs how to act rather than reason. The closest work on learning how to reason via
RL is STaR (Zelikman et al., 2022), a bootstrapping-based approach that is discussed in Section 4.1
16Published in Transactions on Machine Learning Research (06/2023)
RL is a natural framework for training LMs to act and use tools since many of these tools are non-differentiable
(e.g. search engines, calculators or programming language interpreters). Additionally, many tasks that benefit
from interacting with tools resemble sequential decision making problems (e.g., navigating a web-browser to
buy a specified product) and have a well-defined reward (e.g.,1 if the model buys the correct product and
0 otherwise). While there are early works focused on models that could interface with external tools, they
employ ad-hoc tool-dependent architectures (Adolphs et al., 2022; Buck et al., 2018; Nogueira and Cho, 2017;
Zhong et al., 2018). We do not cover them here since the main focus of our survey is instead on the acting
and reasoning capabilities of standard general-purpose LM architectures trained with the language modeling
objective.
Hard-coded reward functions. When teaching a LM how to use external tools, the standard practice is
to update the weights of the model using a scalar reward generated by a hard-coded reward function. This
task-dependent function is computed based on the tool output. The LM agent takes a textual input, which in
RL terminology corresponds to the current state of the environment, and generates a sequence of tokens, or
actions in RL terms. Optimization is done through policy gradient algorithms like REINFORCE (Williams,
1992), PPO and similar variants (Schulman et al., 2017; Ramamurthy et al., 2022).
Initial works on training LMs to use tools via RL mostly focused on searching and fetching additional
factual information. Common tools for such information-seeking tasks aredocument retrievers, question
answering systems, andsearch engines. The first two consist in retrieving document from a pre-defined
set of text documents, or in retrieving an answer based on some input query. However, a search engine
allows for more structured interactive search where, for instance, the model further refines the initial query or
performs additional actions based on the initial output of the tool. For example, Wu et al. (2022d) perform
conversational question-answering by teaching a LM via RL to rewrite queries in order to feed them to an
off-the-shelf retriever. The reward function is a contrastive retrieval-accuracy metric based on the token
overlap between following conversation rounds and retrieved passages. Another example is the work from
Liu et al. (2022a):RAINIER is a LM able to generate contextually relevant questions that are optimized to
query a frozenQA system. After distilling knowledge from a largerGPT3 (Brown et al., 2020) model into
a smallerT5 model (Raffel et al., 2020),RAINIER is finetuned using PPO (Schulman et al., 2017) with
feedback provided by the pre-trained question answering model from Khashabi et al. (2020). Interestingly,
this work is an example of a LM learning to use another frozen neural model as an external tool.
Yao et al. (2022a) use RL to teach a language model to navigate avirtual shop and buy items constrained
on attributes like color and price. Similar toWebGPT (Nakano et al., 2021), the model is given a goal in
textual format and allowed to perform a limited set of actions. Prompted with a user-generated instruction,
in a multi-task learning setup, the model needs to simultaneously understand the query and browse the web
to search for the right product. The reward is a hard-coded text-matching function based on the similarity
between the model-purchased written description of the item and the given shopping instruction. Optimization
is performed with the A3C algorithm (Mnih et al., 2016), a variant of the standard actor-critic method. While
the model still lags behind human experts, they found that fine-tuning with RL after training on human
demonstrations improves performance. This provides additional evidence of the benefits of reward-based
learning for endowing LMs with the ability to interact with external tools.
While interacting with asearch engine or adocument retriever allows a model to augment its current
context with additional input, it is often necessary to process structured information when interacting with
tools like aknowledge base. Dognin et al. (2021) train a LM to learn how to interface with a graph-based
knowledge base by performing the text2graph and graph2text tasks. The model, based on aT5 architecture
(Raffel et al., 2020) and trained with the vanilla policy gradient algorithm REINFORCE (Williams, 1992),
can perform bidirectional generation of text and graphs and shows state-of-the-art performance on tasks
related to knowledge base automated construction from text and vice versa. TheT5-based agent is trained
to directly maximize graph2text metrics such as BLEU (Papineni et al., 2002a), METEOR (Banerjee and
Lavie, 2005), and chrF++ (Popović, 2017), or text2graph ones such as F1, Precision, and Recall.
Finally, it is also possible to leverage RL at inference time only. For example, Cao et al. (2023) propose a
method to avoid generating tokens that will likely lead to toxic content by framing it as a RL problem. More
precisely, a reward model and a value function are trained to evaluate respectively the toxicity of a sentence
17Published in Transactions on Machine Learning Research (06/2023)
and the probability of the so-far generated tokens to lead to a toxic generation. At inference time, the latter
is used to truncate the next token probability distribution towards the tokens that are less likely to lead to a
toxic generation.
Human feedback. Evaluating the quality of machine-generated text is non-trivial because it can vary
depending on the context, individual preferences, and user’s intentions. For example, in some contexts, a
user might require creative writing, while in others it may just require factual information. Model outputs
should be judged accordingly and should be able to capture such intent differences. Several metrics based on
heuristics like BLEU (Papineni et al., 2002b) and ROUGE (Lin, 2004) have been developed for comparing
model outputs to reference texts. However, they fail to fully capture the quality of generations with respect
to human intentions. Human feedback can be exploited to improve the quality of machine-generated text, for
example for dialog agents (Xu et al., 2022). In particular, Reinforcement Learning from Human Feedback
(RLHF) (Knox and Stone, 2008; MacGlashan et al., 2017; Christiano et al., 2017; Warnell et al., 2018) aims to
overcome these limitations by using human preferences as an evaluation metric and as an objective function
to optimize the language model. Using RLHF allows LMs to be more closely aligned with complex human
preferences and values which are difficult to capture by hard-coded reward functions.
RLHF works by using a pre-trained LM to generate text, which is then evaluated by humans by, for example,
ranking two model generations for the same prompt. This data is then collected to learn a reward model that
predicts a scalar reward given any generated text. The reward captures human preferences when judging
model output. Finally, the LM is optimized against such reward model using RL policy gradient algorithms
like PPO (Schulman et al., 2017). RLHF can be applied directly on top of a general-purpose LM pre-trained
via self-supervised learning. However, for more complex tasks, the model’s generations may not be good
enough. In such cases, RLHF is typically applied after an initial supervised fine-tuning phase using a small
number of expert demonstrations for the corresponding downstream task (Ramamurthy et al., 2022; Ouyang
et al., 2022; Stiennon et al., 2020).
A successful example of RLHF used to teach a LM to use an external tool stems fromWebGPT (Nakano
et al., 2021), discussed in 3.2.3, a model capable of answering questions using asearch engine and providing
references to support such answers. The tool interface is a simplified text-based web-browser. The model
architecture is based onGPT3 (Brown et al., 2020) and is trained to perform browsing actions expressed in
natural language. The model is fine-tuned on question-human demonstration pairs, before further optimization
via RLHF. On two QA datasets,WebGPT’s answers are preferred relative to human-generated ones and tend
to be more factual than the original vanillaGPT3 model. Similarly, Menick et al. (2022) proposeGopherCite,
a Gopher-based LM model (Rae et al., 2021) fine-tuned with RLHF that can cite supporting evidence when
answering questions and abstain from answering when unsure. In contrast withWebGPT, GopherCite uses an
information retrieval external module rather than a web-browser to find relevant information that improves
its question answering capabilities. Besides learning to use external tools, RLHF has also proven useful for a
wide range of language generation tasks, from summarization (Ziegler et al., 2019; Wu et al., 2021; Stiennon
et al., 2020) to training more helpful, harmless, and accurate assistants (Glaese et al., 2022; Cohen et al.,
2022; Ouyang et al., 2022; Bai et al., 2022). Since these works do not focus on training models to reason and
act, they are out of the scope of this survey.
4.3 Limitations and future directions
Despite recent algorithmic progress and performance improvements, current RL methods still suffer from
instability issues which can make training difficult and slow (Ramamurthy et al., 2022; Snell et al., 2022).
While supervised learning has been an efficient and robust way to fine-tune language models on specific
tasks (Mishra et al., 2021; Sanh et al., 2022; Wang et al., 2022b), this assumes the existence of a large
number of expert demonstrations, which can be difficult and costly to obtain. This is particularly true for
tasks that require reasoning and acting where we do not have readily available data. A possible solution
to the lack of quality data problem could come from bootstrapping methods and offline RL. The promise
of these methods is that a dataset can be generated via feedback and interactions of any behavior policy
and can be used to train an improved policy. Therefore, combining a data-driven approach with offline
RL could give the "best of both worlds": learning from counterfactual events in a scalable and stable
18Published in Transactions on Machine Learning Research (06/2023)
way (Levine et al., 2020). In the offline regime, though, there are some open challenges. The maximum
improvement can be limited by a number of factors such as: the suboptimality of the initial behavior policy,
the dimensionality of the state and the action space, the length of the effective horizon, the accumulation of
errors and distributional shifts due to the discrepancy between the behavior policy and the learned one (Levine
et al., 2020). Recent works (Zelikman et al., 2022; Snell et al., 2022) have shown that such approaches
could reach performance that goes beyond that of the expert demonstrations or improve over initial model
generations. For example, Snell et al. (2022) introduce a new offline RL algorithm called ILQL which learns
from a static dataset of demonstrations and their associated rewards by estimating a value function and using
it to optimize LM generations. ILQL combines online RL flexible optimization framework with the simplicity
and ability to learn from existing datasets of supervised learning, resulting in good performance on dialogue
tasks. As explained in Section 4, Zelikman et al. (2022) employ a bootstrapping approach for teaching LMs
to reason, which can be seen as an approximation to policy gradient algorithms.
Recently, Schick et al. (2023) proposedToolformer, a model that teaches itself to use tools in a self-supervised
way. This is achieved by first using the few-shot abilities of an existing LM to sample a large amount of
potential tool uses. For instance, the model can call a calculator API to augment its context, e.g., “Out
of 1400 participants, 400 (or [Calculator(400 / 1400)→ 0.29] 29% passed the test.” Then, the model is
fine-tuned on its own generations, filtering them based on whether they reduce perplexity for future tokens
generations. This method enables using several tools (e.g., acalendar, acalculator, or aninformation
retrieval system). However, it was tested in a limited setup of using a single tool at once, since examples
of tool use were independently sampled. We believe that studying how this approach could be extended to
more complex multi-step tool uses is a promising research direction for a generalist LM-based agent.
5 Discussion
Moving away from language modeling.Is a model trained to do intermediate reasoning steps or having
access to the internet still purely performing language modeling? Indeed, in NLP, language modeling (Bahl
et al., 1983) is generally defined as the task of predicting missing tokens given a context and is relied heavily
on for pre-training models. However, several techniques have been developed to later fine-tune models (Ziegler
et al., 2019; Wei et al., 2022a; Sanh et al., 2022) to perform various natural language tasks, which could be
seen as moving away from traditional language modeling. In particular, the texts used to fine-tune LMs
are not just found on the internet, but rather designed to explicitly inject some level of grounding. One of
the argument advocated recently in Goldberg (2023) is that “it might be much easier to learn from direct
instructions like these than it is to learn from non-instruction data”. This argument can be supported by
the recent work of Giannou et al. (2023), showing both theoretically and in practice that even shallow
looped transformers can follow instructions and be programmed as general purpose computers. Intuitively, a
text is the result of complex intermediate thoughts that are hidden. Therefore, the superficial text used for
supervision can be seen as representing only the logs of these thoughts, thus lacking of context. Conversely,
with task-oriented supervised data, we can explicitly ground the answer with the intermediate steps. In this
regard, the resulting model may not be considered as a language model. And yet, the task is still about
predicting the next token given text only. The argument is all the more true for ALMs since they can augment
their context.
In particular, tool-augmented LMs might actually lose the ability to assign a probability to the next token -
which is at the core of language modeling: whereas a regular LM can easily computep(xt |x1,...,x t−1), a tool-
augmented LM has to consider all possible tool uses, e.g.p(xt |x1,...,x t−1) =∑
c p(c|x1,...,x t−1) ·p(xt |
x1,...,x t−1,c) where c is a tool, which might not be tractable. First, in the general case, marginalizing over
all the tools is not enough, one also has to marginalize over all possible tool use. For many tools however,
the probabilityp(c|x1,...,x t−1) associated with each possible tool output cannot be modeled. Take for
example web browsing: for a fixed query, the result may vary day to day unpredictably as both the internet
and the search algorithms are constantly evolving, making the evaluation ofp(c|x1,...,x t−1) impossible
unless the tool uses a past snapshot of the internet, which is generally not wanted. Second, in the case where
a tool can be called within another tool call, the number of tool uses can become exponential in the number
of tools. For example, within a tool call, an ALM could browse the web by reading the content of a web page,
19Published in Transactions on Machine Learning Research (06/2023)
which is text, then apply another of the tools to the text it found to refine its query, etc. If for some reason
we require a depth that is equal to the number of tools, we have an exponential complexity.
For these reasons, we refer to Augmented Language Models (ALMs) in this survey, to distinguish from
Language Modeling in the traditional sense.
A tradeoff between memorizing and querying tools.Is it preferable to memorize information in the
model weights, or to leverage external tools? Some situations arguably require external tools, for example
computing 213443344. However, many information are well known facts such as “The Eiffel tower is located
in Paris” or1 + 2 = 3, and should not be offloaded. And, when learning world representations, memorization
is not only desirable, but also deeply connected to reasoning (Hayes et al., 2014). Can ALMs be calibrated
enough to decide when and when not to use a tool? Could a computation budget for each tool be integrated
into the loss to let the model learn to do so?
Generalizing the non-parametric framework. A motivation behind information retrieval augmented
LMs such asRETRO (Borgeaud et al., 2022) andAtlas (Izacard et al., 2022) is to develop a class of LM
requiring less parameters through relying on an external non-parametric memory. The motivation for using
other kind of tools such ascode interpreter or calculator has been slightly different so far: for instance,
Cobbe et al. (2021) use a calculator to improve accuracy on tasks requiring arithmetic. Yet, the paradigm
of tool-augmented LMs can be seen as a generalization of the non-parametric framework. Indeed, beyond
information retrieval, LMs can delegate any kind of abilities such as calculus to the corresponding external
tools. By avoiding to store rarely accessed knowledge in their weights, tool-augmented LMs may have better
scaling laws and thus yield smaller models retaining the capabilities of their largest counterpart. Combined
with the possibility to access recent information from the external world thus avoiding frequent updates,
non-parametric generalization holds great benefits for ALMs.
A path towards autonomous machine intelligence?A concept for an autonomous intelligent agent
was proposed by LeCun (2022). We now discuss to what extent ALMs instantiate this idea. In LeCun (2022),
the agent is composed of different modules starting from a world model and a short-term memory. Essentially,
the agent takes actions via an actor module based on its world model, perception module, and short-term
memory so as to minimize some cost. The agent is also equipped with a configurator module for modulating
the world model, the perception, the actor and the cost given the task at hand.
Translating into this framework, the ALM’s weights essentially contain the world model, perception and
actor modules. The short-term memory can be identified with the ALM’s context or prompt. Based on its
perception of the context and its world model, the ALM would take actions by outputting special tokens,
and perceive the result. The configurator module remains elusive but may be implicit: it can be seen as
the conditioning induced by the ALM’s context, for example an initial prompt such as “You are a kind and
helpful assistant”. Finally, the cost remains fixed in this framework, and could be the ALM’s perplexity
mixed with a computational cost associated to reasoning and using external tools.
However, an important feature of the agent in LeCun (2022) is its ability to plan, defined by the decomposition
of a complex task into subtasks: in the ALM’s context, planning is akin to reasoning, a slight abuse of
terminology as it is not clear whether LMs reason as humans do as noted in Section 2. LeCun (2022)
propose to implement reasoning (under the term planning) as the minimization of an energy with respect to
a hierarchical combination of actions. Since ALMs only perform predictions at the token level, they cannot
reason according to LeCun (2022)’s view and may be still limited to System 1 tasks,i.e. that rely on reflex
rather than logic and thinking. Whether System 2,i.e. the opposite abilities can be obtained by pushing
current methods remains uncertain. For example, LMs are deprived from global consistency beyond their
maximum sequence length: as an illustration, two different discussions with the same LM will result in
inconsistencies. This is a strong limitation when it comes to solving complex problems that require to perform
a large number of sub-goals such as writing a research paper, where one has an initial mental state that
includes the current results and the angle of the paper. This process is not linear and results from different
interactions, e.g., new ideas while reading some related works. The mental state is maintained although
updated trough all the process, such that we keep in mind the big picture. Although more compute and
20Published in Transactions on Machine Learning Research (06/2023)
larger input size could mitigate the issue, another solution may be to endow LMs with adequate components.
In this regard, a model architecture that intrinsically makes the LM consistent with an energy function as
suggested in LeCun (2022) could constitute a promising venue.
Finally, our survey sees LMs as the central piece of a generalist agent that could reason in natural language
and interact with external tools. Along these lines, Wang et al. (2023) uses a LM as a centralized planner to
generate goal sequences for solving tasks in the game of Minecraft. Through a feedback loop and intermediate
checks on subgoals execution, the LM can explain mistakes of the goal executor and refine its original plan.
However, we note that a LM-based controller might not be the only viable approach for a generalist agent.
Recent work on the game of Diplomacy (Bakhtin et al., 2022), a long-standing challenge for AI agents due to
its complex planning and reasoning dynamics, employs an ad-hoc planning model trained via self-play and
reinforcement learning. Here the LM is used to interact with other players, thus as an external communication
module grounded in the current state of the game. This offers an alternative view of LMs as agents specialized
to communicate with humans, albeit in the restricted setting of a Diplomacy game. We believe that (A)LMs
will play a central role in the next generation of powerful interactive systems, whether as centralized controller
of a modular system or as a language-only module that needs to interact with an orchestrator remains an
open research question.
Augmented Language Models benefits.Overall, ALMs offer many potential advantages over traditional
LMs.
• Truthfulness: As the current LM’s training objective is arguably responsible for inciting the generation
of seemingly plausible but not factual information, grounding the predictions through some tools
should lead to more trustworthy models. However, although this conclusion is straightforward when
equipping a LM with a calculator, there is surprisingly little evidence of it for information retrieval
augmented LMs (Krishna et al., 2021). One of the reasons is the presence of a lot of non-truthful
information in the web. Investigating this direction will be critical for making LM reliable.
• Estimating and reducing uncertainty: Extending the maximum-likelihood paradigm by letting the
model reason and access additional information could help models to learn what they know and what
they don’t. Some papers suggest that LMs are already well calibrated (Kadavath et al., 2022), i.e.
there is a high correlation between the accuracy of their predictions and the corresponding likelihood.
This uncertainty could be directly exploited by ALMs to know when to rely on their own weights, or
when to query an external tool.
• Interpretability: Deep learning models are often considered to be black boxes, and their predictions
are difficult to interpret. Providing intermediate reasoning steps and relying on tools should help to
make ALMs more interpretable. In particular, we can expect that being able to cite the sources used
to compose the answer to be critical. However, some works Lewkowycz et al. (2022) pointed out that
chain-of-thoughts can lead to the correct predictions even though the intermediate reasoning doesn’t
make any sense, indicating clear challenges for researchers exploring this direction.
• Enhanced capabilities: ALMs with improved reasoning abilities and tools can be more helpful
assistants and solve a wider range of tasks than standard LMs. For example, an ALM connected to a
python interpreter can run code and experiments on a user’s behalf, which a vanilla LM cannot do.
In addition, a feedback loop can emerge between reasoning and acting, where each ability further
improves the other (Yao et al., 2022b). Interacting with external tools, entities, and environments
can improve reasoning since it allows the ALM to collect additional information and ground itself in
the real-world. Similarly, reasoning can improve the ALM’s decision making abilities such as when
and how to use a certain tool.
Cost of using tools. To the best of our knowledge, the cost of using tools has not yet been taken into
account comprehensively.
Overall, using tools seem to be beneficial in terms of energy use. Retrieval-augmented language models are a
good example of a more efficient way to handle new information since updating an external data store can be
21Published in Transactions on Machine Learning Research (06/2023)
orders of magnitude cheaper than re-training a LLM for scratch every time we get new data (as argued for
example in Borgeaud et al. (2022)). This stays true for using a calculator or browsing the web for example.
One can therefore assume that relying on tools rather than storing knowledge in parameters is generally more
energy-efficient than training, frequently updating a LLM, and scaling it if some ability remains out of reach,
if one assumes an optimal tool usage (i.e., not using each tool at each token). Then, how to assess the cost of
using a tool versus another?
We believe that the most natural way of estimating such cost is to consider two factors: (i) the price per tool
request, and (ii) the time required to fulfill the request. These factors make sense from the perspective of the
ALM, and are a reasonable way to take into account the cost of creating and maintaining the tool since this
should typically be reflected in the pricing. Interestingly, including such cost at training and inference time
under the form of a loss term for example could help ALMs to develop optimal tool use, and lead to the
emergence of models that know how to balance between tool use or internal chain of thoughts.
Ethical concerns. ALMs raise new ethical concerns. LM predictions based on tools may look more
trustworthy and authoritative at a first glance, when in fact many of them will still be incorrect. Moreover, we
can expect this phenomenon to be amplified as LMs reason in quite a similar manner to humans (Dasgupta
et al., 2022), making it even harder to detect mistakes. Hence, ALMs may be leveraged to generate more
convincing fake news and conspiracy theories. Conversely, conditioning ALMs on “safe” and trusted sources
should lead to more factuality and less toxicity in their answers.
While such ethical concerns apply to most tools, it is important to distinguish between passive and active
tools. The former only collects external information and passes it to the LM’s context, while the latter allows
it to act on the virtual or physical world without human validation in the loop. An example of active tool is
letting the LM control a search engine. Hence, there exists a broad spectrum of possible harmful consequences
of LM usage. We are moving from passive LMs that generate text in isolation of the external environment,
towards ALMs that act in the real world. In this context, the aforementioned ethical concerns may resonate
even further, as ALM will be connected to more and more tools and environments.
Currently, training LLM results in a large number of gas emissions, leveraging tools may result in smaller
hence cheaper models that are more environmentally friendly both at training, inference, and update. ALMs
are therefore a promising avenue to decrease the environmental impact of LLMs.
ALMs also have the potential to transform the LM landscape. Nowadays, state of the art LLMs have been
the product of large research organizations, fine-tuning these models towards more versatility (via reasoning
and learning to use different tools) and agency (via actions) may be within range of smaller entities and
perhaps individuals. We may therefore see the burgeoning of a market of open, possibly specialized and
even personalized ALM agents, learning to use new tools, handling various data modalities and interacting
with each other. We may also see the emergence of a new breed of app store, dedicated (via appropriate
documentation or API constraints for example) to be used by ALMs. Overall, this new ecosystem may further
accelerate the current pace of LLM research and application to the real world, while also making its control
even more uncertain.
6 Conclusion
This survey presented works in which LMs are augmented with better reasoning and tools. In most of the
works, LMs augment their context with additional relevant information useful to perform missing token
prediction. As many of these augmentations are non-parametric,i.e. involve calling an external, possibly
non-parametric module, such LMs arguably depart from the classical language modeling paradigm, hence our
decision to dub them Augmented Language Models. Although several works focus either on reasoning or
acting skills of LMs, most of them rely on human annotation which may not be scalable, e.g., hand-crafted
few-shot prompting, or reinforcement learning from human feedback. How to equip language models with
meaningful augmentations in a fully self-supervised fashion remains an open research question. Additionally,
as very few works combine reasoning and tools, future efforts should study the integration and fruitful
interaction between these two skills. Overall, we believe studying Augmented Language Models is a promising
22Published in Transactions on Machine Learning Research (06/2023)
and exciting research avenue towards the next generation of deep learning systems capable of complex and
useful human-machine interaction.
References
Leonard Adolphs, Benjamin Boerschinger, Christian Buck, Michelle Chen Huebscher, Massimiliano Ciaramita,
Lasse Espeholt, Thomas Hofmann, and Yannic Kilcher. Boosting search engines with interactive agents.
Transactions on Machine Learning Research (TMLR), 2022.
Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn,
Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et al. Do as i can, not as i say: Grounding
language in robotic affordances.arXiv preprint arXiv:2204.01691, 2022.
Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron, Alex
Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, et al. Solving rubik’s cube with a robot hand.
arXiv preprint arXiv:1910.07113, 2019.
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,
Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot
learning. Advances in Neural Information Processing Systems (NeurIPS), 2022.
Daniel Andor, Luheng He, Kenton Lee, and Emily Pitler. Giving BERT a calculator: Finding operations and
arguments with reading comprehension. InProceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th International Joint Conference on Natural Language Processing
(EMNLP-IJCNLP), 2019.
Akari Asai, Xinyan Yu, Jungo Kasai, and Hannaneh Hajishirzi. One question answering model for many
languages with cross-lingual dense passage retrieval.Advances in Neural Information Processing Systems
(NeurIPS), 2021.
Akari Asai, Timo Schick, Patrick Lewis, Xilun Chen, Gautier Izacard, Sebastian Riedel, Hannaneh Hajishirzi,
and Wen-tau Yih. Task-aware retrieval with instructions.arXiv preprint arXiv:2211.09260, 2022.
Lalit R. Bahl, Frederick Jelinek, and Robert L. Mercer. A maximum likelihood approach to continuous speech
recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, PAMI-5(2):179–190, 1983.
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen,
Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai
feedback. arXiv preprint arXiv:2212.08073, 2022.
Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew Goff,
Jonathan Gray, Hengyuan Hu, Athul Paul Jacob, Mojtaba Komeili, Karthik Konath, Minae Kwon, Adam
Lerer, Mike Lewis, Alexander H. Miller, Sandra Mitts, Adithya Renduchintala, Stephen Roller, Dirk Rowe,
Weiyan Shi, Joe Spisak, Alexander Wei, David J. Wu, Hugh Zhang, and Markus Zijlstra. Human-level
play in the game of diplomacy by combining language models with strategic reasoning.Science, 378:1067 –
1074, 2022.
Satanjeev Banerjee and Alon Lavie. METEOR: An automatic metric for MT evaluation with improved corre-
lation with human judgments. InProceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation
Measures for Machine Translation and/or Summarization, pages 65–72. Association for Computational
Linguistics, 2005.
Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George
van den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego de Las Casas, Aurelia Guy,
Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Maggiore, Chris Jones, Albin Cassirer,
Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack W.
Rae, Erich Elsen, and Laurent Sifre. Improving language models by retrieving from trillions of tokens. In
International Conference on Machine Learning (ICML), 2022.
23Published in Transactions on Machine Learning Research (06/2023)
Sergey Brin. Extracting patterns and relations from the world wide web. InThe World Wide Web and
Databases, pages 172–183. Springer Berlin Heidelberg, 1999.
Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana
Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for
real-world control at scale.arXiv preprint arXiv:2212.06817, 2022.
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen
Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,
Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,
Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models
are few-shot learners, 2020.
Christian Buck, Jannis Bulian, Massimiliano Ciaramita, Wojciech Gajewski, Andrea Gesmundo, Neil Houlsby,
and Wei Wang. Ask the right questions: Active question reformulation with reinforcement learning.
International Conference on Learning Representations (ICLR), 2018.
Meng Cao, Mehdi Fatemi, Jackie Chi Kit Cheung, and Samira Shabanian. Systematic rectification of language
models via dead-end analysis. InInternational Conference on Learning Representations (ICLR), 2023.
Thomas Carta, Clément Romac, Thomas Wolf, Sylvain Lamprier, Olivier Sigaud, and Pierre-Yves Oudeyer.
Grounding large language models in interactive environments with online reinforcement learning, 2023.
Boyuan Chen, Fei Xia, Brian Ichter, Kanishka Rao, Keerthana Gopalakrishnan, Michael S Ryoo, Austin
Stone, and Daniel Kappler. Open-vocabulary queryable scene representations for real world planning.
arXiv preprint arXiv:2209.09874, 2022a.
Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading wikipedia to answer open-domain
questions. arXiv preprint arXiv:1704.00051, 2017.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan,
Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger,
Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder,
Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet,
Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel
Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin,
Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh
Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati,
Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech
Zaremba. Evaluating large language models trained on code, 2021.
Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. Program of thoughts prompting:
Disentangling computation from reasoning for numerical reasoning tasks, 2022b.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,
Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi,
Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar
Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael
Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk
Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito,
David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani
Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor
Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang,
Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck,
Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways.arXiv, 2022.
24Published in Transactions on Machine Learning Research (06/2023)
Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement
learning from human preferences.Advances in Neural Information Processing Systems (NeurIPS), 2017.
Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,
Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv
preprint arXiv:2210.11416, 2022.
Christopher Clark and Matt Gardner. Simple and effective multi-paragraph reading comprehension.arXiv
preprint arXiv:1710.10723, 2017.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training
verifiers to solve math word problems.arXiv preprint arXiv:2110.14168, 2021.
Deborah Cohen, Moonkyung Ryu, Yinlam Chow, Orgad Keller, Ido Greenberg, Avinatan Hassidim, Michael
Fink, Yossi Matias, Idan Szpektor, Craig Boutilier, et al. Dynamic planning in open-ended dialogue using
reinforcement learning.arXiv preprint arXiv:2208.02294, 2022.
Antonia Creswell and Murray Shanahan. Faithful reasoning using large language models.arXiv preprint
arXiv:2208.14271, 2022.
Antonia Creswell, Murray Shanahan, and Irina Higgins. Selection-inference: Exploiting large language models
for interpretable logical reasoning.arXiv preprint arXiv:2205.09712, 2022.
Ishita Dasgupta, Christine Kaeser-Chen, Kenneth Marino, Arun Ahuja, Sheila Babayan, Felix Hill, and Rob
Fergus. Collaborating with language models for embodied reasoning, 2023.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional
transformers for language understanding. InProceedings of the North American Chapter of the Association
for Computational Linguistics (NAACL), 2019.
Pierre L Dognin, Inkit Padhi, Igor Melnyk, and Payel Das. Regen: Reinforcement learning for text and
knowledge base generation using pretrained language models.Conference on Empirical Methods in Natural
Language Processing (EMNLP), 2021.
Chris Donahue, Mina Lee, and Percy Liang. Enabling language models to fill in the blanks. InProceedings of
the Annual Meeting of the Association for Computational Linguistics (ACL), 2020.
Iddo Drori, Sarah Zhang, Reece Shuttleworth, Leonard Tang, Albert Lu, Elizabeth Ke, Kevin Liu, Linda
Chen, Sunny Tran, Newman Cheng, et al. A neural network solves, explains, and generates university
math problems by program synthesis and few-shot learning at human level.Proceedings of the National
Academy of Sciences, 119(32), 2022.
Andrew Drozdov, Nathanael Schärli, Ekin Akyürek, Nathan Scales, Xinying Song, Xinyun Chen, Olivier
Bousquet, and Denny Zhou. Compositional semantic parsing with large language models.arXiv preprint
arXiv:2209.15003, 2022.
Dheeru Dua, Shivanshu Gupta, Sameer Singh, and Matt Gardner. Successive prompting for decomposing
complex questions. Conference on Empirical Methods in Natural Language Processing (EMNLP), 2022.
Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham
Neubig. Pal: Program-aided language models, 2022.
Angeliki Giannou, Shashank Rajput, Jy-yong Sohn, Kangwook Lee, Jason D Lee, and Dimitris Papailiopoulos.
Looped transformers as programmable computers.arXiv preprint arXiv:2301.13196, 2023.
Amelia Glaese, Nat McAleese, Maja Trębacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh,
Laura Weidinger, Martin Chadwick, Phoebe Thacker, Lucy Campbell-Gillingham, Jonathan Uesato, Po-
Sen Huang, Ramona Comanescu, Fan Yang, Abigail See, Sumanth Dathathri, Rory Greig, Charlie Chen,
Doug Fritz, Jaume Sanchez Elias, Richard Green, Soňa Mokrá, Nicholas Fernando, Boxi Wu, Rachel
25Published in Transactions on Machine Learning Research (06/2023)
Foley, Susannah Young, Iason Gabriel, William Isaac, John Mellor, Demis Hassabis, Koray Kavukcuoglu,
Lisa Anne Hendricks, and Geoffrey Irving. Improving alignment of dialogue agents via targeted human
judgements. arXiv preprint arXiv:2209.14375, 2022.
Yoav Goldberg. Some remarks on large language models, 2023. URLhttps://gist.github.com/yoavg/
59d174608e92e845c8994ac2e234c8a9.
Edouard Grave, Armand Joulin, and Nicolas Usunier. Improving neural language models with a continuous
cache. InInternational Conference on Learning Representations (ICLR), 2017.
Shixiang Gu, Ethan Holly, Timothy Lillicrap, and Sergey Levine. Deep reinforcement learning for robotic
manipulation with asynchronous off-policy updates. In2017 IEEE international conference on robotics
and automation (ICRA), pages 3389–3396, 2017.
Izzeddin Gur, Ulrich Rueckert, Aleksandra Faust, and Dilek Hakkani-Tur. Learning to navigate the web.
International Conference on Learning Representations (ICLR), 2019.
Izzeddin Gur, Natasha Jaques, Kevin Malta, Manoj Tiwari, Honglak Lee, and Aleksandra Faust. Adversarial
environment generation for learning to navigate the web.arXiv preprint arXiv:2103.01991, 2021.
Izzeddin Gur, Ofir Nachum, Yingjie Miao, Mustafa Safdari, Austin Huang, Aakanksha Chowdhery, Sharan
Narang, Noah Fiedel, and Aleksandra Faust. Understanding html with large language models.arXiv
preprint arXiv:2210.03945, 2022.
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmented language
model pre-training. InInternational Conference on Machine Learning (ICML), 2020.
Yaru Hao, Haoyu Song, Li Dong, Shaohan Huang, Zewen Chi, Wenhui Wang, Shuming Ma, and Furu Wei.
Language models are general-purpose interfaces.arXiv preprint arXiv:2206.06336, 2022.
Brett K Hayes, Evan Heit, and Caren M Rotello. Memory, reasoning, and categorization: parallels and
common mechanisms.Frontiers in Psychology, 5:529, 2014.
Hangfeng He, Hongming Zhang, and Dan Roth. Rethinking with retrieval: Faithful large language model
inference. arXiv preprint arXiv:2301.00303, 2022.
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
Measuring massive multitask language understanding. InAdvances in Neural Information Processing
Systems (NeurIPS), 2021.
Namgyu Ho, Laura Schmid, and Se-Young Yun. Large language models are reasoning teachers, 2022.
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford,
Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal
large language models.arXiv preprint arXiv:2203.15556, 2022.
Jie Huang and Kevin Chen-Chuan Chang. Towards reasoning in large language models: A survey.arXiv
preprint arXiv:2212.10403, 2022.
Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners:
Extracting actionable knowledge for embodied agents.arXiv preprint arXiv:2201.07207, 2022a.
Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson,
Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through planning with
language models. arXiv preprint arXiv:2207.05608, 2022b.
Peter C Humphreys, David Raposo, Tobias Pohlen, Gregory Thornton, Rachita Chhaparia, Alistair Muldal,
Josh Abramson, Petko Georgiev, Adam Santoro, and Timothy Lillicrap. A data-driven approach for
learning to control computers. InInternational Conference on Machine Learning (ICML), 2022.
26Published in Transactions on Machine Learning Research (06/2023)
Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Daniel Simig, Ping Yu, Kurt
Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura, Xian Li, Brian O’Horo, Gabriel Pereyra, Jeff Wang,
Christopher Dewan, Asli Celikyilmaz, Luke Zettlemoyer, and Ves Stoyanov. Opt-iml: Scaling language
model instruction meta learning through the lens of generalization.arXiv preprint arXiv:2212.12017, 2022.
Gautier Izacard and Edouard Grave. Leveraging passage retrieval with generative models for open domain
question answering.arXiv preprint arXiv:2007.01282, 2020.
Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu,
Armand Joulin, Sebastian Riedel, and Edouard Grave. Atlas: Few-shot learning with retrieval augmented
language models. arXiv preprint arXiv:2208.03299, 2022.
Albert Q Jiang, Sean Welleck, Jin Peng Zhou, Wenda Li, Jiacheng Liu, Mateja Jamnik, Timothée Lacroix,
Yuhuai Wu, and Guillaume Lample. Draft, sketch, and prove: Guiding formal theorem provers with
informal proofs. arXiv preprint arXiv:2210.12283, 2022.
Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig. How can we know what language models
know? Transactions of the Association for Computational Linguistics, 8, 2020.
Jeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with gpus.IEEE Transactions
on Big Data, 7(3):535–547, 2021. doi: 10.1109/TBDATA.2019.2921572.
Herve Jégou, Matthijs Douze, and Cordelia Schmid. Product quantization for nearest neighbor search.IEEE
Transactions on Pattern Analysis and Machine Intelligence, 33(1):117–128, 2011. doi: 10.1109/TPAMI.
2010.57.
Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer,
Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, Sheer El-Showk, Andy Jones,
Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bowman, Stanislav Fort, Deep Ganguli,
Danny Hernandez, Josh Jacobson, Jackson Kernion, Shauna Kravec, Liane Lovitt, Kamal Ndousse,
Catherine Olsson, Sam Ringer, Dario Amodei, Tom Brown, Jack Clark, Nicholas Joseph, Ben Mann, Sam
McCandlish, Chris Olah, and Jared Kaplan. Language models (mostly) know what they know.arXiv
preprint arXiv:2207.05221, 2022.
Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre Quillen,
Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, et al. Qt-opt: Scalable deep reinforcement learning
for vision-based robotic manipulation. arxiv e-prints, page.arXiv preprint arXiv:1806.10293, 2018.
Seyed Mehran Kazemi, Najoung Kim, Deepti Bhatia, Xin Xu, and Deepak Ramachandran. Lambadal
backward chaining for automated reasoning in natural language, 2022.
Daniel Keysers, Nathanael Schärli, Nathan Scales, Hylke Buisman, Daniel Furrer, Sergii Kashubin, Nikola
Momchev, Danila Sinopalnikov, Lukasz Stafiniak, Tibor Tihon, et al. Measuring compositional generaliza-
tion: A comprehensive method on realistic data. InInternational Conference on Learning Representations,
2019.
Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization through
Memorization: Nearest Neighbor Language Models. InInternational Conference on Learning Representa-
tions (ICLR), 2020.
Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh
Hajishirzi. Unifiedqa: Crossing format boundaries with a single qa system.arXiv preprint arXiv:2005.00700,
2020.
Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal.
Decomposed prompting: A modular approach for solving complex tasks.arXiv preprint arXiv:2210.02406,
2022.
27Published in Transactions on Machine Learning Research (06/2023)
W Bradley Knox and Peter Stone. Tamer: Training an agent manually via evaluative reinforcement. In2008
7th IEEE international conference on development and learning, pages 292–297. IEEE, 2008.
Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language
models are zero-shot reasoners. InAdvances in Neural Information Processing Systems (NeurIPS), 2022.
Mojtaba Komeili, Kurt Shuster, and Jason Weston. Internet-augmented dialogue generation. ArXiv,
abs/2107.07566, 2021.
Kalpesh Krishna, Aurko Roy, and Mohit Iyyer. Hurdles to progress in long-form question answering.arXiv
preprint arXiv:2103.06332, 2021.
Sawan Kumar and Partha Talukdar. Reordering examples helps during priming-based few-shot learning. In
Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4507–4518, Online,
August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-acl.395. URL
https://aclanthology.org/2021.findings-acl.395.
Brenden Lake and Marco Baroni. Generalization without systematicity: On the compositional skills of
sequence-to-sequence recurrent networks. InInternational conference on machine learning, pages 2873–2882.
PMLR, 2018.
Guillaume Lample, Marie-Anne Lachaux, Thibaut Lavril, Xavier Martinet, Amaury Hayat, Gabriel Ebner,
Aurélien Rodriguez, and Timothée Lacroix. Hypertree proof search for neural theorem proving. InAdvances
in Neural Information Processing Systems (NeurIPS), 2022.
Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev. Internet-augmented
language models through few-shot prompting for open-domain question answering, 2022. URLhttps:
//arxiv.org/abs/2203.05115.
Yann LeCun. A path towards autonomous machine intelligence, 2022.
Joonho Lee, Jemin Hwangbo, Lorenz Wellhausen, Vladlen Koltun, and Marco Hutter. Learning quadrupedal
locomotion over challenging terrain.Science robotics, 5(47):eabc5986, 2020.
Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised open domain
question answering.arXiv preprint arXiv:1906.00300, 2019.
Chris Lengerich, Gabriel Synnaeve, Amy Zhang, Hugh Leather, Kurt Shuster, François Charton, and Charysse
Redwood. Contrastive distillation is a sample-efficient self-supervised loss policy for transfer learning.arXiv
preprint arXiv:2212.11353, 2022.
Hector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. InThirteenth
international conference on the principles of knowledge representation and reasoning, 2012.
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review,
and perspectives on open problems, 2020.
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich
Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. Retrieval-
augmented generation for knowledge-intensive nlp tasks. InAdvances in Neural Information Processing
Systems (NeurIPS), 2020.
Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh,
Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy
Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models, 2022.
Belinda Li, Jane Yu, Madian Khabsa, Luke Zettlemoyer, Alon Halevy, and Jacob Andreas. Quantifying
adaptability in pre-trained language models with 500 tasks. InProceedings of the 2022 Conference of the
North American Chapter of the Association for Computational Linguistics: Human Language Technologies,
pages 4696–4715, Seattle, United States, July 2022a. Association for Computational Linguistics. doi:
10.18653/v1/2022.naacl-main.346. URL https://aclanthology.org/2022.naacl-main.346.
28Published in Transactions on Machine Learning Research (06/2023)
Shuang Li, Xavier Puig, Yilun Du, Clinton Wang, Ekin Akyurek, Antonio Torralba, Jacob Andreas, and Igor
Mordatch. Pre-trained language models for interactive decision-making.arXiv preprint arXiv:2202.01771,
2022b.
Xiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori B Hashimoto. Diffusion-lm
improves controllable text generation.arXiv preprint arXiv:2205.14217, 2022c.
Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy
Zeng. Code as policies: Language model programs for embodied control.arXiv preprint arXiv:2209.07753,
2022.
Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries. InText summarization branches
out, pages 74–81, 2004.
Jiacheng Liu, Skyler Hallinan, Ximing Lu, Pengfei He, Sean Welleck, Hannaneh Hajishirzi, and Yejin
Choi. Rainier: Reinforced knowledge introspector for commonsense question answering.arXiv preprint
arXiv:2210.03078, 2022a.
Ruibo Liu, Jason Wei, Shixiang Shane Gu, Te-Yen Wu, Soroush Vosoughi, Claire Cui, Denny Zhou, and
Andrew M Dai. Mind’s eye: Grounded language model reasoning through simulation.arXiv preprint
arXiv:2210.05359, 2022b.
Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter
Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question
answering. arXiv preprint arXiv:2209.09513, 2022a.
Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts
and where to find them: Overcoming few-shot prompt order sensitivity. InProceedings of the 60th Annual
Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8086–8098,
Dublin, Ireland, May 2022b. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.556.
URL https://aclanthology.org/2022.acl-long.556.
Yi Luan, Jacob Eisenstein, Kristina Toutanova, and Michael Collins. Sparse, Dense, and Attentional
RepresentationsforTextRetrieval. Transactions of the Association for Computational Linguistics, 9:329–345,
04 2021. ISSN 2307-387X. doi: 10.1162/tacl_a_00369. URLhttps://doi.org/10.1162/tacl_a_00369.
James MacGlashan, Mark K Ho, Robert Loftin, Bei Peng, Guan Wang, David L Roberts, Matthew E Taylor,
and Michael L Littman. Interactive learning from policy-dependent human feedback. InInternational
Conference on Machine Learning, pages 2285–2294. PMLR, 2017.
Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, and Graham Neubig. Language models of code are
few-shot commonsense learners.arXiv preprint arXiv:2210.07128, 2022.
John McCarthy et al.Programs with common sense. RLE and MIT computation center Cambridge, MA,
USA, 1960.
Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick, Mia Glaese,
Susannah Young, Lucy Campbell-Gillingham, Geoffrey Irving, et al. Teaching language models to support
answers with verified quotes.arXiv preprint arXiv:2203.11147, 2022.
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. In
International Conference on Learning Representations (ICLR), 2017.
Sewon Min, Victor Zhong, Luke Zettlemoyer, and Hannaneh Hajishirzi. Multi-hop reading comprehension
through question decomposition and rescoring. InProceedings of the 57th Annual Meeting of the Association
for Computational Linguistics, pages 6097–6109, 2019.
Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer.
Rethinking the role of demonstrations: What makes in-context learning work?, 2022. URL https:
//arxiv.org/abs/2202.12837.
29Published in Transactions on Machine Learning Research (06/2023)
Swaroop Mishra and Elnaz Nouri. Help me think: A simple prompting strategy for non-experts to create
customized content with models.arXiv preprint arXiv:2208.08232, 2022.
Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Natural instructions: Bench-
marking generalization to new tasks from natural language instructions.arXiv preprint arXiv:2104.08773,
2021.
Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay Rajpurohit,
Oyvind Tafjord, Ashish Sabharwal, Peter Clark, et al. Lila: A unified benchmark for mathematical reasoning.
arXiv preprint arXiv:2210.17517, 2022a.
Swaroop Mishra, Arindam Mitra, Neeraj Varshney, Bhavdeep Sachdeva, Peter Clark, Chitta Baral, and
Ashwin Kalyan. NumGLUE: A suite of fundamental yet challenging mathematical reasoning tasks. In
Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers), pages 3505–3523, Dublin, Ireland, May 2022b. Association for Computational Linguistics. doi:
10.18653/v1/2022.acl-long.246. URL https://aclanthology.org/2022.acl-long.246.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex
Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir
Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis
Hassabis. Human-level control through deep reinforcement learning.Nature, 518(7540):529–533, February
2015. ISSN 00280836. URLhttp://dx.doi.org/10.1038/nature14236.
Volodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Tim Harley, Timothy P. Lillicrap,
David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In
Proceedings of the 33rd International Conference on International Conference on Machine Learning -
Volume 48, ICML’16, page 1928–1937. JMLR.org, 2016.
Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse,
Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted question-answering
with human feedback.arXiv preprint arXiv:2112.09332, 2021.
Rodrigo Nogueira and Kyunghyun Cho. Task-oriented query reformulation with reinforcement learning. In
Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 574–583,
Copenhagen, Denmark, September 2017. Association for Computational Linguistics. doi: 10.18653/v1/
D17-1061. URL https://aclanthology.org/D17-1061.
Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber,
David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton, and Augustus Odena.
Show your work: Scratchpads for intermediate computation with language models. arXiv preprint
arXiv:2112.00114, 2021.
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,
Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training
language models to follow instructions with human feedback.arXiv preprint arXiv:2203.02155, 2022.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of
machine translation. InProceedings of the Annual Meeting of the Association for Computational Linguistics
(ACL), 2002a.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation
of machine translation. InProceedings of the 40th annual meeting of the Association for Computational
Linguistics, pages 311–318, 2002b.
Aaron Parisi, Yao Zhao, and Noah Fiedel. Talm: Tool augmented language models. arXiv preprint
arXiv:2205.12255, 2022.
30Published in Transactions on Machine Learning Research (06/2023)
Pruthvi Patel, Swaroop Mishra, Mihir Parmar, and Chitta Baral. Is a question decomposition unit all we
need? arXiv preprint arXiv:2205.12538, 2022.
Ethan Perez, Patrick Lewis, Wen-tau Yih, Kyunghyun Cho, and Douwe Kiela. Unsupervised question
decomposition for question answering. InProceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing (EMNLP), 2020.
Maja Popović. chrF++: words helping character n-grams. InProceedings of the Second Conference on
Machine Translation, pages 612–618. Association for Computational Linguistics, 2017.
Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, and Mike Lewis. Measuring and
narrowing the compositionality gap in language models, 2022.
Jing Qian, Hong Wang, Zekun Li, Shiyang Li, and Xifeng Yan. Limitations of language models in arithmetic
and symbolic induction, 2022.
Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang,
and Huajun Chen. Reasoning with language model prompting: A survey, 2022.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models
are unsupervised multitask learners, 2019.
Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John
Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob
Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh,
Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John
Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena
Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena
Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato,
Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug
Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson
d’Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas,
Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger, Iason
Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem
Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scaling
language models: Methods, analysis & insights from training gopher, 2021.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.
Journal of Machine Learning Research (JMLR), 2020.
Rajkumar Ramamurthy, Prithviraj Ammanabrolu, Kianté Brantley, Jack Hessel, Rafet Sifa, Christian
Bauckhage, Hannaneh Hajishirzi, and Yejin Choi. Is reinforcement learning (not) for natural language
processing?: Benchmarks, baselines, and building blocks for natural language policy optimization.arXiv
preprint arXiv:2210.01241, 2022.
Stephen Robertson and Hugo Zaragoza.The probabilistic relevance framework: BM25 and beyond. Now
Publishers Inc, 2009.
Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin,
Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma
Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan
Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey,
Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault
Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and
Alexander M Rush. Multitask prompted training enables zero-shot task generalization. InInternational
Conference on Learning Representations (ICLR), 2022.
31Published in Transactions on Machine Learning Research (06/2023)
Timo Schick, Jane Dwivedi-Yu, Zhengbao Jiang, Fabio Petroni, Patrick Lewis, Gautier Izacard, Qingfei You,
Christoforos Nalmpantis, Edouard Grave, and Sebastian Riedel. Peer: A collaborative language model.
arXiv preprint arXiv:2208.11663, 2022.
Timo Schick, Jane Dwivedi-Yu, Roberto Dessì†, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola
Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools.arXiv
preprint arXiv:2302.04761, 2023.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization
algorithms. arXiv preprint arXiv:1707.06347, 2017.
Thomas Scialom, Tuhin Chakrabarty, and Smaranda Muresan. Continual-t0: Progressively instructing 50+
tasks to language models without forgetting. Conference on Empirical Methods in Natural Language
Processing (EMNLP), 2022.
Tianxiao Shen, Victor Quach, Regina Barzilay, and Tommi Jaakkola. Blank language models. InConference
on Empirical Methods in Natural Language Processing (EMNLP), 2020.
Freda Shi, Daniel Fried, Marjan Ghazvininejad, Luke Zettlemoyer, and Sida I Wang. Natural language to
code translation with execution.arXiv preprint arXiv:2204.11454, 2022.
Tianlin Shi, Andrej Karpathy, Linxi Fan, Jonathan Hernandez, and Percy Liang. World of bits: An
open-domain platform for web-based agents. InInternational Conference on Machine Learning (ICML),
2017.
Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and
Wen tau Yih. Replug: Retrieval-augmented black-box language models, 2023.
Kumar Shridhar, Alessandro Stolfo, and Mrinmaya Sachan. Distilling multi-step reasoning capabilities of
large language models into smaller models via semantic decompositions.arXiv preprint arXiv:2212.00193,
2022.
Kurt Shuster, Mojtaba Komeili, Leonard Adolphs, Stephen Roller, Arthur Szlam, and Jason Weston. Language
models that seek for knowledge: Modular search & generation for dialogue and prompt completion.arXiv
preprint arXiv:2203.13224, 2022a.
Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller, Megan Ung, Moya
Chen, Kushal Arora, Joshua Lane, Morteza Behrooz, William Ngan, Spencer Poff, Naman Goyal, Arthur
Szlam, Y-Lan Boureau, Melanie Kambadur, and Jason Weston. Blenderbot 3: a deployed conversational
agent that continually learns to responsibly engage.arXiv preprint arXiv:2208.03188, 2022b.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian
Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go
with deep neural networks and tree search.Nature, 529(7587):484–489, 2016.
Charlie Snell, Ilya Kostrikov, Yi Su, Mengjiao Yang, and Sergey Levine. Offline rl for natural language
generation with implicit language q learning.arXiv preprint arXiv:2206.11871, 2022.
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch,
Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. Beyond the imitation game:
Quantifying and extrapolating the capabilities of language models.arXiv preprint arXiv:2206.04615, 2022.
Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario
Amodei, and Paul F Christiano. Learning to summarize with human feedback. InAdvances in Neural
Information Processing Systems (NeurIPS), 2020.
Richard S Sutton and Andrew G Barto.Reinforcement learning: An introduction. MIT press, 2018.
32Published in Transactions on Machine Learning Research (06/2023)
Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha
Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, and Jason Wei. Challenging big-bench tasks and whether
chain-of-thought can solve them, 2022. URLhttps://arxiv.org/abs/2210.09261.
Alon Talmor and Jonathan Berant. The web as a knowledge-base for answering complex questions. In
Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL),
2018.
Yi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng,
Neil Houlsby, and Donald Metzler. Unifying language learning paradigms.arXiv preprint arXiv:2205.05131,
2022.
Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew
Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science.arXiv preprint
arXiv:2211.09085, 2022.
Open Ended Learning Team, Adam Stooke, Anuj Mahajan, Catarina Barros, Charlie Deck, Jakob Bauer,
Jakub Sygnowski, Maja Trebacz, Max Jaderberg, Michael Mathieu, et al. Open-ended learning leads to
generally capable agents.arXiv preprint arXiv:2107.12808, 2021.
Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,
Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin
Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen,
Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Vincent Zhao, Yanqi Zhou, Chung-Ching
Chang, Igor Krivokon, Will Rusch, Marc Pickett, Pranesh Srinivasan, Laichee Man, Kathleen Meier-
Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben
Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina,
Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya
Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui,
Marian Croak, Ed Chi, and Quoc Le. Lamda: Language models for dialog applications.arXiv preprint
arXiv:2201.08239, 2022.
Kushal Tirumala, Aram H. Markosyan, Luke Zettlemoyer, and Armen Aghajanyan. Memorization without
overfitting: Analyzing the training dynamics of large language models. InAdvances in Neural Information
Processing Systems (NeurIPS), 2022.
Daniel Toyama, Philippe Hamel, Anita Gergely, Gheorghe Comanici, Amelia Glaese, Zafarali Ahmed, Tyler
Jackson, Shibl Mourad, and Doina Precup. Androidenv: a reinforcement learning platform for android.
arXiv preprint arXiv:2105.13231, 2021.
Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. Interleaving retrieval with
chain-of-thought reasoning for knowledge-intensive multi-step questions.arXiv preprint arXiv:2212.10509,
2022.
Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michaël Mathieu, Andrew Dudzik, Junyoung Chung,
David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using
multi-agent reinforcement learning.Nature, 575(7782):350–354, 2019.
Boshi Wang, Xiang Deng, and Huan Sun. Iteratively prompt pre-trained language models for chain of thought.
Conference on Empirical Methods in Natural Language Processing (EMNLP), 2022a.
Ruoyao Wang, Peter Jansen, Marc-Alexandre Côté, and Prithviraj Ammanabrolu. Behavior cloned trans-
formers are neurosymbolic reasoners.arXiv preprint arXiv:2210.07382, 2022b.
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and
Denny Zhou. Self-consistency improves chain of thought reasoning in language models.Advances in Neural
Information Processing Systems (NeurIPS), 2022c.
33Published in Transactions on Machine Learning Research (06/2023)
Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana
Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, Eshaan Pathak, Giannis
Karamanolakis, Haizhi Gary Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima
Doshi, Maitreya Patel, Kuntal Kumar Pal, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj
Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Shailaja Keyur
Sampat, Savan Doshi, Siddhartha Mishra, Sujan Reddy, Sumanta Patro, Tanay Dixit, Xudong Shen, Chitta
Baral, Yejin Choi, Noah A. Smith, Hannaneh Hajishirzi, and Daniel Khashabi. Super-natural instructions:
Generalization via declarative instructions on 1600+ nlp tasks. InConference on Empirical Methods in
Natural Language Processing (EMNLP), 2022d.
Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and select:
Interactive planning with large language models enables open-world multi-task agents, 2023. URL
https://arxiv.org/abs/2302.01560.
Garrett Warnell, Nicholas Waytowich, Vernon Lawhern, and Peter Stone. Deep tamer: Interactive agent
shaping in high-dimensional state spaces. InProceedings of the AAAI conference on artificial intelligence,
volume 32, 1, 2018.
Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M
Dai, and Quoc V Le. Finetuned language models are zero-shot learners.International Conference on
Learning Representations (ICLR), 2022a.
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten
Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models.Transactions on
Machine Learning Research (TMLR), 2022b.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of
thought prompting elicits reasoning in large language models.arXiv preprint arXiv:2201.11903, 2022c.
Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and Jason Weston. Neural text
generation with unlikelihood training. InInternational Conference on Learning Representations (ICLR),
2020.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning.
Machine learning, 8(3):229–256, 1992.
Jeff Wu, Long Ouyang, Daniel M Ziegler, Nisan Stiennon, Ryan Lowe, Jan Leike, and Paul Christiano.
Recursively summarizing books with human feedback.arXiv preprint arXiv:2109.10862, 2021.
Tongshuang Wu, Ellen Jiang, Aaron Donsbach, Jeff Gray, Alejandra Molina, Michael Terry, and Carrie J Cai.
Promptchainer: Chaining large language model prompts through visual programming. InCHI Conference
on Human Factors in Computing Systems Extended Abstracts, pages 1–10, 2022a.
Tongshuang Wu, Michael Terry, and Carrie Jun Cai. Ai chains: Transparent and controllable human-ai
interaction by chaining large language model prompts. InCHI Conference on Human Factors in Computing
Systems, pages 1–22, 2022b.
Yuhuai Wu, Albert Q Jiang, Wenda Li, Markus N Rabe, Charles Staats, Mateja Jamnik, and Christian
Szegedy. Autoformalization with large language models. Advances in Neural Information Processing
Systems (NeurIPS), 2022c.
Zeqiu Wu, Yi Luan, Hannah Rashkin, David Reitter, and Gaurav Singh Tomar. Conqrr: Conversational
query rewriting for retrieval with reinforcement learning.Conference on Empirical Methods in Natural
Language Processing (EMNLP), 2022d.
Ted Xiao, Harris Chan, Pierre Sermanet, Ayzaan Wahid, Anthony Brohan, Karol Hausman, Sergey Levine,
and Jonathan Tompson. Robotic skill acquisition via instruction augmentation with vision-language models.
arXiv preprint arXiv:2211.11736, 2022.
34Published in Transactions on Machine Learning Research (06/2023)
Jing Xu, Megan Ung, Mojtaba Komeili, Kushal Arora, Y-Lan Boureau, and Jason Weston. Learning new
skills after deployment: Improving open-domain internet-driven dialogue with human feedback, 2022.
Jingfeng Yang, Haoming Jiang, Qingyu Yin, Danqing Zhang, Bing Yin, and Diyi Yang. Seqzero: Few-shot
compositional semantic parsing with sequential prompts and zero-shot models.Proceedings of the North
American Chapter of the Association for Computational Linguistics (NAACL), 2022a.
Kevin Yang, Dan Klein, Nanyun Peng, and Yuandong Tian. Doc: Improving long story coherence with
detailed outline control.arXiv preprint arXiv:2212.10077, 2022b.
Kevin Yang, Nanyun Peng, Yuandong Tian, and Dan Klein. Re3: Generating longer stories with recursive
reprompting and revision.Conference on Empirical Methods in Natural Language Processing (EMNLP),
2022c.
Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web
interaction with grounded language agents.Advances in Neural Information Processing Systems (NeurIPS),
2022a.
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React:
Synergizing reasoning and acting in language models.arXiv preprint arXiv:2210.03629, 2022b.
David Yarowsky. Unsupervised word sense disambiguation rivaling supervised methods. InProceedings of the
Annual Meeting of the Association for Computational Linguistics (ACL), 1995.
Ping Yu, Tianlu Wang, Olga Golovneva, Badr Alkhamissy, Gargi Ghosh, Mona Diab, and Asli Celikyilmaz.
Alert: Adapting language models to reasoning tasks.arXiv preprint arXiv:2212.08286, 2022.
Eric Zelikman, Jesse Mu, Noah D Goodman, and Yuhuai Tony Wu. Star: Self-taught reasoner bootstrapping
reasoning with reasoning.Advances in Neural Information Processing Systems (NeurIPS), 2022.
Andy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choromanski, Adrian Wong, Stefan Welker, Federico
Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, and Pete
Florence. Socratic models: Composing zero-shot multimodal reasoning with language, 2022. URL
https://arxiv.org/abs/2204.00598.
Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal chain-of-
thought reasoning in language models, 2023.
Victor Zhong, Caiming Xiong, and Richard Socher. Seq2SQL: Generating structured queries from natural
language using reinforcement learning, 2018. URLhttps://openreview.net/forum?id=Syx6bz-Ab.
Zexuan Zhong, Tao Lei, and Danqi Chen. Training language models with memory augmentation. In
Conference on Empirical Methods in Natural Language Processing (EMNLP), 2022.
Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire
Cui, Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in large
language models. arXiv preprint arXiv:2205.10625, 2022.
Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano,
and Geoffrey Irving. Fine-tuning language models from human preferences.arXiv preprint arXiv:1909.08593,
2019.
35

